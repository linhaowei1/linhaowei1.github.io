---
layout: about
title: About
permalink: /
subtitle:

profile:
  align: right
  image: haowei.jpg
  image_circular: false # crops the image to make it circular
  address: >

announcements:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

E-mail: linhaowei (at) pku (dot) edu (dot) cn

I am Haowei Lin, a third year Ph.D. student at the Institute for Artificial Intelligence, Peking University, co-advised by [Prof. Yitao Liang](http://web.cs.ucla.edu/~yliang/), [Prof. Di He](https://dihe-pku.github.io/), and [Prof. Jianzhu Ma](https://majianzhu.com/).

I received my Bachelor's degree in Artificial Intelligence from [Yuanpei College](https://yuanpei.pku.edu.cn/en/aboutyuanpei/collegeprofile/index.htm), Peking University. During my undergraduate studies, I was fortunate to work with [Prof. Bing Liu](https://www.cs.uic.edu/~liub/) and [Dr. Zixuan Ke](https://vincent950129.github.io/) on OOD detection, continual learning, and language models. We are the first to propose the task of **continual pre-training** for LLMs ([EMNLP22](https://arxiv.org/abs/2210.05549), [ICLR23](https://openreview.net/pdf?id=m_GDIItaI3o)), and the first to apply OOD detection methods to continual learning ([EMNLP23](https://arxiv.org/abs/2310.05083), [ICLR24](https://arxiv.org/abs/2309.15048)).

My Ph.D. research focuses on **AI for Science**, spanning two complementary directions. On one hand, I study **generative foundation models** across modalities such as text, video, MDPs, 3D, and molecules. In collaboration with [Prof. Stefano Ermon](https://cs.stanford.edu/~ermon/) and [Prof. Yilun Du](https://yilundu.github.io/), we conducted a series of studies on inference-time scaling for non-autoregressive models (the TFG series: [NeurIPS24](https://arxiv.org/abs/2409.15761), [ICLR25](https://arxiv.org/abs/2501.14216), [ICLR26](https://arxiv.org/abs/2505.23614)). On the other hand, I am interested in building **AI Scientists** and treating AI research itself as a new axis for AI4Science. As an initial step, we studied Scaling Laws Discovery ([ICML24](https://arxiv.org/pdf/2402.02314), [ICLR26](https://arxiv.org/pdf/2507.21184)) to unveil the physics of AI.

Beyond research, I enjoy contributing to open source. I am a core developer of [Harbor Adapters](https://harborframework.com/docs/adapters), a subproject of [Terminal Bench](https://www.tbench.ai/), an evaluation harness widely used in modern model releases (e.g., GPT, Gemini, Claude). 

If youâ€™re interested in collaborating on **generative foundation models** or **AI Scientist** research, feel free to reach out via email.
