<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Parallel Scaling Law - SLDAgent + Claude Sonnet 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Parallel Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Sonnet 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.999970 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.999900</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.999870</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999970 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Hybrid 4-parameter scaling law for parallel LLM training
Form: loss = (a * (N/1e9)^b + c) * (1 + d/P^0.5)
Combines additive base term with multiplicative parallel scaling
Key insight: Base loss has constant + power components, parallel benefit scales with sqrt
Simpler form than previous attempts while capturing key behaviors
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Hybrid scaling: loss = (a * (N/1e9)^b + c) * (1 + d/P^0.5)
    params: [a, b, c, d] - exactly 4 parameters
    Fixed parallel exponent at 0.5 (sqrt) for simplicity and stability
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).flatten()
    
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]
    
    # Normalize to billions for stability
    N_norm = num_params / 1e9
    
    # Extract parameters with safe defaults
    a = params[0] if len(params) &gt; 0 else 1.5
    b = params[1] if len(params) &gt; 1 else -0.08
    c = params[2] if len(params) &gt; 2 else 0.5
    d = params[3] if len(params) &gt; 3 else 0.12
    
    # Hybrid base term: power law + constant
    # This captures both scaling behavior and baseline
    base_loss = np.abs(a) * np.power(np.maximum(N_norm, 1e-9), b) + np.abs(c)
    
    # Multiplicative parallel factor with fixed sqrt exponent
    # Simpler than variable exponent, still captures diminishing returns
    parallel_factor = 1.0 + np.abs(d) / np.sqrt(np.maximum(parallel_size, 1.0))
    
    pred = base_loss * parallel_factor
    
    return np.clip(pred, 0.5, 3.0)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Streamlined fitting with smart initialization and efficient optimization
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    loss_values = np.asarray(loss_values).flatten()
    
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]
    N_norm = num_params / 1e9
    
    def objective(params):
        pred = scaling_law_func(data_points, params)
        mse = np.mean((pred - loss_values) ** 2)
        reg = 1e-7 * np.sum(params ** 2)
        return mse + reg
    
    # Smart initialization from P=1 data
    p1_mask = parallel_size == 1
    if np.any(p1_mask):
        # For hybrid form: loss(P=1) = a*N^b + c
        # Use robust estimation
        loss_p1 = loss_values[p1_mask]
        N_p1 = N_norm[p1_mask]
        
        # Estimate c as minimum baseline
        c_init = np.min(loss_p1) * 0.7
        
        # Fit power law for remaining variation
        adjusted_loss = loss_p1 - c_init
        log_N = np.log(np.maximum(N_p1, 1e-9))
        log_adj_loss = np.log(np.maximum(adjusted_loss, 1e-10))
        
        # Linear regression in log space
        A = np.column_stack([np.ones(len(log_N)), log_N])
        coeffs = np.linalg.lstsq(A, log_adj_loss, rcond=None)[0]
        
        a_init = np.exp(coeffs[0])
        b_init = coeffs[1]
        
        # Estimate parallel benefit
        p_multi_mask = parallel_size &gt; 1
        if np.any(p_multi_mask):
            p1_avg = loss_p1.mean()
            p_multi_avg = loss_values[p_multi_mask].mean()
            rel_benefit = (p1_avg - p_multi_avg) / p1_avg
            # For sqrt form: d/sqrt(P_avg) ≈ rel_benefit * base_loss
            P_avg = parallel_size[p_multi_mask].mean()
            d_init = rel_benefit * np.sqrt(P_avg) * 0.9
        else:
            d_init = 0.12
    else:
        # Fallback initialization
        mean_loss = np.mean(loss_values)
        a_init = mean_loss * 0.8
        b_init = -0.08
        c_init = mean_loss * 0.3
        d_init = 0.12
    
    init_guess = np.array([a_init, b_init, c_init, d_init])
    
    # Bounds optimized for hybrid form
    bounds = [
        (0.1, 5.0),      # a: power law coefficient
        (-0.5, 0.2),     # b: scaling exponent (allow slight positive)
        (0.01, 2.0),     # c: baseline constant
        (0.01, 0.6)      # d: parallel benefit (smaller for sqrt form)
    ]
    
    # Clip initialization to bounds
    init_guess = np.clip(init_guess, [b[0] for b in bounds], [b[1] for b in bounds])
    
    # Two-stage optimization: global then local
    result = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=250,
        atol=1e-9,
        tol=1e-9,
        workers=1,
        polish=True,
        updating=&#x27;immediate&#x27;,
        init=&#x27;sobol&#x27;
    )
    
    best_params = result.x if result.success else init_guess
    best_score = result.fun if result.success else objective(init_guess)
    
    # Local refinement with high precision
    try:
        refined = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11, &#x27;maxiter&#x27;: 2500}
        )
        if refined.success and refined.fun &lt; best_score:
            best_params = refined.x
    except:
        pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999904 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Ultra-compact 4-parameter scaling law with hybrid parallel modeling
Form: loss = a * N^(-b) * (1 + c * p^(-0.4)) + d

Key innovations:
- Exponent -0.4 is between -0.5 (sqrt) and -1 (inverse), optimized for balance
- More compact than previous versions (~350 chars in functions)
- Enhanced numerical stability with adaptive parameter scaling
- Faster convergence with smart initialization from data moments
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;Predict: a * N^(-b) * (1 + c * p^(-0.4)) + d&quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    a, b, c, d = np.asarray(params).flatten()
    
    N_norm = X[:, 0] / 1e9
    p_size = X[:, 1]
    
    return a * np.power(N_norm, -b) * (1.0 + c * np.power(p_size, -0.4)) + d


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;Fit with smart initialization and hybrid optimization&quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).flatten()
    
    def objective(params):
        pred = scaling_law_func(X, params)
        return np.mean((pred - y) ** 2) + 1e-7 * np.sum(params ** 2)
    
    # Smart initialization: estimate offset from min loss
    d_init = np.min(y) - 0.5
    
    # Tighter bounds optimized for -0.4 exponent
    bounds = [
        (0.3, 6.5),     # a: scale coefficient
        (0.02, 0.45),   # b: power law exponent
        (0.02, 3.0),    # c: parallel efficiency (0.4 exponent needs moderate values)
        (-1.2, 2.2)     # d: baseline offset
    ]
    
    # Global search with enhanced settings
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=420,
        popsize=22,
        atol=1e-9,
        tol=1e-9,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.6, 1.1),
        recombination=0.75,
        init=&#x27;latinhypercube&#x27;
    )
    
    # Local refinement with tighter tolerance
    result = minimize(
        objective,
        result_de.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 1000}
    )
    
    return result.x if result.success else result_de.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999880 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized multiplicative scaling law for parallel LLM training
Uses 4 parameters: L = a * N^b * (1 + c / P^d)

Key improvements:
- Multiplicative form captures realistic interaction effects
- Streamlined initialization from data patterns
- Efficient multi-start optimization
- Better numerical stability with tighter bounds
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = a * N^b * (1 + c / P^d)
    params: [a, b, c, d] - exactly 4 parameters
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).ravel()[:4]
    
    if len(params) &lt; 4:
        params = np.pad(params, (0, 4 - len(params)), constant_values=[1.0, -0.1, 0.15, 0.35])
    
    N = np.maximum(data_points[:, 0], 1e6)
    P = np.maximum(data_points[:, 1], 1.0)
    
    a, b, c, d = params
    
    # Stability constraints
    a = np.maximum(np.abs(a), 1e-10)
    b = np.clip(b, -0.35, 0.05)
    c = np.clip(np.abs(c), 0.0, 1.0)
    d = np.clip(d, 0.25, 1.5)
    
    # Multiplicative form
    base_loss = a * np.power(N, b)
    parallel_factor = 1.0 + c / np.power(P, d)
    
    return base_loss * parallel_factor


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Efficient fitting with data-driven initialization
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    loss_values = np.asarray(loss_values).ravel()
    
    N = data_points[:, 0]
    P = data_points[:, 1]
    y = loss_values
    
    # Step 1: Estimate base scaling from P=1 data
    mask_p1 = (P == 1.0)
    if np.sum(mask_p1) &gt;= 2:
        log_N = np.log(N[mask_p1])
        log_y = np.log(np.maximum(y[mask_p1], 1e-10))
        coef = np.polyfit(log_N, log_y, 1)
        b_init = np.clip(coef[0], -0.25, 0.0)
        log_a_c = coef[1]  # log(a*(1+c))
    else:
        b_init = -0.1
        log_a_c = np.log(np.mean(y))
    
    # Step 2: Estimate parallel parameters from loss ratios
    y_p1 = np.mean(y[P == 1.0]) if np.any(P == 1.0) else np.max(y)
    y_p2 = np.mean(y[P == 2.0]) if np.any(P == 2.0) else y_p1 * 0.98
    y_p4 = np.mean(y[P == 4.0]) if np.any(P == 4.0) else y_p1 * 0.94
    
    # Analyze loss ratios: (1+c) / (1+c/P^d)
    ratio_12 = y_p1 / max(y_p2, 1e-10)
    ratio_14 = y_p1 / max(y_p4, 1e-10)
    
    # Estimate d from ratio pattern
    if ratio_14 &gt; ratio_12 &gt; 1.0:
        log_r12 = np.log(max(ratio_12 - 1.0, 1e-6))
        log_r14 = np.log(max(ratio_14 - 1.0, 1e-6))
        d_init = np.clip((log_r14 - log_r12) / np.log(2.0), 0.3, 0.5)
    else:
        d_init = 0.38
    
    # Estimate c from ratio: (1+c) / (1+c/4^d) = ratio_14
    denom = ratio_14 / (4.0 ** d_init) - 1.0
    if abs(denom) &gt; 1e-6:
        c_init = np.clip((1.0 - ratio_14) / denom, 0.08, 0.25)
    else:
        c_init = 0.16
    
    # Estimate a from a*(1+c) = exp(log_a_c)
    a_init = np.exp(log_a_c) / (1.0 + c_init)
    
    init_params = np.array([a_init, b_init, c_init, d_init])
    
    def objective(params):
        pred = scaling_law_func(data_points, params)
        return np.mean((pred - y) ** 2)
    
    bounds = [
        (1e-8, 1e10),
        (-0.35, 0.05),
        (0.0, 1.0),
        (0.25, 1.5)
    ]
    
    # Multi-start optimization with focused variants
    best_params = init_params
    best_loss = float(&#x27;inf&#x27;)
    
    starts = [
        init_params,
        [a_init * 0.93, b_init, c_init * 1.07, d_init * 0.98],
        [a_init * 1.07, b_init, c_init * 0.93, d_init * 1.02],
        [np.mean(y) * 0.94, -0.11, 0.18, 0.39],
        [np.mean(y) * 1.06, -0.09, 0.14, 0.37]
    ]
    
    for start in starts:
        try:
            result = minimize(
                objective,
                start,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 1500, &#x27;ftol&#x27;: 1e-12}
            )
            
            if result.fun &lt; best_loss:
                best_loss = result.fun
                best_params = result.x
        except:
            continue
    
    # Final refinement with best parameters
    try:
        result = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-13}
        )
        if result.success and result.fun &lt; best_loss:
            best_params = result.x
    except:
        pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999875 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for parallel LLM training
Uses multiplicative form: L(N, P) = a * N^(-b) * (1 + c/P^d)
This captures:
- Power law decrease with model size
- Multiplicative benefit from parallelization with diminishing returns
- 4 parameters: [a, b, c, d]
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L(N, P) = a * N^(-b) * (1 + c/P^d)
    params = [a, b, c, d] (exactly 4 parameters)
    
    This form models:
    - a: base loss scale
    - b: parameter scaling exponent (how fast loss decreases with N)
    - c: parallel efficiency coefficient
    - d: parallel scaling exponent (diminishing returns)
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).ravel()
    
    if len(params) != 4:
        raise ValueError(f&quot;Expected 4 parameters, got {len(params)}&quot;)
    
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]
    
    a, b, c, d = params
    
    # Normalize num_params to billions for numerical stability
    N_billions = num_params / 1e9
    
    # Ensure numerical stability
    N_billions = np.maximum(N_billions, 1e-3)
    parallel_size = np.maximum(parallel_size, 1.0)
    
    # Clip exponents to reasonable ranges
    b_safe = np.clip(b, 0.01, 2.0)
    d_safe = np.clip(d, 0.01, 2.0)
    
    # Scaling law: multiplicative form
    # L(N, P) = a * N^(-b) * (1 + c/P^d)
    term1 = a * np.power(N_billions, -b_safe)
    term2 = 1.0 + c / np.power(parallel_size, d_safe)
    
    pred = term1 * term2
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit the 4-parameter scaling law using hybrid optimization strategy
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    loss_values = np.asarray(loss_values).ravel()
    
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]
    
    # Analyze data to get better initial estimates
    loss_min = np.min(loss_values)
    loss_max = np.max(loss_values)
    loss_mean = np.mean(loss_values)
    loss_range = loss_max - loss_min
    
    # Estimate parameter-only effect (at parallel_size=1)
    mask_p1 = parallel_size == 1
    if np.any(mask_p1):
        loss_at_p1 = np.mean(loss_values[mask_p1])
    else:
        loss_at_p1 = loss_max
    
    # Smart initialization based on data characteristics
    # For multiplicative form: L = a * N^(-b) * (1 + c/P^d)
    # At P=1: L ≈ a * N^(-b) * (1 + c)
    # As P increases: parallel term (1 + c/P^d) approaches 1
    
    init_a = loss_mean * 0.9  # Base scale
    init_b = 0.12              # Typical parameter scaling
    init_c = 0.15              # Parallel contribution
    init_d = 0.5               # Parallel diminishing returns
    
    init_params = np.array([init_a, init_b, init_c, init_d])
    
    def objective(params):
        try:
            pred = scaling_law_func(data_points, params)
            residuals = pred - loss_values
            mse = np.mean(residuals ** 2)
            
            # Regularization to prefer physically reasonable parameters
            # Penalize extreme values
            reg = 1e-5 * (
                np.abs(params[0] - loss_mean)**2 +  # a near mean loss
                (params[1] - 0.15)**2 +              # b near typical value
                params[2]**2 +                        # c small
                (params[3] - 0.5)**2                  # d near 0.5
            )
            
            return mse + reg
        except:
            return 1e10
    
    # Parameter bounds based on physical constraints
    bounds = [
        (loss_min * 0.5, loss_max * 2.0),  # a: reasonable loss scale
        (0.01, 0.5),                        # b: parameter scaling exponent
        (0.0, 1.0),                         # c: parallel efficiency
        (0.1, 1.5)                          # d: parallel exponent
    ]
    
    # Strategy 1: Differential evolution for global search
    result_de = differential_evolution(
        objective, 
        bounds,
        seed=42,
        maxiter=400,
        popsize=20,
        atol=1e-9,
        tol=1e-9,
        workers=1
    )
    
    # Strategy 2: Local refinement with multiple starts
    best_params = result_de.x
    best_loss = result_de.fun
    
    for trial in range(3):
        if trial == 0:
            x0 = result_de.x
        elif trial == 1:
            x0 = init_params
        else:
            # Random perturbation around best found
            x0 = result_de.x * np.random.uniform(0.8, 1.2, size=4)
            x0 = np.clip(x0, [b[0] for b in bounds], [b[1] for b in bounds])
        
        result_local = minimize(
            objective,
            x0,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-10}
        )
        
        if result_local.success and result_local.fun &lt; best_loss:
            best_loss = result_local.fun
            best_params = result_local.x
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999870 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Improved 4-parameter scaling law:
L = a * N^b * (1 + c / P^d)

This form:
- Power law in parameters (a, b)
- Flexible inverse power in parallel_size (c, d)
- Multiplicative interaction captures cross-effects
- Reduces to simpler forms when d≈1 or c≈0
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    L = a * N^b * (1 + c / P^d)
    where N = num_params (normalized), P = parallel_size
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    a, b, c, d = np.asarray(params).ravel()[:4]
    
    N_norm = X[:, 0] / 1e9  # Normalize to billions
    P = X[:, 1]
    
    # Clip for numerical stability
    b_safe = np.clip(b, -2.0, 1.0)
    d_safe = np.clip(d, 0.01, 5.0)
    P_safe = np.maximum(P, 0.1)
    
    # Main scaling law with multiplicative interaction
    power_term = a * np.power(N_norm, b_safe)
    parallel_factor = 1.0 + c / np.power(P_safe, d_safe)
    
    return power_term * parallel_factor


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Robust two-stage fitting with better initialization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).ravel()
    
    def objective(params):
        pred = scaling_law_func(X, params)
        mse = np.mean((pred - y) ** 2)
        # Light regularization to prefer stable parameters
        reg = 1e-7 * (params[0]**2 + params[2]**2)
        return mse + reg
    
    # Informed bounds based on data analysis
    # Loss ranges: pile [1.79, 2.11], stack [0.99, 1.17]
    # Normalized params: 0.536 to 4.38 (billions)
    bounds = [
        (0.5, 3.0),      # a: base coefficient (around mean loss)
        (-0.3, 0.1),     # b: power exponent (slightly negative)
        (0.0, 1.5),      # c: parallel interaction strength
        (0.1, 3.0)       # d: parallel power (likely near 1)
    ]
    
    # Smart initialization: start near additive inverse form
    # L ≈ 1.5 * N^(-0.05) * (1 + 0.3 / P^1)
    init_guess = np.array([1.5, -0.05, 0.3, 1.0])
    
    # Global search with differential evolution
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=250,
        popsize=12,
        atol=1e-8,
        tol=1e-8,
        workers=1,
        init=&#x27;latinhypercube&#x27;,
        updating=&#x27;deferred&#x27;
    )
    
    # Local refinement with multiple methods for robustness
    best_result = result_de
    best_score = result_de.fun
    
    # Try L-BFGS-B refinement
    for start in [result_de.x, init_guess]:
        try:
            result_local = minimize(
                objective,
                start,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-10}
            )
            if result_local.success and result_local.fun &lt; best_score:
                best_result = result_local
                best_score = result_local.fun
        except:
            continue
    
    return best_result.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
