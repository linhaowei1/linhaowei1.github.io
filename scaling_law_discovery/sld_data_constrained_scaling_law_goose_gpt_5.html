<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Data-Constrained Scaling Law - goose + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Data-Constrained Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">goose</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.913528 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.893520</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.866873</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.913528 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">from __future__ import annotations
from typing import Dict, List

# Global exponents shared across groups (discovered via grid-search least squares)
_ALPHA_PARAMS = 0.50275
_BETA_TOKENS = 0.5658333333333334
_GAMMA_UNIQUE = 0.1328333333333333

# Group-specific linear coefficients [c, A, B, D] for the additive inverse-power model
# Fitted on the provided dataset. A default is provided for unknown groups.
_COEFFICIENTS: Dict[str, List[float]] = {
    # loss = c + A * params^{-alpha} + B * tokens^{-beta} + D * unique_tokens^{-gamma}
    &quot;all_data&quot;: [1.8793173316766316, 4879.203039121107, 113188.27489200784, 14.824566834048097],
    &quot;default&quot;:  [1.8793173316766316, 4879.203039121107, 113188.27489200784, 14.824566834048097],
}

# Small epsilon to guard against any accidental zero-valued inputs
_EPS = 1e-12


def _predict_single(x: Dict[str, float], coef: List[float]) -&gt; float:
    c, A, B, D = coef
    p = max(float(x.get(&quot;params&quot;, 0.0)), _EPS)
    t = max(float(x.get(&quot;tokens&quot;, 0.0)), _EPS)
    u = max(float(x.get(&quot;unique_tokens&quot;, 0.0)), _EPS)
    return (
        c
        + A * (p ** (-_ALPHA_PARAMS))
        + B * (t ** (-_BETA_TOKENS))
        + D * (u ** (-_GAMMA_UNIQUE))
    )


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The law used here is an additive inverse-power scaling model:
        loss = c + A * params^{-alpha} + B * tokens^{-beta} + D * unique_tokens^{-gamma}

    Exponents (alpha, beta, gamma) are shared across groups; the linear
    coefficients (c, A, B, D) are group-specific (with a default fallback).

    Args:
        input_data: A list of dictionaries, each containing the numeric inputs:
            - &#x27;params&#x27; (float): model parameter count
            - &#x27;tokens&#x27; (float): total pre-training tokens
            - &#x27;unique_tokens&#x27; (float): number of unique tokens in the dataset
        group: The experimental group for which to make predictions.

    Returns:
        A list of dictionaries, each containing:
            - &#x27;loss&#x27; (float): predicted final validation loss
    &quot;&quot;&quot;
    coef = _COEFFICIENTS.get(group, _COEFFICIENTS[&quot;default&quot;])
    return [{&quot;loss&quot;: _predict_single(row, coef)} for row in input_data]</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.912524 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">from __future__ import annotations
from typing import Dict, List

# Discovered scaling law (functional form shared across groups):
#   loss = L0 + a * P^(-ap) + b * T^(-bt) + c * U^(-cu) + d * (P*T)^(-dx)
# where
#   P = params, T = tokens, U = unique_tokens
# Coefficients below are fitted per group. If an unknown group is provided,
# we fall back to the &#x27;all_data&#x27; coefficients.

_GROUP_PARAMS: Dict[str, Dict[str, float]] = {
    &quot;all_data&quot;: {
        # Fitted with nonnegative coefficients using non-linear least squares
        # on the provided dataset.
        &quot;L0&quot;: 1.89642926,
        &quot;a&quot;: 3220.35969,
        &quot;ap&quot;: 0.488875099,
        &quot;b&quot;: 138466.144,
        &quot;bt&quot;: 0.584352928,
        &quot;c&quot;: 16.2409846,
        &quot;cu&quot;: 0.136988374,
        &quot;d&quot;: 19125.4726,
        &quot;dx&quot;: 0.29439468,
    }
}

_DEFAULT_GROUP = &quot;all_data&quot;


def _predict_single(P: float, T: float, U: float, params: Dict[str, float]) -&gt; float:
    # Guard against nonpositive values (outside training distribution)
    if P &lt;= 0 or T &lt;= 0 or U &lt;= 0:
        return float(&quot;nan&quot;)
    L0 = params[&quot;L0&quot;]
    a, ap = params[&quot;a&quot;], params[&quot;ap&quot;]
    b, bt = params[&quot;b&quot;], params[&quot;bt&quot;]
    c, cu = params[&quot;c&quot;], params[&quot;cu&quot;]
    d, dx = params[&quot;d&quot;], params[&quot;dx&quot;]
    return (
        L0
        + a * (P ** (-ap))
        + b * (T ** (-bt))
        + c * (U ** (-cu))
        + d * ((P * T) ** (-dx))
    )


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected keys: &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) under key &#x27;loss&#x27;.
    &quot;&quot;&quot;
    params = _GROUP_PARAMS.get(group, _GROUP_PARAMS[_DEFAULT_GROUP])

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        P = float(row.get(&quot;params&quot;, 0.0))
        T = float(row.get(&quot;tokens&quot;, 0.0))
        U = float(row.get(&quot;unique_tokens&quot;, 0.0))
        pred = _predict_single(P, T, U, params)
        outputs.append({&quot;loss&quot;: float(pred)})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.905629 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">from __future__ import annotations

# Discovered scaling law (data-constrained LM pre-training):
#   loss(params, tokens, unique_tokens) = L_inf
#       + A * params^{-alpha}
#       + B * tokens^{-beta}
#       + C * unique_tokens^{-gamma}
# The functional form is identical across groups; only the coefficients differ.
# If an unknown group is provided, we fall back to the &quot;default&quot; coefficients.

# Fitted on the provided dataset (group == &quot;all_data&quot;).
# Coefficients obtained via nonlinear least squares with random restarts.
_COEFFICIENTS: dict[str, dict[str, float]] = {
    # Best RMSE on provided data ≈ 0.272 (see /app/explain.md)
    &quot;all_data&quot;: {
        &quot;L_inf&quot;: 2.29977243,
        &quot;A&quot;: 1101.09385, &quot;alpha&quot;: 0.40907593,
        &quot;B&quot;: 106860.325, &quot;beta&quot;: 0.56202189,
        &quot;C&quot;: 166.571827, &quot;gamma&quot;: 0.29285241,
    },
    # Fallback coefficients, identical to all_data for now.
    &quot;default&quot;: {
        &quot;L_inf&quot;: 2.29977243,
        &quot;A&quot;: 1101.09385, &quot;alpha&quot;: 0.40907593,
        &quot;B&quot;: 106860.325, &quot;beta&quot;: 0.56202189,
        &quot;C&quot;: 166.571827, &quot;gamma&quot;: 0.29285241,
    },
}

_REQUIRED_KEYS = (&quot;params&quot;, &quot;tokens&quot;, &quot;unique_tokens&quot;)


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coeffs = _COEFFICIENTS.get(group, _COEFFICIENTS[&quot;default&quot;])

    Linf = coeffs[&quot;L_inf&quot;]
    A, alpha = coeffs[&quot;A&quot;], coeffs[&quot;alpha&quot;]
    B, beta = coeffs[&quot;B&quot;], coeffs[&quot;beta&quot;]
    C, gamma = coeffs[&quot;C&quot;], coeffs[&quot;gamma&quot;]

    outputs: list[dict[str, float]] = []
    for row in input_data:
        # Validate expected keys
        if not all(k in row for k in _REQUIRED_KEYS):
            outputs.append({&quot;loss&quot;: float(&quot;nan&quot;)})
            continue
        # Extract and guard values (strictly positive for power laws)
        p = float(row.get(&quot;params&quot;, 0.0))
        t = float(row.get(&quot;tokens&quot;, 0.0))
        u = float(row.get(&quot;unique_tokens&quot;, 0.0))
        eps = 1.0
        p = p if p &gt; 0.0 else eps
        t = t if t &gt; 0.0 else eps
        u = u if u &gt; 0.0 else eps

        loss_val = (
            Linf
            + A * (p ** (-alpha))
            + B * (t ** (-beta))
            + C * (u ** (-gamma))
        )
        outputs.append({&quot;loss&quot;: float(loss_val)})

    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.869045 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations
from typing import Dict, List


def _predict_loss(params: float, tokens: float, unique_tokens: float, coef: Dict[str, float]) -&gt; float:
    # Numerical safety: enforce strictly positive inputs
    eps = 1e-12
    N = max(float(params), eps)
    D = max(float(tokens), eps)
    U = max(float(unique_tokens), eps)

    c = coef[&quot;c&quot;]
    a = coef[&quot;a&quot;]
    alpha = coef[&quot;alpha&quot;]
    b = coef[&quot;b&quot;]
    beta = coef[&quot;beta&quot;]
    s = coef[&quot;s&quot;]

    # Effective data after accounting for duplication / limited uniqueness
    Deff = min(D, s * U)

    # Scaling law: independent capacity- and data-limited improvements + irreducible floor
    # L = c + a * N^{-alpha} + b * Deff^{-beta}
    loss = c + a * (N ** (-alpha)) + b * (Deff ** (-beta))
    return float(loss)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Required keys: &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;loss&#x27;: float}.
    &quot;&quot;&quot;
    # Per-group coefficients for the law. If an unseen group is provided, fall back to &#x27;all_data&#x27;.
    COEFFICIENTS: Dict[str, Dict[str, float]] = {
        # Fitted on the provided dataset (see /app/explain.md for details)
        # L = c + a * N^{-alpha} + b * min(D, s * U)^{-beta}
        # where N=params, D=tokens, U=unique_tokens
        &quot;all_data&quot;: {
            &quot;c&quot;: 2.255038883,   # irreducible loss floor
            &quot;a&quot;: 4.24239542e04, # parameter-scaling amplitude
            &quot;alpha&quot;: 0.645550388, # parameter-scaling exponent
            &quot;b&quot;: 3.44184023e03, # data-scaling amplitude
            &quot;beta&quot;: 0.361914566, # data-scaling exponent
            &quot;s&quot;: 2.40311025e01,  # effective-uniqueness multiplier
        },
    }

    coef = COEFFICIENTS.get(group, COEFFICIENTS[&quot;all_data&quot;])

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        loss = _predict_loss(
            params=row.get(&quot;params&quot;, 0.0),
            tokens=row.get(&quot;tokens&quot;, 0.0),
            unique_tokens=row.get(&quot;unique_tokens&quot;, 0.0),
            coef=coef,
        )
        outputs.append({&quot;loss&quot;: loss})

    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.866873 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">from __future__ import annotations

from typing import Dict, List
import math

# Discovered scaling law (same functional form for all groups):
#   loss = L0 + A * params^(-alpha) + B * Neff^(-beta)
# with an effective data term that accounts for limited uniqueness in the corpus:
#   Neff = (tokens * (c * unique_tokens)) / (tokens + c * unique_tokens)
# which behaves like a smooth minimum of tokens and c * unique_tokens.
#
# Fitted coefficients per group. If an unknown group is provided, we fall back to
# the &quot;all_data&quot; coefficients.
_GROUP_COEFFS: Dict[str, Dict[str, float]] = {
    # Fitted on the provided dataset using non-linear least squares
    # L0, A, alpha, B, beta, c
    &quot;all_data&quot;: {
        &quot;L0&quot;: 2.38717219,
        &quot;A&quot;: 1.60700128e04,
        &quot;alpha&quot;: 5.81892030e-01,
        &quot;B&quot;: 9.76230068e03,
        &quot;beta&quot;: 4.22008080e-01,
        &quot;c&quot;: 2.54449411e01,
    },
}

# Default group to use when the provided group is not found
_DEFAULT_GROUP = &quot;all_data&quot;


def _predict_single(P: float, T: float, U: float, coeffs: Dict[str, float]) -&gt; float:
    &quot;&quot;&quot;Apply the scaling law for a single data point.

    Args:
        P: params (parameter count)
        T: tokens (total training tokens)
        U: unique_tokens (number of unique tokens)
        coeffs: dictionary with keys {L0, A, alpha, B, beta, c}

    Returns:
        Predicted loss (float)
    &quot;&quot;&quot;
    L0 = float(coeffs[&quot;L0&quot;])  # irreducible loss floor
    A = float(coeffs[&quot;A&quot;])    # capacity scaling amplitude
    alpha = float(coeffs[&quot;alpha&quot;])  # capacity exponent (&gt;0)
    B = float(coeffs[&quot;B&quot;])    # data scaling amplitude
    beta = float(coeffs[&quot;beta&quot;])    # data exponent (&gt;0)
    c = float(coeffs[&quot;c&quot;])          # uniqueness-to-tokens coupling scale

    # Numerical safety
    eps = 1e-12
    P = max(float(P), eps)
    T = max(float(T), 0.0)
    U = max(float(U), 0.0)

    # Effective number of independent tokens (smooth min between T and c*U)
    CU = c * U
    denom = T + CU
    if denom &lt;= eps:
        Neff = 0.0
    else:
        Neff = (T * CU) / denom

    # Clamp Neff minimally to avoid division by zero in power with negative exponent
    Neff = max(Neff, eps)

    loss = L0 + A * (P ** (-abs(alpha))) + B * (Neff ** (-abs(beta)))
    return float(loss)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Required keys: &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;loss&#x27;: &lt;float&gt;}.
    &quot;&quot;&quot;
    coeffs = _GROUP_COEFFS.get(group, _GROUP_COEFFS[_DEFAULT_GROUP])

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        P = row.get(&quot;params&quot;)
        T = row.get(&quot;tokens&quot;)
        U = row.get(&quot;unique_tokens&quot;)
        if P is None or T is None or U is None:
            raise ValueError(&quot;Each input dict must contain &#x27;params&#x27;, &#x27;tokens&#x27;, and &#x27;unique_tokens&#x27;.&quot;)
        pred = _predict_single(P, T, U, coeffs)
        outputs.append({&quot;loss&quot;: pred})

    return outputs</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
