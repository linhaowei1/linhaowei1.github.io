<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - Data-Constrained Scaling Law - aider + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>Data-Constrained Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">aider</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">GPT-5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.963475
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.717744</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.103650</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.963475
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0">from __future__ import annotations

from typing import Dict, List
import math


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # We keep the module limited to a single public function by placing helpers inside.
    import numpy as np

    # Fit-on-first-use and cache learned parameters on the function object
    if not hasattr(law, &quot;_fitted&quot;):

        def _safe_array(x):
            arr = np.asarray(x, dtype=float)
            # Avoid zeros/negatives that could cause under/overflow in power transforms
            return np.maximum(arr, 1e-12)

        def _as_dataset_array(ds, key: str) -&gt; np.ndarray:
            return _safe_array(ds[key] if isinstance(ds[key], list) else list(ds[key]))

        def _kfold_indices(n: int, k: int = 5, rng: np.random.Generator | None = None):
            if n &lt; k:
                # Degenerate: use leave-one-out if very small
                idx = np.arange(n)
                for i in range(n):
                    test_idx = idx[i : i + 1]
                    train_idx = np.delete(idx, i)
                    yield train_idx, test_idx
                return
            if rng is None:
                rng = np.random.default_rng(42)
            idx = np.arange(n)
            rng.shuffle(idx)
            folds = np.array_split(idx, k)
            for i in range(k):
                test_idx = folds[i]
                train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
                yield train_idx, test_idx

        def _fit_group(y: np.ndarray, p: np.ndarray, t: np.ndarray, u: np.ndarray):
            # Grid over exponents for the three inverse power-law terms
            exp_grid = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0])
            best = {
                &quot;rmse&quot;: np.inf,
                &quot;alpha_p&quot;: 0.5,
                &quot;alpha_t&quot;: 0.5,
                &quot;alpha_u&quot;: 0.5,
                &quot;coef&quot;: np.zeros(4),
            }
            n = y.shape[0]
            rng = np.random.default_rng(123)
            for ap in exp_grid:
                fp = np.power(p, -ap)
                for at in exp_grid:
                    ft = np.power(t, -at)
                    for au in exp_grid:
                        fu = np.power(u, -au)
                        # K-fold CV to pick exponents
                        rmses = []
                        for tr, te in _kfold_indices(n, k=5, rng=rng):
                            Xtr = np.column_stack(
                                [np.ones(tr.shape[0]), fp[tr], ft[tr], fu[tr]]
                            )
                            ytr = y[tr]
                            # OLS with small ridge to improve stability
                            XtX = Xtr.T @ Xtr
                            ridge = 1e-8 * np.eye(XtX.shape[0])
                            coef = np.linalg.solve(XtX + ridge, Xtr.T @ ytr)
                            Xte = np.column_stack(
                                [np.ones(te.shape[0]), fp[te], ft[te], fu[te]]
                            )
                            yhat = Xte @ coef
                            rmse = float(np.sqrt(np.mean((yhat - y[te]) ** 2)))
                            rmses.append(rmse)
                        mean_rmse = float(np.mean(rmses))
                        if mean_rmse &lt; best[&quot;rmse&quot;]:
                            # Refit on all data with chosen exponents
                            X = np.column_stack([np.ones(n), fp, ft, fu])
                            XtX = X.T @ X
                            ridge = 1e-8 * np.eye(XtX.shape[0])
                            coef = np.linalg.solve(XtX + ridge, X.T @ y)
                            best = {
                                &quot;rmse&quot;: mean_rmse,
                                &quot;alpha_p&quot;: float(ap),
                                &quot;alpha_t&quot;: float(at),
                                &quot;alpha_u&quot;: float(au),
                                &quot;coef&quot;: coef,
                            }
            # Enforce non-negativity on contribution coefficients (except intercept)
            coef = best[&quot;coef&quot;].copy()
            coef[1:] = np.maximum(coef[1:], 0.0)
            best[&quot;coef&quot;] = coef
            return best

        def _load_training():
            try:
                from datasets import load_from_disk  # type: ignore
            except Exception:
                return None

            try:
                ds = load_from_disk(&quot;/app/data&quot;)
            except Exception:
                return None

            # Support both Dataset and DatasetDict
            records = []
            if hasattr(ds, &quot;select&quot;):  # Dataset
                records = [row for row in ds]
            elif isinstance(ds, dict) or hasattr(ds, &quot;keys&quot;):
                # Concatenate all splits
                for key in ds.keys():
                    split = ds[key]
                    records.extend([row for row in split])
            else:
                return None

            # Extract to simple arrays
            def _get_col(name: str, default=None):
                vals = [r.get(name, default) for r in records]
                return vals

            params = _get_col(&quot;params&quot;)
            tokens = _get_col(&quot;tokens&quot;)
            uniq = _get_col(&quot;unique_tokens&quot;)
            loss = _get_col(&quot;loss&quot;)
            grp = _get_col(&quot;group&quot;, &quot;GLOBAL&quot;)

            # Validate essential fields
            if any(v is None for v in (params, tokens, uniq, loss)):
                return None

            return {
                &quot;params&quot;: np.asarray(params, dtype=float),
                &quot;tokens&quot;: np.asarray(tokens, dtype=float),
                &quot;unique_tokens&quot;: np.asarray(uniq, dtype=float),
                &quot;loss&quot;: np.asarray(loss, dtype=float),
                &quot;group&quot;: np.asarray(grp),
            }

        # Default/fallback parameters
        law._params_by_group = {}  # type: ignore[attr-defined]
        data = _load_training()
        if data is not None:
            P = np.maximum(data[&quot;params&quot;], 1e-12)
            T = np.maximum(data[&quot;tokens&quot;], 1e-12)
            U = np.maximum(data[&quot;unique_tokens&quot;], 1e-12)
            Y = np.asarray(data[&quot;loss&quot;], dtype=float)
            G = data[&quot;group&quot;].astype(str)

            # Fit per group
            unique_groups = sorted(list({g for g in G}))
            for g in unique_groups:
                mask = (G == g)
                if not np.any(mask):
                    continue
                best = _fit_group(Y[mask], P[mask], T[mask], U[mask])
                law._params_by_group[g] = {  # type: ignore[attr-defined]
                    &quot;c&quot;: float(best[&quot;coef&quot;][0]),
                    &quot;b_p&quot;: float(best[&quot;coef&quot;][1]),
                    &quot;b_t&quot;: float(best[&quot;coef&quot;][2]),
                    &quot;b_u&quot;: float(best[&quot;coef&quot;][3]),
                    &quot;alpha_p&quot;: float(best[&quot;alpha_p&quot;]),
                    &quot;alpha_t&quot;: float(best[&quot;alpha_t&quot;]),
                    &quot;alpha_u&quot;: float(best[&quot;alpha_u&quot;]),
                }

            # Also fit a GLOBAL model over all data for fallback
            best_global = _fit_group(Y, P, T, U)
            law._params_by_group[&quot;GLOBAL&quot;] = {  # type: ignore[attr-defined]
                &quot;c&quot;: float(best_global[&quot;coef&quot;][0]),
                &quot;b_p&quot;: float(best_global[&quot;coef&quot;][1]),
                &quot;b_t&quot;: float(best_global[&quot;coef&quot;][2]),
                &quot;b_u&quot;: float(best_global[&quot;coef&quot;][3]),
                &quot;alpha_p&quot;: float(best_global[&quot;alpha_p&quot;]),
                &quot;alpha_t&quot;: float(best_global[&quot;alpha_t&quot;]),
                &quot;alpha_u&quot;: float(best_global[&quot;alpha_u&quot;]),
            }
        else:
            # If dataset is unavailable, fall back to a plausible generic prior.
            # Typical cross-entropy losses range ~1-5; choose a conservative baseline.
            law._params_by_group = {  # type: ignore[attr-defined]
                &quot;GLOBAL&quot;: {
                    &quot;c&quot;: 2.5,
                    &quot;b_p&quot;: 1.0,
                    &quot;b_t&quot;: 1.0,
                    &quot;b_u&quot;: 0.5,
                    &quot;alpha_p&quot;: 0.5,
                    &quot;alpha_t&quot;: 0.5,
                    &quot;alpha_u&quot;: 0.3,
                }
            }

        law._fitted = True  # type: ignore[attr-defined]

    # Retrieve parameters for the requested group; fall back to GLOBAL then any available group
    params_by_group = getattr(law, &quot;_params_by_group&quot;, {})  # type: ignore[attr-defined]
    gkey = group if group in params_by_group else (&quot;GLOBAL&quot; if &quot;GLOBAL&quot; in params_by_group else (next(iter(params_by_group.keys())) if params_by_group else None))

    if gkey is None:
        # Absolute fallback if nothing is available
        model = {&quot;c&quot;: 2.5, &quot;b_p&quot;: 1.0, &quot;b_t&quot;: 1.0, &quot;b_u&quot;: 0.5, &quot;alpha_p&quot;: 0.5, &quot;alpha_t&quot;: 0.5, &quot;alpha_u&quot;: 0.3}
    else:
        model = params_by_group[gkey]

    def _predict_one(x: Dict[str, float]) -&gt; float:
        p = float(x.get(&quot;params&quot;, 1.0))
        t = float(x.get(&quot;tokens&quot;, 1.0))
        u = float(x.get(&quot;unique_tokens&quot;, 1.0))
        # Numerical guards
        p = max(p, 1e-12)
        t = max(t, 1e-12)
        u = max(u, 1e-12)

        # Inverse power-law contributions with group-specific exponents and weights:
        # loss = c + b_p * params^{-alpha_p} + b_t * tokens^{-alpha_t} + b_u * unique_tokens^{-alpha_u}
        val = (
            float(model[&quot;c&quot;])
            + float(model[&quot;b_p&quot;]) * (p ** (-float(model[&quot;alpha_p&quot;])))
            + float(model[&quot;b_t&quot;]) * (t ** (-float(model[&quot;alpha_t&quot;])))
            + float(model[&quot;b_u&quot;]) * (u ** (-float(model[&quot;alpha_u&quot;])))
        )
        # Loss should be non-negative
        return max(0.0, float(val))

    return [{&quot;loss&quot;: _predict_one(x)} for x in input_data]</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.884699
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1">from __future__ import annotations

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected keys: &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups, but
               parameters are fit per-group from /app/data at first use.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;loss&#x27;: float}.
    &quot;&quot;&quot;
    # All helpers are nested so this module provides a single top-level function.
    import math
    from typing import Dict, List, Tuple, Optional
    import numpy as np

    # Lazy import to avoid import-time errors in environments without datasets.
    def _load_dataset():
        try:
            from datasets import load_from_disk  # type: ignore
        except Exception as e:  # pragma: no cover
            raise RuntimeError(&quot;HuggingFace &#x27;datasets&#x27; package is required to load /app/data.&quot;) from e
        ds = load_from_disk(&quot;/app/data&quot;)
        # Accept either Dataset or DatasetDict
        if hasattr(ds, &quot;keys&quot;) and callable(getattr(ds, &quot;keys&quot;)):
            # DatasetDict
            if &quot;train&quot; in ds:
                return ds[&quot;train&quot;]
            # Fallback: take the first available split
            for k in ds.keys():
                return ds[k]
        return ds

    def _np_log_safe(x: np.ndarray, min_pos: float = 1e-12) -&gt; np.ndarray:
        return np.log(np.clip(x, min_pos, None))

    def _to_numpy_col(dataset, name: str, default: Optional[float] = None) -&gt; np.ndarray:
        if name in dataset.column_names:
            return np.asarray(dataset[name], dtype=float)
        if default is None:
            raise KeyError(f&quot;Required column &#x27;{name}&#x27; not found in dataset.&quot;)
        return np.full(len(dataset), float(default))

    def _kfold_indices(n: int, k: int = 5, seed: int = 1337) -&gt; List[Tuple[np.ndarray, np.ndarray]]:
        k = max(2, min(k, n)) if n &gt;= 4 else 2
        rng = np.random.default_rng(seed)
        idx = np.arange(n)
        rng.shuffle(idx)
        folds = np.array_split(idx, k)
        splits: List[Tuple[np.ndarray, np.ndarray]] = []
        for i in range(k):
            val_idx = folds[i]
            train_idx = np.concatenate([folds[j] for j in range(k) if j != i]) if k &gt; 1 else idx
            splits.append((train_idx, val_idx))
        return splits

    def _ridge_solve(X: np.ndarray, y: np.ndarray, lam: float = 1e-12, no_reg_cols: Optional[List[int]] = None) -&gt; np.ndarray:
        XtX = X.T @ X
        reg = np.eye(X.shape[1]) * lam
        if no_reg_cols:
            for c in no_reg_cols:
                reg[c, c] = 0.0
        A = XtX + reg
        b = X.T @ y
        try:
            return np.linalg.solve(A, b)
        except np.linalg.LinAlgError:
            # Fallback to lstsq if ill-conditioned
            return np.linalg.lstsq(X, y, rcond=None)[0]

    def _fit_additive(p: np.ndarray, t: np.ndarray, u: Optional[np.ndarray], y: np.ndarray) -&gt; Dict:
        # Additive power-law with irreducible floor:
        #   loss ≈ L_inf + a * p^{-α} + b * t^{-β} [+ c * u^{-γ}]
        # Grid-search small set of plausible exponents, solve linear coefs via ridge for each.
        alphas = np.array([0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20])
        betas  = np.array([0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20])
        gammas = np.array([0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.15, 0.20]) if u is not None else np.array([])

        # Numerically safe bases
        eps = 1e-18
        p = np.clip(p, eps, None)
        t = np.clip(t, eps, None)
        if u is not None:
            u = np.clip(u, eps, None)

        # Cross-validation to pick exponents
        n = len(y)
        splits = _kfold_indices(n, k=5)
        best = {&quot;score&quot;: math.inf}

        # Precompute logs for speed
        lp = np.log(p)
        lt = np.log(t)
        lu = np.log(u) if u is not None else None

        def _score_combo(alpha: float, beta: float, gamma: Optional[float]) -&gt; float:
            # Use CV RMSE in y-space
            errs: List[float] = []
            for tr_idx, va_idx in splits:
                # Build features on train
                F_cols = []
                F_cols.append(np.exp(-alpha * lp[tr_idx]))
                F_cols.append(np.exp(-beta * lt[tr_idx]))
                if gamma is not None and lu is not None:
                    F_cols.append(np.exp(-gamma * lu[tr_idx]))
                F = np.column_stack(F_cols)  # (n, m)
                # Solve for coefficients with L_inf as free intercept via two-step stable approach
                # First, unconstrained with intercept:
                X = np.column_stack([np.ones(F.shape[0]), F])
                theta = _ridge_solve(X, y[tr_idx], lam=1e-10, no_reg_cols=[0])
                L_inf = float(min(theta[0], float(np.min(y[tr_idx]) - 1e-9)))  # do not exceed min observed loss
                # Refit non-negative weights on residual y - L_inf
                r = y[tr_idx] - L_inf
                r = np.maximum(r, 0.0)
                # Solve and then clip negatives to zero, refit using only positive columns
                w = _ridge_solve(F, r, lam=1e-10)
                w = np.where(w &lt; 0.0, 0.0, w)
                if np.all(w == 0.0):
                    # avoid degenerate
                    y_hat_va = np.full(len(va_idx), L_inf)
                else:
                    # Refit using only columns with positive weights
                    keep = w &gt; 0.0
                    Fk = F[:, keep]
                    wk = _ridge_solve(Fk, r, lam=1e-10) if Fk.shape[1] &gt; 0 else np.zeros(0)
                    Fv_cols = []
                    Fv_cols.append(np.exp(-alpha * lp[va_idx]))
                    Fv_cols.append(np.exp(-beta * lt[va_idx]))
                    if gamma is not None and lu is not None:
                        Fv_cols.append(np.exp(-gamma * lu[va_idx]))
                    Fv = np.column_stack(Fv_cols)[:, keep] if keep.any() else np.zeros((len(va_idx), 0))
                    y_hat_va = L_inf + (Fv @ wk if Fv.shape[1] &gt; 0 else 0.0)
                err = float(np.sqrt(np.mean((y_hat_va - y[va_idx]) ** 2)))
                errs.append(err)
            return float(np.mean(errs))

        # Iterate grid
        for a in alphas:
            for b in betas:
                if u is None:
                    score = _score_combo(a, b, None)
                    if score &lt; best[&quot;score&quot;]:
                        best = {&quot;score&quot;: score, &quot;alpha&quot;: float(a), &quot;beta&quot;: float(b), &quot;gamma&quot;: None}
                else:
                    for c in gammas:
                        score = _score_combo(a, b, c)
                        if score &lt; best[&quot;score&quot;]:
                            best = {&quot;score&quot;: score, &quot;alpha&quot;: float(a), &quot;beta&quot;: float(b), &quot;gamma&quot;: float(c)}

        # Fit final model on all data with chosen exponents
        alpha = best[&quot;alpha&quot;]
        beta = best[&quot;beta&quot;]
        gamma = best[&quot;gamma&quot;]
        F_cols = [np.exp(-alpha * lp), np.exp(-beta * lt)]
        if gamma is not None and lu is not None:
            F_cols.append(np.exp(-gamma * lu))
        F = np.column_stack(F_cols)
        X = np.column_stack([np.ones(F.shape[0]), F])
        theta = _ridge_solve(X, y, lam=1e-10, no_reg_cols=[0])
        L_inf = float(min(theta[0], float(np.min(y) - 1e-9)))
        r = np.maximum(y - L_inf, 0.0)
        w = _ridge_solve(F, r, lam=1e-10)
        w = np.where(w &lt; 0.0, 0.0, w)

        # Keep only positive-weight features
        keep = w &gt; 0.0
        if not np.any(keep):
            keep = np.ones_like(w, dtype=bool)
        w = w[keep]
        # Map kept indices back to variable names
        var_names = [&quot;params&quot;, &quot;tokens&quot;] + ([&quot;unique_tokens&quot;] if gamma is not None else [])
        kept_vars = [var_names[i] for i, k in enumerate(keep) if k]

        return {
            &quot;model&quot;: &quot;additive&quot;,
            &quot;exponents&quot;: {&quot;params&quot;: alpha, &quot;tokens&quot;: beta, **({&quot;unique_tokens&quot;: gamma} if gamma is not None else {})},
            &quot;L_inf&quot;: L_inf,
            &quot;weights&quot;: {name: float(wi) for name, wi in zip(kept_vars, w)},
            &quot;score&quot;: best[&quot;score&quot;],
        }

    def _fit_loglinear(p: np.ndarray, t: np.ndarray, u: Optional[np.ndarray], y: np.ndarray) -&gt; Dict:
        # Multiplicative power-law without explicit floor:
        #   log loss ≈ c0 + c1 log p + c2 log t [+ c3 log u]
        eps = 1e-18
        p = np.clip(p, eps, None)
        t = np.clip(t, eps, None)
        lp, lt = np.log(p), np.log(t)
        cols = [np.ones_like(lp), lp, lt]
        if u is not None:
            u = np.clip(u, eps, None)
            lu = np.log(u)
            cols.append(lu)
        X = np.column_stack(cols)
        y_safe = np.clip(y, 1e-12, None)
        ly = np.log(y_safe)

        # CV score in y-space
        splits = _kfold_indices(len(y), k=5)
        errs = []
        for tr_idx, va_idx in splits:
            theta = _ridge_solve(X[tr_idx], ly[tr_idx], lam=1e-10, no_reg_cols=[0])
            y_hat_va = np.exp(X[va_idx] @ theta)
            errs.append(float(np.sqrt(np.mean((y_hat_va - y[va_idx]) ** 2))))
        score = float(np.mean(errs))

        theta = _ridge_solve(X, ly, lam=1e-10, no_reg_cols=[0])
        params = {&quot;c0&quot;: float(theta[0]), &quot;c1&quot;: float(theta[1]), &quot;c2&quot;: float(theta[2])}
        if u is not None and X.shape[1] == 4:
            params[&quot;c3&quot;] = float(theta[3])
        return {&quot;model&quot;: &quot;loglinear&quot;, &quot;theta&quot;: params, &quot;score&quot;: score}

    def _predict_additive(model: Dict, p: float, t: float, u: Optional[float]) -&gt; float:
        L_inf = model[&quot;L_inf&quot;]
        exps = model[&quot;exponents&quot;]
        w = model[&quot;weights&quot;]
        val = L_inf
        if &quot;params&quot; in w:
            val += w[&quot;params&quot;] * (max(p, 1e-18) ** (-exps[&quot;params&quot;]))
        if &quot;tokens&quot; in w:
            val += w[&quot;tokens&quot;] * (max(t, 1e-18) ** (-exps[&quot;tokens&quot;]))
        if u is not None and &quot;unique_tokens&quot; in w and &quot;unique_tokens&quot; in exps:
            val += w[&quot;unique_tokens&quot;] * (max(u, 1e-18) ** (-exps[&quot;unique_tokens&quot;]))
        return float(max(val, 1e-9))

    def _predict_loglinear(model: Dict, p: float, t: float, u: Optional[float]) -&gt; float:
        theta = model[&quot;theta&quot;]
        val = theta[&quot;c0&quot;] + theta[&quot;c1&quot;] * math.log(max(p, 1e-18)) + theta[&quot;c2&quot;] * math.log(max(t, 1e-18))
        if u is not None and &quot;c3&quot; in theta:
            val += theta[&quot;c3&quot;] * math.log(max(u, 1e-18))
        return float(max(math.exp(val), 1e-9))

    # Fit parameters once per process and cache them
    if not hasattr(law, &quot;_cache&quot;):
        # Load and extract columns
        dataset = _load_dataset()
        # Gather columns safely
        try:
            params_all = _to_numpy_col(dataset, &quot;params&quot;)
            tokens_all = _to_numpy_col(dataset, &quot;tokens&quot;)
            unique_all = _to_numpy_col(dataset, &quot;unique_tokens&quot;, None) if &quot;unique_tokens&quot; in dataset.column_names else None
            loss_all = _to_numpy_col(dataset, &quot;loss&quot;)
            groups = dataset[&quot;group&quot;] if &quot;group&quot; in dataset.column_names else [&quot;default&quot;] * len(loss_all)
        except Exception as e:
            # As a hard fallback, create a trivial model if dataset schema is unexpected
            law._cache = {
                &quot;groups&quot;: {},
                &quot;global&quot;: {&quot;model&quot;: &quot;loglinear&quot;, &quot;theta&quot;: {&quot;c0&quot;: 0.0, &quot;c1&quot;: 0.0, &quot;c2&quot;: 0.0}, &quot;score&quot;: float(&quot;inf&quot;)},
                &quot;medians&quot;: {&quot;params&quot;: 1.0, &quot;tokens&quot;: 1.0, &quot;unique_tokens&quot;: 1.0},
            }
            # Proceed to prediction with defaults
            pass
        else:
            # Group indices
            groups = np.asarray(groups)
            uniq_groups = list(dict.fromkeys(groups.tolist()))
            group_models: Dict[str, Dict] = {}
            # Precompute medians for imputing missing features at prediction time
            med_params = float(np.median(params_all))
            med_tokens = float(np.median(tokens_all))
            med_unique = float(np.median(unique_all)) if unique_all is not None else 1.0

            for g in uniq_groups:
                m = (groups == g)
                p = params_all[m]
                t = tokens_all[m]
                u = unique_all[m] if unique_all is not None else None
                y = loss_all[m]

                # If unique_tokens has negligible variation, ignore it
                use_u = None
                if u is not None and np.isfinite(u).all():
                    if np.ptp(u) &gt; 1e-12 * max(1.0, float(np.median(u))):
                        use_u = u

                add_model = _fit_additive(p, t, use_u, y)
                log_model = _fit_loglinear(p, t, use_u, y)
                model = add_model if add_model[&quot;score&quot;] &lt;= log_model[&quot;score&quot;] else log_model
                group_models[g] = model

            # Also fit a global fallback model on all data
            use_u_all = None
            if unique_all is not None and np.isfinite(unique_all).all():
                if np.ptp(unique_all) &gt; 1e-12 * max(1.0, float(np.median(unique_all))):
                    use_u_all = unique_all
            add_model_all = _fit_additive(params_all, tokens_all, use_u_all, loss_all)
            log_model_all = _fit_loglinear(params_all, tokens_all, use_u_all, loss_all)
            global_model = add_model_all if add_model_all[&quot;score&quot;] &lt;= log_model_all[&quot;score&quot;] else log_model_all

            law._cache = {
                &quot;groups&quot;: group_models,
                &quot;global&quot;: global_model,
                &quot;medians&quot;: {&quot;params&quot;: med_params, &quot;tokens&quot;: med_tokens, &quot;unique_tokens&quot;: med_unique},
            }

    # Prepare predictions
    cache = getattr(law, &quot;_cache&quot;, None)
    if cache is None:
        # Should not happen, but ensure a safe default
        return [{&quot;loss&quot;: 1.0} for _ in input_data]

    # Pick model for requested group or global fallback
    model = cache[&quot;groups&quot;].get(group, cache[&quot;global&quot;])
    med = cache[&quot;medians&quot;]

    results: List[Dict[str, float]] = []
    for row in input_data:
        p = float(row.get(&quot;params&quot;, med[&quot;params&quot;]))
        t = float(row.get(&quot;tokens&quot;, med[&quot;tokens&quot;]))
        u = float(row.get(&quot;unique_tokens&quot;, med[&quot;unique_tokens&quot;]))
        if model[&quot;model&quot;] == &quot;additive&quot;:
            pred = _predict_additive(model, p, t, u)
        else:
            pred = _predict_loglinear(model, p, t, u)
        results.append({&quot;loss&quot;: float(pred)})

    return results</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.841468
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2">from __future__ import annotations

from typing import Dict, List

import math
import os

import numpy as np


# Global cache populated on first use from /app/data
_FITTED: bool = False
_EXPONENTS: Dict[str, float] = {&quot;alpha&quot;: 0.12, &quot;beta&quot;: 0.20, &quot;gamma&quot;: 0.50}  # sensible defaults
_COEFFS: Dict[str, Dict[str, float]] = {}  # group -&gt; {&quot;L_inf&quot;: ..., &quot;A&quot;: ..., &quot;B&quot;: ..., &quot;C&quot;: ...}
_EPS = 1e-12
_DATA_PATH = &quot;/app/data&quot;
_EXPLAIN_PATH = &quot;/app/explain.md&quot;


def _safe_get(d: dict, keys: List[str], default: float | None = None) -&gt; float | None:
    for k in keys:
        if k in d and d[k] is not None:
            return d[k]
    return default


def _load_dataset_rows() -&gt; Dict[str, np.ndarray]:
    &quot;&quot;&quot;
    Load dataset from /app/data using datasets.load_from_disk(), returning numpy arrays.
    Expected fields:
      - loss (float)
      - params (float)
      - tokens (float)
      - unique_tokens (float)
      - group (str)
    &quot;&quot;&quot;
    try:
        from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore
    except Exception:
        raise RuntimeError(
            &quot;The &#x27;datasets&#x27; package is required to fit the scaling law from /app/data.&quot;
        )

    ds_any = load_from_disk(_DATA_PATH)
    rows: List[dict] = []

    def _extend_from_dataset(ds):
        for r in ds:
            rows.append(
                {
                    &quot;loss&quot;: _safe_get(r, [&quot;loss&quot;]),
                    &quot;params&quot;: _safe_get(r, [&quot;params&quot;, &quot;P&quot;, &quot;n_params&quot;, &quot;parameters&quot;]),
                    &quot;tokens&quot;: _safe_get(r, [&quot;tokens&quot;, &quot;N&quot;, &quot;train_tokens&quot;, &quot;n_tokens&quot;]),
                    &quot;unique_tokens&quot;: _safe_get(
                        r, [&quot;unique_tokens&quot;, &quot;U&quot;, &quot;n_unique_tokens&quot;, &quot;vocab_coverage&quot;]
                    ),
                    &quot;group&quot;: _safe_get(r, [&quot;group&quot;, &quot;group_name&quot;, &quot;dataset_group&quot;]),
                }
            )

    if hasattr(ds_any, &quot;values&quot;) and isinstance(ds_any, DatasetDict):  # multiple splits
        for split in ds_any.values():
            _extend_from_dataset(split)
    else:
        _extend_from_dataset(ds_any)

    # Filter and coerce
    rows = [
        r
        for r in rows
        if r[&quot;loss&quot;] is not None
        and r[&quot;params&quot;] is not None
        and r[&quot;tokens&quot;] is not None
        and r[&quot;unique_tokens&quot;] is not None
        and r[&quot;group&quot;] is not None
    ]

    if not rows:
        raise RuntimeError(&quot;No valid rows found in /app/data&quot;)

    loss = np.asarray([float(r[&quot;loss&quot;]) for r in rows], dtype=float)
    P = np.asarray([float(r[&quot;params&quot;]) for r in rows], dtype=float)
    T = np.asarray([float(r[&quot;tokens&quot;]) for r in rows], dtype=float)
    U = np.asarray([float(r[&quot;unique_tokens&quot;]) for r in rows], dtype=float)
    groups = np.asarray([str(r[&quot;group&quot;]) for r in rows], dtype=object)

    # Basic sanitization
    P = np.maximum(P, _EPS)
    T = np.maximum(T, _EPS)
    U = np.clip(U, _EPS, None)

    # Ensure U &lt;= T (if not, clip to T; dataset glitches)
    U = np.minimum(U, T)

    # Finite-only
    mask = np.isfinite(loss) &amp; np.isfinite(P) &amp; np.isfinite(T) &amp; np.isfinite(U)
    return {
        &quot;loss&quot;: loss[mask],
        &quot;P&quot;: P[mask],
        &quot;T&quot;: T[mask],
        &quot;U&quot;: U[mask],
        &quot;groups&quot;: groups[mask],
    }


def _design(P: np.ndarray, T: np.ndarray, U: np.ndarray, alpha: float, beta: float, gamma: float):
    x1 = np.power(P + _EPS, -alpha)
    x2 = np.power(T + _EPS, -beta)
    ratio = np.clip(U / (T + _EPS), _EPS, None)
    x3 = np.power(ratio, gamma)
    return x1, x2, x3


def _fit_per_group(loss: np.ndarray, x1: np.ndarray, x2: np.ndarray, x3: np.ndarray, groups: np.ndarray):
    coeffs: Dict[str, Dict[str, float]] = {}
    uniq = np.unique(groups)
    for g in uniq:
        idx = groups == g
        y = loss[idx]
        X = np.column_stack([np.ones_like(y), x1[idx], x2[idx], x3[idx]])
        # Linear least squares: y ≈ L_inf + A*x1 + B*x2 + C*x3
        b, *_ = np.linalg.lstsq(X, y, rcond=None)
        coeffs[str(g)] = {&quot;L_inf&quot;: float(b[0]), &quot;A&quot;: float(b[1]), &quot;B&quot;: float(b[2]), &quot;C&quot;: float(b[3])}
    return coeffs


def _mse(loss: np.ndarray, pred: np.ndarray) -&gt; float:
    return float(np.mean((loss - pred) ** 2))


def _predict_with_coeffs(
    loss: np.ndarray, x1: np.ndarray, x2: np.ndarray, x3: np.ndarray, groups: np.ndarray, coeffs: Dict[str, Dict[str, float]]
):
    # Build predictions respecting group membership
    yhat = np.empty_like(loss, dtype=float)
    uniq = np.unique(groups)
    for g in uniq:
        idx = groups == g
        c = coeffs[str(g)]
        yhat[idx] = c[&quot;L_inf&quot;] + c[&quot;A&quot;] * x1[idx] + c[&quot;B&quot;] * x2[idx] + c[&quot;C&quot;] * x3[idx]
    return yhat


def _grid(values: List[float], around: float | None = None, scale: float = 2.0) -&gt; List[float]:
    if around is None:
        return values
    lo = max(values[0], around / scale)
    hi = around * scale
    grid = sorted(set([values[0], values[-1], around, lo, hi]))
    return grid


def _fit_from_disk() -&gt; None:
    global _FITTED, _EXPONENTS, _COEFFS

    data = _load_dataset_rows()
    loss, P, T, U, groups = data[&quot;loss&quot;], data[&quot;P&quot;], data[&quot;T&quot;], data[&quot;U&quot;], data[&quot;groups&quot;]

    # Coarse grids inspired by LLM scaling literature
    coarse_alpha = [0.05, 0.08, 0.10, 0.12, 0.15, 0.20, 0.30]
    coarse_beta = [0.05, 0.08, 0.10, 0.12, 0.15, 0.20, 0.30]
    coarse_gamma = [0.25, 0.33, 0.50, 0.75, 1.00]

    best = {&quot;mse&quot;: math.inf, &quot;alpha&quot;: None, &quot;beta&quot;: None, &quot;gamma&quot;: None, &quot;coeffs&quot;: None}

    for a in coarse_alpha:
        x1a, _, _ = _design(P, T, U, a, 0.0, 1.0)  # precompute x1 dependency
        for b in coarse_beta:
            _, x2b, _ = _design(P, T, U, 0.0, b, 1.0)
            for gma in coarse_gamma:
                _, _, x3g = _design(P, T, U, 0.0, 0.0, gma)
                # Now combine without recomputing many times
                x1, x2, x3 = x1a, x2b, x3g
                coeffs = _fit_per_group(loss, x1, x2, x3, groups)
                pred = _predict_with_coeffs(loss, x1, x2, x3, groups, coeffs)
                e = _mse(loss, pred)
                if e &lt; best[&quot;mse&quot;]:
                    best.update(mse=e, alpha=a, beta=b, gamma=gma, coeffs=coeffs)

    # Optional fine pass around coarse best
    a0, b0, g0 = float(best[&quot;alpha&quot;]), float(best[&quot;beta&quot;]), float(best[&quot;gamma&quot;])
    fine_alpha = sorted(set([a0 / 1.5, a0 / 1.2, a0, a0 * 1.2, a0 * 1.5]))
    fine_beta = sorted(set([b0 / 1.5, b0 / 1.2, b0, b0 * 1.2, b0 * 1.5]))
    fine_gamma = sorted(set([max(0.1, g0 / 2), g0 / 1.5, g0, g0 * 1.5, g0 * 2.0]))

    for a in fine_alpha:
        x1a, _, _ = _design(P, T, U, a, 0.0, 1.0)
        for b in fine_beta:
            _, x2b, _ = _design(P, T, U, 0.0, b, 1.0)
            for gma in fine_gamma:
                _, _, x3g = _design(P, T, U, 0.0, 0.0, gma)
                x1, x2, x3 = x1a, x2b, x3g
                coeffs = _fit_per_group(loss, x1, x2, x3, groups)
                pred = _predict_with_coeffs(loss, x1, x2, x3, groups, coeffs)
                e = _mse(loss, pred)
                if e &lt; best[&quot;mse&quot;]:
                    best.update(mse=e, alpha=a, beta=b, gamma=gma, coeffs=coeffs)

    _EXPONENTS = {&quot;alpha&quot;: float(best[&quot;alpha&quot;]), &quot;beta&quot;: float(best[&quot;beta&quot;]), &quot;gamma&quot;: float(best[&quot;gamma&quot;])}
    _COEFFS = dict(best[&quot;coeffs&quot;])  # type: ignore
    _FITTED = True

    # Generate explain.md
    try:
        _write_explain_md(
            exps=_EXPONENTS,
            coeffs=_COEFFS,
            n_rows=int(loss.shape[0]),
            groups=list(np.unique(groups).astype(str)),
        )
    except Exception:
        # Writing explain is best-effort; ignore failures during evaluation
        pass


def _write_explain_md(exps: Dict[str, float], coeffs: Dict[str, Dict[str, float]], n_rows: int, groups: List[str]) -&gt; None:
    lines: List[str] = []
    lines.append(&quot;# Discovered Scaling Law for Data-Constrained LLM Pre-Training&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;This document is auto-generated by /app/law.py after fitting on /app/data.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Functional Form&quot;)
    lines.append(
        &quot;We model the final validation loss as a group-wise affine combination of power-law terms in model parameters (P), total tokens (T), and the dataset diversity ratio (U/T):&quot;
    )
    lines.append(&quot;&quot;)
    lines.append(&quot;loss ≈ L_inf[g] + A[g] · P^(−α) + B[g] · T^(−β) + C[g] · (U/T)^(γ)&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;- α, β, γ are shared across groups (global exponents).&quot;)
    lines.append(&quot;- L_inf[g], A[g], B[g], C[g] are group-specific coefficients.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Fitting Procedure&quot;)
    lines.append(&quot;- Load all rows from /app/data.&quot;)
    lines.append(&quot;- Perform a coarse-to-fine grid search over global exponents α, β, γ.&quot;)
    lines.append(&quot;- For each exponent triplet, solve group-specific linear least squares for [L_inf, A, B, C].&quot;)
    lines.append(&quot;- Select the triplet that minimizes overall mean squared error.&quot;)
    lines.append(&quot;&quot;)
    lines.append(f&quot;Fitted on {n_rows} rows with {len(groups)} group(s).&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Global Exponents&quot;)
    lines.append(f&quot;- α = {exps[&#x27;alpha&#x27;]:.6g}&quot;)
    lines.append(f&quot;- β = {exps[&#x27;beta&#x27;]:.6g}&quot;)
    lines.append(f&quot;- γ = {exps[&#x27;gamma&#x27;]:.6g}&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Group-Specific Coefficients&quot;)
    for g in sorted(coeffs.keys()):
        c = coeffs[g]
        lines.append(f&quot;- {g}: L_inf={c[&#x27;L_inf&#x27;]:.6g}, A={c[&#x27;A&#x27;]:.6g}, B={c[&#x27;B&#x27;]:.6g}, C={c[&#x27;C&#x27;]:.6g}&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Usage&quot;)
    lines.append(&quot;Call law(input_data, group) with input_data containing keys: params, tokens, unique_tokens.&quot;)
    lines.append(&quot;&quot;)
    with open(_EXPLAIN_PATH, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.write(&quot;\n&quot;.join(lines))


def _ensure_fitted() -&gt; None:
    if _FITTED:
        return
    # Attempt to fit from disk; fall back to defaults if unavailable
    try:
        if os.path.exists(_DATA_PATH):
            _fit_from_disk()
        else:
            # No data; remain with defaults and empty coeffs
            pass
    except Exception:
        # Swallow to keep prediction available with defaults
        pass
    finally:
        # If we still have no coeffs, create a generic default to avoid KeyErrors
        if not _COEFFS:
            _COEFFS[&quot;__default__&quot;] = {&quot;L_inf&quot;: 2.5, &quot;A&quot;: 1.0, &quot;B&quot;: 1.0, &quot;C&quot;: 0.2}


def _predict_row(p: float, t: float, u: float, group: str) -&gt; float:
    a, b, g = _EXPONENTS[&quot;alpha&quot;], _EXPONENTS[&quot;beta&quot;], _EXPONENTS[&quot;gamma&quot;]
    x1 = (max(p, _EPS)) ** (-a)
    x2 = (max(t, _EPS)) ** (-b)
    ratio = max(min(u, t), _EPS) / max(t, _EPS)
    x3 = (ratio) ** (g)
    c = _COEFFS.get(group, _COEFFS.get(&quot;__default__&quot;))
    return c[&quot;L_inf&quot;] + c[&quot;A&quot;] * x1 + c[&quot;B&quot;] * x2 + c[&quot;C&quot;] * x3


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    _ensure_fitted()

    out: List[Dict[str, float]] = []
    for row in input_data:
        p = float(_safe_get(row, [&quot;params&quot;], 0.0) or 0.0)
        t = float(_safe_get(row, [&quot;tokens&quot;], 0.0) or 0.0)
        u = float(_safe_get(row, [&quot;unique_tokens&quot;], 0.0) or 0.0)
        y = _predict_row(p, t, u, group)
        out.append({&quot;loss&quot;: float(y)})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.795429
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3">def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected keys: &quot;params&quot;, &quot;tokens&quot;, &quot;unique_tokens&quot;.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups, but the
               coefficients are fit per group. If the group is not found, a global
               fit (across all groups) is used.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&quot;loss&quot;: float}.
    &quot;&quot;&quot;
    # Lazy-fit and cache coefficients on the function object so this module can
    # contain a single public function as required.
    if not hasattr(law, &quot;_coeffs&quot;):
        # Fit once at first invocation.
        import math
        from typing import Any, Iterable, Dict
        import numpy as np

        try:
            from datasets import load_from_disk, Dataset, DatasetDict
        except Exception as e:  # pragma: no cover
            raise RuntimeError(&quot;The &#x27;datasets&#x27; library is required to fit the scaling law.&quot;) from e

        # Load dataset from disk (provided path).
        data_obj = load_from_disk(&quot;/app/data&quot;)

        # Iterate through all rows across splits if needed.
        def _iter_rows(d: Any) -&gt; Iterable[Dict[str, Any]]:
            if hasattr(d, &quot;values&quot;) and callable(d.values):  # DatasetDict-like
                for split in d.values():
                    for row in split:
                        yield row
            else:
                for row in d:  # Single Dataset
                    yield row

        # Collect data per group.
        by_group: dict[str, dict[str, list[float]]] = {}
        # Also collect global.
        global_store = {&quot;params&quot;: [], &quot;tokens&quot;: [], &quot;unique_tokens&quot;: [], &quot;loss&quot;: []}
        for row in _iter_rows(data_obj):
            try:
                g = str(row[&quot;group&quot;])
                P = float(row[&quot;params&quot;])
                T = float(row[&quot;tokens&quot;])
                U = float(row[&quot;unique_tokens&quot;])
                L = float(row[&quot;loss&quot;])
            except Exception as e:
                # Skip rows that do not contain the required fields
                # to keep fitting robust.
                continue
            if g not in by_group:
                by_group[g] = {&quot;params&quot;: [], &quot;tokens&quot;: [], &quot;unique_tokens&quot;: [], &quot;loss&quot;: []}
            by_group[g][&quot;params&quot;].append(P)
            by_group[g][&quot;tokens&quot;].append(T)
            by_group[g][&quot;unique_tokens&quot;].append(U)
            by_group[g][&quot;loss&quot;].append(L)
            global_store[&quot;params&quot;].append(P)
            global_store[&quot;tokens&quot;].append(T)
            global_store[&quot;unique_tokens&quot;].append(U)
            global_store[&quot;loss&quot;].append(L)

        def _safe_log(x: np.ndarray) -&gt; np.ndarray:
            return np.log(np.clip(x, 1e-12, None))

        def _fit_block(block: dict[str, list[float]]) -&gt; dict[str, float]:
            # Convert to arrays
            params = np.asarray(block[&quot;params&quot;], dtype=float)
            tokens = np.asarray(block[&quot;tokens&quot;], dtype=float)
            uniq = np.asarray(block[&quot;unique_tokens&quot;], dtype=float)
            loss = np.asarray(block[&quot;loss&quot;], dtype=float)

            n = loss.size
            if n == 0:
                # Degenerate: return a conservative default.
                return {&quot;L0&quot;: float(np.nan), &quot;a&quot;: 0.0, &quot;b&quot;: -0.05, &quot;c&quot;: -0.1, &quot;d&quot;: -0.02}

            # Robust estimate of irreducible loss floor L0 slightly below the minimum observed loss.
            # This lets the model capture diminishing returns.
            lmin = float(np.min(loss))
            p10 = float(np.percentile(loss, 10.0)) if n &gt;= 5 else lmin
            p90 = float(np.percentile(loss, 90.0)) if n &gt;= 5 else float(np.max(loss))
            spread = max(0.0, p90 - p10)
            delta = max(1e-6, 0.05 * spread)
            L0 = lmin - delta

            resid = loss - L0
            # Ensure strictly positive residuals for the log.
            resid = np.clip(resid, 1e-12, None)

            # Design matrix for: log(resid) = a + b*log(P) + c*log(T) + d*log(U)
            X = np.column_stack([
                np.ones_like(resid),
                _safe_log(params),
                _safe_log(tokens),
                _safe_log(uniq),
            ])
            y = np.log(resid)

            # Solve least squares; fall back to simple defaults on failure.
            try:
                coeffs, *_ = np.linalg.lstsq(X, y, rcond=None)
                a, b, c, d = map(float, coeffs.tolist())
            except Exception:
                a, b, c, d = 0.0, -0.05, -0.1, -0.02

            return {&quot;L0&quot;: float(L0), &quot;a&quot;: a, &quot;b&quot;: b, &quot;c&quot;: c, &quot;d&quot;: d}

        coeffs_by_group: dict[str, dict[str, float]] = {}
        for g, block in by_group.items():
            coeffs_by_group[g] = _fit_block(block)

        # Global fallback using all data across groups.
        coeffs_by_group[&quot;__GLOBAL__&quot;] = _fit_block(global_store)

        # Cache on the function object.
        law._coeffs = coeffs_by_group  # type: ignore[attr-defined]

    # Use group-specific coefficients if available, else fall back to global.
    coeffs = law._coeffs.get(group) if hasattr(law, &quot;_coeffs&quot;) else None  # type: ignore[attr-defined]
    if coeffs is None:
        coeffs = law._coeffs.get(&quot;__GLOBAL__&quot;)  # type: ignore[attr-defined]
        if coeffs is None:
            # Final fallback if fitting failed entirely.
            coeffs = {&quot;L0&quot;: 0.0, &quot;a&quot;: 0.0, &quot;b&quot;: -0.05, &quot;c&quot;: -0.1, &quot;d&quot;: -0.02}

    L0 = float(coeffs[&quot;L0&quot;])
    a = float(coeffs[&quot;a&quot;])
    b = float(coeffs[&quot;b&quot;])
    c = float(coeffs[&quot;c&quot;])
    d = float(coeffs[&quot;d&quot;])

    # Prepare predictions
    out: list[dict[str, float]] = []
    # Compute with safe logs.
    import numpy as np
    for row in input_data:
        P = float(row.get(&quot;params&quot;, 1.0))
        T = float(row.get(&quot;tokens&quot;, 1.0))
        U = float(row.get(&quot;unique_tokens&quot;, 1.0))

        lp = np.log(max(P, 1e-12))
        lt = np.log(max(T, 1e-12))
        lu = np.log(max(U, 1e-12))

        pred = L0 + float(np.exp(a + b * lp + c * lt + d * lu))
        # Guard against any NaNs/Infs
        if not np.isfinite(pred):
            pred = float(L0)

        out.append({&quot;loss&quot;: float(pred)})

    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.103650
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4">&quot;&quot;&quot;
Scaling law predictor for language model pre-training validation loss.

We assume a multiplicative power-law relationship between loss and the inputs
(model parameters P, training tokens T, and unique tokens U), which becomes
affine after taking logs:

    loss ≈ c0_g + cP_g * ln(P) + cT_g * ln(T) + cU_g * ln(U)

The functional form is shared across groups g, while coefficients are group-specific.
Coefficients are fit via ridge-regularized least squares on the dataset found at
/app/data (loaded with datasets.load_from_disk). If the dataset is unavailable,
we fall back to conservative defaults.

Run this module as a script to print the fitted coefficients per group:
    python /app/law.py
&quot;&quot;&quot;
from __future__ import annotations

from typing import Dict, List
import os
import math

# Optional dependency; handled gracefully if missing at runtime.
try:
    import numpy as np
except Exception:  # pragma: no cover
    np = None  # type: ignore

try:
    from datasets import load_from_disk  # type: ignore
except Exception:  # pragma: no cover
    load_from_disk = None  # type: ignore


# Global storage for fitted coefficients per group.
# Each value is a numpy array [c0, cP, cT, cU].
_COEFFS: Dict[str, &quot;np.ndarray&quot;] = {}
# Fallback coefficients if fitting isn&#x27;t possible.
_DEFAULT_COEFFS = (0.0, -0.05, -0.05, -0.01)  # (c0, cP, cT, cU)
# Small epsilon to avoid log(0).
_EPS = 1.0


def _safe_log(x: float) -&gt; float:
    # Natural log with guard against non-positive inputs.
    if not isinstance(x, (int, float)) or not math.isfinite(x):
        return 0.0
    if x &lt;= 0:
        x = _EPS
    return math.log(x)


def _fit_group_coeffs(X: &quot;np.ndarray&quot;, y: &quot;np.ndarray&quot;, lam: float = 1e-6) -&gt; &quot;np.ndarray&quot;:
    &quot;&quot;&quot;
    Solve ridge-regularized least squares:
        minimize ||X w - y||^2 + lam * ||w||^2
    where columns of X are [1, ln(P), ln(T), ln(U)].
    &quot;&quot;&quot;
    n_features = X.shape[1]
    XtX = X.T @ X
    reg = lam * np.eye(n_features)
    Xty = X.T @ y
    return np.linalg.solve(XtX + reg, Xty)


def _try_fit_from_disk(path: str = &quot;/app/data&quot;) -&gt; None:
    &quot;&quot;&quot;
    Attempt to load the dataset and fit per-group coefficients.
    Expected columns: &#x27;loss&#x27;, &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;, &#x27;group&#x27;
    &quot;&quot;&quot;
    global _COEFFS
    if np is None or load_from_disk is None:
        return
    if not os.path.exists(path):
        return

    try:
        ds = load_from_disk(path)
    except Exception:
        return

    # Some datasets may be DatasetDict; prefer &#x27;train&#x27; if present, else merge.
    try:
        if hasattr(ds, &quot;keys&quot;):  # DatasetDict-like
            if &quot;train&quot; in ds:
                d = ds[&quot;train&quot;]
            else:
                # Concatenate all splits
                splits = [ds[k] for k in ds.keys()]
                d = splits[0].concatenate_datasets(splits[1:]) if len(splits) &gt; 1 else splits[0]
        else:
            d = ds
    except Exception:
        return

    required = {&quot;loss&quot;, &quot;params&quot;, &quot;tokens&quot;, &quot;unique_tokens&quot;, &quot;group&quot;}
    missing = [c for c in required if c not in d.column_names]
    if missing:
        return

    # Collect by group
    try:
        groups = set(d[&quot;group&quot;])
    except Exception:
        return

    fitted: Dict[str, &quot;np.ndarray&quot;] = {}
    # Also collect a global fit as fallback for unseen groups.
    X_all = []
    y_all = []

    for g in groups:
        # Filter rows for this group
        sub = d.filter(lambda r: r[&quot;group&quot;] == g)
        if len(sub) == 0:
            continue
        P = sub[&quot;params&quot;]
        T = sub[&quot;tokens&quot;]
        U = sub[&quot;unique_tokens&quot;]
        Y = sub[&quot;loss&quot;]

        # Build design matrix X with columns [1, ln(P), ln(T), ln(U)]
        rows = []
        for p, t, u in zip(P, T, U):
            rows.append([1.0, _safe_log(float(p)), _safe_log(float(t)), _safe_log(float(u))])
        X = np.array(rows, dtype=float)
        y = np.array([float(v) for v in Y], dtype=float)

        # Keep for global fit
        X_all.append(X)
        y_all.append(y)

        # Fit coefficients for this group
        try:
            w = _fit_group_coeffs(X, y, lam=1e-6)
            fitted[str(g)] = w
        except Exception:
            # Skip this group if singular; we&#x27;ll rely on global later.
            continue

    if X_all:
        try:
            Xg = np.vstack(X_all)
            yg = np.concatenate(y_all)
            wg = _fit_group_coeffs(Xg, yg, lam=1e-6)
            fitted[&quot;__GLOBAL__&quot;] = wg
        except Exception:
            pass

    if fitted:
        _COEFFS = fitted


# Try to learn coefficients once at import time.
_try_fit_from_disk()


def _get_coeffs(group: str) -&gt; tuple[float, float, float, float]:
    &quot;&quot;&quot;
    Retrieve coefficients for a specific group, falling back to global or defaults.
    Returns (c0, cP, cT, cU).
    &quot;&quot;&quot;
    # Prefer exact group
    if _COEFFS and group in _COEFFS:
        w = _COEFFS[group]
        return float(w[0]), float(w[1]), float(w[2]), float(w[3])
    # Fallback to a global fit if available
    if _COEFFS and &quot;__GLOBAL__&quot; in _COEFFS:
        w = _COEFFS[&quot;__GLOBAL__&quot;]
        return float(w[0]), float(w[1]), float(w[2]), float(w[3])
    # Final fallback: conservative defaults
    return _DEFAULT_COEFFS


def _predict_loss(params: float, tokens: float, unique_tokens: float, coeffs: tuple[float, float, float, float]) -&gt; float:
    c0, cP, cT, cU = coeffs
    lp = _safe_log(params)
    lt = _safe_log(tokens)
    lu = _safe_log(unique_tokens)
    y = c0 + cP * lp + cT * lt + cU * lu
    # Keep predictions within sane numeric bounds
    if not math.isfinite(y):
        y = float(&quot;nan&quot;)
    return y


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected keys: &#x27;params&#x27;, &#x27;tokens&#x27;, &#x27;unique_tokens&#x27;.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the coefficients differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;loss&#x27;: float}.
    &quot;&quot;&quot;
    coeffs = _get_coeffs(str(group) if group is not None else &quot;&quot;)
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        p = float(row.get(&quot;params&quot;, _EPS))
        t = float(row.get(&quot;tokens&quot;, _EPS))
        u = float(row.get(&quot;unique_tokens&quot;, _EPS))
        yhat = _predict_loss(p, t, u, coeffs)
        outputs.append({&quot;loss&quot;: float(yhat)})
    return outputs


if __name__ == &quot;__main__&quot;:  # Utility: print fitted coefficients for inspection
    # Ensure we have attempted fitting; then print what we have.
    if not _COEFFS:
        _try_fit_from_disk()
    print(&quot;Fitted coefficients per group (loss = c0 + cP*ln(P) + cT*ln(T) + cU*ln(U))&quot;)
    if _COEFFS:
        for k in sorted(_COEFFS.keys()):
            w = _COEFFS[k]
            print(f&quot;{k}: c0={w[0]:.6f}, cP={w[1]:.6f}, cT={w[2]:.6f}, cU={w[3]:.6f}&quot;)
    else:
        c0, cP, cT, cU = _DEFAULT_COEFFS
        print(&quot;No dataset found; using defaults&quot;)
        print(f&quot;__DEFAULT__: c0={c0:.6f}, cP={cP:.6f}, cT={cT:.6f}, cU={cU:.6f}&quot;)</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>