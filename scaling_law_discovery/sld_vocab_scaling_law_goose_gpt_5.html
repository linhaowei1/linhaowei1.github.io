<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Vocabulary Scaling Law - goose + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Vocabulary Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">goose</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.980344 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.961611</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.933122</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.980344 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">import math
from typing import List, Dict

# Quadratic polynomial in natural logs of inputs, fitted via ridge regression (alpha ~= 1e-6)
# Target: unigram_normalized_loss
# Features: 1, ln N, ln D, ln V, (ln N)^2, (ln N)(ln D), (ln N)(ln V), (ln D)^2, (ln D)(ln V), (ln V)^2
# Where N = non_vocab_parameters, D = num_characters, V = vocab_size

# Per-group coefficients. Functional form is identical across groups; coefficients may differ.
# Only &#x27;all_data&#x27; was present in the released dataset; we use it as the default for any unknown group.
_COEFFICIENTS_BY_GROUP: Dict[str, Dict[str, float]] = {
    &quot;all_data&quot;: {
        &quot;c0&quot;: 43.65241359337898,
        &quot;c_log_N&quot;: 0.5845970659865998,
        &quot;c_log_D&quot;: -4.504391609574668,
        &quot;c_log_V&quot;: 0.7794943512417376,
        &quot;c_log_N2&quot;: 0.02581377699971201,
        &quot;c_log_N_log_D&quot;: -0.0813545696437359,
        &quot;c_log_N_log_V&quot;: 0.022588042542338404,
        &quot;c_log_D2&quot;: 0.13736449927091602,
        &quot;c_log_D_log_V&quot;: -0.0738696772199968,
        &quot;c_log_V2&quot;: 0.0285489527696865,
    }
}

# Fallback order when an unknown group is requested
_FALLBACK_GROUP = &quot;all_data&quot;


def _predict_single(x: Dict[str, float], coeffs: Dict[str, float]) -&gt; float:
    N = float(x.get(&quot;non_vocab_parameters&quot;, 0.0))
    D = float(x.get(&quot;num_characters&quot;, 0.0))
    V = float(x.get(&quot;vocab_size&quot;, 0.0))
    if N &lt;= 0 or D &lt;= 0 or V &lt;= 0:
        # Guard against invalid inputs for logarithms; return NaN to signal invalid prediction
        return float(&quot;nan&quot;)
    lnN = math.log(N)
    lnD = math.log(D)
    lnV = math.log(V)
    y = (
        coeffs[&quot;c0&quot;]
        + coeffs[&quot;c_log_N&quot;] * lnN
        + coeffs[&quot;c_log_D&quot;] * lnD
        + coeffs[&quot;c_log_V&quot;] * lnV
        + coeffs[&quot;c_log_N2&quot;] * (lnN ** 2)
        + coeffs[&quot;c_log_N_log_D&quot;] * (lnN * lnD)
        + coeffs[&quot;c_log_N_log_V&quot;] * (lnN * lnV)
        + coeffs[&quot;c_log_D2&quot;] * (lnD ** 2)
        + coeffs[&quot;c_log_D_log_V&quot;] * (lnD * lnV)
        + coeffs[&quot;c_log_V2&quot;] * (lnV ** 2)
    )
    return float(y)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coeffs = _COEFFICIENTS_BY_GROUP.get(group, _COEFFICIENTS_BY_GROUP[_FALLBACK_GROUP])
    outputs: List[Dict[str, float]] = []
    for x in input_data:
        y = _predict_single(x, coeffs)
        outputs.append({&quot;unigram_normalized_loss&quot;: y})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.980329 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">from __future__ import annotations

from math import log
from typing import Dict, List

# Quadratic-in-logs scaling law fitted on the provided dataset
# Targets: unigram_normalized_loss
# Inputs: vocab_size (V), non_vocab_parameters (P), num_characters (D)
#
# y = c0
#     + c1*ln(V) + c2*ln(P) + c3*ln(D)
#     + c4*ln(V)^2 + c5*ln(V)*ln(P) + c6*ln(V)*ln(D)
#     + c7*ln(P)^2 + c8*ln(P)*ln(D) + c9*ln(D)^2
#
# The functional form is identical for all groups; coefficients may differ per group.
# The dataset contains a single group &#x27;all_data&#x27;. If an unknown group is requested,
# we fall back to &#x27;all_data&#x27;.

_COEFFICIENTS: Dict[str, Dict[str, float]] = {
    # Coefficients obtained via ordinary least squares on degree-2 polynomial features of
    # natural logs using the provided dataset.
    # Metrics (on provided data): R^2 ≈ 0.988, RMSE ≈ 0.088; holdout R^2 ≈ 0.986.
    &quot;all_data&quot;: {
        &quot;c0&quot;: 43.65302340313523,
        &quot;c1&quot;: 0.7794957511937278,   # ln(V)
        &quot;c2&quot;: 0.5846007123502589,   # ln(P)
        &quot;c3&quot;: -4.504394566402874,  # ln(D)
        &quot;c4&quot;: 0.028553981965247575,    # ln(V)^2
        &quot;c5&quot;: 0.022592838156027455,    # ln(V)*ln(P)
        &quot;c6&quot;: -0.07386461582128316,    # ln(V)*ln(D)
        &quot;c7&quot;: 0.025813565754715825,    # ln(P)^2
        &quot;c8&quot;: -0.08135643672422146,    # ln(P)*ln(D)
        &quot;c9&quot;: 0.13736040362701446,     # ln(D)^2
    }
}

_EPS = 1e-12  # numerical safety for logs


def _predict_one(vocab_size: float, non_vocab_parameters: float, num_characters: float, coefs: Dict[str, float]) -&gt; float:
    # Guard against non-positive inputs for logarithms
    V = max(float(vocab_size), _EPS)
    P = max(float(non_vocab_parameters), _EPS)
    D = max(float(num_characters), _EPS)

    lv = log(V)
    lp = log(P)
    ld = log(D)

    return (
        coefs[&quot;c0&quot;]
        + coefs[&quot;c1&quot;] * lv
        + coefs[&quot;c2&quot;] * lp
        + coefs[&quot;c3&quot;] * ld
        + coefs[&quot;c4&quot;] * (lv ** 2)
        + coefs[&quot;c5&quot;] * (lv * lp)
        + coefs[&quot;c6&quot;] * (lv * ld)
        + coefs[&quot;c7&quot;] * (lp ** 2)
        + coefs[&quot;c8&quot;] * (lp * ld)
        + coefs[&quot;c9&quot;] * (ld ** 2)
    )


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coefs = _COEFFICIENTS.get(group, _COEFFICIENTS[&quot;all_data&quot;])
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        y = _predict_one(
            vocab_size=row.get(&quot;vocab_size&quot;, 0.0),
            non_vocab_parameters=row.get(&quot;non_vocab_parameters&quot;, 0.0),
            num_characters=row.get(&quot;num_characters&quot;, 0.0),
            coefs=coefs,
        )
        outputs.append({&quot;unigram_normalized_loss&quot;: float(y)})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.980329 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">from __future__ import annotations

import math
from typing import Dict, List

# Quadratic-in-logs scaling law discovered on the provided dataset.
# Variables: 
#   P = non_vocab_parameters
#   V = vocab_size
#   D = num_characters
# Prediction: unigram_normalized_loss
#
# Functional form (natural logarithms):
#   y = b0
#       + b1 * ln P + b2 * ln V + b3 * ln D
#       + b4 * (ln P)^2 + b5 * (ln P)(ln V) + b6 * (ln P)(ln D)
#       + b7 * (ln V)^2 + b8 * (ln V)(ln D) + b9 * (ln D)^2
#
# Coefficients can differ per experimental group, but the form is the same.
# Below are the coefficients fitted for the groups present in the training data.

COEFFICIENTS: Dict[str, Dict[str, float]] = {
    # Fitted on the provided dataset (single group present: &#x27;all_data&#x27;).
    # Intercept and coefficients learned via ordinary least squares on
    # the full quadratic features in ln P, ln V, ln D.
    &quot;all_data&quot;: {
        &quot;b0&quot;: 43.65302340313573,
        &quot;b1&quot;: 0.5846007123502085,   # ln P
        &quot;b2&quot;: 0.779495751193743,    # ln V
        &quot;b3&quot;: -4.504394566402879,   # ln D
        &quot;b4&quot;: 0.02581356575471539,  # (ln P)^2
        &quot;b5&quot;: 0.02259283815602762,  # (ln P)(ln V)
        &quot;b6&quot;: -0.08135643672421937, # (ln P)(ln D)
        &quot;b7&quot;: 0.028553981965246198, # (ln V)^2
        &quot;b8&quot;: -0.07386461582128219, # (ln V)(ln D)
        &quot;b9&quot;: 0.1373604036270136,   # (ln D)^2
    },
}

# Fallback group to use when an unknown group is requested.
_DEFAULT_GROUP = &quot;all_data&quot;


def _predict_single(d: Dict[str, float], coeffs: Dict[str, float]) -&gt; float:
    &quot;&quot;&quot;Predicts unigram_normalized_loss for a single datum using given coefficients.&quot;&quot;&quot;
    try:
        P = float(d[&quot;non_vocab_parameters&quot;])  # non-vocabulary parameters
        V = float(d[&quot;vocab_size&quot;])           # vocabulary size
        D = float(d[&quot;num_characters&quot;])       # number of characters (data)
    except KeyError as e:
        raise KeyError(
            f&quot;Missing required key {e!s}. Required keys: &#x27;non_vocab_parameters&#x27;, &#x27;vocab_size&#x27;, &#x27;num_characters&#x27;&quot;
        )

    if P &lt;= 0 or V &lt;= 0 or D &lt;= 0:
        raise ValueError(&quot;All inputs must be positive to compute logarithms.&quot;)

    lnP = math.log(P)
    lnV = math.log(V)
    lnD = math.log(D)

    y = (
        coeffs[&quot;b0&quot;]
        + coeffs[&quot;b1&quot;] * lnP
        + coeffs[&quot;b2&quot;] * lnV
        + coeffs[&quot;b3&quot;] * lnD
        + coeffs[&quot;b4&quot;] * (lnP ** 2)
        + coeffs[&quot;b5&quot;] * (lnP * lnV)
        + coeffs[&quot;b6&quot;] * (lnP * lnD)
        + coeffs[&quot;b7&quot;] * (lnV ** 2)
        + coeffs[&quot;b8&quot;] * (lnV * lnD)
        + coeffs[&quot;b9&quot;] * (lnD ** 2)
    )
    return float(y)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coeffs = COEFFICIENTS.get(group, COEFFICIENTS[_DEFAULT_GROUP])
    preds = []
    for d in input_data:
        y = _predict_single(d, coeffs)
        preds.append({&quot;unigram_normalized_loss&quot;: y})
    return preds</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.933929 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations
from typing import List, Dict

# Discovered scaling law (inverse power-law saturation):
#   L = L_inf + A * P^{-alpha} + B * C^{-beta} + D * V^{-gamma}
# where
#   L: unigram_normalized_loss (lower is better)
#   P: non_vocab_parameters (parameters not allocated to the embedding)
#   C: num_characters (training data size in characters)
#   V: vocab_size (number of tokens in the vocabulary)
# The same functional form is used for all groups; coefficients differ per group.
#
# Coefficients below were fit on the provided dataset using non-linear least squares.
# If an unknown group is requested, we fall back to the &#x27;all_data&#x27; parameters.

_PARAMS_BY_GROUP: Dict[str, Dict[str, float]] = {
    # Fitted on the provided dataset (group == &#x27;all_data&#x27;)
    # Methodology: SciPy curve_fit with bounds, model described above.
    # See /app/explain.md for details.
    &quot;all_data&quot;: {
        &quot;Linf&quot;: -5.803541619999999,  # asymptotic loss at infinite scale
        &quot;A&quot;: 2.68428369e+01,         # coefficient for parameters term
        &quot;alpha&quot;: 2.36765535e-01,     # exponent for parameters term
        &quot;B&quot;: 6.70168939e+03,         # coefficient for data term
        &quot;beta&quot;: 3.80181291e-01,      # exponent for data term
        &quot;D&quot;: 2.22135843e+02,         # coefficient for vocab term
        &quot;gamma&quot;: 2.80988265e+00,     # exponent for vocab term
    },
}

# Fallback group name to use when an unknown group is requested
_FALLBACK_GROUP = &quot;all_data&quot;


def _predict_one(x: Dict[str, float], params: Dict[str, float]) -&gt; float:
    # Safeguard against zero/negative values (should not happen in valid data)
    P = max(float(x.get(&quot;non_vocab_parameters&quot;, 0.0)), 1e-12)
    C = max(float(x.get(&quot;num_characters&quot;, 0.0)), 1e-12)
    V = max(float(x.get(&quot;vocab_size&quot;, 0.0)), 1e-12)

    Linf = params[&quot;Linf&quot;]
    A = params[&quot;A&quot;]; alpha = params[&quot;alpha&quot;]
    B = params[&quot;B&quot;]; beta = params[&quot;beta&quot;]
    D = params[&quot;D&quot;]; gamma = params[&quot;gamma&quot;]

    # Inverse power-law saturation
    return Linf + A * (P ** (-alpha)) + B * (C ** (-beta)) + D * (V ** (-gamma))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = _PARAMS_BY_GROUP.get(group, _PARAMS_BY_GROUP[_FALLBACK_GROUP])
    out: List[Dict[str, float]] = []
    for x in input_data:
        y = _predict_one(x, params)
        out.append({&quot;unigram_normalized_loss&quot;: float(y)})
    return out</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.933122 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">from __future__ import annotations
from typing import List, Dict

# Discovered scaling law (additive power-law with a constant floor):
#   y = L + A * Np^{-alpha} + B * D^{-beta} + C * V^{-gamma}
# where
#   y  = unigram_normalized_loss
#   Np = non_vocab_parameters
#   D  = num_characters
#   V  = vocab_size
# The functional form is shared across groups; coefficients may vary by group.
# This repository&#x27;s dataset only contains a single group (&quot;all_data&quot;). We also
# provide a &quot;default&quot; set of parameters that mirrors the same fit.

PARAMS_BY_GROUP = {
    # Fit obtained by cross-validated grid-search on exponents
    # (alpha, beta, gamma) and least-squares on coefficients with
    # nonnegativity encouraged for A, B, C. Vocabulary-size effect
    # is negligible in this dataset (C ~ 0), so gamma is included
    # for completeness but contributes little.
    &quot;all_data&quot;: {
        # Refined fit (nonnegative A,B,C with intercept re-fit), full-data:
        # exponents: alpha=0.06, beta=0.35, gamma=0.02
        # coefficients: L=-6.45718219, A=2.51363526, B=3866.31610, C=0.0
        &quot;alpha&quot;: 0.06,
        &quot;beta&quot;: 0.35,
        &quot;gamma&quot;: 0.02,
        &quot;L&quot;: -6.45718219,
        &quot;A&quot;: 2.51363526,
        &quot;B&quot;: 3866.31610,
        &quot;C&quot;: 0.0,
    },
    &quot;default&quot;: {
        &quot;alpha&quot;: 0.06,
        &quot;beta&quot;: 0.35,
        &quot;gamma&quot;: 0.02,
        &quot;L&quot;: -6.45718219,
        &quot;A&quot;: 2.51363526,
        &quot;B&quot;: 3866.31610,
        &quot;C&quot;: 0.0,
    },
}


def _predict_one(x: Dict[str, float], p: Dict[str, float]) -&gt; float:
    # Extract and guard against non-positive inputs (should not occur in sane data)
    V = float(x.get(&quot;vocab_size&quot;, 0.0))
    Np = float(x.get(&quot;non_vocab_parameters&quot;, 0.0))
    D = float(x.get(&quot;num_characters&quot;, 0.0))

    eps = 1e-12
    V = max(V, eps)
    Np = max(Np, eps)
    D = max(D, eps)

    alpha = p[&quot;alpha&quot;]; beta = p[&quot;beta&quot;]; gamma = p[&quot;gamma&quot;]
    L = p[&quot;L&quot;]; A = p[&quot;A&quot;]; B = p[&quot;B&quot;]; C = p[&quot;C&quot;]

    y = (
        L
        + A * (Np ** (-alpha))
        + B * (D ** (-beta))
        + C * (V ** (-gamma))
    )
    return float(y)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = PARAMS_BY_GROUP.get(group, PARAMS_BY_GROUP[&quot;default&quot;])  # fallback if unseen group
    out: List[Dict[str, float]] = []
    for row in input_data:
        y = _predict_one(row, params)
        out.append({&quot;unigram_normalized_loss&quot;: y})
    return out</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
