<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - SFT Scaling Law - aider + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>SFT Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">aider</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.892971 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.131226</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">-1.000000</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.892971 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">from __future__ import annotations

import math
import os
from typing import Dict, Iterable, List, Tuple

# Try to import datasets; fall back gracefully if unavailable.
try:
    from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore
except Exception:  # pragma: no cover
    load_from_disk = None  # type: ignore
    Dataset = object  # type: ignore
    DatasetDict = dict  # type: ignore


# Parameters: group -&gt; (L_inf, A, alpha)
_PARAMS_BY_GROUP: Dict[str, Tuple[float, float, float]] = {}
_GLOBAL_PARAMS: Tuple[float, float, float] = (0.0, 1.0, 0.5)  # sensible default fallback


def _linear_fit(x: List[float], y: List[float]) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;
    Simple unweighted least squares fit for y = m*x + b
    Returns (m, b)
    &quot;&quot;&quot;
    n = len(x)
    if n == 0:
        return (0.0, 0.0)
    mean_x = sum(x) / n
    mean_y = sum(y) / n
    sxx = sum((xi - mean_x) ** 2 for xi in x)
    if sxx &lt;= 0.0:
        return (0.0, mean_y)
    sxy = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))
    m = sxy / sxx
    b = mean_y - m * mean_x
    return (m, b)


def _fit_power_law_with_asymptote(xs: List[float], ys: List[float]) -&gt; Tuple[float, float, float]:
    &quot;&quot;&quot;
    Fit the three-parameter scaling law:
        loss(N) = L_inf + A * N^(-alpha)
    via a coarse grid-search over L_inf and linear regression on log-space for A, alpha.

    Returns (L_inf, A, alpha)
    &quot;&quot;&quot;
    # Sanitize and filter data
    data = [(float(x), float(y)) for x, y in zip(xs, ys) if x is not None and y is not None]
    data = [(x, y) for x, y in data if x &gt; 0 and math.isfinite(x) and math.isfinite(y)]
    if not data:
        return (0.0, 1.0, 0.5)

    xs = [x for x, _ in data]
    ys = [y for _, y in data]
    y_min = min(ys)
    y_max = max(ys)

    # If no variation, fall back to a simpler 2-parameter power law with L_inf=0
    if not math.isfinite(y_min) or not math.isfinite(y_max) or abs(y_max - y_min) &lt; 1e-12:
        # Fit y = A * N^(-alpha) in log space
        t = [math.log(x) for x in xs]
        z = [math.log(max(y, 1e-12)) for y in ys]
        m, b = _linear_fit(t, z)
        alpha = -m
        A = math.exp(b)
        if not (math.isfinite(alpha) and alpha &gt; 0 and math.isfinite(A) and A &gt; 0):
            alpha, A = 0.5, max(y_min, 1e-6)
        return (0.0, A, alpha)

    # Define a grid for L_inf below the minimum observed loss
    span = max(y_max - y_min, 1e-6)
    upper = y_min - 1e-9  # must be strictly below min(y)
    lower = max(0.0, y_min - 0.25 * span)
    if lower &gt;= upper:
        lower = max(0.0, 0.5 * upper)

    candidates: List[float] = []
    steps = 50
    for i in range(steps):
        frac = (i + 0.5) / steps
        L = lower + frac * (upper - lower)
        if L &lt; upper:
            candidates.append(L)
    # Also try L_inf = 0 explicitly
    if 0.0 &lt; upper:
        candidates.append(0.0)

    best_err = float(&quot;inf&quot;)
    best_params = (0.0, 1.0, 0.5)

    t_vals = [math.log(x) for x in xs]

    for L in candidates:
        # Compute transformed targets z = log(y - L)
        # Safe because L &lt; min(y) by construction
        z_vals = [math.log(y - L) for y in ys]
        m, b = _linear_fit(t_vals, z_vals)
        alpha = -m
        A = math.exp(b)

        # Discard invalid fits
        if not (math.isfinite(alpha) and alpha &gt; 0 and math.isfinite(A) and A &gt; 0 and math.isfinite(L) and L &gt;= 0):
            continue

        # Evaluate SSE in natural space
        err = 0.0
        for x, y in zip(xs, ys):
            y_hat = L + A * (x ** (-alpha))
            if not math.isfinite(y_hat):
                err = float(&quot;inf&quot;)
                break
            diff = y_hat - y
            err += diff * diff

        if err &lt; best_err:
            best_err = err
            best_params = (L, A, alpha)

    return best_params


def _load_all_records(path: str = &quot;/app/data&quot;) -&gt; List[dict]:
    &quot;&quot;&quot;
    Load all rows from a HuggingFace dataset or dataset dict located at path.
    Returns a list of Python dict records.
    &quot;&quot;&quot;
    records: List[dict] = []
    if load_from_disk is None:
        return records
    try:
        ds = load_from_disk(path)  # type: ignore
    except Exception:
        return records

    def _iter_rows(d) -&gt; Iterable[dict]:
        try:
            return iter(d)  # HuggingFace Datasets are iterable
        except Exception:
            return iter([])

    # DatasetDict: combine splits
    try:
        if isinstance(ds, DatasetDict):  # type: ignore
            for split_name in ds.keys():  # type: ignore
                split_ds = ds[split_name]  # type: ignore
                for row in _iter_rows(split_ds):
                    records.append(row)
        elif isinstance(ds, Dataset):  # type: ignore
            for row in _iter_rows(ds):
                records.append(row)
        else:
            # Fallback: try dict-like
            if hasattr(ds, &quot;values&quot;):
                for part in ds.values():  # type: ignore
                    for row in _iter_rows(part):
                        records.append(row)
    except Exception:
        # As a last resort, attempt to iterate ds directly
        try:
            for row in _iter_rows(ds):
                records.append(row)
        except Exception:
            pass

    return records


def _fit_all_groups() -&gt; None:
    &quot;&quot;&quot;
    Fit parameters per group and globally, storing them in module-level caches.
    Also writes/updates /app/explain.md with the discovered parameters if possible.
    &quot;&quot;&quot;
    global _PARAMS_BY_GROUP, _GLOBAL_PARAMS

    records = _load_all_records(&quot;/app/data&quot;)
    # Extract columns robustly
    def get_val(rec: dict, key: str, default=None):
        return rec.get(key, default)

    # Determine group field
    group_field_candidates = [&quot;group&quot;, &quot;sft_group&quot;, &quot;family&quot;, &quot;model_group&quot;]
    group_field = None
    if records:
        sample = records[0]
        for k in group_field_candidates:
            if k in sample:
                group_field = k
                break
    if group_field is None:
        group_field = &quot;group&quot;  # default name; treat all as one group

    # Partition data by group
    by_group: Dict[str, Tuple[List[float], List[float]]] = {}
    xs_all: List[float] = []
    ys_all: List[float] = []

    for rec in records:
        x = get_val(rec, &quot;sft_data_size&quot;)
        y = get_val(rec, &quot;sft_loss&quot;)
        g = get_val(rec, group_field, &quot;default&quot;)
        try:
            xf = float(x)
            yf = float(y)
        except Exception:
            continue
        if not (math.isfinite(xf) and math.isfinite(yf) and xf &gt; 0):
            continue

        xs_all.append(xf)
        ys_all.append(yf)
        if g not in by_group:
            by_group[g] = ([], [])
        by_group[g][0].append(xf)
        by_group[g][1].append(yf)

    # Global fit (pooled)
    if xs_all and ys_all:
        _GLOBAL_PARAMS = _fit_power_law_with_asymptote(xs_all, ys_all)
    else:
        # Keep default fallback
        _GLOBAL_PARAMS = _GLOBAL_PARAMS

    # Per-group fit
    params_by_group: Dict[str, Tuple[float, float, float]] = {}
    if by_group:
        for g, (xs, ys) in by_group.items():
            params_by_group[g] = _fit_power_law_with_asymptote(xs, ys)
    else:
        # No groups available; use a single default group
        params_by_group[&quot;default&quot;] = _GLOBAL_PARAMS

    _PARAMS_BY_GROUP = params_by_group

    # Attempt to write an explain file with discovered parameters
    try:
        lines: List[str] = []
        lines.append(&quot;# SFT Scaling Law\n&quot;)
        lines.append(&quot;We model the supervised fine-tuning loss as a function of the number of fine-tuning examples N using a three-parameter power law with an asymptote:\n&quot;)
        lines.append(&quot;L(N) = L_inf + A * N^(-alpha)\n&quot;)
        lines.append(&quot;\nMethodology:\n&quot;)
        lines.append(&quot;- For each group, we sweep a grid of candidate L_inf values below the minimum observed loss.\n&quot;)
        lines.append(&quot;- For each candidate L_inf, we fit log(L - L_inf) = log A - alpha * log N via linear least squares to estimate A and alpha.\n&quot;)
        lines.append(&quot;- We pick the parameters (L_inf, A, alpha) that minimize squared error in the original loss space.\n&quot;)
        lines.append(&quot;\nFitted parameters by group:\n&quot;)
        for g, (L_inf, A, alpha) in sorted(_PARAMS_BY_GROUP.items(), key=lambda kv: str(kv[0])):
            lines.append(f&quot;- {g}: L_inf={L_inf:.6g}, A={A:.6g}, alpha={alpha:.6g}\n&quot;)
        lines.append(&quot;\nGlobal pooled fit (used as fallback for unknown groups):\n&quot;)
        L_inf, A, alpha = _GLOBAL_PARAMS
        lines.append(f&quot;- GLOBAL: L_inf={L_inf:.6g}, A={A:.6g}, alpha={alpha:.6g}\n&quot;)

        with open(&quot;/app/explain.md&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(&quot;\n&quot;.join(lines))
    except Exception:
        # Non-fatal if we cannot write the explanation.
        pass


# Fit once at import time (best-effort; safe no-op if dataset unavailable)
_fit_all_groups()


def _params_for_group(group: str) -&gt; Tuple[float, float, float]:
    if group in _PARAMS_BY_GROUP:
        return _PARAMS_BY_GROUP[group]
    # Try case-insensitive key match
    group_lower = group.lower()
    for g in _PARAMS_BY_GROUP.keys():
        if str(g).lower() == group_lower:
            return _PARAMS_BY_GROUP[g]
    return _GLOBAL_PARAMS


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # Ensure parameters are available (import-time fit may have been skipped in some environments)
    if not _PARAMS_BY_GROUP:
        _fit_all_groups()

    L_inf, A, alpha = _params_for_group(group)

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&quot;sft_data_size&quot;, 0.0))
        if not (math.isfinite(n) and n &gt; 0):
            # Graceful handling for invalid N: predict using N=1 as minimal meaningful size
            n = 1.0
        y_hat = L_inf + A * (n ** (-alpha))
        # Safety: ensure finite
        if not math.isfinite(y_hat):
            y_hat = float(&quot;nan&quot;)
        outputs.append({&quot;sft_loss&quot;: float(y_hat)})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.887761 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">from __future__ import annotations

import math
from typing import Dict, Iterable, List, Tuple

# Optional, but provides fast array ops if available.
try:
    import numpy as np  # type: ignore
except Exception:  # pragma: no cover
    np = None  # type: ignore

# Optional dependency. We fail gracefully if not present or data is missing.
try:
    from datasets import load_from_disk  # type: ignore
except Exception:  # pragma: no cover
    load_from_disk = None  # type: ignore

DATA_PATH = &quot;/app/data&quot;

# Fitted parameters are stored here as:
#   PARAMS[group] = {&quot;L_inf&quot;: float, &quot;A&quot;: float, &quot;alpha&quot;: float}
PARAMS: Dict[str, Dict[str, float]] = {}


def _to_np(x: Iterable[float]):
    if np is None:
        # Minimal shim with list semantics when numpy is unavailable
        return list(float(v) for v in x)
    return np.asarray(list(float(v) for v in x), dtype=float)


def _linear_regression(x: Iterable[float], y: Iterable[float]) -&gt; Tuple[float, float]:
    &quot;&quot;&quot;
    Fit y = b + m * x by least squares.
    Returns (b, m).
    &quot;&quot;&quot;
    X = _to_np(x)
    Y = _to_np(y)
    if np is None:
        n = len(X)
        if n == 0:
            return 0.0, 0.0
        mean_x = sum(X) / n
        mean_y = sum(Y) / n
        num = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(X, Y))
        den = sum((xi - mean_x) ** 2 for xi in X)
        m = 0.0 if den == 0 else (num / den)
        b = mean_y - m * mean_x
        return b, m
    else:
        x_mean = float(np.mean(X)) if X.size else 0.0
        y_mean = float(np.mean(Y)) if Y.size else 0.0
        num = float(np.sum((X - x_mean) * (Y - y_mean)))
        den = float(np.sum((X - x_mean) ** 2))
        m = 0.0 if den == 0.0 else (num / den)
        b = y_mean - m * x_mean
        return b, m


def _sse_loss(N, L, L_inf: float, A: float, alpha: float) -&gt; float:
    # Compute mean squared error in original space for stability/interpretability
    if np is None:
        preds = [L_inf + A * (max(n, 1e-12) ** (-alpha)) for n in N]
        residuals = [(lp - lt) for lp, lt in zip(preds, L)]
        return sum(r * r for r in residuals) / (len(residuals) or 1)
    else:
        N = _to_np(N)
        L = _to_np(L)
        preds = L_inf + A * np.power(np.maximum(N, 1e-12), -alpha)
        residuals = preds - L
        return float(np.mean(residuals ** 2))


def _fit_group(N_raw: Iterable[float], L_raw: Iterable[float]) -&gt; Tuple[float, float, float]:
    &quot;&quot;&quot;
    Fit the scaling law:
        L(N) = L_inf + A * N^{-alpha}
    using a grid-search over L_inf and closed-form linear regression for (log A, alpha).
    &quot;&quot;&quot;
    # Clean data: positive N, finite values
    N = []
    L = []
    for n, l in zip(N_raw, L_raw):
        if n is None or l is None:
            continue
        try:
            n_val = float(n)
            l_val = float(l)
        except Exception:
            continue
        if not (math.isfinite(n_val) and math.isfinite(l_val)):
            continue
        if n_val &lt;= 0.0:
            continue
        N.append(n_val)
        L.append(l_val)

    if len(N) &lt; 2:
        # Fallback: insufficient data
        L_inf = min(L) if L else 0.0
        A = max((max(L) - L_inf), 1e-6) if L else 1.0
        alpha = 0.5
        return float(L_inf), float(A), float(alpha)

    # Establish a reasonable L_inf search range:
    L_min = min(L)
    L_max = max(L)
    if L_min &lt;= 0:
        low = L_min * 0.5
        high = min(L_min * 0.99, L_min - 1e-6)
    else:
        low = max(0.0, 0.1 * L_min)
        high = 0.99 * L_min

    # If range is degenerate, expand conservatively
    if not math.isfinite(low) or not math.isfinite(high) or low &gt;= high:
        low = max(0.0, L_min * 0.25)
        high = 0.99 * L_min if L_min &gt; 0 else (L_min * 0.9)

    # Build candidate grid for L_inf
    grid_count = 101
    if np is None:
        L_grid = [low + (high - low) * i / (grid_count - 1) for i in range(grid_count)]
    else:
        L_grid = list(np.linspace(low, high, grid_count))

    best = {
        &quot;sse&quot;: float(&quot;inf&quot;),
        &quot;L_inf&quot;: None,  # type: ignore
        &quot;A&quot;: None,      # type: ignore
        &quot;alpha&quot;: None,  # type: ignore
    }

    logN = [math.log(n) for n in N]

    for L0 in L_grid:
        # Ensure positivity of (L - L0)
        y = [l - L0 for l in L]
        if any(v &lt;= 0 for v in y):
            continue
        logy = [math.log(v) for v in y]
        b, m = _linear_regression(logN, logy)  # log(y) = b + m * logN
        # Here, m = -alpha and A = exp(b)
        alpha = -m
        if not math.isfinite(alpha) or alpha &lt;= 0:
            continue
        A = math.exp(b)
        if not math.isfinite(A) or A &lt;= 0:
            continue
        sse = _sse_loss(N, L, L0, A, alpha)
        if sse &lt; best[&quot;sse&quot;]:
            best.update({&quot;sse&quot;: sse, &quot;L_inf&quot;: L0, &quot;A&quot;: A, &quot;alpha&quot;: alpha})

    # If grid search failed (e.g., numerical issues), fallback
    if best[&quot;L_inf&quot;] is None:
        L_inf = max(0.0, 0.5 * L_min)
        A = max(L_max - L_inf, 1e-6)
        alpha = 0.5
        return float(L_inf), float(A), float(alpha)

    # Optional local refinement around the best L_inf
    L0 = float(best[&quot;L_inf&quot;])
    span = max(1e-12, 0.1 * abs(L0) + 1e-6)
    candidates = [L0 + d for d in ( -span, -span/2, 0.0, span/2, span )]
    for Lc in candidates:
        y = [l - Lc for l in L]
        if any(v &lt;= 0 for v in y):
            continue
        logy = [math.log(v) for v in y]
        b, m = _linear_regression(logN, logy)
        alpha = -m
        if not math.isfinite(alpha) or alpha &lt;= 0:
            continue
        A = math.exp(b)
        if not math.isfinite(A) or A &lt;= 0:
            continue
        sse = _sse_loss(N, L, Lc, A, alpha)
        if sse &lt; best[&quot;sse&quot;]:
            best.update({&quot;sse&quot;: sse, &quot;L_inf&quot;: Lc, &quot;A&quot;: A, &quot;alpha&quot;: alpha})

    return float(best[&quot;L_inf&quot;]), float(best[&quot;A&quot;]), float(best[&quot;alpha&quot;])


def _load_all_rows_from_disk(path: str):
    if load_from_disk is None:
        return []

    try:
        ds = load_from_disk(path)
    except Exception:
        return []

    rows = []
    try:
        # DatasetDict (multiple splits)
        values = getattr(ds, &quot;values&quot;, None)
        if callable(values):
            for split in ds.values():  # type: ignore[attr-defined]
                for ex in split:
                    rows.append(ex)
        else:
            # Single Dataset
            for ex in ds:
                rows.append(ex)
    except Exception:
        # As a very last resort, try iterating directly
        try:
            for ex in ds:
                rows.append(ex)
        except Exception:
            return []

    return rows


def _detect_group_key(example: dict) -&gt; str | None:
    candidates = (
        &quot;group&quot;,
        &quot;sft_group&quot;,
        &quot;exp_group&quot;,
        &quot;setting&quot;,
        &quot;task&quot;,
        &quot;model&quot;,
    )
    for k in candidates:
        if k in example:
            return k
    return None


def _detect_size_key(example: dict) -&gt; str | None:
    candidates = (
        &quot;sft_data_size&quot;,
        &quot;data_size&quot;,
        &quot;n&quot;,
        &quot;N&quot;,
        &quot;examples&quot;,
        &quot;num_examples&quot;,
        &quot;train_examples&quot;,
    )
    for k in candidates:
        if k in example:
            return k
    return None


def _detect_loss_key(example: dict) -&gt; str | None:
    candidates = (
        &quot;sft_loss&quot;,
        &quot;loss&quot;,
        &quot;final_loss&quot;,
    )
    for k in candidates:
        if k in example:
            return k
    return None


def _fit_params_from_data() -&gt; Dict[str, Dict[str, float]]:
    rows = _load_all_rows_from_disk(DATA_PATH)
    if not rows:
        # Fallback defaults if no data
        return {&quot;GLOBAL&quot;: {&quot;L_inf&quot;: 0.0, &quot;A&quot;: 1.0, &quot;alpha&quot;: 0.5}}

    # Detect key names
    g_key = _detect_group_key(rows[0]) or &quot;group&quot;
    n_key = _detect_size_key(rows[0]) or &quot;sft_data_size&quot;
    l_key = _detect_loss_key(rows[0]) or &quot;sft_loss&quot;

    groups: Dict[str, Tuple[List[float], List[float]]] = {}
    allN: List[float] = []
    allL: List[float] = []

    for ex in rows:
        if n_key not in ex or l_key not in ex:
            continue
        g = str(ex.get(g_key, &quot;GLOBAL&quot;))
        try:
            n = float(ex[n_key])
            l = float(ex[l_key])
        except Exception:
            continue
        if not (math.isfinite(n) and math.isfinite(l)) or n &lt;= 0:
            continue
        allN.append(n)
        allL.append(l)
        if g not in groups:
            groups[g] = ([], [])
        groups[g][0].append(n)
        groups[g][1].append(l)

    params: Dict[str, Dict[str, float]] = {}

    # Global fit (useful fallback)
    L_inf_g, A_g, alpha_g = _fit_group(allN, allL)
    params[&quot;GLOBAL&quot;] = {&quot;L_inf&quot;: L_inf_g, &quot;A&quot;: A_g, &quot;alpha&quot;: alpha_g}

    # Per-group fits
    for g, (Ns, Ls) in groups.items():
        L_inf, A, alpha = _fit_group(Ns, Ls)
        params[g] = {&quot;L_inf&quot;: L_inf, &quot;A&quot;: A, &quot;alpha&quot;: alpha}

    return params


# Eagerly fit at import time for reproducibility and speed at inference.
try:
    PARAMS = _fit_params_from_data()
except Exception:
    # Robust fallback
    PARAMS = {&quot;GLOBAL&quot;: {&quot;L_inf&quot;: 0.0, &quot;A&quot;: 1.0, &quot;alpha&quot;: 0.5}}


def _get_param_set(group: str) -&gt; Dict[str, float]:
    # Exact group match
    if group in PARAMS:
        return PARAMS[group]
    # Case-insensitive match
    for g in PARAMS:
        if g.lower() == group.lower():
            return PARAMS[g]
    # Fallback to GLOBAL
    return PARAMS.get(&quot;GLOBAL&quot;, {&quot;L_inf&quot;: 0.0, &quot;A&quot;: 1.0, &quot;alpha&quot;: 0.5})


def _extract_size_from_input(d: dict) -&gt; float:
    # Support several synonymous keys for convenience.
    for k in (&quot;sft_data_size&quot;, &quot;data_size&quot;, &quot;n&quot;, &quot;N&quot;, &quot;examples&quot;, &quot;num_examples&quot;, &quot;train_examples&quot;):
        if k in d:
            try:
                v = float(d[k])
                if math.isfinite(v) and v &gt; 0:
                    return v
            except Exception:
                continue
    # If missing/invalid, use a tiny positive to avoid division-by-zero
    return 1e-12


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).

    Scaling law implemented:
        sft_loss(N) = L_inf + A * N^(-alpha)
    &quot;&quot;&quot;
    params = _get_param_set(group)
    L_inf = float(params.get(&quot;L_inf&quot;, 0.0))
    A = float(params.get(&quot;A&quot;, 1.0))
    alpha = float(params.get(&quot;alpha&quot;, 0.5))

    outputs: List[dict[str, float]] = []
    for row in input_data:
        N = _extract_size_from_input(row)
        # Numerical guard
        N = max(float(N), 1e-12)
        pred = float(L_inf + A * (N ** (-alpha)))
        outputs.append({&quot;sft_loss&quot;: pred})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.875399 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">from __future__ import annotations

import math
import os
from typing import Dict, List, Tuple

# We use numpy if available for robust fitting; otherwise fall back to simple heuristics.
try:
    import numpy as np
except Exception:  # pragma: no cover
    np = None  # type: ignore

_PARAMS: Dict[str, Tuple[float, float, float]] = {}
_GLOBAL_PARAMS: Tuple[float, float, float] | None = None
_FITTED: bool = False


def _safe_log(x: float) -&gt; float:
    return math.log(max(x, 1e-12))


def _fit_floor_powerlaw(xs: List[float], ys: List[float]) -&gt; Tuple[float, float, float]:
    &quot;&quot;&quot;
    Fit L(N) = L_inf + A * N^{-alpha} using a grid search over L_inf and
    linear regression in log-log space for (A, alpha).
    Returns (L_inf, A, alpha). All parameters are constrained to be &gt; 0.
    &quot;&quot;&quot;
    # Basic sanitation
    data = [(float(x), float(y)) for x, y in zip(xs, ys) if x is not None and y is not None]
    data = [(x, y) for x, y in data if x &gt; 0 and y &gt; 0 and not (math.isinf(x) or math.isinf(y) or math.isnan(x) or math.isnan(y))]
    if len(data) == 0:
        # Degenerate fallback
        return (0.01, 1.0, 0.3)
    xs = [x for x, _ in data]
    ys = [y for _, y in data]

    if np is None or len(data) &lt; 2:  # Not enough data or numpy missing
        # Heuristic: pick a floor slightly below min loss, alpha=0.3, set A to match median
        y_min = min(ys)
        y_max = max(ys)
        L0 = max(1e-8, y_min - 0.1 * max(y_max - y_min, 1e-6))
        alpha = 0.3
        vals = []
        for x, y in data:
            vals.append(max(1e-12, (y - L0) * (x ** alpha)))
        A = sorted(vals)[len(vals) // 2] if vals else 1.0
        return (float(L0), float(max(1e-12, A)), float(max(1e-6, alpha)))

    x_arr = np.array(xs, dtype=float)
    y_arr = np.array(ys, dtype=float)

    y_min = float(np.min(y_arr))
    y_max = float(np.max(y_arr))
    spread = max(y_max - y_min, 1e-6)

    upper = y_min - 1e-9  # L_inf must be strictly below the minimum observed loss
    lower = max(1e-12, y_min - 0.8 * spread)
    if lower &gt;= upper:
        lower = max(1e-12, y_min * 0.5)

    grid = np.linspace(lower, upper, num=60)

    best = None  # (score, L0, A, alpha)
    xlog = np.log(np.maximum(x_arr, 1.0))

    for L0 in grid:
        # Ensure positive differences
        diff = y_arr - L0
        mask = diff &gt; 1e-12
        if not np.any(mask):
            continue

        ylog = np.log(diff[mask])
        xlog_masked = xlog[mask]
        if ylog.size &lt; 2:
            continue

        # Linear regression in log space: ylog = b + m * xlog; alpha = -m; A = exp(b)
        m, b = np.polyfit(xlog_masked, ylog, deg=1)
        alpha = -float(m)
        A = float(np.exp(b))
        if alpha &lt;= 0 or not np.isfinite(alpha) or A &lt;= 0 or not np.isfinite(A):
            continue

        y_pred = L0 + A * np.power(np.maximum(x_arr, 1.0), -alpha)

        # Use log-space MSE for relative fit robustness
        score = float(np.mean((np.log(np.maximum(y_arr, 1e-12)) - np.log(np.maximum(y_pred, 1e-12))) ** 2))
        if not math.isfinite(score):
            continue

        if best is None or score &lt; best[0]:
            best = (score, float(L0), float(A), float(alpha))

    if best is not None:
        _, L0, A, alpha = best
        return (L0, A, alpha)

    # Fallback: L0 = 0, straight log-log fit
    m, b = np.polyfit(xlog, np.log(np.maximum(y_arr, 1e-12)), deg=1)
    alpha = -float(m)
    A = float(np.exp(b))
    L0 = 1e-8
    alpha = float(max(alpha, 1e-6))
    A = float(max(A, 1e-12))
    return (L0, A, alpha)


def _collect_rows_from_hf_dataset() -&gt; List[dict]:
    &quot;&quot;&quot;
    Load the dataset from /app/data via datasets.load_from_disk.
    Returns a flat list of dicts across all splits if present.
    &quot;&quot;&quot;
    try:
        from datasets import load_from_disk
    except Exception:
        return []

    path = &quot;/app/data&quot;
    if not os.path.exists(path):
        return []

    try:
        ds = load_from_disk(path)
    except Exception:
        return []

    rows: List[dict] = []
    try:
        # DatasetDict
        if hasattr(ds, &quot;keys&quot;) and hasattr(ds, &quot;__getitem__&quot;) and not hasattr(ds, &quot;column_names&quot;):
            for split in ds.keys():
                part = ds[split]
                for ex in part:
                    rows.append(dict(ex))
        else:
            # Single Dataset
            for ex in ds:
                rows.append(dict(ex))
    except Exception:
        return []
    return rows


def _fit_all() -&gt; None:
    global _PARAMS, _GLOBAL_PARAMS, _FITTED

    if _FITTED:
        return

    rows = _collect_rows_from_hf_dataset()

    size_key = &quot;sft_data_size&quot;
    loss_key = &quot;sft_loss&quot;
    group_key = &quot;group&quot;

    # Filter and group rows
    by_group: Dict[str, List[Tuple[float, float]]] = {}
    all_pairs: List[Tuple[float, float]] = []

    for r in rows:
        if size_key not in r or loss_key not in r:
            continue
        try:
            n = float(r[size_key])
            l = float(r[loss_key])
        except Exception:
            continue
        if not (n &gt; 0 and l &gt; 0 and math.isfinite(n) and math.isfinite(l)):
            continue

        g = str(r.get(group_key, &quot;default&quot;))
        by_group.setdefault(g, []).append((n, l))
        all_pairs.append((n, l))

    # If no data was found, install a safe default
    if not all_pairs:
        _GLOBAL_PARAMS = (0.01, 1.0, 0.3)
        _PARAMS = {}
        _FITTED = True
        _write_report(_PARAMS, _GLOBAL_PARAMS, rows_present=False)
        return

    # Global fit
    xs_all = [x for x, _ in all_pairs]
    ys_all = [y for _, y in all_pairs]
    global_params = _fit_floor_powerlaw(xs_all, ys_all)
    _GLOBAL_PARAMS = global_params

    # Per-group fit with fallbacks
    params: Dict[str, Tuple[float, float, float]] = {}
    for g, pairs in by_group.items():
        xs = [x for x, _ in pairs]
        ys = [y for _, y in pairs]
        if len(pairs) &gt;= 3:
            params[g] = _fit_floor_powerlaw(xs, ys)
        else:
            # Use global alpha and L_inf, solve A by robust statistic
            L0_g, A_g, alpha_g = global_params
            if len(pairs) &gt;= 1:
                transformed = []
                for x, y in pairs:
                    transformed.append(max(1e-12, (y - L0_g) * (x ** alpha_g)))
                transformed.sort()
                A_est = transformed[len(transformed) // 2] if transformed else A_g
                params[g] = (L0_g, float(A_est), alpha_g)
            else:
                params[g] = global_params

    _PARAMS = params
    _FITTED = True
    _write_report(_PARAMS, _GLOBAL_PARAMS, rows_present=True)


def _write_report(params: Dict[str, Tuple[float, float, float]],
                  global_params: Tuple[float, float, float] | None,
                  rows_present: bool) -&gt; None:
    &quot;&quot;&quot;
    Write a human-readable explanation and the fitted parameters to /app/explain.md.
    &quot;&quot;&quot;
    try:
        lines: List[str] = []
        lines.append(&quot;# Discovered SFT scaling law&quot;)
        lines.append(&quot;&quot;)
        lines.append(&quot;We model the final SFT loss as a function of the number of SFT examples N with a saturating power law:&quot;)
        lines.append(&quot;&quot;)
        lines.append(&quot;    L_hat(N) = L_inf + A * N^{-alpha}&quot;)
        lines.append(&quot;&quot;)
        lines.append(&quot;- L_inf (asymptotic loss floor) &gt;= 0&quot;)
        lines.append(&quot;- A &gt; 0 is the scale factor at N = 1&quot;)
        lines.append(&quot;- alpha &gt; 0 controls how quickly the loss decreases with more data&quot;)
        lines.append(&quot;&quot;)
        lines.append(&quot;Fitting procedure:&quot;)
        lines.append(&quot;- For each group, we search over candidate values of L_inf strictly below the minimum observed loss for that group.&quot;)
        lines.append(&quot;- For each candidate L_inf, we fit log(L - L_inf) = log A - alpha * log N by linear regression in log-log space.&quot;)
        lines.append(&quot;- We select the parameters that minimize the mean squared error in log space, which emphasizes relative error.&quot;)
        lines.append(&quot;&quot;)
        if global_params is not None:
            L0, A, alpha = global_params
            lines.append(&quot;Global fit across all groups (used as a fallback):&quot;)
            lines.append(f&quot;- L_inf = {L0:.6g}, A = {A:.6g}, alpha = {alpha:.6g}&quot;)
            lines.append(&quot;&quot;)
        if not params:
            if rows_present:
                lines.append(&quot;No distinct groups were found in the dataset; the global fit above is used for all predictions.&quot;)
            else:
                lines.append(&quot;Dataset was not available at runtime; default heuristic parameters are used.&quot;)
        else:
            lines.append(&quot;Per-group fitted parameters:&quot;)
            lines.append(&quot;&quot;)
            lines.append(&quot;| group | L_inf | A | alpha |&quot;)
            lines.append(&quot;|---|---:|---:|---:|&quot;)
            for g in sorted(params.keys()):
                L0, A, alpha = params[g]
                lines.append(f&quot;| {g} | {L0:.6g} | {A:.6g} | {alpha:.6g} |&quot;)
            lines.append(&quot;&quot;)
        lines.append(&quot;This functional form is shared across groups; only the constants (L_inf, A, alpha) differ by group.&quot;)

        with open(&quot;/app/explain.md&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(&quot;\n&quot;.join(lines) + &quot;\n&quot;)
    except Exception:
        # Best-effort only; silently ignore I/O errors to keep prediction path reliable.
        pass


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    global _FITTED
    if not _FITTED:
        _fit_all()

    params = _PARAMS.get(group, _GLOBAL_PARAMS if _GLOBAL_PARAMS is not None else (0.01, 1.0, 0.3))
    L_inf, A, alpha = params

    outputs: list[dict[str, float]] = []
    for point in input_data:
        n = float(point.get(&quot;sft_data_size&quot;, 0.0))
        if not (n &gt; 0 and math.isfinite(n)):
            # Graceful handling of nonpositive sizes: treat as N=1
            n = 1.0
        y_hat = L_inf + A * (n ** (-alpha))
        # Numerical safety
        y_hat = float(max(1e-12, y_hat))
        outputs.append({&quot;sft_loss&quot;: y_hat})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -1.000000 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations

import math
from dataclasses import dataclass
from typing import Dict, Iterable, List, Tuple, Optional

# Public API: law(input_data, group) -&gt; predictions
# We discover and cache per-group parameters at import time by fitting
# a three-parameter power law with offset to the dataset in /app/data:
#     sft_loss(N) = L_inf[group] + A[group] * N ** (-alpha[group])
#
# The functional form is identical for all groups; only the constants differ.


@dataclass(frozen=True)
class Params:
    L_inf: float
    A: float
    alpha: float


DATA_PATH = &quot;/app/data&quot;
# Cache of fitted parameters per group name
PARAMS: Dict[str, Params] = {}
# Fallback/global params if a specific group isn&#x27;t found
GLOBAL_KEY = &quot;__ALL__&quot;


def _safe_log(arr: Iterable[float]) -&gt; List[float]:
    eps = 1e-12
    return [math.log(max(eps, float(x))) for x in arr]


def _polyfit_loglog_with_offset(
    Ns: List[float],
    losses: List[float],
    n_grid: int = 128,
) -&gt; Optional[Params]:
    &quot;&quot;&quot;
    Fit y = L_inf + A * N^(-alpha) by:
      1) Grid search over L_inf in [0, min(loss)*(1-1e-6)]
      2) For each L_inf, linear regression in log space on (log N, log(y - L_inf))
      3) Choose L_inf minimizing squared error in original space
    Returns Params or None if fitting is not feasible.
    &quot;&quot;&quot;
    if len(Ns) &lt; 2:
        return None

    # Clean and ensure strictly positive Ns and finite y
    pairs: List[Tuple[float, float]] = []
    for n, y in zip(Ns, losses):
        if n is None or y is None:
            continue
        try:
            nf = float(n)
            yf = float(y)
        except (TypeError, ValueError):
            continue
        if not (math.isfinite(nf) and math.isfinite(yf)):
            continue
        if nf &lt;= 0:
            continue
        pairs.append((nf, yf))

    if len(pairs) &lt; 2:
        return None

    Ns_clean = [p[0] for p in pairs]
    ys_clean = [p[1] for p in pairs]

    y_min = min(ys_clean)
    if not math.isfinite(y_min):
        return None

    # Define L_inf search range; keep strictly below y_min to ensure log(y - L_inf) &gt; 0
    upper = y_min * (1.0 - 1e-6) if y_min &gt; 0 else y_min - 1e-6
    lower = 0.0
    if upper &lt;= lower:
        # Degenerate range; fall back to small positive fraction of min
        upper = y_min * 0.9 if y_min &gt; 0 else -1e-6
        lower = 0.0

    best_params: Optional[Params] = None
    best_sse: float = float(&quot;inf&quot;)

    # Precompute log N
    logN = _safe_log(Ns_clean)

    # Grid search over L_inf
    for i in range(n_grid):
        L_inf = lower + (upper - lower) * (i + 0.5) / n_grid  # midpoints
        # Prepare dependent variable: z = log(y - L_inf)
        resid = [y - L_inf for y in ys_clean]
        # Ensure positivity
        if any(r &lt;= 0 for r in resid):
            continue
        log_resid = _safe_log(resid)

        # Simple least squares fit of z = c + m * logN
        # Compute slope (m) and intercept (c)
        # Use numerically stable formulas for simple linear regression
        n = float(len(logN))
        sum_x = sum(logN)
        sum_y = sum(log_resid)
        sum_xx = sum(x * x for x in logN)
        sum_xy = sum(x * y for x, y in zip(logN, log_resid))

        denom = n * sum_xx - sum_x * sum_x
        if denom == 0:
            continue
        m = (n * sum_xy - sum_x * sum_y) / denom
        c = (sum_y - m * sum_x) / n

        alpha = -m
        # Enforce a reasonable range for alpha to avoid pathological fits
        if not math.isfinite(alpha) or alpha &lt; 0:
            continue
        # Recover A
        A = math.exp(c)
        if not (math.isfinite(A) and A &gt; 0):
            continue

        # Evaluate SSE in original space
        sse = 0.0
        for N, y in zip(Ns_clean, ys_clean):
            y_hat = L_inf + A * (N ** (-alpha))
            diff = y_hat - y
            sse += diff * diff

        if sse &lt; best_sse:
            best_sse = sse
            best_params = Params(L_inf=L_inf, A=A, alpha=alpha)

    return best_params


def _load_dataset_records() -&gt; List[dict]:
    &quot;&quot;&quot;
    Load dataset from disk and return a list of records (dicts).
    &quot;&quot;&quot;
    try:
        from datasets import load_from_disk
    except Exception:
        return []

    try:
        ds = load_from_disk(DATA_PATH)
    except Exception:
        return []

    # Handle DatasetDict vs Dataset
    try:
        # DatasetDict: pick &#x27;train&#x27; if present else first split
        if hasattr(ds, &quot;keys&quot;) and callable(getattr(ds, &quot;keys&quot;, None)):
            split = None
            if &quot;train&quot; in ds:
                split = ds[&quot;train&quot;]
            else:
                # First available split
                for k in ds.keys():
                    split = ds[k]
                    break
            dataset = split
        else:
            dataset = ds
    except Exception:
        dataset = ds

    records: List[dict] = []
    try:
        # Iterate rows as dicts; HF datasets are iterable
        for row in dataset:
            if isinstance(row, dict):
                records.append(row)
    except Exception:
        # Fallback: try to convert columns to rows
        try:
            cols = {k: list(v) for k, v in dataset.items()}  # type: ignore[attr-defined]
            n = min(len(v) for v in cols.values())
            for i in range(n):
                rec = {k: cols[k][i] for k in cols}
                records.append(rec)
        except Exception:
            pass

    return records


def _discover_group_key(records: List[dict]) -&gt; Optional[str]:
    if not records:
        return None
    candidates = [
        &quot;group&quot;,
        &quot;grp&quot;,
        &quot;setting&quot;,
        &quot;experiment_group&quot;,
        &quot;exp_group&quot;,
        &quot;arch&quot;,
        &quot;model_group&quot;,
    ]
    first = records[0]
    for k in candidates:
        if k in first:
            return k
    # If a &quot;group&quot; isn&#x27;t present, treat all as one group
    return None


def _fit_params_from_data() -&gt; Dict[str, Params]:
    records = _load_dataset_records()
    if not records:
        return {}

    group_key = _discover_group_key(records)

    # Build per-group arrays
    per_group: Dict[str, Tuple[List[float], List[float]]] = {}
    for r in records:
        if &quot;sft_data_size&quot; not in r or &quot;sft_loss&quot; not in r:
            continue
        try:
            N = float(r[&quot;sft_data_size&quot;])
            y = float(r[&quot;sft_loss&quot;])
        except Exception:
            continue
        if not (math.isfinite(N) and math.isfinite(y)):
            continue
        g = str(r[group_key]) if group_key and (group_key in r) else GLOBAL_KEY
        per_group.setdefault(g, ([], []))
        per_group[g][0].append(N)
        per_group[g][1].append(y)

    params: Dict[str, Params] = {}
    # First fit per specific group
    for g, (Ns, ys) in per_group.items():
        fitted = _polyfit_loglog_with_offset(Ns, ys)
        if fitted is not None:
            params[g] = fitted

    # Also fit a global model across all data for fallback
    all_Ns: List[float] = []
    all_ys: List[float] = []
    for (Ns, ys) in per_group.values():
        all_Ns.extend(Ns)
        all_ys.extend(ys)
    global_fit = _polyfit_loglog_with_offset(all_Ns, all_ys)
    if global_fit is not None:
        params[GLOBAL_KEY] = global_fit

    return params


# Fit on import (best-effort). If it fails, PARAMS remains empty and we use a heuristic fallback.
try:
    PARAMS = _fit_params_from_data()
except Exception:
    PARAMS = {}


def _get_params_for_group(group: str) -&gt; Params:
    # Exact group
    if group in PARAMS:
        return PARAMS[group]
    # Fallback to global fit
    if GLOBAL_KEY in PARAMS:
        return PARAMS[GLOBAL_KEY]
    # Heuristic safety net
    return Params(L_inf=0.0, A=1.0, alpha=0.3)


def _predict_loss(N: float, p: Params) -&gt; float:
    N = float(N)
    if N &lt;= 0 or not math.isfinite(N):
        return float(&quot;nan&quot;)
    return float(p.L_inf + p.A * (N ** (-p.alpha)))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Must include &#x27;sft_data_size&#x27;.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups, but the
               constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) under key &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    params = _get_params_for_group(str(group))

    preds: List[dict] = []
    for row in input_data:
        if &quot;sft_data_size&quot; not in row:
            raise KeyError(&quot;Each input row must contain &#x27;sft_data_size&#x27;.&quot;)
        N = float(row[&quot;sft_data_size&quot;])
        y_hat = _predict_loss(N, params)
        preds.append({&quot;sft_loss&quot;: y_hat})
    return preds</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -1.000000 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">from __future__ import annotations

import json
import math
import os
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional

# Numpy is widely available and sufficient for our small optimization routine.
import numpy as np

# We load the dataset to fit group-specific parameters.
try:
    from datasets import load_from_disk, Dataset, DatasetDict
except Exception:  # pragma: no cover - graceful fallback if datasets isn&#x27;t available
    load_from_disk = None
    Dataset = None
    DatasetDict = None


DATA_PATH = &quot;/app/data&quot;
PARAMS_PATH = &quot;/app/params.json&quot;
EXPLAIN_PATH = &quot;/app/explain.md&quot;


@dataclass(frozen=True)
class Params:
    L_inf: float
    A: float
    alpha: float

    def predict(self, n: float) -&gt; float:
        n = float(n)
        if not math.isfinite(n) or n &lt;= 0:
            # Extremely conservative fallback for non-positive/invalid n
            n = 1.0
        # L(N) = L_inf + A * N^{-alpha}
        return float(self.L_inf + self.A * (n ** (-self.alpha)))


# Module-level cache of learned parameters per group
_PARAMS: Dict[str, Params] = {}
_FITTED: bool = False


def _safe_float_array(x: Iterable) -&gt; np.ndarray:
    arr = np.asarray(list(x), dtype=float)
    # Filter out non-finite entries
    return arr[np.isfinite(arr)]


def _load_all_rows():
    if load_from_disk is None:
        return None

    try:
        ds = load_from_disk(DATA_PATH)
    except Exception:
        return None

    # Concatenate splits if a DatasetDict, else take the Dataset as-is
    try:
        from datasets import concatenate_datasets  # type: ignore
    except Exception:
        concatenate_datasets = None

    if DatasetDict is not None and isinstance(ds, DatasetDict):
        if concatenate_datasets is None:
            # Fallback: just use the first split if we cannot concatenate
            first_split = next(iter(ds.values()))
            return first_split
        return concatenate_datasets(list(ds.values()))
    return ds


def _find_group_column(column_names: List[str]) -&gt; Optional[str]:
    candidates = [
        &quot;group&quot;,
        &quot;exp_group&quot;,
        &quot;Group&quot;,
        &quot;model&quot;,
        &quot;arch&quot;,
        &quot;task&quot;,
        &quot;family&quot;,
        &quot;variant&quot;,
        &quot;experiment&quot;,
    ]
    lowercase = {c.lower(): c for c in column_names}
    for name in candidates:
        if name in column_names:
            return name
        if name in lowercase:
            return lowercase[name]
    return None


def _fit_power_law(N: np.ndarray, Y: np.ndarray) -&gt; Optional[Params]:
    # Clean and validate
    mask = np.isfinite(N) &amp; np.isfinite(Y) &amp; (N &gt; 0) &amp; (Y &gt; 0)
    N = N[mask]
    Y = Y[mask]
    if N.size &lt; 3:
        return None

    y_min = float(np.min(Y))
    # Guardrails: L_inf must be less than min observed loss
    # Create a grid for L_inf and fit linear model in log-space for each.
    eps = max(1e-12, 1e-6 * y_min)
    low = max(eps, 0.25 * y_min)
    high = 0.99 * y_min
    if not math.isfinite(low) or not math.isfinite(high) or low &gt;= high:
        # Degenerate case: fall back to L_inf = 0
        low, high = 0.0, 0.0

    # Include 0 in the grid to allow pure power law fits if appropriate
    l_grid = np.unique(np.concatenate([np.linspace(low, high, 60), np.array([0.0])]))

    best = None  # (mse, L_inf, A, alpha)
    x_log = np.log(N)

    for L_inf in l_grid:
        z = Y - L_inf
        if np.any(z &lt;= 0):
            # invalid for log-space; skip
            continue
        t = np.log(z)

        # OLS: t = c0 + c1 * x_log  =&gt; log(z) = log(A) - alpha * log(N)
        # So: alpha = -c1,  A = exp(c0)
        try:
            c1, c0 = np.polyfit(x_log, t, 1)  # returns slope, intercept
        except Exception:
            continue

        alpha = -float(c1)
        A = float(np.exp(c0))

        if not (math.isfinite(alpha) and math.isfinite(A)) or A &lt;= 0:
            continue
        if alpha &lt;= 0 or alpha &gt; 5.0:
            # Reasonable prior: alpha in (0, 5]
            continue

        y_hat = L_inf + A * (N ** (-alpha))
        mse = float(np.mean((Y - y_hat) ** 2))
        cand = (mse, L_inf, A, alpha)
        if best is None or mse &lt; best[0]:
            best = cand

    # If grid search failed, try pure log-log (L_inf = 0)
    if best is None:
        try:
            t = np.log(Y)
            c1, c0 = np.polyfit(x_log, t, 1)
            alpha = -float(c1)
            A = float(np.exp(c0))
            if alpha &gt; 0 and A &gt; 0 and math.isfinite(alpha) and math.isfinite(A):
                best = (float(np.mean((Y - (A * (N ** (-alpha)))) ** 2)), 0.0, A, alpha)
        except Exception:
            pass

    if best is None:
        return None

    _, L_inf, A, alpha = best

    # Small local refinement around L_inf with narrower grid
    # This keeps the method SciPy-free while improving L_inf.
    rng = max(1e-3 * y_min, abs(0.1 * (high - low)))
    if rng &gt; 0:
        l_ref_grid = np.linspace(max(0.0, L_inf - rng), max(0.0, L_inf + rng), 21)
        best_ref = None
        for L_inf_ref in l_ref_grid:
            z = Y - L_inf_ref
            if np.any(z &lt;= 0):
                continue
            t = np.log(z)
            try:
                c1, c0 = np.polyfit(x_log, t, 1)
            except Exception:
                continue
            alpha_ref = -float(c1)
            A_ref = float(np.exp(c0))
            if alpha_ref &lt;= 0 or not (math.isfinite(alpha_ref) and math.isfinite(A_ref)):
                continue
            y_hat = L_inf_ref + A_ref * (N ** (-alpha_ref))
            mse = float(np.mean((Y - y_hat) ** 2))
            cand = (mse, L_inf_ref, A_ref, alpha_ref)
            if best_ref is None or mse &lt; best_ref[0]:
                best_ref = cand
        if best_ref is not None and best_ref[0] &lt;= best[0]:
            _, L_inf, A, alpha = best_ref

    return Params(L_inf=float(L_inf), A=float(A), alpha=float(alpha))


def _fit_params_per_group() -&gt; Dict[str, Params]:
    # If a cached params file exists, load it to avoid re-fitting.
    if os.path.exists(PARAMS_PATH):
        try:
            with open(PARAMS_PATH, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                raw = json.load(f)
            loaded: Dict[str, Params] = {
                g: Params(**vals) for g, vals in raw.items()
            }
            if loaded:
                return loaded
        except Exception:
            pass  # fall through to refit

    ds_all = _load_all_rows()
    if ds_all is None:
        # Fallback defaults if dataset is unavailable at runtime
        return {&quot;default&quot;: Params(L_inf=0.5, A=0.5, alpha=0.5)}

    colnames = list(ds_all.column_names)
    # Required columns
    if &quot;sft_data_size&quot; not in colnames or &quot;sft_loss&quot; not in colnames:
        # Try a couple of common aliases before giving up
        name_map = {
            &quot;sft_data_size&quot;: next((c for c in colnames if c.lower() in {&quot;n&quot;, &quot;size&quot;, &quot;num_examples&quot;, &quot;data_size&quot;}), None),
            &quot;sft_loss&quot;: next((c for c in colnames if c.lower() in {&quot;loss&quot;, &quot;final_loss&quot;}), None),
        }
        size_col = name_map[&quot;sft_data_size&quot;] or &quot;sft_data_size&quot;
        loss_col = name_map[&quot;sft_loss&quot;] or &quot;sft_loss&quot;
    else:
        size_col, loss_col = &quot;sft_data_size&quot;, &quot;sft_loss&quot;

    group_col = _find_group_column(colnames)

    # Build per-group arrays
    by_group: Dict[str, Dict[str, List[float]]] = {}
    for row in ds_all:
        try:
            n = float(row[size_col])
            y = float(row[loss_col])
        except Exception:
            continue
        g = str(row[group_col]) if group_col and group_col in row else &quot;default&quot;
        d = by_group.setdefault(g, {&quot;N&quot;: [], &quot;Y&quot;: []})
        d[&quot;N&quot;].append(n)
        d[&quot;Y&quot;].append(y)

    params: Dict[str, Params] = {}
    # Fit each group
    for g, data in by_group.items():
        N = _safe_float_array(data[&quot;N&quot;])
        Y = _safe_float_array(data[&quot;Y&quot;])
        p = _fit_power_law(N, Y)
        if p is None:
            p = Params(L_inf=float(np.min(Y)) * 0.5 if Y.size else 0.5, A=0.5, alpha=0.5)
        params[g] = p

    # Persist for reuse
    try:
        with open(PARAMS_PATH, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            json.dump({g: vars(p) for g, p in params.items()}, f, indent=2, sort_keys=True)
    except Exception:
        pass

    # Update explain.md with the fitted values
    try:
        _write_explain(params)
    except Exception:
        pass

    return params


def _write_explain(params: Dict[str, Params]) -&gt; None:
    lines = []
    lines.append(&quot;# Scaling law for SFT loss vs. dataset size&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;We model the final SFT loss as a simple, data-efficient power law with an asymptote:&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;L(N) = L_inf + A * N^{-alpha}&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;Where:&quot;)
    lines.append(&quot;- N is the SFT dataset size (number of examples).&quot;)
    lines.append(&quot;- L_inf is the irreducible loss as N → ∞.&quot;)
    lines.append(&quot;- A &gt; 0 scales the improvement from finite data.&quot;)
    lines.append(&quot;- alpha &gt; 0 is the data-scaling exponent.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;Fitting methodology:&quot;)
    lines.append(&quot;- Load all rows from /app/data (concatenating splits if present).&quot;)
    lines.append(&quot;- Group by the experimental group column (auto-detected; defaults to a single &#x27;default&#x27; group if missing).&quot;)
    lines.append(&quot;- For each group, grid-search L_inf (including 0), and for each candidate fit log(Y - L_inf) ~ log(N) by OLS to obtain A and alpha.&quot;)
    lines.append(&quot;- Select the candidate minimizing MSE in the original loss space, then locally refine L_inf with a short 1D search.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;This functional form is widely used for learning curves and extrapolates well: as N grows, the loss declines as a power law towards L_inf.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Fitted parameter values per group&quot;)
    lines.append(&quot;&quot;)
    if not params:
        lines.append(&quot;_No parameters fitted yet. Importing `law` will trigger fitting automatically._&quot;)
    else:
        lines.append(&quot;| group | L_inf | A | alpha |&quot;)
        lines.append(&quot;|---|---:|---:|---:|&quot;)
        for g, p in sorted(params.items()):
            lines.append(f&quot;| {g} | {p.L_inf:.6f} | {p.A:.6f} | {p.alpha:.6f} |&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;_This file is auto-updated when /app/law.py fits parameters (on first import)._&quot; )
    content = &quot;\n&quot;.join(lines)
    try:
        with open(EXPLAIN_PATH, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(content)
    except Exception:
        # best-effort; ignore write errors
        pass


def _ensure_fitted() -&gt; None:
    global _FITTED, _PARAMS
    if _FITTED:
        return
    _PARAMS = _fit_params_per_group()
    _FITTED = True


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected key: &#x27;sft_data_size&#x27;.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) under key &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    _ensure_fitted()

    # Choose parameters for the requested group; if missing, pick a reasonable fallback.
    params = _PARAMS.get(group)
    if params is None:
        # Fallback hierarchy:
        params = _PARAMS.get(&quot;default&quot;)
        if params is None and _PARAMS:
            # Use the median parameters across groups
            L_infs = np.array([p.L_inf for p in _PARAMS.values()])
            As = np.array([p.A for p in _PARAMS.values()])
            alphas = np.array([p.alpha for p in _PARAMS.values()])
            params = Params(
                L_inf=float(np.median(L_infs)),
                A=float(np.median(As)),
                alpha=float(np.median(alphas)),
            )
        if params is None:
            # Final safety fallback
            params = Params(L_inf=0.5, A=0.5, alpha=0.5)

    out: List[Dict[str, float]] = []
    for row in input_data:
        # Primary expected key
        n = row.get(&quot;sft_data_size&quot;, None)
        # Common aliases to be robust to input variations
        if n is None:
            for k in (&quot;N&quot;, &quot;n&quot;, &quot;size&quot;, &quot;data_size&quot;, &quot;num_examples&quot;):
                if k in row:
                    n = row[k]
                    break
        if n is None:
            # If completely missing, predict using N=1.0 to avoid NaN
            n = 1.0

        y_pred = params.predict(n)
        out.append({&quot;sft_loss&quot;: float(y_pred)})

    return out


# Fit at import so the parameters and explanation are available immediately.
try:
    _ensure_fitted()
except Exception:
    # Defer fitting errors to first call of law()
    pass</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
