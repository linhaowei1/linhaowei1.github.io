<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Data-Constrained Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Data-Constrained Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Gemini 2.5 Flash</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.986048 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.835004</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.725164</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.986048 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss values based on unique_tokens (U), params (P), and tokens (D)
    using an additive power law form: L = C_U * U^E_U + C_P * P^E_P + C_D * D^E_D + Bias.

    This function is designed for numerical stability and adheres to the common
    structure of scaling laws in LLMs, where increased resources reduce loss.

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]. All values must be positive.
    params: Array of 7 parameters: [C_U, C_P, C_D, E_U, E_P, E_D, Bias]
            - C_U, C_P, C_D: Coefficients for unique_tokens, params, tokens terms respectively.
            - E_U, E_P, E_D: Exponents for unique_tokens, params, tokens terms respectively.
            - Bias: Irreducible loss component.

    Returns: Predicted loss values (N,)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))  # (N, F) where F=3
    
    # Extract features. Add a small epsilon (1e-9) to ensure they are strictly
    # positive for log calculations. Input data ranges are already large and positive,
    # so this is mainly for robustness against extreme edge cases or future data.
    U = X[:, 0] + 1e-9
    P = X[:, 1] + 1e-9
    D = X[:, 2] + 1e-9
    
    # Unpack the 7 parameters passed by the optimizer.
    flat_params = np.asarray(params).flatten()
    C_U, C_P, C_D, E_U, E_P, E_D, Bias = flat_params

    # Calculate each power law term using the log-exp trick for numerical stability:
    # C * X^E = exp(log(C) + E * log(X)). This method is robust, especially for
    # large numbers and fractional exponents.
    # Coefficients (C_U, C_P, C_D) and Bias are guaranteed to be positive by the
    # optimization bounds (1e-9 minimum), so direct use in np.log is safe.
    term_U = np.exp(np.log(C_U) + E_U * np.log(U))
    term_P = np.exp(np.log(C_P) + E_P * np.log(P))
    term_D = np.exp(np.log(C_D) + E_D * np.log(D))
    
    # Sum the individual power law contributions and add the irreducible bias.
    pred_loss = term_U + term_P + term_D + Bias

    # Loss values are always non-negative. Ensure the predicted loss respects this.
    # The lowest observed loss (1.8) is significantly above zero, so 1e-6 is a
    # safe and reasonable floor to prevent non-physical negative predictions.
    return np.maximum(pred_loss, 1e-6)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the additive power law scaling function to data using the L-BFGS-B algorithm.
    This implementation features enhanced initial parameter guessing and carefully
    defined bounds for better numerical stability, faster convergence, and
    physically interpretable results, with a focus on cross-dataset generalization.
    It employs a weighted Mean Squared Error (WMSE) objective, prioritizing accurate
    fitting of lower loss values, which are critical for model performance.

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]
    loss_values: Array of corresponding loss values (N,)

    Returns: Optimized parameters (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # Ensure loss_values is a 1D array for consistent calculations throughout.
    y_flat = y.flatten()

    # --- Initial Parameter Guess Strategy ---
    # 1. Initial Bias: Estimate as a fraction of the minimum observed loss, ensuring positivity.
    # This heuristic places the irreducible loss in a reasonable range for a good starting point.
    min_loss_obs = np.min(y_flat) if len(y_flat) &gt; 0 else 1.0
    init_bias_val = max(0.1, min_loss_obs * 0.5)

    # 2. Average Feature Values: Use median for robustness against potential outliers
    # in the input data (unique_tokens, params, tokens). Provide defaults for safety.
    avg_U = np.median(X[:, 0]) if X.shape[0] &gt; 0 else 1e8
    avg_P = np.median(X[:, 1]) if X.shape[0] &gt; 0 else 5e8
    avg_D = np.median(X[:, 2]) if X.shape[0] &gt; 0 else 5e10
    
    # 3. Initial Exponent Guess: A common, physically plausible value for negative
    # scaling exponents in LLMs.
    init_E = -0.3 
    
    # 4. Initial Coefficients (C_U, C_P, C_D): Scale them so each power law term
    # contributes roughly equally to the &#x27;reducible&#x27; part of the total loss.
    # The &#x27;reducible&#x27; part is total mean loss minus the estimated bias.
    target_sum_terms = max(0.1, np.mean(y_flat) - init_bias_val)
    target_term_val = target_sum_terms / 3.0 # Assume equal contribution from each of 3 terms

    # Calculate initial C values for each feature.
    # A small epsilon (1e-9) is added to the base to avoid issues if avg_X is
    # extremely small, especially with negative exponents.
    init_C_U = target_term_val / (avg_U + 1e-9)**init_E
    init_C_P = target_term_val / (avg_P + 1e-9)**init_E
    init_C_D = target_term_val / (avg_D + 1e-9)**init_E
    
    # Clip initial C values to prevent excessively large or small starting points,
    # which can destabilize the optimization process. This helps in robust convergence.
    init_C_U = np.clip(init_C_U, 1e-2, 1e12)
    init_C_P = np.clip(init_C_P, 1e-2, 1e12)
    init_C_D = np.clip(init_C_D, 1e-2, 1e12)

    # Compile the full initial guess for parameters.
    init_params = np.array([
        init_C_U,   # C_U
        init_C_P,   # C_P
        init_C_D,   # C_D
        init_E,     # E_U
        init_E,     # E_P
        init_E,     # E_D
        init_bias_val # Bias
    ])

    # --- Bounds for L-BFGS-B Optimization ---
    # Define bounds to ensure physical realism, numerical stability, and
    # adherence to typical LLM scaling behavior for improved generalization.
    
    # Coefficients (C_U, C_P, C_D) and Bias must be strictly positive.
    min_positive_param = 1e-9
    
    # Exponents (E_U, E_P, E_D) must be negative (more resources -&gt; lower loss).
    # Tighter bounds (-1.0 to -1e-3) are used to enforce common LLM scaling law
    # behavior, promoting theoretical stability and cross-dataset generalization.
    # -1.0 allows for reasonably steep scaling, -1e-3 allows for very weak but still negative effects.
    min_exponent_val = -1.0
    max_exponent_val = -1e-3 

    bounds = [
        (min_positive_param, None),        # C_U (coefficient must be positive)
        (min_positive_param, None),        # C_P (coefficient must be positive)
        (min_positive_param, None),        # C_D (coefficient must be positive)
        (min_exponent_val, max_exponent_val), # E_U (exponent must be negative within reasonable range)
        (min_exponent_val, max_exponent_val), # E_P (exponent must be negative within reasonable range)
        (min_exponent_val, max_exponent_val), # E_D (exponent must be negative within reasonable range)
        (min_positive_param, None)          # Bias (irreducible loss must be positive)
    ]

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function to minimize, using Weighted Mean Squared Error (WMSE).
        Weights are inversely proportional to the true loss values, giving more
        importance to accurately fitting lower loss points, which is often crucial
        for scaling law models.
        Includes a guard for non-finite WMSE to guide the optimizer away from problematic regions.
        &quot;&quot;&quot;
        pred = scaling_law_func(X, flat_params)
        
        # Calculate weights inversely proportional to the true loss values.
        # This gives more importance to accurately fitting lower loss points.
        # Since y_flat (actual losses) are strictly positive (range 1.8 to 7.2),
        # 1.0 / y_flat is safe and well-behaved, ensuring numerical stability.
        weights = 1.0 / y_flat
        
        # Calculate Weighted Mean Squared Error (WMSE)
        wmse = np.mean(weights * (pred - y_flat) ** 2)
        
        # Return a very large value if WMSE is non-finite, indicating a failure
        # in parameter calculation (e.g., due to numerical overflow).
        if not np.isfinite(wmse):
            return 1e15 
        return wmse

    # Use L-BFGS-B for bounded optimization. This method is well-suited for
    # nonlinear least squares problems with bounds, providing a balance of
    # speed and robustness.
    # Increased maxiter and tightened ftol/gtol to encourage better convergence.
    result = minimize(objective, init_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                      options={&#x27;maxiter&#x27;: 10000, &#x27;ftol&#x27;: 1e-8, &#x27;gtol&#x27;: 1e-8})
    
    # Return the optimized parameters. If the optimization was not successful,
    # fall back to the robust initial guess to prevent erroneous output.
    return result.x if result.success else init_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.937614 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
This evolved program refines the scaling law form by explicitly coupling both the
exponents and coefficients for total tokens and unique tokens, which is particularly relevant
for data-constrained conditions. It uses log-transformed coefficients for stability
and bounded optimization with improved initialization, aiming for better parameter
efficiency and generalization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LLM loss based on model parameters, tokens, and unique tokens
    using a specific scaling law form where data scaling exponents and coefficients are coupled.

    The scaling law is L = L0 + A * P_m^(-alpha_P) + K * (w_coeff * D_t^(-alpha_D) + (1.0 - w_coeff) * U^(-alpha_U)).

    Here:
    - L0 is the irreducible loss.
    - A is the coefficient for the model parameter term.
    - alpha_P is the exponent for the model parameter term.
    - K is the overall coefficient for the combined data terms.
    - w_coeff is a weighting factor (0 to 1) that distributes K&#x27;s influence between the
      total tokens (D_t) and unique tokens (U) terms.
    - alpha_D and alpha_U are derived from an overall data scaling exponent
      alpha_total and a weighting factor w_dt:
        alpha_D = alpha_total * w_dt
        alpha_U = alpha_total * (1.0 - w_dt)
    - w_dt is a weighting factor (0 to 1) that distributes alpha_total&#x27;s influence between the
      exponents of total tokens (D_t) and unique tokens (U).

    A and K are modeled as exp(logA) and exp(logK) for positive coefficients.
    w_coeff and w_dt are explicitly bounded between 0 and 1.

    Args:
        data_points (np.ndarray): (N,3) array with columns [unique_tokens, params, tokens].
                                  - unique_tokens (U): Number of unique tokens seen during training.
                                  - params (P_m): Number of model parameters.
                                  - tokens (D_t): Total number of tokens seen during training.
        params (np.ndarray): Array of 7 parameters for the scaling law:
                             [logA, alpha_P, logK, w_coeff, alpha_total, w_dt, L0]

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    
    # Extract features from data_points
    unique_tokens = X[:, 0]  # U
    model_params = X[:, 1]   # P_m
    tokens = X[:, 2]         # D_t

    # Unpack scaling law parameters
    if len(params) != 7:
        raise ValueError(f&quot;scaling_law_func expects exactly 7 parameters, got {len(params)}.&quot;)

    logA, alpha_P, logK, w_coeff, alpha_total, w_dt, L0 = params

    # Convert logged coefficients back to actual coefficients (ensures they are positive)
    A = np.exp(logA)
    K = np.exp(logK)
    
    # Add a small epsilon to bases to prevent numerical issues if inputs could be zero or very small.
    # This is critical for robustness with power laws and potential log(0) issues.
    epsilon = 1e-9 

    # Derive alpha_D and alpha_U from the coupled parameters
    # The w_dt parameter balances the exponents between total tokens and unique tokens.
    # It&#x27;s bounded between 0 and 1 by the optimization, ensuring alpha_D and alpha_U are non-negative.
    alpha_D = alpha_total * w_dt
    alpha_U = alpha_total * (1.0 - w_dt)
    
    # Calculate the individual power law terms for data dimensions
    term_D = np.power(np.maximum(tokens, epsilon), -alpha_D)
    term_U = np.power(np.maximum(unique_tokens, epsilon), -alpha_U)

    # Combine data terms with coefficient coupling using K and w_coeff
    # This formulation effectively makes B = K * w_coeff and C = K * (1 - w_coeff)
    combined_data_term = K * (w_coeff * term_D + (1.0 - w_coeff) * term_U)
    
    # Calculate predicted loss using the specialized scaling law
    predicted_loss = L0 + \
                     A * np.power(np.maximum(model_params, epsilon), -alpha_P) + \
                     combined_data_term
    
    return predicted_loss

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the evolved LLM scaling law function to observed data using optimization.

    Args:
        data_points (np.ndarray): (N,3) array with columns [unique_tokens, params, tokens].
        loss_values (np.ndarray): Array of corresponding loss values.

    Returns:
        np.ndarray: Optimized parameters (7 parameters) for the scaling law.
                    [logA, alpha_P, logK, w_coeff, alpha_total, w_dt, L0]
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # --- Initial parameter guesses ---
    # The order of parameters: [logA, alpha_P, logK, w_coeff, alpha_total, w_dt, L0]
    
    min_loss = np.min(y)
    mean_loss = np.mean(y)
    
    # Extract features for median calculations to make initial guesses robust to outliers
    unique_tokens_data = X[:, 0]
    model_params_data = X[:, 1]
    tokens_data = X[:, 2]

    median_P = np.median(model_params_data)
    median_D = np.median(tokens_data)
    median_U = np.median(unique_tokens_data)
    
    # Initialize exponents with values common in LLM scaling laws or reasonable starting points.
    alpha_P_init = 0.34     # Common exponent for model parameters (e.g., from Chinchilla/Gopher)
    alpha_total_init = 0.34 # Initial overall exponent for data-related terms
    w_dt_init = 0.5         # Initial weight for exponent distribution (neutral starting point)
    w_coeff_init = 0.5      # Initial weight for coefficient distribution (neutral starting point)
    
    # Initialize L0 (irreducible loss). It should be non-negative and often a small fraction of the minimum observed loss.
    L0_init = max(0.01, min_loss * 0.05) 

    # Guard against zero or negative log arguments for medians using a small epsilon,
    # important as features can be large numbers, but log function needs positive input.
    epsilon_log = 1e-9 
    log_median_P = np.log(max(median_P, epsilon_log)) 
    log_median_D = np.log(max(median_D, epsilon_log))
    log_median_U = np.log(max(median_U, epsilon_log))

    # Calculate initial derived exponents for consistency with alpha_total_init and w_dt_init
    alpha_D_init_derived = alpha_total_init * w_dt_init
    alpha_U_init_derived = alpha_total_init * (1.0 - w_dt_init)

    # Calculate the total reducible loss magnitude for initialization scaling
    reducible_loss_magnitude = max(1e-5, (mean_loss - L0_init))
    
    # Initialize logA: Assume model term contributes roughly 1/3 of the reducible loss at median P
    target_P_contribution = reducible_loss_magnitude / 3.0
    logA_init = np.log(target_P_contribution) + alpha_P_init * log_median_P

    # Initialize logK: Assume the combined data term contributes roughly 2/3 of the reducible loss at median D, U
    target_data_contribution = reducible_loss_magnitude * 2.0 / 3.0
    
    # Calculate the power law terms for median D and U using initial derived exponents
    median_D_power_term = np.power(max(median_D, epsilon_log), -alpha_D_init_derived)
    median_U_power_term = np.power(max(median_U, epsilon_log), -alpha_U_init_derived)

    # Combined base for data term: (w_coeff * D_t_term + (1-w_coeff) * U_term) at median values
    # Ensure this combined base is not zero to prevent division by zero for K_init
    combined_median_data_base = (w_coeff_init * median_D_power_term + (1.0 - w_coeff_init) * median_U_power_term)
    
    # Initialize K: target_data_contribution = K_init * combined_median_data_base
    # Add a small epsilon to prevent division by zero or log of zero/negative if combined_median_data_base is too small
    if combined_median_data_base &lt;= epsilon_log: # Use epsilon_log as a robust threshold
        K_init_val = target_data_contribution + epsilon_log # Fallback to a small positive value
    else:
        K_init_val = target_data_contribution / combined_median_data_base
    
    logK_init = np.log(K_init_val)


    initial_params = np.array([
        logA_init, alpha_P_init,
        logK_init, w_coeff_init,
        alpha_total_init, w_dt_init,
        L0_init
    ])

    # --- Parameter bounds ---
    # Order: [logA, alpha_P, logK, w_coeff, alpha_total, w_dt, L0]
    # These bounds help guide the optimizer towards physically meaningful solutions and prevent divergence.
    bounds = [
        (-15.0, 15.0),          # logA: Allows A to range from exp(-15) to exp(15) (~3e-7 to ~3e6)
        (0.01, 1.0),            # alpha_P: Exponents are typically positive and less than 1 (e.g., 0.1 to 0.7)
        (-15.0, 15.0),          # logK: Similar range for overall data coefficient K
        (0.0, 1.0),             # w_coeff: Weight for coefficient distribution, must be between 0 and 1
        (0.01, 1.0),            # alpha_total: Overall data exponent, positive and less than 1
        (0.0, 1.0),             # w_dt: Weight for exponent distribution, must be between 0 and 1
        (0.0, min_loss)         # L0: Non-negative irreducible loss. Upper bound is min_loss (from Inspiration 2)
                                # as irreducible loss should be lower than any observed finite loss.
    ]

    def objective(params):
        &quot;&quot;&quot;
        Objective function for optimization (Mean Squared Error).
        Includes a penalty for physically implausible predictions (e.g., very low or negative loss).
        &quot;&quot;&quot;
        pred = scaling_law_func(X, params)
        
        # Add a check for NaN/Inf predictions, which can occur with problematic parameters
        if not np.all(np.isfinite(pred)):
            return np.inf # Heavily penalize non-finite predictions to guide optimizer away from unstable regions

        mse = np.mean((pred - y)**2)
        
        # Add a penalty for predictions that are too low or negative.
        # Loss values are typically positive. Penalize predictions significantly below a reasonable minimum.
        # This helps enforce the physical constraint of positive loss.
        penalty = 0.0
        min_acceptable_pred = 0.001 # A fixed small positive value for cross-entropy, more robust than min(y)*0.5
        negative_pred_mask = pred &lt; min_acceptable_pred
        if np.any(negative_pred_mask):
            # Apply a quadratic penalty for predictions falling below the threshold
            penalty = np.sum((min_acceptable_pred - pred[negative_pred_mask])**2) * 1000 
        
        return mse + penalty

    # Use L-BFGS-B, which is a quasi-Newton method suitable for bounded optimization.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)
    
    # Return optimized parameters if the optimization was successful; otherwise,
    # return the initial parameters as a reasonable fallback to prevent errors.
    optimized_params = result.x if result.success else initial_params
    
    return optimized_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.793827 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss values using a combined multiplicative and an explicit repetition power law form.
    This form directly addresses data-constrained conditions by including a term for data repetition.

    L = K * (unique_tokens^E_U * model_params^E_P * tokens^E_D_mult) + C_REP * (tokens / unique_tokens)^E_REP + Bias

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]. All values must be positive.
                 unique_tokens: Amount of unique text in the training data.
                 params: Number of model parameters.
                 tokens: Total number of tokens used for training.
    params: Array of 7 parameters: [K, E_U, E_P, E_D_mult, C_REP, E_REP, Bias]
            - K, C_REP, Bias: Coefficients/base for the terms (expected positive).
            - E_U, E_P, E_D_mult: Exponents (expected negative) indicating loss reduction.
            - E_REP: Exponent for the repetition term (expected positive) indicating
                     loss increase with more data repetition.

    Returns: Predicted loss values (N,)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    unique_tokens = X[:, 0]
    model_params = X[:, 1]
    tokens = X[:, 2]

    # Ensure params is a 2D array (1, 7) for consistent indexing.
    params_arr = np.asarray(params)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :] 

    # Extract the 7 parameters.
    K, E_U, E_P, E_D_mult, C_REP, E_REP, Bias = params_arr[0]

    # Compute power law terms using the log-exp trick for numerical stability.
    # X^E = exp(E * log(X)). Input &#x27;data_points&#x27; are guaranteed positive.
    log_unique_tokens = np.log(unique_tokens)
    log_model_params = np.log(model_params)
    log_tokens = np.log(tokens)

    # Multiplicative term: K * unique_tokens^E_U * model_params^E_P * tokens^E_D_mult
    log_mult_term_components = (
        E_U * log_unique_tokens,
        E_P * log_model_params,
        E_D_mult * log_tokens
    )
    term_multiplicative = K * np.exp(np.sum(log_mult_term_components, axis=0))
    
    # Repetition term: C_REP * (tokens / unique_tokens)^E_REP
    # This term explicitly models the effect of data repetition on loss.
    # A positive E_REP means higher repetition (tokens/unique_tokens) increases this term, thus increasing loss.
    log_repetition_ratio = log_tokens - log_unique_tokens
    term_repetition = C_REP * np.exp(E_REP * log_repetition_ratio)

    # The total predicted loss is the sum of the multiplicative, repetition, and bias terms.
    predicted_loss = term_multiplicative + term_repetition + Bias

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the evolved scaling law function to data using the L-BFGS-B optimization method.
    This function incorporates refined initial guesses and tighter, physically plausible
    bounds for parameters, which are crucial for stable and accurate optimization,
    especially under data-constrained conditions and with complex model forms.

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]
    loss_values: Array of corresponding loss values (N,)

    Returns: Optimized parameters (7 parameters) as a 1D numpy array.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    P_total = 7 # The total number of parameters in the scaling_law_func

    # Determine the minimum and mean observed loss, critical for setting initial guesses and bounds.
    min_loss = np.min(y)
    mean_loss = np.mean(y)
    
    # Calculate geometric mean of features as a representative &#x27;average&#x27; point.
    geo_mean_unique_tokens = np.exp(np.mean(np.log(X[:, 0])))
    geo_mean_model_params = np.exp(np.mean(np.log(X[:, 1])))
    geo_mean_tokens = np.exp(np.mean(np.log(X[:, 2])))
    
    # Calculate geometric mean of the repetition ratio
    geo_mean_repetition_ratio = np.exp(np.mean(np.log(X[:, 2] / X[:, 0])))

    # Refined initial guesses for exponents based on common LLM scaling literature and new term&#x27;s logic.
    init_E_U = -0.06      # Exponent for unique tokens (negative, loss decreases)
    init_E_P = -0.09      # Exponent for model parameters (negative, loss decreases)
    init_E_D_mult = -0.12 # Exponent for tokens in multiplicative term (negative, loss decreases)
    init_E_REP = 0.05     # Exponent for repetition (positive, loss increases with repetition)
    
    # Estimate the scale of the power terms at the geometric mean of features,
    # using the initial exponent guesses. This helps in dynamically setting K and C_REP.
    estimated_mult_base = (geo_mean_unique_tokens ** init_E_U) * \
                          (geo_mean_model_params ** init_E_P) * \
                          (geo_mean_tokens ** init_E_D_mult)
    
    estimated_rep_base = (geo_mean_repetition_ratio ** init_E_REP)

    # Calculate initial Bias guess. Bias must be positive and less than min_loss.
    initial_Bias = max(0.5, min_loss * 0.9) 

    # The sum of the power law terms should approximately equal (mean_loss - initial_Bias).
    target_power_law_sum = mean_loss - initial_Bias
    
    # Ensure target_power_law_sum is positive, if not, adjust it.
    if target_power_law_sum &lt; 0.1: 
        target_power_law_sum = 0.5 

    # Proportionally assign the target sum to K and C_REP.
    # The multiplicative term is usually dominant, so it takes a larger share.
    # Fallback values prevent division by zero or extreme initial values.
    initial_K = (target_power_law_sum * 0.8) / (estimated_mult_base if estimated_mult_base &gt; 1e-10 else 1.0)
    initial_C_REP = (target_power_law_sum * 0.2) / (estimated_rep_base if estimated_rep_base &gt; 1e-10 else 1.0)
    
    # Clip initial coefficients to reasonable ranges.
    initial_K = np.clip(initial_K, 1e-3, 5e3) 
    initial_C_REP = np.clip(initial_C_REP, 1e-3, 50.0)

    initial_guess = np.array([
        initial_K,      
        init_E_U,       
        init_E_P,       
        init_E_D_mult,  
        initial_C_REP,  
        init_E_REP,   
        initial_Bias    
    ])

    # Refined bounds for parameters to enforce physical plausibility and aid optimization stability.
    eps_val = 1e-6 
    
    bounds = [
        (eps_val, 5e3),            # K: Coefficient for multiplicative term (positive).
        (-0.5, -eps_val),          # E_U: Exponent for unique tokens (negative).
        (-0.5, -eps_val),          # E_P: Exponent for model parameters (negative).
        (-0.5, -eps_val),          # E_D_mult: Exponent for tokens in mult. term (negative).
        (eps_val, 50.0),           # C_REP: Coefficient for repetition term (positive).
        (eps_val, 0.5),            # E_REP: Exponent for repetition term (positive, but capped to avoid instability).
        (0.5, min_loss - eps_val)  # Bias: Irreducible loss (positive, less than min observed loss).
    ]
    
    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function to minimize, calculated as Mean Squared Error (MSE)
        between predicted and actual loss values.
        &quot;&quot;&quot;
        params_reshaped = flat_params.reshape(1, P_total) 
        pred = scaling_law_func(X, params_reshaped)
        mse = np.mean((pred - y) ** 2)
        return mse

    result = minimize(
        objective, 
        initial_guess, 
        method=&#x27;L-BFGS-B&#x27;, 
        bounds=bounds, 
        options={&#x27;maxiter&#x27;: 3000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10} 
    )
    
    params_opt = result.x if result.success else initial_guess

    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.732369 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss values based on unique_tokens, params, and tokens using a novel
    scaling law form that combines parameter and data tokens into an interactive term.

    L = C_U * U^E_U + C_PD * (P^alpha_P * D^beta_D)^E_PD + Bias

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]. All values must be positive.
    params: Array of 7 parameters: [C_U, E_U, C_PD, alpha_P, beta_D, E_PD, Bias]
            - C_U: Coefficient for unique_tokens term (expected positive)
            - E_U: Exponent for unique_tokens term (expected negative)
            - C_PD: Coefficient for the combined P*D term (expected positive)
            - alpha_P: Exponent for parameters within the combined term (expected positive)
            - beta_D: Exponent for tokens within the combined term (expected positive)
            - E_PD: Overall exponent for the combined P*D term (expected negative)
            - Bias: Irreducible loss component (expected positive)

    Returns: Predicted loss values (N,)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))  # (N, 3)
    
    # Extract features, ensuring they are positive for log operations.
    # Adding a small epsilon for robustness against potential exact zeros,
    # though problem description implies large positive numbers.
    U = X[:, 0] + 1e-10 # unique_tokens
    P = X[:, 1] + 1e-10 # params
    D = X[:, 2] + 1e-10 # tokens
    
    # Unpack parameters (assuming a single set of parameters as T=1 for this problem)
    C_U, E_U, C_PD, alpha_P, beta_D, E_PD, Bias = params

    # Term 1: Unique tokens component
    # Calculated using log-exp trick for numerical stability: C_U * exp(E_U * log(U))
    term_U = C_U * np.exp(E_U * np.log(U))

    # Term 2: Combined Parameters and Data tokens component
    # C_PD * (P^alpha_P * D^beta_D)^E_PD = C_PD * exp(E_PD * log(P^alpha_P * D^beta_D))
    # = C_PD * exp(E_PD * (alpha_P * log(P) + beta_D * log(D)))
    log_inner_term = alpha_P * np.log(P) + beta_D * np.log(D)
    term_PD = C_PD * np.exp(E_PD * log_inner_term)
    
    # Total predicted loss
    pred_loss = term_U + term_PD + Bias
    
    return pred_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function to data using L-BFGS-B with bounds, optimized
    for the specific 7-parameter model.

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]
    loss_values: Array of corresponding loss values (N,)

    Returns: Optimized parameters (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))  # (N, F)
    y = np.asarray(loss_values)
    
    # Define initial guess for the 7 parameters:
    # [C_U, E_U, C_PD, alpha_P, beta_D, E_PD, Bias]
    # Initial values are chosen based on typical scaling law behaviors and
    # to provide a reasonable starting point for optimization.
    initial_guess = np.array([1.0, -0.5, 1.0, 0.5, 0.5, -0.5, 1.0])

    # Define bounds for parameters for L-BFGS-B method:
    # These bounds enforce physical interpretability and aid optimization stability.
    # A small epsilon (1e-6) is used to avoid exact zero for coefficients and exponents
    # where log/power operations might be sensitive or would indicate a non-scaling behavior
    # (e.g., exponent of 0 implies a constant term).
    bounds = [
        (1e-6, None),   # C_U: Positive coefficient
        (None, -1e-6),  # E_U: Negative exponent (loss decreases with more unique tokens)
        (1e-6, None),   # C_PD: Positive coefficient for the combined term
        (1e-6, 5.0),    # alpha_P: Positive exponent for P within the combined term, bounded to prevent extremes
        (1e-6, 5.0),    # beta_D: Positive exponent for D within the combined term, bounded to prevent extremes
        (None, -1e-6),  # E_PD: Negative overall exponent for the combined term (loss decreases with more P*D resource)
        (1e-6, None)    # Bias: Positive irreducible loss component
    ]

    def objective(params):
        &quot;&quot;&quot;Objective function to minimize (Mean Squared Error).&quot;&quot;&quot;
        pred = scaling_law_func(X, params)  # (N,)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Use L-BFGS-B for bounded optimization, which is suitable for constrained problems.
    result = minimize(objective, initial_guess, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)
    
    # Return optimized parameters if successful, otherwise return the initial guess
    # (though a robust optimizer should ideally succeed or indicate failure clearly).
    params_opt = result.x if result.success else initial_guess

    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.725164 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss values using a composite scaling law designed for LLM training scenarios
    under data-constrained conditions. The model combines a global multiplicative power law
    across unique_tokens, parameters, and total tokens, with an additional additive power law
    term specifically for parameters, and an irreducible bias.

    Model form: L = C * (U^E_U * P^E_P * D^E_D) + C_P_ADD * P^E_P_ADD + Bias

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens]. All values must be positive.
    params: Array of 7 parameters: [C, E_U, E_P, E_D, C_P_ADD, E_P_ADD, Bias]
            - C, C_P_ADD: Coefficients for the respective terms (expected positive).
            - E_U, E_P, E_D, E_P_ADD: Exponents for unique_tokens, params, tokens, and the additive
              parameter term respectively (expected negative, as more resources typically reduce loss).
            - Bias: Irreducible loss component (expected positive).

    Returns: Predicted loss values (N, 1) - a 2D array for compatibility with the fitting function.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    
    # Extract features: Unique Tokens (U), Parameters (P), Data Tokens (D)
    U_arr = X[:, 0]  # unique_tokens
    P_arr = X[:, 1]  # params
    D_arr = X[:, 2]  # tokens

    # Ensure params is a 1D array for direct unpacking, handling (1, P) or (P,) input
    if params.ndim == 2:
        current_params = params[0]
    else:
        current_params = params

    C, E_U, E_P, E_D, C_P_ADD, E_P_ADD, Bias = current_params

    # Calculate logarithms of features for numerical stability in power law computations.
    # The problem description guarantees positive input values, so np.log is safe.
    log_U = np.log(U_arr)
    log_P = np.log(P_arr)
    log_D = np.log(D_arr)

    # Term 1: Multiplicative scaling component (C * U^E_U * P^E_P * D^E_D)
    # Computed as C * exp(E_U*log(U) + E_P*log(P) + E_D*log(D)) for stability.
    term1_log_sum = E_U * log_U + E_P * log_P + E_D * log_D
    term1 = C * np.exp(term1_log_sum)

    # Term 2: Additive power law component focused solely on parameters (C_P_ADD * P^E_P_ADD)
    # Computed as C_P_ADD * exp(E_P_ADD*log(P)) for stability.
    term2_log_sum = E_P_ADD * log_P
    term2 = C_P_ADD * np.exp(term2_log_sum)
    
    # Total predicted loss is the sum of these components plus the irreducible bias.
    predicted_loss = term1 + term2 + Bias
    
    # Return as a 2D array (N, 1) to maintain compatibility with the calling optimization routine.
    return predicted_loss[:, None]


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function to data using L-BFGS-B, a bounded optimization method.
    This approach helps in finding physically interpretable parameters and improves
    numerical stability compared to unconstrained optimization.

    Parameters:
    data_points: (N,3) array with columns [unique_tokens, params, tokens].
    loss_values: Array of corresponding loss values (N,).

    Returns: Optimized parameters (7 parameters) as a 1D NumPy array.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))  # Ensure data_points is a 2D array
    y = np.asarray(loss_values)                 # Ensure loss_values is a NumPy array
    
    N, F = X.shape  # N = number of data points, F = number of features (3 in this case)
    P = 7           # Total number of parameters for the `scaling_law_func`

    # Reshape `y` to (N, 1) to match the expected output shape of `scaling_law_func`
    # and handle general T (number of targets) logic within the optimization framework.
    if y.ndim == 1:
        y2d = y[:, None]
    else:
        y2d = y
    T = y2d.shape[1] # For this problem, T will always be 1.

    # Initial guess for the 7 parameters: [C, E_U, E_P, E_D, C_P_ADD, E_P_ADD, Bias]
    # These values are chosen based on typical scaling law parameter ranges.
    # An adaptive initial guess for Bias, related to the minimum observed loss.
    min_observed_loss = np.min(y)
    init_bias = max(0.1, min_observed_loss * 0.5) # ensure bias is positive and not too high

    init_params_single = [1.0, -0.1, -0.2, -0.2, 1.0, -0.5, init_bias] 
    init = np.array(init_params_single * T).reshape(T, P)

    # Define bounds for each parameter to constrain the search space for L-BFGS-B.
    # Using a small epsilon for lower bounds of positive parameters to prevent numerical issues
    # if they go exactly to zero. Exponents are typically negative or zero.
    bounds_single = [
        (1e-9, None),   # C: coefficient must be non-negative (strictly positive)
        (-2.0, 0.0),    # E_U: exponent for unique_tokens, typically negative
        (-2.0, 0.0),    # E_P: exponent for params, typically negative
        (-2.0, 0.0),    # E_D: exponent for tokens, typically negative
        (1e-9, None),   # C_P_ADD: additive parameter coefficient must be non-negative (strictly positive)
        (-2.0, 0.0),    # E_P_ADD: exponent for additive parameter term, typically negative
        (1e-9, None)    # Bias: irreducible loss must be non-negative (strictly positive)
    ]
    bounds = bounds_single * T # Repeat bounds for each target if T&gt;1

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function to minimize, which is the Mean Squared Error (MSE)
        between the predicted loss and the actual loss values.
        &quot;&quot;&quot;
        params_reshaped = flat_params.reshape(T, P) # Reshape flat parameters back to (T, P)
        pred = scaling_law_func(X, params_reshaped) # Get predictions (N, T)
        
        # Add a penalty for physically impossible negative loss predictions.
        # Loss values should always be non-negative.
        penalty = 0
        neg_preds = pred[pred &lt; 1e-9] 
        if neg_preds.size &gt; 0:
            # Apply a large quadratic penalty for predictions below a small positive epsilon
            penalty = np.sum((neg_preds - 1e-9)**2) * 1e6 
            
        mse = np.mean((pred - y2d) ** 2) + penalty            # Calculate MSE
        return mse

    # Perform optimization using L-BFGS-B, which is suitable for bounded problems.
    # Increased maxiter and stricter tolerances for potentially better convergence.
    result = minimize(objective, init.ravel(), method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                      options={&#x27;maxiter&#x27;: 5000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-8})
    
    # Return the optimized parameters. If optimization fails, return the initial guess.
    params_opt = result.x.reshape(T, P) if result.success else init

    # For this problem, T=1, so we return the single set of optimized parameters.
    return params_opt[0]
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
