<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Vocabulary Scaling Law - claude-code + claude-haiku-4-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Vocabulary Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">claude-code</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">claude-haiku-4-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.949582 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.895340</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.861120</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.949582 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">import numpy as np


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;

    # Scaling law coefficients discovered through regression analysis
    # The model is: loss = a + b1*log(vocab) + b2*log(params) + b3*log(chars) + b4*log(params)*log(chars)

    # Group-specific parameters
    group_params = {
        &#x27;all_data&#x27;: {
            &#x27;intercept&#x27;: 65.573639301665,
            &#x27;coef_vocab&#x27;: 0.065643930083,
            &#x27;coef_params&#x27;: -3.059110450551,
            &#x27;coef_chars&#x27;: -3.086349037920,
            &#x27;coef_interaction&#x27;: 0.133786043000
        }
    }

    # Use &#x27;all_data&#x27; parameters as default for any group
    if group not in group_params:
        params = group_params[&#x27;all_data&#x27;]
    else:
        params = group_params[group]

    results = []

    for data_point in input_data:
        # Extract input variables
        vocab_size = data_point[&#x27;vocab_size&#x27;]
        non_vocab_parameters = data_point[&#x27;non_vocab_parameters&#x27;]
        num_characters = data_point[&#x27;num_characters&#x27;]

        # Compute logarithmic features
        log_vocab = np.log(vocab_size)
        log_params = np.log(non_vocab_parameters)
        log_chars = np.log(num_characters)
        interaction = log_params * log_chars

        # Compute prediction using the scaling law
        prediction = (
            params[&#x27;intercept&#x27;] +
            params[&#x27;coef_vocab&#x27;] * log_vocab +
            params[&#x27;coef_params&#x27;] * log_params +
            params[&#x27;coef_chars&#x27;] * log_chars +
            params[&#x27;coef_interaction&#x27;] * interaction
        )

        # Return the predicted unigram_normalized_loss
        results.append({
            &#x27;unigram_normalized_loss&#x27;: float(prediction)
        })

    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.938178 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">import numpy as np


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The scaling law is based on a quadratic model in log-log space:

    log(loss) = a₀ + a₁·log(V) + a₂·log(P) + a₃·log(D) +
                a₄·[log(V)]² + a₅·[log(P)]² + a₆·[log(D)]²

    where V is vocab_size, P is non_vocab_parameters, and D is num_characters.

    The loss is unigram_normalized_loss, which is negative. The model predicts log(-loss).

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;

    # Fitted coefficients for the scaling law
    # These are derived from linear regression in log-log space with squared terms
    coefficients = {
        &#x27;all_data&#x27;: {
            &#x27;intercept&#x27;: -20.58579395,
            &#x27;log_vocab_size&#x27;: 0.04095321,
            &#x27;log_non_vocab_parameters&#x27;: -0.43878558,
            &#x27;log_num_characters&#x27;: 2.12979100,
            &#x27;log_vocab_size_sq&#x27;: -0.00309215,
            &#x27;log_non_vocab_parameters_sq&#x27;: 0.01159718,
            &#x27;log_num_characters_sq&#x27;: -0.04320388,
        }
    }

    # Get coefficients for the specified group
    if group not in coefficients:
        # Default to &#x27;all_data&#x27; if group not found (only one group in dataset)
        group = &#x27;all_data&#x27;

    coeff = coefficients[group]

    results = []

    for data_point in input_data:
        # Extract input variables
        vocab_size = data_point.get(&#x27;vocab_size&#x27;, 1.0)
        non_vocab_parameters = data_point.get(&#x27;non_vocab_parameters&#x27;, 1.0)
        num_characters = data_point.get(&#x27;num_characters&#x27;, 1.0)

        # Compute log values
        log_vocab_size = np.log(vocab_size)
        log_non_vocab_parameters = np.log(non_vocab_parameters)
        log_num_characters = np.log(num_characters)

        # Compute log(loss) using the quadratic model
        log_loss = (
            coeff[&#x27;intercept&#x27;] +
            coeff[&#x27;log_vocab_size&#x27;] * log_vocab_size +
            coeff[&#x27;log_non_vocab_parameters&#x27;] * log_non_vocab_parameters +
            coeff[&#x27;log_num_characters&#x27;] * log_num_characters +
            coeff[&#x27;log_vocab_size_sq&#x27;] * (log_vocab_size ** 2) +
            coeff[&#x27;log_non_vocab_parameters_sq&#x27;] * (log_non_vocab_parameters ** 2) +
            coeff[&#x27;log_num_characters_sq&#x27;] * (log_num_characters ** 2)
        )

        # Convert back from log space: loss = -exp(log_loss)
        # (negative because the unigram_normalized_loss is negative)
        unigram_normalized_loss = -np.exp(log_loss)

        results.append({
            &#x27;unigram_normalized_loss&#x27;: float(unigram_normalized_loss)
        })

    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.866698 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">import math

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The discovered scaling law follows the form:
    unigram_normalized_loss = a*log(vocab_size) + b*log(non_vocab_parameters) + c*log(num_characters) + d

    Where:
    - a is a shared coefficient across all groups
    - b, c, d are parameters that vary by group (where group is identified by vocab_size)

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                In this dataset, the group is identified by the vocab_size value.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;

    # Unified coefficient across all groups
    a = 0.0634011567

    # Group-specific parameters indexed by vocab_size
    # Each group has its own b, c, d values
    group_params = {
        4096.0: {&#x27;b&#x27;: 0.0103157282, &#x27;c&#x27;: -0.4387568777, &#x27;d&#x27;: 5.0740243542},
        6144.0: {&#x27;b&#x27;: 0.0019714668, &#x27;c&#x27;: -0.4572441132, &#x27;d&#x27;: 5.6220287899},
        8192.0: {&#x27;b&#x27;: 0.0035061757, &#x27;c&#x27;: -0.4762909418, &#x27;d&#x27;: 6.0311809571},
        10240.0: {&#x27;b&#x27;: 0.0097884790, &#x27;c&#x27;: -0.4849630808, &#x27;d&#x27;: 6.1153025956},
        16384.0: {&#x27;b&#x27;: 0.0080820317, &#x27;c&#x27;: -0.5083907212, &#x27;d&#x27;: 6.6778105051},
        24576.0: {&#x27;b&#x27;: 0.0128570922, &#x27;c&#x27;: -0.5238156652, &#x27;d&#x27;: 6.9554173182},
        32768.0: {&#x27;b&#x27;: 0.0118341620, &#x27;c&#x27;: -0.5321262189, &#x27;d&#x27;: 7.1667365668},
        48128.0: {&#x27;b&#x27;: 0.0572019544, &#x27;c&#x27;: -0.5462202420, &#x27;d&#x27;: 6.6517005780},
        64512.0: {&#x27;b&#x27;: 0.0299395040, &#x27;c&#x27;: -0.5454710483, &#x27;d&#x27;: 7.1626047194},
    }

    results = []

    for data_point in input_data:
        vocab_size = data_point[&#x27;vocab_size&#x27;]
        non_vocab_parameters = data_point[&#x27;non_vocab_parameters&#x27;]
        num_characters = data_point[&#x27;num_characters&#x27;]

        # Find the group parameters for this vocab_size
        # If exact match not found, use the closest vocab_size
        if vocab_size in group_params:
            params = group_params[vocab_size]
        else:
            # Find closest vocab_size in available groups
            available_sizes = list(group_params.keys())
            closest_size = min(available_sizes, key=lambda x: abs(x - vocab_size))
            params = group_params[closest_size]

        b = params[&#x27;b&#x27;]
        c = params[&#x27;c&#x27;]
        d = params[&#x27;d&#x27;]

        # Calculate prediction using the scaling law
        # loss = a*log(vocab_size) + b*log(non_vocab_parameters) + c*log(num_characters) + d
        prediction = (
            a * math.log(vocab_size) +
            b * math.log(non_vocab_parameters) +
            c * math.log(num_characters) +
            d
        )

        results.append({&#x27;unigram_normalized_loss&#x27;: prediction})

    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.861121 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">import numpy as np


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # Coefficients fitted for the &#x27;all_data&#x27; group
    # Formula: loss = c + b1*log10(vocab_size) + b2*log10(non_vocab_parameters) + b3*log10(num_characters)

    # Group-specific coefficients (currently only &#x27;all_data&#x27; group in the training data)
    group_coefficients = {
        &#x27;all_data&#x27;: {
            &#x27;constant&#x27;: 6.3805912366,
            &#x27;vocab_size&#x27;: 0.1459881172,
            &#x27;non_vocab_parameters&#x27;: 0.0377878723,
            &#x27;num_characters&#x27;: -1.1552084671,
        }
    }

    # Get coefficients for the requested group
    # If group not found, use &#x27;all_data&#x27; as fallback
    if group in group_coefficients:
        coeffs = group_coefficients[group]
    else:
        # Fallback to &#x27;all_data&#x27; if group not found
        coeffs = group_coefficients[&#x27;all_data&#x27;]

    results = []

    for data_point in input_data:
        # Extract input variables
        vocab_size = data_point.get(&#x27;vocab_size&#x27;)
        non_vocab_parameters = data_point.get(&#x27;non_vocab_parameters&#x27;)
        num_characters = data_point.get(&#x27;num_characters&#x27;)

        # Compute prediction using the fitted formula
        # loss = c + b1*log10(vocab_size) + b2*log10(non_vocab_parameters) + b3*log10(num_characters)
        prediction = (
            coeffs[&#x27;constant&#x27;]
            + coeffs[&#x27;vocab_size&#x27;] * np.log10(vocab_size)
            + coeffs[&#x27;non_vocab_parameters&#x27;] * np.log10(non_vocab_parameters)
            + coeffs[&#x27;num_characters&#x27;] * np.log10(num_characters)
        )

        results.append({
            &#x27;unigram_normalized_loss&#x27;: float(prediction)
        })

    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.861120 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">import math


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The scaling law is based on a log-linear regression model that relates the
    unigram-normalized loss to three input variables: vocabulary size, non-vocabulary
    parameters, and number of characters.

    Formula: loss = a + b*ln(vocab_size) + c*ln(non_vocab_parameters) + d*ln(num_characters)

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law must be the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;

    # Fitted parameters for the &quot;all_data&quot; group
    # These coefficients are derived from log-linear regression on the training dataset
    params = {
        &quot;all_data&quot;: {
            &quot;intercept&quot;: 6.380591,
            &quot;vocab_size_coeff&quot;: 0.063402,
            &quot;non_vocab_parameters_coeff&quot;: 0.016411,
            &quot;num_characters_coeff&quot;: -0.501701,
        }
    }

    # Use the parameters for the specified group; default to &quot;all_data&quot; if not found
    if group not in params:
        group = &quot;all_data&quot;

    coefficients = params[group]

    # Make predictions for each data point
    predictions = []
    for data_point in input_data:
        vocab_size = data_point.get(&quot;vocab_size&quot;, 1.0)
        non_vocab_parameters = data_point.get(&quot;non_vocab_parameters&quot;, 1.0)
        num_characters = data_point.get(&quot;num_characters&quot;, 1.0)

        # Compute the prediction using the log-linear formula
        # loss = a + b*ln(vocab_size) + c*ln(non_vocab_parameters) + d*ln(num_characters)
        predicted_loss = (
            coefficients[&quot;intercept&quot;]
            + coefficients[&quot;vocab_size_coeff&quot;] * math.log(vocab_size)
            + coefficients[&quot;non_vocab_parameters_coeff&quot;] * math.log(non_vocab_parameters)
            + coefficients[&quot;num_characters_coeff&quot;] * math.log(num_characters)
        )

        predictions.append({
            &quot;unigram_normalized_loss&quot;: predicted_loss
        })

    return predictions</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
