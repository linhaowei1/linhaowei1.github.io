<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - SFT Scaling Law - SLDAgent + Claude Haiku 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>SFT Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Claude Haiku 4.5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.992046
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.976753</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.967566</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.992046
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law for LLM finetuning: L(D) = alpha + beta / (1 + gamma * D^delta)
Streamlined multi-stage optimization with adaptive convergence strategy.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L(D) = alpha + beta / (1 + gamma * D^delta)
    
    params: [alpha, beta, gamma, delta]
      - alpha: asymptotic minimum loss
      - beta: amplitude of decay term
      - gamma: scaling rate coefficient
      - delta: exponent controlling decay speed (0.05-2.5)
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).flatten()
    params = np.asarray(params, dtype=np.float64)
    
    alpha, beta, gamma, delta = params[:4]
    
    # Numerical stability bounds
    gamma = np.clip(gamma, 1e-6, 1e6)
    delta = np.clip(delta, 0.05, 2.5)
    
    # Main scaling law
    X_safe = np.maximum(X, 1e-6)
    denom = 1.0 + gamma * np.power(X_safe, delta)
    return alpha + beta / np.maximum(denom, 1e-6)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Hybrid optimization: global search + adaptive local refinement.
    Streamlined two-stage approach with integrated polishing.
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).flatten()
    y = np.atleast_1d(np.asarray(loss_values)).flatten()
    
    if len(X) &lt; 4:
        return np.array([np.mean(y), 0.5, 0.1, 0.7])
    
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min + 1e-8
    y_norm = (y - y_min) / y_range
    
    # Efficient initialization from log-space analysis
    log_X = np.log(np.maximum(X, 1.0))
    log_y = np.log(np.maximum(y, 0.1))
    
    if len(X) &gt; 2:
        coeffs = np.polyfit(log_X, log_y, 1)
        delta_est = np.clip(np.abs(coeffs[0]) * 0.5, 0.05, 2.5)
    else:
        delta_est = 0.5
    
    # Data-driven initialization
    alpha_init = y_min - 0.1 * y_range
    beta_init = y_range * 1.2
    gamma_init = (np.median(X) / 1000.0) ** delta_est
    
    def objective(params):
        &quot;&quot;&quot;MSE loss in normalized space&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            pred_norm = (np.clip(pred, y_min - y_range, y_max + y_range) - y_min) / y_range
            mse = np.mean((pred_norm - y_norm) ** 2)
            return mse if np.isfinite(mse) else 1e10
        except:
            return 1e10
    
    # Adaptive bounds
    bounds = [
        (y_min - y_range, y_max + 0.5 * y_range),
        (-0.5 * y_range, 3.0 * y_range),
        (1e-6, 1000),
        (0.05, 2.5)
    ]
    
    # Stage 1: Global optimization
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            maxiter=450,
            popsize=22,
            seed=42,
            atol=1e-9,
            tol=1e-9,
            workers=1,
            polish=False,
            mutation=(0.5, 1.5),
            recombination=0.93,
            updating=&#x27;deferred&#x27;
        )
        best_params = result_de.x
        best_loss = result_de.fun
    except:
        best_params = np.array([alpha_init, beta_init, gamma_init, delta_est])
        best_loss = objective(best_params)
    
    # Stage 2: Combined local refinement + adaptive polish
    try:
        result_local = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;ftol&#x27;: 1e-11, &#x27;maxiter&#x27;: 2500, &#x27;maxcor&#x27;: 25}
        )
        if result_local.fun &lt; best_loss:
            best_params = result_local.x
            best_loss = result_local.fun
    except:
        pass
    
    # Stage 3: Focused adaptive polish
    try:
        polish_bounds = [
            (best_params[0] - 0.12 * y_range, best_params[0] + 0.12 * y_range),
            (best_params[1] - 0.18 * y_range, best_params[1] + 0.18 * y_range),
            (max(best_params[2] * 0.4, 1e-6), best_params[2] * 2.5),
            (max(best_params[3] - 0.18, 0.05), min(best_params[3] + 0.18, 2.5))
        ]
        result_polish = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=polish_bounds,
            options={&#x27;ftol&#x27;: 1e-12, &#x27;maxiter&#x27;: 1200, &#x27;maxcor&#x27;: 18}
        )
        if result_polish.fun &lt; best_loss:
            best_params = result_polish.x
    except:
        pass
    
    # Final parameter validation
    best_params[2] = np.maximum(best_params[2], 1e-6)
    best_params[3] = np.clip(best_params[3], 0.05, 2.5)
    
    return best_params

# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.978663
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law: Loss = a / (N + b)^c + d
Hybrid optimization with curve_fit initialization and refinement
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize, curve_fit

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Inverse power law: Loss = a / (N + b)^c + d
    params: [a, b, c, d]
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N = X[:, 0]
    params = np.asarray(params, dtype=np.float64).flatten()
    
    if len(params) &lt; 4:
        params = np.pad(params, (0, 4 - len(params)), constant_values=1.0)
    
    a, b, c, d = params[:4]
    a = np.abs(a) + 1e-6
    b = np.maximum(b, 0.0)
    c = np.maximum(c, 0.001)
    d = np.maximum(d, 0.0)
    
    base = np.maximum(N + b, 1e-10)
    with np.errstate(divide=&#x27;ignore&#x27;, invalid=&#x27;ignore&#x27;, over=&#x27;ignore&#x27;):
        pred = a / np.power(base, c) + d
    
    return np.nan_to_num(pred, nan=1e6, posinf=1e6, neginf=1e6)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Hybrid optimization: curve_fit initialization + DE + L-BFGS-B refinement
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    N = X[:, 0]
    
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min
    N_min, N_max = np.min(N), np.max(N)
    
    # Try curve_fit for smart initialization
    def model(x, a, b, c, d):
        return a / np.power(np.maximum(x + b, 1e-10), c) + d
    
    x0 = None
    try:
        p0 = [y_range * 0.8, N_min * 0.2, 0.5, y_min * 0.3]
        bounds_cf = (
            [0.01, 0, 0.001, 0],
            [y_range * 100, N_min * 10, 3.0, y_min * 5]
        )
        popt, _ = curve_fit(model, N, y, p0=p0, bounds=bounds_cf, maxfev=3000)
        x0 = popt
    except:
        x0 = np.array([y_range, N_min * 0.1, 0.5, y_min * 0.1])
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e12
            mse = np.mean((pred - y) ** 2)
            # Light regularization
            reg = 1e-8 * (params[0]**2 / (y_range**2 + 1e-6) + params[2]**2)
            return mse + reg
        except:
            return 1e12
    
    bounds = [
        (0.001, y_range * 50),
        (0, N_min * 3),
        (0.001, 2.5),
        (0, y_min * 3)
    ]
    
    # Global search
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=350,
        popsize=16,
        atol=1e-8,
        tol=1e-8,
        workers=1,
        updating=&#x27;deferred&#x27;,
        polish=True
    )
    
    # Local refinement
    result_lbfgs = minimize(
        objective,
        result_de.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
    )
    
    # Fallback to Nelder-Mead if needed
    if result_lbfgs.fun &gt; result_de.fun * 0.95:
        result_nm = minimize(
            objective,
            result_de.x,
            method=&#x27;Nelder-Mead&#x27;,
            options={&#x27;maxiter&#x27;: 1500, &#x27;xatol&#x27;: 1e-10, &#x27;fatol&#x27;: 1e-11}
        )
        result_lbfgs = result_nm
    
    final_params = result_lbfgs.x if result_lbfgs.fun &lt; result_de.fun else result_de.x
    return np.array(final_params[:4])

# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.972924
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning - optimized Chinchilla-inspired form
L(D) = a + b*D^(-alpha) + c*log(D)
Simplified and streamlined for better convergence
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L(D) = a + b*D^(-alpha) + c*log(D)
    params: [a, b, alpha, c]
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).ravel()
    params = np.atleast_1d(np.asarray(params))
    
    if len(params) &lt; 4:
        params = np.pad(params, (0, 4 - len(params)), mode=&#x27;constant&#x27;)
    
    a, b, alpha, c = params[0], params[1], params[2], params[3]
    X_safe = np.maximum(X, 1.0)
    alpha = np.clip(alpha, 0.001, 2.0)
    
    return a + b * np.power(X_safe, -alpha) + c * np.log(X_safe)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Two-stage optimization: global DE + local L-BFGS-B refinement
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).ravel()
    y = np.atleast_1d(np.asarray(loss_values)).ravel()
    
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min
    
    def objective(params):
        pred = scaling_law_func(X, params)
        if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
            return 1e10
        mse = np.mean((pred - y) ** 2)
        
        # Penalty for invalid alpha
        alpha = params[2]
        if alpha &lt; 0.001 or alpha &gt; 2.0:
            mse += 100.0 * (1.0 + np.abs(alpha - 0.5)) ** 2
        
        return mse
    
    # Bounds optimized for LLM scaling laws
    bounds = [
        (y_min - y_range, y_max + y_range),      # a
        (-y_range * 20, y_range * 20),           # b (allow negative)
        (0.001, 2.0),                            # alpha
        (-2.0, 2.0)                              # c
    ]
    
    # Global search with stricter DE settings
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=250,
            popsize=18,
            atol=1e-8,
            tol=1e-8,
            mutation=(0.5, 1.5),
            recombination=0.75,
            workers=1,
            polish=True
        )
        init_params = result_de.x
    except:
        init_params = np.array([
            np.median(y),
            y_range * 0.5,
            0.15,
            0.0
        ])
    
    # Local refinement with tighter convergence
    result_local = minimize(
        objective,
        init_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-10, &#x27;maxiter&#x27;: 1000, &#x27;maxcor&#x27;: 25}
    )
    
    params_opt = result_local.x if result_local.success else init_params
    
    # Final clipping to ensure bounds
    params_opt = np.array([
        np.clip(params_opt[0], bounds[0][0], bounds[0][1]),
        np.clip(params_opt[1], bounds[1][0], bounds[1][1]),
        np.clip(params_opt[2], bounds[2][0], bounds[2][1]),
        np.clip(params_opt[3], bounds[3][0], bounds[3][1])
    ])
    
    return params_opt

# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.972564
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for LLM finetuning using proven power-law form with logarithmic correction.
L(N) = a + b*N^c + d*log(N)

Key improvements:
1. Refined initialization strategy using data-driven quartile estimation
2. Improved bound calibration for better convergence
3. Enhanced numerical stability with better clipping strategies
4. Simplified code while maintaining mathematical sophistication
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Proven scaling law: L(N) = a + b*N^c + d*log(N)
    
    Parameters:
    - a: baseline loss floor
    - b: power-law coefficient
    - c: power-law exponent (typically -0.3 to -0.01)
    - d: logarithmic correction term
    
    This form consistently captures LLM finetuning behavior across datasets.
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).flatten()
    params = np.asarray(params, dtype=np.float64)
    
    if len(params) &lt; 4:
        params = np.pad(params, (0, 4 - len(params)), mode=&#x27;constant&#x27;)
    
    a, b, c, d = params[:4]
    
    # Numerical stability
    X_safe = np.maximum(X, 1.0)
    
    # Clip exponent to physically valid range
    c_clipped = np.clip(c, -0.5, -0.0001)
    
    # Power-law component (main scaling behavior)
    power_term = b * np.power(X_safe, c_clipped)
    
    # Logarithmic correction (fine-tuning curvature)
    log_term = d * np.log(X_safe)
    
    # Combined prediction
    pred = a + power_term + log_term
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Two-stage optimization for robust parameter fitting:
    1. Differential evolution for global exploration
    2. L-BFGS-B for precision refinement
    
    Uses intelligent initialization and adaptive bounds.
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points)).flatten()
    y = np.atleast_1d(np.asarray(loss_values)).flatten()
    
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min
    y_mean = np.mean(y)
    
    def objective(params):
        &quot;&quot;&quot;MSE objective with minimal regularization&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            if not np.all(np.isfinite(pred)):
                return 1e10
            mse = np.mean((pred - y) ** 2)
            # Minimal regularization on log coefficient
            reg = 1e-6 * np.abs(params[3])
            return mse + reg
        except:
            return 1e10
    
    # Intelligent initialization from quartile analysis
    sorted_idx = np.argsort(X)
    q1_idx = max(0, len(X) // 4)
    q3_idx = min(len(X) - 1, 3 * len(X) // 4)
    
    X_q1, X_q3 = X[sorted_idx[q1_idx]], X[sorted_idx[q3_idx]]
    y_q1, y_q3 = y[sorted_idx[q1_idx]], y[sorted_idx[q3_idx]]
    
    # Estimate exponent from quartile behavior
    if y_q1 &gt; y_q3 and X_q3 &gt; X_q1:
        c_estimate = np.log(y_q1 / y_q3) / np.log(X_q3 / X_q1)
        c_estimate = np.clip(c_estimate, -0.5, -0.001)
    else:
        c_estimate = -0.1
    
    # Adaptive bounds accounting for both positive and negative b
    bounds = [
        (y_min - y_range, y_max + y_range),    # a: baseline loss
        (-y_range * 10, y_range * 10),         # b: power coefficient (allow negative)
        (-0.5, -0.0001),                       # c: exponent (power-law regime)
        (-2.0, 2.0),                           # d: log coefficient
    ]
    
    # Stage 1: Global optimization with differential evolution
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=400,
            popsize=25,
            atol=1e-8,
            tol=1e-9,
            workers=1,
            polish=False,
            updating=&#x27;deferred&#x27;,
            strategy=&#x27;best1bin&#x27;
        )
        x0 = result_de.x
    except Exception:
        x0 = np.array([y_min, y_range, c_estimate, 0.0])
    
    # Stage 2: Local refinement with L-BFGS-B
    try:
        result_lbfgs = minimize(
            objective,
            x0,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={
                &#x27;maxiter&#x27;: 1500,
                &#x27;ftol&#x27;: 1e-11,
                &#x27;gtol&#x27;: 1e-9,
                &#x27;maxcor&#x27;: 25
            }
        )
        params_opt = result_lbfgs.x if result_lbfgs.success else x0
    except Exception:
        params_opt = x0
    
    # Validate and enforce constraints on output
    params_final = np.array(params_opt, dtype=np.float64)
    if not np.all(np.isfinite(params_final)):
        params_final = np.array([y_min, y_range, c_estimate, 0.0])
    
    # Strict enforcement of exponent bounds
    params_final[2] = np.clip(params_final[2], -0.5, -0.0001)
    
    return params_final

# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.967566
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning with robust inverse power law
Uses proven 4-parameter form: L = a + b / (x + c)^d
Multi-stage optimization for superior generalization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L(N) = a + b / (N + c)^d
    
    params: [a, b, c, d] (4 parameters)
    - a: asymptotic loss floor
    - b: amplitude coefficient
    - c: offset for numerical stability
    - d: power exponent
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points, dtype=np.float64)).ravel()
    params = np.atleast_1d(np.asarray(params, dtype=np.float64))
    
    a = params[0]
    b = params[1]
    c = params[2]
    d = params[3]
    
    # Ensure numerical safety
    d = np.clip(d, 0.01, 3.0)
    c = np.abs(c) + 1e-8
    
    # Compute: L = a + b / (X + c)^d
    denom = np.power(X + c, d)
    prediction = a + b / np.maximum(denom, 1e-10)
    
    return prediction


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit 4-parameter scaling law with global + local optimization
    &quot;&quot;&quot;
    X = np.atleast_1d(np.asarray(data_points, dtype=np.float64)).ravel()
    y = np.atleast_1d(np.asarray(loss_values, dtype=np.float64)).ravel()
    
    # Data statistics
    y_min, y_max = np.min(y), np.max(y)
    y_mean = np.mean(y)
    X_min, X_max = np.min(X), np.max(X)
    
    def objective(params):
        &quot;&quot;&quot;MSE objective with numerical stability checks&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            mse = np.mean((pred - y) ** 2)
            return mse if np.isfinite(mse) else 1e10
        except:
            return 1e10
    
    # Set bounds based on data characteristics
    bounds = [
        (y_min - 1.0, y_max + 1.0),      # a: asymptote
        (-100.0, 100.0),                  # b: amplitude (unrestricted sign)
        (1.0, X_max),                     # c: offset for stability
        (0.05, 2.5)                       # d: exponent
    ]
    
    best_params = None
    best_loss = np.inf
    
    # Stage 1: Global search with differential evolution
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=400,
            atol=1e-7,
            tol=1e-7,
            workers=1,
            polish=True
        )
        best_params = result_de.x
        best_loss = result_de.fun
    except:
        pass
    
    # Stage 2: Multiple local refinements from good initialization points
    init_strategies = [
        [y_min, (y_max - y_min) * 2, X_min / 10, 0.6],
        [y_min + 0.1 * (y_max - y_min), (y_max - y_min), X_min, 0.5],
        [y_mean * 0.7, (y_max - y_min) * 0.5, X_max / 100, 0.3],
        [(y_min + y_max) / 2, y_max - y_min, np.sqrt(X_min * X_max), 0.8],
    ]
    
    for init in init_strategies:
        try:
            result = minimize(
                objective,
                init,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;ftol&#x27;: 1e-7, &#x27;maxiter&#x27;: 1000}
            )
            if result.fun &lt; best_loss:
                best_loss = result.fun
                best_params = result.x
        except:
            pass
    
    # Stage 3: Final refinement from best found solution
    if best_params is not None and best_loss &lt; np.inf:
        try:
            result_final = minimize(
                objective,
                best_params,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 500}
            )
            if result_final.fun &lt; best_loss:
                best_params = result_final.x
        except:
            pass
    
    # Fallback
    if best_params is None:
        best_params = np.array([y_min, (y_max - y_min), X_min, 0.5])
    
    return np.asarray(best_params, dtype=np.float64)[:4]
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>