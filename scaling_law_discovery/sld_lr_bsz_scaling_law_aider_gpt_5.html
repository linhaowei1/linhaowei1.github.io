<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - LR & Batch Size Scaling Law - aider + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>LR & Batch Size Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">aider</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #d2691e; color: white"> 0.025235 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">-0.659042</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">-1.000000</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.025235 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">from __future__ import annotations

import json
import math
import os
from typing import Dict, List

import numpy as np

# Attempt to import datasets lazily. If unavailable, we can still run with a fallback.
try:
    from datasets import load_from_disk  # type: ignore
except Exception:  # pragma: no cover
    load_from_disk = None  # type: ignore


# Global container for fitted coefficients, filled at import-time (lazily) from /app/data if available.
# Keys are group names; values are dicts with the coefficients.
COEFFS: Dict[str, Dict[str, float]] = {}
GLOBAL_GROUP_KEY = &quot;__ALL__&quot;

# Numerical safety epsilon for logs
_EPS = 1e-12

# Description of the functional form
FORMULA_DESC = (
    &quot;log(lm_loss) = beta0_g + a_g*log(lr) + a2_g*(log(lr))^2 + &quot;
    &quot;b_g*log(bsz) + c_g*log(data_size) + d_g*log(non_embedding_param_size)\n&quot;
    &quot;=&gt; lm_loss = exp(beta0_g + a_g*log(lr) + a2_g*(log(lr))^2 + &quot;
    &quot;b_g*log(bsz) + c_g*log(data_size) + d_g*log(non_embedding_param_size))&quot;
)


def _safe_log(x: float) -&gt; float:
    return math.log(max(float(x), _EPS))


def _design_row(lr: float, bsz: float, data_size: float, non_emb_params: float) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Build a single feature row for the regression:
    [1, log(lr), (log(lr))^2, log(bsz), log(data_size), log(non_embedding_param_size)]
    &quot;&quot;&quot;
    z_lr = _safe_log(lr)
    return np.array(
        [
            1.0,
            z_lr,
            z_lr * z_lr,
            _safe_log(bsz),
            _safe_log(data_size),
            _safe_log(non_emb_params),
        ],
        dtype=np.float64,
    )


def _fit_group(X: np.ndarray, y: np.ndarray, lam: float = 1e-6) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Ridge-regularized least squares:
        (X^T X + lam I) w = X^T y
    &quot;&quot;&quot;
    XT = X.T
    A = XT @ X
    # Ridge on all parameters including bias (small lam)
    A[np.diag_indices_from(A)] += lam
    b = XT @ y
    w = np.linalg.solve(A, b)
    return w


def _extract_dataset_rows(ds_item: dict) -&gt; tuple[float, float, float, float, float, str | None]:
    &quot;&quot;&quot;
    Extract lr, bsz, data_size, non_embedding_param_size, lm_loss, group (if present) from a dataset item.
    Returns tuple: (lr, bsz, data_size, non_emb_params, lm_loss, group)
    &quot;&quot;&quot;
    lr = float(ds_item.get(&quot;lr&quot;))
    bsz = float(ds_item.get(&quot;bsz&quot;))
    data_size = float(ds_item.get(&quot;data_size&quot;))
    non_emb = float(ds_item.get(&quot;non_embedding_param_size&quot;))
    lm_loss = float(ds_item.get(&quot;lm_loss&quot;))
    group = ds_item.get(&quot;group&quot;)
    if group is not None:
        group = str(group)
    return lr, bsz, data_size, non_emb, lm_loss, group


def _load_and_fit(path: str = &quot;/app/data&quot;) -&gt; Dict[str, Dict[str, float]]:
    &quot;&quot;&quot;
    Load dataset from disk and fit per-group coefficients according to FORMULA_DESC.
    If datasets API is not available or loading fails, return a robust default.
    &quot;&quot;&quot;
    coeffs: Dict[str, Dict[str, float]] = {}

    if load_from_disk is None:
        # Fallback: very conservative defaults (weak dependence)
        coeffs[GLOBAL_GROUP_KEY] = {
            &quot;beta0&quot;: 1.0,
            &quot;a_lr&quot;: 0.0,
            &quot;a2_lr2&quot;: 0.1,
            &quot;b_bsz&quot;: -0.02,
            &quot;c_data&quot;: -0.1,
            &quot;d_param&quot;: -0.1,
        }
        return coeffs

    # Load dataset (can be Dataset or DatasetDict)
    try:
        ds = load_from_disk(path)
    except Exception:
        # Fallback defaults if loading fails
        coeffs[GLOBAL_GROUP_KEY] = {
            &quot;beta0&quot;: 1.0,
            &quot;a_lr&quot;: 0.0,
            &quot;a2_lr2&quot;: 0.1,
            &quot;b_bsz&quot;: -0.02,
            &quot;c_data&quot;: -0.1,
            &quot;d_param&quot;: -0.1,
        }
        return coeffs

    # Collect all rows across splits if needed
    rows = []
    if hasattr(ds, &quot;values&quot;):  # DatasetDict
        for split in ds.values():
            rows.extend(list(split))
    else:  # Single Dataset
        rows = list(ds)

    # Partition by group (or GLOBAL group if group missing)
    groups: Dict[str, list[tuple[float, float, float, float, float]]] = {}
    for it in rows:
        try:
            lr, bsz, data_size, non_emb, lm_loss, group = _extract_dataset_rows(it)
        except Exception:
            continue

        # Filter invalid values
        if not all(v is not None for v in (lr, bsz, data_size, non_emb, lm_loss)):
            continue
        if lr &lt;= 0 or bsz &lt;= 0 or data_size &lt;= 0 or non_emb &lt;= 0 or lm_loss &lt;= 0:
            continue

        gname = group if group is not None else GLOBAL_GROUP_KEY
        groups.setdefault(gname, []).append((lr, bsz, data_size, non_emb, lm_loss))

    # If no groups found, bail to fallback
    if not groups:
        coeffs[GLOBAL_GROUP_KEY] = {
            &quot;beta0&quot;: 1.0,
            &quot;a_lr&quot;: 0.0,
            &quot;a2_lr2&quot;: 0.1,
            &quot;b_bsz&quot;: -0.02,
            &quot;c_data&quot;: -0.1,
            &quot;d_param&quot;: -0.1,
        }
        return coeffs

    # Also fit a global group across all data to use as fallback for unknown groups
    all_data = [rec for glist in groups.values() for rec in glist]
    groups_with_global = dict(groups)
    groups_with_global[GLOBAL_GROUP_KEY] = all_data

    # Fit per group
    for gname, glist in groups_with_global.items():
        if len(glist) &lt; 6:  # Need at least as many points as parameters for a good fit
            continue
        X = np.vstack([_design_row(*rec[:4]) for rec in glist])  # n x 6
        y = np.array([_safe_log(rec[4]) for rec in glist], dtype=np.float64)  # log(lm_loss)

        try:
            w = _fit_group(X, y, lam=1e-6)
        except np.linalg.LinAlgError:
            # Very small increase in regularization if ill-conditioned
            w = _fit_group(X, y, lam=1e-3)

        coeffs[gname] = {
            &quot;beta0&quot;: float(w[0]),
            &quot;a_lr&quot;: float(w[1]),
            &quot;a2_lr2&quot;: float(w[2]),
            &quot;b_bsz&quot;: float(w[3]),
            &quot;c_data&quot;: float(w[4]),
            &quot;d_param&quot;: float(w[5]),
        }

    # In rare case fitting failed for some groups, ensure we at least have a global fallback
    if GLOBAL_GROUP_KEY not in coeffs:
        # Fit a quick global from whatever we have (if any), else use defaults
        if all_data:
            X = np.vstack([_design_row(*rec[:4]) for rec in all_data])
            y = np.array([_safe_log(rec[4]) for rec in all_data], dtype=np.float64)
            try:
                w = _fit_group(X, y, lam=1e-6)
            except np.linalg.LinAlgError:
                w = _fit_group(X, y, lam=1e-3)
            coeffs[GLOBAL_GROUP_KEY] = {
                &quot;beta0&quot;: float(w[0]),
                &quot;a_lr&quot;: float(w[1]),
                &quot;a2_lr2&quot;: float(w[2]),
                &quot;b_bsz&quot;: float(w[3]),
                &quot;c_data&quot;: float(w[4]),
                &quot;d_param&quot;: float(w[5]),
            }
        else:
            coeffs[GLOBAL_GROUP_KEY] = {
                &quot;beta0&quot;: 1.0,
                &quot;a_lr&quot;: 0.0,
                &quot;a2_lr2&quot;: 0.1,
                &quot;b_bsz&quot;: -0.02,
                &quot;c_data&quot;: -0.1,
                &quot;d_param&quot;: -0.1,
            }

    return coeffs


def _write_explain_md(coeffs: Dict[str, Dict[str, float]], path: str = &quot;/app/explain.md&quot;) -&gt; None:
    &quot;&quot;&quot;
    Generate a detailed explanation file including the functional form and fitted coefficients.
    &quot;&quot;&quot;
    lines: List[str] = []
    lines.append(&quot;# Scaling Law for Final Language Modeling Loss\n&quot;)
    lines.append(&quot;This document describes the discovered scaling law relating the final language modeling loss (lm_loss) to training hyperparameters.\n&quot;)
    lines.append(&quot;## Functional Form\n&quot;)
    lines.append(&quot;We fit a log-linear model with a quadratic term in log(learning rate) to capture the typical U-shaped dependence on learning rate:\n&quot;)
    lines.append(&quot;log(lm_loss) = beta0_g + a_g*log(lr) + a2_g*(log(lr))^2 + b_g*log(bsz) + c_g*log(data_size) + d_g*log(non_embedding_param_size)\n&quot;)
    lines.append(&quot;\nEquivalently:\n&quot;)
    lines.append(&quot;lm_loss = exp(beta0_g + a_g*log(lr) + a2_g*(log(lr))^2 + b_g*log(bsz) + c_g*log(data_size) + d_g*log(non_embedding_param_size))\n&quot;)
    lines.append(&quot;\n- g denotes the experimental group. The functional form is identical across groups, while coefficients vary per group.\n&quot;)
    lines.append(&quot;\n## Methodology\n&quot;)
    lines.append(&quot;- Loaded the dataset from `/app/data` using `datasets.load_from_disk()`.\n&quot;)
    lines.append(&quot;- Filtered rows to ensure all variables are positive (required for logarithms).\n&quot;)
    lines.append(&quot;- Regressed log(lm_loss) on [1, log(lr), (log(lr))^2, log(bsz), log(data_size), log(non_embedding_param_size)] using ridge-regularized least squares (λ = 1e-6).\n&quot;)
    lines.append(&quot;- Fitted the model per group and also a global model across all data as a fallback.\n&quot;)
    lines.append(&quot;\n## Fitted Coefficients by Group\n&quot;)
    lines.append(&quot;The following coefficients were fitted programmatically at import time of `law.py`:\n&quot;)
    lines.append(&quot;\n&quot;)
    # Nicely format coefficients per group
    # Sort groups, showing GLOBAL first if present
    keys = list(coeffs.keys())
    if GLOBAL_GROUP_KEY in keys:
        keys.remove(GLOBAL_GROUP_KEY)
        keys = [GLOBAL_GROUP_KEY] + sorted(keys)
    else:
        keys = sorted(keys)
    for g in keys:
        c = coeffs[g]
        lines.append(f&quot;### Group: {g}\n&quot;)
        lines.append(f&quot;- beta0: {c[&#x27;beta0&#x27;]:.8f}\n&quot;)
        lines.append(f&quot;- a (log lr): {c[&#x27;a_lr&#x27;]:.8f}\n&quot;)
        lines.append(f&quot;- a2 (log lr)^2: {c[&#x27;a2_lr2&#x27;]:.8f}\n&quot;)
        lines.append(f&quot;- b (log bsz): {c[&#x27;b_bsz&#x27;]:.8f}\n&quot;)
        lines.append(f&quot;- c (log data_size): {c[&#x27;c_data&#x27;]:.8f}\n&quot;)
        lines.append(f&quot;- d (log non_embedding_param_size): {c[&#x27;d_param&#x27;]:.8f}\n&quot;)
        lines.append(&quot;\n&quot;)

    try:
        with open(path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(&quot;\n&quot;.join(lines))
    except Exception:
        # If writing fails, silently ignore (not critical for predictions)
        pass


def _ensure_fitted() -&gt; None:
    &quot;&quot;&quot;
    Ensure that COEFFS is populated. If empty, attempt to load and fit.
    Also writes/updates /app/explain.md with the fitted coefficients.
    &quot;&quot;&quot;
    global COEFFS
    if COEFFS:
        return
    coeffs = _load_and_fit(&quot;/app/data&quot;)
    COEFFS = coeffs
    # Best-effort write explain.md so the fitted numbers are visible
    _write_explain_md(COEFFS, &quot;/app/explain.md&quot;)


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Required keys per item:
                    - &#x27;lr&#x27;
                    - &#x27;bsz&#x27;
                    - &#x27;data_size&#x27;
                    - &#x27;non_embedding_param_size&#x27;
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups, but the
               coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;lm_loss&#x27;: float}.
    &quot;&quot;&quot;
    _ensure_fitted()

    # Select coefficients for the requested group, fallback to global, then to any available group
    c = COEFFS.get(group)
    if c is None:
        c = COEFFS.get(GLOBAL_GROUP_KEY)
    if c is None and COEFFS:
        # Fallback to any one group deterministically
        any_group = sorted(COEFFS.keys())[0]
        c = COEFFS[any_group]
    if c is None:
        # Last-resort defaults (should not happen if fitting succeeded)
        c = {
            &quot;beta0&quot;: 1.0,
            &quot;a_lr&quot;: 0.0,
            &quot;a2_lr2&quot;: 0.1,
            &quot;b_bsz&quot;: -0.02,
            &quot;c_data&quot;: -0.1,
            &quot;d_param&quot;: -0.1,
        }

    beta0 = c[&quot;beta0&quot;]
    a_lr = c[&quot;a_lr&quot;]
    a2_lr2 = c[&quot;a2_lr2&quot;]
    b_bsz = c[&quot;b_bsz&quot;]
    c_data = c[&quot;c_data&quot;]
    d_param = c[&quot;d_param&quot;]

    outputs: list[dict[str, float]] = []
    for item in input_data:
        try:
            lr = float(item[&quot;lr&quot;])
            bsz = float(item[&quot;bsz&quot;])
            data_size = float(item[&quot;data_size&quot;])
            non_emb_params = float(item[&quot;non_embedding_param_size&quot;])
        except Exception as e:
            raise ValueError(&quot;Each input item must contain &#x27;lr&#x27;, &#x27;bsz&#x27;, &#x27;data_size&#x27;, and &#x27;non_embedding_param_size&#x27; as numeric values.&quot;) from e

        z_lr = _safe_log(lr)
        z_bsz = _safe_log(bsz)
        z_data = _safe_log(data_size)
        z_param = _safe_log(non_emb_params)

        ln_loss = (
            beta0
            + a_lr * z_lr
            + a2_lr2 * (z_lr * z_lr)
            + b_bsz * z_bsz
            + c_data * z_data
            + d_param * z_param
        )
        loss = float(math.exp(ln_loss))
        outputs.append({&quot;lm_loss&quot;: loss})

    return outputs


# Ensure coefficients are fitted when the module is imported, so explain.md is generated.
try:
    _ensure_fitted()
except Exception:
    # Don&#x27;t fail import in case of transient dataset issues; prediction will use defaults if necessary.
    pass</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -0.773483 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # Lazy-fit group-specific coefficients the first time this function is called.
    if not hasattr(law, &quot;_coefs&quot;):
        # Delayed imports to keep this file self-contained with a single function.
        import math
        import numpy as np
        from datasets import load_from_disk
        from typing import Dict, Any, Iterable

        # Load and prepare training data
        ds_obj = load_from_disk(&quot;/app/data&quot;)

        # Flatten to a single dataset (prefer &#x27;train&#x27; if present)
        try:
            from datasets import Dataset, DatasetDict, concatenate_datasets  # type: ignore
        except Exception:
            Dataset = object  # type: ignore
            DatasetDict = dict  # type: ignore

        if isinstance(ds_obj, dict) and &quot;train&quot; in ds_obj:
            ds = ds_obj[&quot;train&quot;]
        elif hasattr(ds_obj, &quot;keys&quot;):
            # Likely a DatasetDict without explicit &#x27;train&#x27;; merge all splits
            try:
                from datasets import concatenate_datasets  # type: ignore
                ds = None
                for k in ds_obj.keys():
                    ds = ds_obj[k] if ds is None else concatenate_datasets([ds, ds_obj[k]])
            except Exception:
                # Fallback: pick an arbitrary split
                first_key = next(iter(ds_obj.keys()))
                ds = ds_obj[first_key]
        else:
            ds = ds_obj  # Already a Dataset

        colnames = set(ds.column_names)

        # Required columns
        required = {&quot;lr&quot;, &quot;bsz&quot;, &quot;data_size&quot;, &quot;non_embedding_param_size&quot;, &quot;lm_loss&quot;}
        missing = [c for c in required if c not in colnames]
        if missing:
            raise KeyError(f&quot;Dataset at /app/data is missing required columns: {missing}&quot;)

        # Identify group column if present; otherwise treat as a single global group
        group_col = &quot;group&quot; if &quot;group&quot; in colnames else ( &quot;Group&quot; if &quot;Group&quot; in colnames else None )

        # Extract arrays
        lr = np.asarray(ds[&quot;lr&quot;], dtype=np.float64)
        bsz = np.asarray(ds[&quot;bsz&quot;], dtype=np.float64)
        data_size = np.asarray(ds[&quot;data_size&quot;], dtype=np.float64)
        nparam = np.asarray(ds[&quot;non_embedding_param_size&quot;], dtype=np.float64)
        lm_loss = np.asarray(ds[&quot;lm_loss&quot;], dtype=np.float64)
        groups = np.asarray(ds[group_col], dtype=object) if group_col is not None else np.asarray([&quot;__global__&quot;] * len(lm_loss), dtype=object)

        # Build design matrix using log-features
        eps = 1e-12
        x1 = np.log(np.clip(lr, eps, None))
        x2 = np.log(np.clip(bsz, eps, None))
        x3 = np.log(np.clip(data_size, eps, None))
        x4 = np.log(np.clip(nparam, eps, None))
        y = np.log(np.clip(lm_loss, eps, None))

        X = np.stack([np.ones_like(x1), x1, x2, x3, x4], axis=1)

        finite_mask = np.isfinite(X).all(axis=1) &amp; np.isfinite(y)
        X = X[finite_mask]
        y = y[finite_mask]
        groups = groups[finite_mask]

        if X.shape[0] &lt; 5:
            raise RuntimeError(&quot;Not enough valid training examples after filtering to fit the scaling law.&quot;)

        # Ridge-regularized closed-form solver
        def ridge_ols(Xm: np.ndarray, ym: np.ndarray, lam: float = 1e-6) -&gt; np.ndarray:
            XT = Xm.T
            A = XT @ Xm
            # Tikhonov regularization (do not penalize intercept excessively)
            I = np.eye(A.shape[0], dtype=Xm.dtype)
            I[0, 0] = 0.0
            A_reg = A + lam * I
            b = XT @ ym
            return np.linalg.solve(A_reg, b)

        # Global coefficients
        global_coef = ridge_ols(X, y, lam=1e-6)

        # Group-specific coefficients (same functional form, coefficients differ by group)
        coefs: Dict[str, np.ndarray] = {}
        unique_groups = np.unique(groups)
        for g in unique_groups:
            mask = (groups == g)
            # Require a minimal number of samples; otherwise fall back to global
            if np.count_nonzero(mask) &gt;= 5:
                try:
                    coefs[str(g)] = ridge_ols(X[mask], y[mask], lam=1e-6)
                except Exception:
                    coefs[str(g)] = global_coef
            else:
                coefs[str(g)] = global_coef

        # Cache for subsequent calls
        law._coefs = coefs  # type: ignore[attr-defined]
        law._global = global_coef  # type: ignore[attr-defined]

    # Prepare predictions
    import math
    import numpy as np

    eps = 1e-12

    # Pick coefficients for requested group
    coefs = getattr(law, &quot;_coefs&quot;)  # type: ignore[attr-defined]
    coef_vec = coefs.get(group, getattr(law, &quot;_global&quot;))  # type: ignore[attr-defined]

    def to_float(v: float) -&gt; float:
        try:
            return float(v)
        except Exception:
            return float(&quot;nan&quot;)

    preds: list[dict[str, float]] = []
    for row in input_data:
        lr = to_float(row.get(&quot;lr&quot;, float(&quot;nan&quot;)))
        bsz = to_float(row.get(&quot;bsz&quot;, float(&quot;nan&quot;)))
        data_size = to_float(row.get(&quot;data_size&quot;, float(&quot;nan&quot;)))
        nparam = to_float(row.get(&quot;non_embedding_param_size&quot;, float(&quot;nan&quot;)))

        if not (math.isfinite(lr) and math.isfinite(bsz) and math.isfinite(data_size) and math.isfinite(nparam)):
            preds.append({&quot;lm_loss&quot;: float(&quot;nan&quot;)})
            continue

        x = np.array(
            [
                1.0,
                math.log(max(lr, eps)),
                math.log(max(bsz, eps)),
                math.log(max(data_size, eps)),
                math.log(max(nparam, eps)),
            ],
            dtype=np.float64,
        )
        y_log = float(x.dot(coef_vec))
        y_hat = float(math.exp(y_log))
        preds.append({&quot;lm_loss&quot;: y_hat})

    return preds</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -0.773483 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">from __future__ import annotations

import math
from typing import Dict, List

import numpy as np

# We load lazily to avoid import-time dependency failures if datasets is unavailable in some contexts.
_DATASET_PATH = &quot;/app/data&quot;
_FEATURES = (&quot;lr&quot;, &quot;bsz&quot;, &quot;data_size&quot;, &quot;non_embedding_param_size&quot;)
_TARGET = &quot;lm_loss&quot;

# Global store for fitted parameters. Filled on first call to `law`.
PARAMS: Dict[str, Dict] | None = None


def _safe_log(x: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;Numerically stable natural log.&quot;&quot;&quot;
    return np.log(np.clip(x, 1e-12, None))


def _coerce_float(v) -&gt; float:
    try:
        return float(v)
    except Exception:
        # If coercion fails, return NaN; caller should handle.
        return math.nan


def _fit_group_power_with_offset(X: np.ndarray, y: np.ndarray) -&gt; Dict[str, float]:
    &quot;&quot;&quot;
    Fit y = L0 + A * prod_i X_i ** a_i via log-linear regression over a grid of L0.

    Args:
        X: shape (n, d) positive features (lr, bsz, data_size, non_embedding_param_size)
        y: shape (n,) target scalar (lm_loss)

    Returns:
        dict with keys: L0, A, exponents (list length d), rmse_log, intercept
    &quot;&quot;&quot;
    n, d = X.shape
    assert d == 4, &quot;Expected 4 features&quot;

    # Ensure strictly positive (for log)
    X = np.clip(X, 1e-12, None)
    y = np.asarray(y, dtype=float)

    # Precompute logs of X
    Xlog = np.column_stack([np.ones(n), _safe_log(X)])  # (n, d+1) including intercept
    Id = np.eye(d + 1)
    Id[0, 0] = 0.0  # don&#x27;t regularize intercept heavily

    # L0 grid: from 0 up to just below the min observed loss (reserve margin)
    y_min = float(np.nanmin(y))
    # Guard: if data are degenerate, fall back.
    if not np.isfinite(y_min) or y_min &lt;= 0:
        return {
            &quot;L0&quot;: 1.0,
            &quot;A&quot;: 1.0,
            &quot;exponents&quot;: [-0.05, -0.10, -0.20, -0.20],
            &quot;rmse_log&quot;: float(&quot;inf&quot;),
            &quot;intercept&quot;: 0.0,
        }

    # Create a grid that includes values close to 0 and close to y_min but less than it.
    # Use a mixture of linear and geometric spacing for robustness.
    n_lin = 25
    n_geo = 25
    lin_grid = np.linspace(0.0, max(0.0, 0.98 * y_min), num=n_lin, endpoint=False)
    # geometric grid avoids 0; start from a tiny fraction of y_min
    geo_start = max(1e-8, 1e-6 * y_min)
    geo_grid = np.geomspace(geo_start, 0.9 * y_min, num=n_geo, endpoint=True)
    L0_candidates = np.unique(np.clip(np.concatenate([lin_grid, geo_grid]), 0.0, y_min - 1e-12))

    best = {
        &quot;score&quot;: float(&quot;inf&quot;),
        &quot;L0&quot;: 0.0,
        &quot;w&quot;: np.zeros(d + 1),
    }
    # Ridge regularization for stability
    lam = 1e-8

    for L0 in L0_candidates:
        resid = y - L0
        # Must be strictly positive for log
        if np.any(resid &lt;= 0):
            continue
        z = _safe_log(resid)  # log(y - L0)
        # Solve (X^T X + lam I) w = X^T z
        XtX = Xlog.T @ Xlog + lam * Id
        Xtz = Xlog.T @ z
        try:
            w = np.linalg.solve(XtX, Xtz)
        except np.linalg.LinAlgError:
            w, *_ = np.linalg.lstsq(XtX, Xtz, rcond=None)
        z_hat = Xlog @ w
        mse = float(np.mean((z - z_hat) ** 2))  # MSE in log-space
        if mse &lt; best[&quot;score&quot;]:
            best.update({&quot;score&quot;: mse, &quot;L0&quot;: float(L0), &quot;w&quot;: w})

    w = best[&quot;w&quot;]
    L0 = float(best[&quot;L0&quot;])
    A = float(np.exp(w[0]))
    exponents = w[1:].tolist()
    return {
        &quot;L0&quot;: L0,
        &quot;A&quot;: A,
        &quot;exponents&quot;: [float(e) for e in exponents],
        &quot;rmse_log&quot;: float(best[&quot;score&quot;]) ** 0.5,
        &quot;intercept&quot;: float(w[0]),
    }


def _load_and_fit() -&gt; Dict[str, Dict]:
    &quot;&quot;&quot;
    Load the dataset from disk and fit parameters per group and a global fallback.

    Returns:
        Dict mapping group name -&gt; params dict
    &quot;&quot;&quot;
    try:
        from datasets import Dataset, DatasetDict, concatenate_datasets, load_from_disk  # type: ignore
    except Exception:
        # No datasets library available: return default generic parameters.
        return {
            &quot;_GLOBAL_&quot;: {
                &quot;L0&quot;: 1.0,
                &quot;A&quot;: 1.0,
                &quot;exponents&quot;: [-0.05, -0.10, -0.20, -0.20],
                &quot;rmse_log&quot;: float(&quot;inf&quot;),
                &quot;intercept&quot;: 0.0,
            }
        }

    try:
        ds = load_from_disk(_DATASET_PATH)
    except Exception:
        # Dataset not available; return defaults.
        return {
            &quot;_GLOBAL_&quot;: {
                &quot;L0&quot;: 1.0,
                &quot;A&quot;: 1.0,
                &quot;exponents&quot;: [-0.05, -0.10, -0.20, -0.20],
                &quot;rmse_log&quot;: float(&quot;inf&quot;),
                &quot;intercept&quot;: 0.0,
            }
        }

    # Merge splits if a DatasetDict
    if isinstance(ds, (dict,)):
        # Unexpected type, fallback: no data
        merged = None
    else:
        try:
            from datasets import DatasetDict as _DD  # noqa
            if isinstance(ds, _DD):
                merged = concatenate_datasets(list(ds.values()))
            else:
                merged = ds
        except Exception:
            # Fallback: try attribute existence
            merged = getattr(ds, &quot;train&quot;, ds)
    if merged is None:
        return {
            &quot;_GLOBAL_&quot;: {
                &quot;L0&quot;: 1.0,
                &quot;A&quot;: 1.0,
                &quot;exponents&quot;: [-0.05, -0.10, -0.20, -0.20],
                &quot;rmse_log&quot;: float(&quot;inf&quot;),
                &quot;intercept&quot;: 0.0,
            }
        }

    # Identify group column if present
    try:
        columns = list(merged.column_names)
    except Exception:
        try:
            columns = list(merged.features.keys())
        except Exception:
            columns = []

    candidate_group_cols = [&quot;group&quot;, &quot;Group&quot;, &quot;grp&quot;, &quot;family&quot;, &quot;cluster&quot;, &quot;exp_group&quot;]
    group_col = next((c for c in candidate_group_cols if c in columns), None)

    # Extract arrays
    feats = {f: [] for f in _FEATURES}
    y = []
    groups = []

    for row in merged:
        try:
            vals = [row.get(f, None) for f in _FEATURES]
            if any(v is None for v in vals):
                continue
            vals = [_coerce_float(v) for v in vals]
            if any(not np.isfinite(v) for v in vals):
                continue
            target = _coerce_float(row.get(_TARGET, math.nan))
            if not np.isfinite(target):
                continue
        except Exception:
            continue

        for f, v in zip(_FEATURES, vals):
            feats[f].append(v)
        y.append(target)
        if group_col is not None:
            groups.append(str(row.get(group_col, &quot;unknown&quot;)))
        else:
            groups.append(&quot;_GLOBAL_&quot;)

    if len(y) == 0:
        return {
            &quot;_GLOBAL_&quot;: {
                &quot;L0&quot;: 1.0,
                &quot;A&quot;: 1.0,
                &quot;exponents&quot;: [-0.05, -0.10, -0.20, -0.20],
                &quot;rmse_log&quot;: float(&quot;inf&quot;),
                &quot;intercept&quot;: 0.0,
            }
        }

    X = np.column_stack([np.asarray(feats[f], dtype=float) for f in _FEATURES])
    y_arr = np.asarray(y, dtype=float)
    groups_arr = np.asarray(groups, dtype=object)

    params_by_group: Dict[str, Dict] = {}

    # Fit per group
    unique_groups = np.unique(groups_arr)
    for g in unique_groups:
        mask = groups_arr == g
        params_by_group[str(g)] = _fit_group_power_with_offset(X[mask], y_arr[mask])

    # Also fit a global fallback on all data (in case unseen group appears)
    params_by_group[&quot;_GLOBAL_&quot;] = _fit_group_power_with_offset(X, y_arr)

    return params_by_group


def _ensure_fitted() -&gt; None:
    global PARAMS
    if PARAMS is None:
        PARAMS = _load_and_fit()


def _predict_from_params(row: Dict[str, float], params: Dict) -&gt; float:
    # Extract features in canonical order, with clipping for numerical stability
    xs = []
    for f in _FEATURES:
        v = _coerce_float(row.get(f, math.nan))
        if not np.isfinite(v):
            raise ValueError(f&quot;Missing or non-finite feature &#x27;{f}&#x27; in input: {row}&quot;)
        xs.append(max(1e-12, float(v)))
    xs = np.asarray(xs, dtype=float)
    L0 = float(params[&quot;L0&quot;])
    A = float(params[&quot;A&quot;])
    exps = np.asarray(params[&quot;exponents&quot;], dtype=float)
    # y = L0 + A * prod_i x_i ** a_i
    return float(L0 + A * float(np.prod(xs ** exps)))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The functional form is a power law with an additive irreducible-loss offset, fit per group:
        lm_loss_g = L0_g + A_g * lr^{a_g} * bsz^{b_g} * data_size^{c_g} * non_embedding_param_size^{d_g}

    The exponents and coefficients (L0_g, A_g, a_g..d_g) are learned from /app/data the first time
    this function (or module) is used.

    Args:
        input_data: List of dicts with keys: &#x27;lr&#x27;, &#x27;bsz&#x27;, &#x27;data_size&#x27;, &#x27;non_embedding_param_size&#x27;
        group: Experimental group name. If unseen, a global fallback fit is used.

    Returns:
        List of dicts with key &#x27;lm_loss&#x27; for each input row.
    &quot;&quot;&quot;
    _ensure_fitted()
    assert PARAMS is not None
    params = PARAMS.get(group)
    if params is None:
        params = PARAMS.get(&quot;_GLOBAL_&quot;, next(iter(PARAMS.values())))
    preds = []
    for row in input_data:
        yhat = _predict_from_params(row, params)
        preds.append({&quot;lm_loss&quot;: yhat})
    return preds</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -0.773483 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations

import math
from typing import Dict, List

import numpy as np


_DATA_PATH = &quot;/app/data&quot;
_EPS = 1e-12  # numeric floor for logs
_MODELS = None  # lazy-fit cache: {&quot;per_group&quot;: {group: model}, &quot;global&quot;: model}


def _safe_log(x: float) -&gt; float:
    return math.log(max(float(x), _EPS))


def _detect_column(columns: List[str], preferred: str, alternatives: List[str]) -&gt; str:
    if preferred in columns:
        return preferred
    for alt in alternatives:
        if alt in columns:
            return alt
    raise KeyError(
        f&quot;Expected column &#x27;{preferred}&#x27; not found. Available: {columns}. &quot;
        f&quot;Tried alternatives: {alternatives}&quot;
    )


def _load_dataset_dicts() -&gt; List[dict]:
    &quot;&quot;&quot;
    Loads the dataset from disk and returns it as a list of dicts.
    Supports either a Dataset or a DatasetDict (uses &#x27;train&#x27; split if present,
    otherwise the first available split).
    &quot;&quot;&quot;
    try:
        from datasets import load_from_disk
    except Exception as e:
        raise RuntimeError(
            &quot;Failed to import &#x27;datasets&#x27;. Ensure the &#x27;datasets&#x27; package is installed.&quot;
        ) from e

    ds = load_from_disk(_DATA_PATH)
    # Normalize to a Dataset instance
    try:
        # DatasetDict (mapping of splits)
        if hasattr(ds, &quot;keys&quot;):
            if &quot;train&quot; in ds:
                ds = ds[&quot;train&quot;]
            else:
                # Pick the first available split
                first_key = next(iter(ds.keys()))
                ds = ds[first_key]
    except Exception:
        pass

    # Convert to Python list of records without requiring pandas
    # HuggingFace Dataset supports to_dict() returning column-wise dict of lists
    cols = ds.column_names
    coldict = ds.to_dict()
    n = len(next(iter(coldict.values()))) if coldict else 0
    records = []
    for i in range(n):
        rec = {c: coldict[c][i] for c in cols}
        records.append(rec)
    return records


def _fit_group_models() -&gt; Dict[str, dict]:
    &quot;&quot;&quot;
    Fit a log-linear (power-law) model per group:
        ln(L) = b0 + b1 ln(lr) + b2 ln(bsz) + b3 ln(data_size) + b4 ln(non_embedding_param_size)

    Returns a dict with:
      {
        &quot;per_group&quot;: { group_name: {&quot;beta&quot;: np.array, &quot;rmse&quot;: float, &quot;n&quot;: int} },
        &quot;global&quot;: {&quot;beta&quot;: np.array, &quot;rmse&quot;: float, &quot;n&quot;: int}
      }
    &quot;&quot;&quot;
    data = _load_dataset_dicts()
    if not data:
        # No data found; return a sane fallback
        beta_fallback = np.array([math.log(3.5), 0.0, 0.0, 0.0, 0.0], dtype=float)
        return {
            &quot;per_group&quot;: {},
            &quot;global&quot;: {&quot;beta&quot;: beta_fallback, &quot;rmse&quot;: float(&quot;nan&quot;), &quot;n&quot;: 0},
        }

    # Detect columns
    columns = list(data[0].keys())
    # Required numeric columns (use exact names specified by the user prompt, with a few safe fallbacks)
    col_lr = _detect_column(columns, &quot;lr&quot;, [&quot;learning_rate&quot;])
    col_bsz = _detect_column(columns, &quot;bsz&quot;, [&quot;batch_size&quot;])
    col_data = _detect_column(columns, &quot;data_size&quot;, [&quot;tokens&quot;, &quot;num_tokens&quot;, &quot;dataset_size&quot;])
    col_params = _detect_column(
        columns, &quot;non_embedding_param_size&quot;, [&quot;non_embedding_params&quot;, &quot;non_embedding_parameters&quot;]
    )
    # Target
    col_loss = _detect_column(columns, &quot;lm_loss&quot;, [&quot;loss&quot;, &quot;val_loss&quot;, &quot;final_loss&quot;])
    # Group column (optional)
    group_col = None
    for cand in [&quot;group&quot;, &quot;group_name&quot;, &quot;exp_group&quot;, &quot;dataset_group&quot;, &quot;task_group&quot;]:
        if cand in columns:
            group_col = cand
            break

    # Filter invalid rows and build per-group buckets
    buckets: Dict[str, List[dict]] = {}
    for row in data:
        try:
            lr = float(row[col_lr])
            bsz = float(row[col_bsz])
            size = float(row[col_data])
            params = float(row[col_params])
            loss = float(row[col_loss])
        except Exception:
            continue

        if not (lr &gt; 0 and bsz &gt; 0 and size &gt; 0 and params &gt; 0 and loss &gt; 0):
            continue

        g = str(row[group_col]) if group_col is not None else &quot;default&quot;
        buckets.setdefault(g, []).append(
            {  # keep only needed keys
                &quot;lr&quot;: lr,
                &quot;bsz&quot;: bsz,
                &quot;data_size&quot;: size,
                &quot;non_embedding_param_size&quot;: params,
                &quot;lm_loss&quot;: loss,
            }
        )

    # If no valid groups, fallback
    if not buckets:
        beta_fallback = np.array([math.log(3.5), 0.0, 0.0, 0.0, 0.0], dtype=float)
        return {
            &quot;per_group&quot;: {},
            &quot;global&quot;: {&quot;beta&quot;: beta_fallback, &quot;rmse&quot;: float(&quot;nan&quot;), &quot;n&quot;: 0},
        }

    def _fit(X: np.ndarray, y: np.ndarray) -&gt; Dict[str, float | np.ndarray]:
        # Solve least squares
        beta, *_ = np.linalg.lstsq(X, y, rcond=None)
        pred = X @ beta
        rmse = float(math.sqrt(np.mean((pred - y) ** 2))) if y.size else float(&quot;nan&quot;)
        return {&quot;beta&quot;: beta, &quot;rmse&quot;: rmse, &quot;n&quot;: int(y.size)}

    models_per_group: Dict[str, dict] = {}
    # Pooled/global data
    X_all, y_all = [], []

    for g, rows in buckets.items():
        # Build design matrix: [1, ln(lr), ln(bsz), ln(data_size), ln(non_embedding_param_size)]
        Xg, yg = [], []
        for r in rows:
            Xg.append(
                [
                    1.0,
                    _safe_log(r[&quot;lr&quot;]),
                    _safe_log(r[&quot;bsz&quot;]),
                    _safe_log(r[&quot;data_size&quot;]),
                    _safe_log(r[&quot;non_embedding_param_size&quot;]),
                ]
            )
            yg.append(_safe_log(r[&quot;lm_loss&quot;]))
        Xg = np.asarray(Xg, dtype=float)
        yg = np.asarray(yg, dtype=float)

        model_g = _fit(Xg, yg)
        models_per_group[g] = model_g

        X_all.append(Xg)
        y_all.append(yg)

    X_all = np.vstack(X_all) if X_all else np.zeros((0, 5))
    y_all = np.concatenate(y_all) if y_all else np.zeros((0,))

    model_global = _fit(X_all, y_all) if y_all.size else {
        &quot;beta&quot;: np.array([math.log(3.5), 0.0, 0.0, 0.0, 0.0], dtype=float),
        &quot;rmse&quot;: float(&quot;nan&quot;),
        &quot;n&quot;: 0,
    }

    return {&quot;per_group&quot;: models_per_group, &quot;global&quot;: model_global}


def _ensure_models():
    global _MODELS
    if _MODELS is None:
        try:
            _MODELS = _fit_group_models()
        except Exception:
            # Absolute fallback to avoid hard failure during import/prediction
            beta_fallback = np.array([math.log(3.5), 0.0, 0.0, 0.0, 0.0], dtype=float)
            _MODELS = {&quot;per_group&quot;: {}, &quot;global&quot;: {&quot;beta&quot;: beta_fallback, &quot;rmse&quot;: float(&quot;nan&quot;), &quot;n&quot;: 0}}


def _predict_one(row: Dict[str, float], group: str) -&gt; float:
    &quot;&quot;&quot;
    Predict lm_loss for a single row using the fitted model for `group`.
    &quot;&quot;&quot;
    _ensure_models()
    assert _MODELS is not None
    model = _MODELS[&quot;per_group&quot;].get(group) or _MODELS[&quot;global&quot;]
    beta: np.ndarray = model[&quot;beta&quot;]

    # feature order must match training: [1, ln(lr), ln(bsz), ln(data_size), ln(non_embedding_param_size)]
    try:
        x = [
            1.0,
            _safe_log(row[&quot;lr&quot;]),
            _safe_log(row[&quot;bsz&quot;]),
            _safe_log(row[&quot;data_size&quot;]),
            _safe_log(row[&quot;non_embedding_param_size&quot;]),
        ]
    except KeyError as e:
        missing = str(e).strip(&quot;&#x27;&quot;)
        raise KeyError(
            f&quot;Missing required input variable &#x27;{missing}&#x27;. &quot;
            &quot;Expected keys: lr, bsz, data_size, non_embedding_param_size&quot;
        )
    y_log = float(np.dot(beta, np.asarray(x, dtype=float)))
    # Ensure strictly positive prediction
    return max(math.exp(y_log), _EPS)


def get_fitted_params() -&gt; Dict[str, dict]:
    &quot;&quot;&quot;
    Returns a dictionary with fitted parameters per group and global model.
    {
      &quot;per_group&quot;: {
          group: {
              &quot;beta&quot;: [b0, b1, b2, b3, b4],
              &quot;A&quot;: exp(b0),
              &quot;rmse&quot;: ...,
              &quot;n&quot;: ...
          }, ...
      },
      &quot;global&quot;: { ... }
    }
    &quot;&quot;&quot;
    _ensure_models()
    assert _MODELS is not None
    # Add derived A = exp(b0)
    def enrich(model: dict) -&gt; dict:
        beta = np.asarray(model[&quot;beta&quot;], dtype=float)
        return {
            &quot;beta&quot;: beta.tolist(),
            &quot;A&quot;: float(math.exp(beta[0])),
            &quot;rmse&quot;: float(model.get(&quot;rmse&quot;, float(&quot;nan&quot;))),
            &quot;n&quot;: int(model.get(&quot;n&quot;, 0)),
        }

    per_group = {g: enrich(m) for g, m in _MODELS[&quot;per_group&quot;].items()}
    global_m = enrich(_MODELS[&quot;global&quot;])
    return {&quot;per_group&quot;: per_group, &quot;global&quot;: global_m}


def dump_explanation(path: str = &quot;/app/explain.md&quot;) -&gt; None:
    &quot;&quot;&quot;
    Writes a detailed explanation and the fitted parameters per group to a Markdown file.
    &quot;&quot;&quot;
    params = get_fitted_params()

    lines = []
    lines.append(&quot;# Scaling Law Explanation&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;We model the final language modeling loss as a multiplicative power-law in the&quot;)
    lines.append(&quot;training hyperparameters, fit in log-space using ordinary least squares (OLS):&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;    ln(L) = b0 + b1 ln(lr) + b2 ln(bsz) + b3 ln(data_size) + b4 ln(non_embedding_param_size)&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;Equivalently, in the original scale:&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;    L = A * lr^b1 * bsz^b2 * data_size^b3 * non_embedding_param_size^b4, where A = exp(b0)&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;We fit one set of coefficients per experimental group (same functional form for all groups).&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Fitted Coefficients&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;| Group | A (exp(b0)) | b1 (lr) | b2 (bsz) | b3 (data_size) | b4 (non_emb_params) | RMSE (ln L) | N |&quot;)
    lines.append(&quot;|---|---:|---:|---:|---:|---:|---:|---:|&quot;)

    def fmt(x: float) -&gt; str:
        if math.isnan(x):
            return &quot;NaN&quot;
        return f&quot;{x:.6g}&quot;

    for g, m in sorted(params[&quot;per_group&quot;].items()):
        b0, b1, b2, b3, b4 = m[&quot;beta&quot;]
        A = m[&quot;A&quot;]
        lines.append(
            f&quot;| {g} | {fmt(A)} | {fmt(b1)} | {fmt(b2)} | {fmt(b3)} | {fmt(b4)} | {fmt(m[&#x27;rmse&#x27;])} | {m[&#x27;n&#x27;]} |&quot;
        )

    # Global/pool summary
    gm = params[&quot;global&quot;]
    b0, b1, b2, b3, b4 = gm[&quot;beta&quot;]
    lines.append(&quot;&quot;)
    lines.append(&quot;### Global (pooled) fit&quot;)
    lines.append(&quot;&quot;)
    lines.append(f&quot;- A = {fmt(gm[&#x27;A&#x27;])}&quot;)
    lines.append(f&quot;- b1 (lr) = {fmt(b1)}, b2 (bsz) = {fmt(b2)}, b3 (data_size) = {fmt(b3)}, b4 (non_emb_params) = {fmt(b4)}&quot;)
    lines.append(f&quot;- RMSE (ln L) = {fmt(gm[&#x27;rmse&#x27;])}, N = {gm[&#x27;n&#x27;]}&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Notes&quot;)
    lines.append(&quot;- Rows with non-positive values are excluded prior to log transform.&quot;)
    lines.append(&quot;- Predictions for unseen groups fall back to the global pooled model.&quot;)
    lines.append(&quot;- This simple log-linear law is robust and extrapolates smoothly; if stronger&quot;)
    lines.append(&quot;  curvature is present, augmenting with interaction terms is a straightforward extension.&quot;)

    content = &quot;\n&quot;.join(lines)
    try:
        with open(path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(content)
    except Exception:
        # Best-effort; ignore write failures
        pass


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Required keys:
                    - lr
                    - bsz
                    - data_size
                    - non_embedding_param_size
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&quot;lm_loss&quot;: float}.
    &quot;&quot;&quot;
    preds = []
    for row in input_data:
        pred = _predict_one(row, group)
        preds.append({&quot;lm_loss&quot;: float(pred)})
    return preds</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #b22222; color: white"> R² = -1.000000 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">from __future__ import annotations

import math
import os
from typing import Dict, Iterable, List, Tuple

# Keep third-party deps minimal and robust to absence.
# datasets is expected to be available in this environment per the prompt.
try:
    from datasets import load_from_disk, Dataset, DatasetDict, concatenate_datasets
except Exception:
    load_from_disk = None  # type: ignore
    Dataset = None  # type: ignore
    DatasetDict = None  # type: ignore
    concatenate_datasets = None  # type: ignore

try:
    import numpy as np
except Exception:  # very unlikely, but keep a safe fallback
    np = None  # type: ignore

# Canonical feature names required by the law()
_CANON_FEATURES = [
    &quot;lr&quot;,
    &quot;bsz&quot;,
    &quot;data_size&quot;,
    &quot;non_embedding_param_size&quot;,
]
_TARGET = &quot;lm_loss&quot;

# Possible aliases to be resilient to column naming differences in the dataset.
_FEATURE_ALIASES: Dict[str, List[str]] = {
    &quot;lr&quot;: [&quot;lr&quot;, &quot;learning_rate&quot;],
    &quot;bsz&quot;: [&quot;bsz&quot;, &quot;batch_size&quot;, &quot;global_batch_size&quot;],
    &quot;data_size&quot;: [&quot;data_size&quot;, &quot;tokens&quot;, &quot;n_tokens&quot;, &quot;train_tokens&quot;, &quot;total_tokens&quot;],
    &quot;non_embedding_param_size&quot;: [
        &quot;non_embedding_param_size&quot;,
        &quot;params_no_embed&quot;,
        &quot;non_embedding_params&quot;,
        &quot;non_embedding_param_count&quot;,
        &quot;non_embedding_parameters&quot;,
    ],
    &quot;lm_loss&quot;: [&quot;lm_loss&quot;, &quot;val_loss&quot;, &quot;validation_loss&quot;, &quot;eval_loss&quot;, &quot;loss&quot;],
}

_GROUP_CANDIDATES = [
    &quot;group&quot;,
    &quot;grp&quot;,
    &quot;exp_group&quot;,
    &quot;experiment&quot;,
    &quot;suite&quot;,
    &quot;dataset&quot;,
    &quot;setting&quot;,
]

_MODELS: Dict[str, Dict[str, object]] = {}
_FEATURE_ORDER: List[str] = _CANON_FEATURES[:]  # order of features in design matrix (logs)
_GROUP_COL: str | None = None
_FITTED: bool = False


def _safe_log(x: float, eps: float = 1e-12) -&gt; float:
    # Guard against non-positive inputs for log transforms
    return math.log(max(float(x), eps))


def _detect_columns(column_names: Iterable[str]) -&gt; Tuple[Dict[str, str], str | None]:
    &quot;&quot;&quot;
    Map canonical names to actual dataset column names and detect the group column.
    &quot;&quot;&quot;
    cols = set(column_names)
    mapping: Dict[str, str] = {}
    for canon, aliases in _FEATURE_ALIASES.items():
        for a in aliases:
            if a in cols:
                mapping[canon] = a
                break

    group_col = None
    for g in _GROUP_CANDIDATES:
        if g in cols:
            group_col = g
            break

    # Ensure required canon features and target exist in mapping
    missing = [k for k in _CANON_FEATURES + [_TARGET] if k not in mapping]
    if missing:
        # If something is missing, we still return what we found; the caller may fallback.
        pass
    return mapping, group_col


def _concat_all_splits(ds_obj):
    if DatasetDict is not None and isinstance(ds_obj, DatasetDict):
        # Concatenate all splits into one dataset
        parts = [ds_obj[k] for k in ds_obj.keys()]
        if len(parts) == 1:
            return parts[0]
        if concatenate_datasets is None:
            # Fallback: naive chaining via .flatten_indices() and .select()
            base = parts[0]
            for p in parts[1:]:
                base = base.concatenate(p)  # type: ignore[attr-defined]
            return base
        return concatenate_datasets(parts)
    return ds_obj


def _fit_group_linear_model(X: &quot;np.ndarray&quot;, y: &quot;np.ndarray&quot;, ridge: float = 1e-6) -&gt; &quot;np.ndarray&quot;:
    &quot;&quot;&quot;
    Fit beta via ridge-regularized normal equations: (X^T X + λI)^{-1} X^T y
    &quot;&quot;&quot;
    XT = X.T
    XTX = XT @ X
    # Ridge on all params except the intercept (index 0)
    I = np.eye(XTX.shape[0])
    I[0, 0] = 0.0
    beta = np.linalg.solve(XTX + ridge * I, XT @ y)
    return beta


def _design_row(d: Dict[str, float]) -&gt; List[float]:
    &quot;&quot;&quot;
    Build a single design-row from an input dict of canonical features.
    Intercept + log-features.
    &quot;&quot;&quot;
    return [1.0] + [_safe_log(d[k]) for k in _FEATURE_ORDER]


def _ensure_fitted() -&gt; None:
    global _MODELS, _FEATURE_ORDER, _GROUP_COL, _FITTED

    if _FITTED:
        return

    models: Dict[str, Dict[str, object]] = {}
    feature_order = _CANON_FEATURES[:]
    group_col: str | None = None

    # Attempt to load and fit from /app/data
    ds = None
    if load_from_disk is not None:
        try:
            ds = load_from_disk(&quot;/app/data&quot;)
        except Exception:
            ds = None

    if ds is not None:
        ds = _concat_all_splits(ds)
        try:
            column_names = list(ds.column_names)  # type: ignore[attr-defined]
        except Exception:
            try:
                column_names = list(ds.features.keys())  # type: ignore[attr-defined]
            except Exception:
                column_names = []

        mapping, group_col = _detect_columns(column_names)

        # Verify that all required features and target are available
        has_all = all((k in mapping) for k in _CANON_FEATURES + [_TARGET])

        if has_all and np is not None:
            # Prepare rows grouped by group_col (or a single default group)
            groups: Dict[str, List[Dict[str, float]]] = {}
            default_group = &quot;all&quot;
            # Iterate rows
            for row in ds:  # type: ignore[assignment]
                # Extract canonical dict
                try:
                    canon = {
                        &quot;lr&quot;: float(row[mapping[&quot;lr&quot;]]),
                        &quot;bsz&quot;: float(row[mapping[&quot;bsz&quot;]]),
                        &quot;data_size&quot;: float(row[mapping[&quot;data_size&quot;]]),
                        &quot;non_embedding_param_size&quot;: float(row[mapping[&quot;non_embedding_param_size&quot;]]),
                    }
                    y = float(row[mapping[&quot;lm_loss&quot;]])
                except Exception:
                    continue

                g = str(row[group_col]) if (group_col is not None and mapping.get(group_col) is None and group_col in row) else (
                    str(row[group_col]) if (group_col is not None and group_col in row) else default_group
                )

                # Stash both x and y
                item = dict(canon)
                item[_TARGET] = y
                groups.setdefault(g, []).append(item)

            # If no group column or empty groups, fallback to all data in one group
            if not groups:
                groups[default_group] = []
                for row in ds:  # type: ignore[assignment]
                    try:
                        groups[default_group].append(
                            {
                                &quot;lr&quot;: float(row[mapping[&quot;lr&quot;]]),
                                &quot;bsz&quot;: float(row[mapping[&quot;bsz&quot;]]),
                                &quot;data_size&quot;: float(row[mapping[&quot;data_size&quot;]]),
                                &quot;non_embedding_param_size&quot;: float(row[mapping[&quot;non_embedding_param_size&quot;]]),
                                _TARGET: float(row[mapping[&quot;lm_loss&quot;]]),
                            }
                        )
                    except Exception:
                        continue

            # Fit per-group models
            for g, rows in groups.items():
                if len(rows) &lt; 2:
                    continue
                X = np.array([_design_row(r) for r in rows], dtype=float)
                y = np.array([r[_TARGET] for r in rows], dtype=float)
                try:
                    beta = _fit_group_linear_model(X, y, ridge=1e-6)
                except Exception:
                    # Fallback to pseudo-inverse if needed
                    try:
                        beta = np.linalg.pinv(X) @ y
                        # Ensure length matches by padding/truncating
                        if beta.shape[0] != len(_FEATURE_ORDER) + 1:
                            beta = np.resize(beta, len(_FEATURE_ORDER) + 1)
                    except Exception:
                        # Ultimate fallback: simple mean model
                        beta = np.zeros(len(_FEATURE_ORDER) + 1, dtype=float)
                        beta[0] = float(y.mean())
                models[g] = {&quot;beta&quot;: beta, &quot;feature_order&quot;: feature_order}

            # Also fit a global model across all data for fallback
            all_rows: List[Dict[str, float]] = [r for rs in groups.values() for r in rs]
            if all_rows:
                X_all = np.array([_design_row(r) for r in all_rows], dtype=float)
                y_all = np.array([r[_TARGET] for r in all_rows], dtype=float)
                try:
                    beta_all = _fit_group_linear_model(X_all, y_all, ridge=1e-6)
                except Exception:
                    try:
                        beta_all = np.linalg.pinv(X_all) @ y_all
                    except Exception:
                        beta_all = np.zeros(len(_FEATURE_ORDER) + 1, dtype=float)
                        beta_all[0] = float(y_all.mean())
                models.setdefault(&quot;all&quot;, {&quot;beta&quot;: beta_all, &quot;feature_order&quot;: feature_order})

    # If fitting failed for any reason, create a conservative default model.
    if not models:
        # Default: constant loss ~3.5 (a typical LM cross-entropy scale) with zero log-coeffs.
        default_beta = [3.5] + [0.0] * len(_FEATURE_ORDER)
        models = {&quot;all&quot;: {&quot;beta&quot;: default_beta, &quot;feature_order&quot;: _FEATURE_ORDER}}

    _MODELS = models
    _FEATURE_ORDER = feature_order
    _GROUP_COL = group_col
    _FITTED = True

    # Attempt to materialize an explain.md with the fitted parameters
    try:
        _write_explain_markdown(&quot;/app/explain.md&quot;)
    except Exception:
        # Do not fail the import if we cannot write the explanation.
        pass


def _format_coeff_table() -&gt; str:
    lines = []
    header = [&quot;group&quot;, &quot;beta0(intercept)&quot;] + [f&quot;beta_{k}=coef(log({k}))&quot; for k in _FEATURE_ORDER]
    lines.append(&quot;| &quot; + &quot; | &quot;.join(header) + &quot; |&quot;)
    lines.append(&quot;| &quot; + &quot; | &quot;.join([&quot;---&quot;] * len(header)) + &quot; |&quot;)
    for g, info in sorted(_MODELS.items(), key=lambda kv: kv[0]):
        beta = info[&quot;beta&quot;]
        if hasattr(beta, &quot;tolist&quot;):
            beta_vals = list(beta.tolist())
        else:
            beta_vals = list(beta)  # type: ignore
        row = [g] + [f&quot;{float(v):.6g}&quot; for v in beta_vals]
        lines.append(&quot;| &quot; + &quot; | &quot;.join(row) + &quot; |&quot;)
    return &quot;\n&quot;.join(lines)


def _write_explain_markdown(path: str) -&gt; None:
    &quot;&quot;&quot;
    Write an explanation file describing the discovered law and fitted coefficients.
    &quot;&quot;&quot;
    template = f&quot;&quot;&quot;# Scaling Law for Language Model Training Loss

We model the final language modeling loss (lm_loss) as an affine function of the logarithms of core training hyperparameters:

Formula (shared functional form across all groups):
    lm_loss ≈ β0_g
              + β1_g · log(lr)
              + β2_g · log(bsz)
              + β3_g · log(data_size)
              + β4_g · log(non_embedding_param_size)

- Functional form is identical for all experimental groups g, but coefficients β•_g are fitted per-group.
- The log-transform captures empirically observed power-law-like scaling of loss with respect to optimization hyperparameters, data scale, and model size.

Fitting methodology:
- Data source: /app/data loaded via datasets.load_from_disk().
- Features: logarithms of lr, bsz, data_size, non_embedding_param_size with an intercept term.
- Target: lm_loss.
- Estimator: per-group ridge-regularized least squares on the design matrix [1, log(lr), log(bsz), log(data_size), log(non_embedding_param_size)].
- Regularization: λ = 1e-6 on non-intercept coefficients to improve numerical stability.
- A global &#x27;all&#x27; model is also fit as a fallback when a group is unseen or has insufficient data.

Fitted coefficients by group (β0_g, β1_g, β2_g, β3_g, β4_g):
{_format_coeff_table()}

Notes:
- log denotes the natural logarithm.
- If any input is non-positive, a small epsilon is used internally for numerical stability.
- For unseen groups at prediction time, the &#x27;all&#x27; model is used as a robust default.
&quot;&quot;&quot;
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.write(template)


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Expected keys:
                      - &#x27;lr&#x27;
                      - &#x27;bsz&#x27;
                      - &#x27;data_size&#x27;
                      - &#x27;non_embedding_param_size&#x27;
        group: The name of the experimental group for which to make predictions.
               The functional form is the same for all groups, but the coefficients
               can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s), currently:
          - &#x27;lm_loss&#x27;
    &quot;&quot;&quot;
    _ensure_fitted()

    # Choose the model for the specified group, fallback to &#x27;all&#x27;
    model = _MODELS.get(group) or _MODELS.get(&quot;all&quot;)
    if model is None:
        # Should not happen; final guard
        return [{&quot;lm_loss&quot;: 3.5} for _ in input_data]

    beta = model[&quot;beta&quot;]
    # Normalize beta to a Python list for computation
    if hasattr(beta, &quot;tolist&quot;):
        beta_vals = list(beta.tolist())  # type: ignore
    else:
        beta_vals = list(beta)  # type: ignore

    preds: List[Dict[str, float]] = []
    for row in input_data:
        # Build canonical dict (allowing both exact and aliased keys)
        canon: Dict[str, float] = {}
        for k in _FEATURE_ORDER:
            v = None
            if k in row:
                v = row[k]
            else:
                # Attempt aliases if user passed a different name
                for alias in _FEATURE_ALIASES.get(k, []):
                    if alias in row:
                        v = row[alias]  # type: ignore[index]
                        break
            if v is None:
                # Missing value: use a neutral default (1.0 for logs -&gt; 0 contribution)
                v = 1.0
            canon[k] = float(v)

        x_vec = [1.0] + [_safe_log(canon[k]) for k in _FEATURE_ORDER]
        # Dot product
        lm = 0.0
        for bi, xi in zip(beta_vals, x_vec):
            lm += float(bi) * float(xi)
        preds.append({&quot;lm_loss&quot;: float(lm)})

    return preds</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
