<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - MoE Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>MoE Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 2.5 Flash</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        0.911320
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.608705</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.056532</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.911320
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts validation loss based on a multiplicative power law with interaction terms for exponents and a bias.
    The model is: loss = A * (P_norm ^ (alpha_p + beta_p * log_E_norm)) * (E_norm ^ (alpha_e + beta_e * log_P_norm)) + L_min

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
        params (np.ndarray): Array of parameters. Can be (P,) or (T, P) where P=6.
                             P = [A_coeff, alpha_param, alpha_exp, beta_param, beta_exp, L_min_bias]

    Returns:
        np.ndarray: Predicted validation loss values (N,) or (N, T).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points)) # Shape (N, 2)
    
    # Ensure params is 2D (T, P) for consistent broadcasting
    params_arr = np.asarray(params)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :] # Make it (1, P) if 1D

    # Unpack parameters for each T set (P=6)
    A_coeffs = params_arr[:, 0]
    alpha_params = params_arr[:, 1] # Base exponent for dense parameters
    alpha_exps = params_arr[:, 2]   # Base exponent for number of experts
    beta_params = params_arr[:, 3]  # Interaction coefficient for dense_param exponent
    beta_exps = params_arr[:, 4]    # Interaction coefficient for num_experts exponent
    L_mins = params_arr[:, 5]

    # Extract features, ensuring they are strictly positive for power function stability
    num_experts = np.maximum(X[:, 0], 1e-9) # (N,)
    dense_param_count = np.maximum(X[:, 1], 1e-9) # (N,)
    
    # --- Normalization of features for numerical stability ---
    # These reference values (geometric means of the observed ranges)
    # help keep the A_coeff in a more manageable range during optimization,
    # without changing the fundamental scaling law form.
    P_ref = 2.8284271247e8 
    E_ref = 8.0    

    normalized_dense_param = dense_param_count / P_ref
    normalized_num_experts = num_experts / E_ref
    
    # Calculate log-normalized features for interaction terms
    # Ensure arguments to log are strictly positive
    log_normalized_dense_param = np.log(np.maximum(normalized_dense_param, 1e-9))
    log_normalized_num_experts = np.log(np.maximum(normalized_num_experts, 1e-9))
    
    # Calculate effective exponents with interaction terms using broadcasting
    effective_alpha_params = alpha_params[None, :] + beta_params[None, :] * log_normalized_num_experts[:, None]
    effective_alpha_exps = alpha_exps[None, :] + beta_exps[None, :] * log_normalized_dense_param[:, None]
    
    # Calculate power terms using broadcasting for (N, T) result
    term_dense = np.power(normalized_dense_param[:, None], effective_alpha_params) # (N, T)
    term_experts = np.power(normalized_num_experts[:, None], effective_alpha_exps) # (N, T)
    
    # Combine terms to get final predictions (N, T)
    pred = A_coeffs[None, :] * term_dense * term_experts + L_mins[None, :]
    
    # Return (N,) if T=1, otherwise (N, T)
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits an enhanced multiplicative power law scaling function with interaction terms to the provided data.
    Improvements include:
    1. Symmetric interaction terms in the exponents (up to 6 parameters).
    2. Feature normalization within the scaling_law_func for numerical stability.
    3. A robust initialization strategy for L_min using a search over candidates
       and selecting the best linear regression fit, improving the starting point
       for the non-linear optimizer.
    4. Interaction terms (beta_p, beta_e) are initialized to zero for the initial linear fit.
    5. The non-linear optimization is run from multiple initial guesses for beta_p and beta_e,
       to help find a better global minimum for the 6-parameter model.

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
        loss_values (np.ndarray): (N,) array of corresponding validation loss values.

    Returns:
        np.ndarray: Optimized parameters [A_coeff, alpha_param, alpha_exp, beta_param, beta_exp, L_min_bias] (1D array).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # The enhanced scaling law uses 6 parameters: [A, alpha_p, alpha_e, beta_p, beta_e, L_min]
    num_params = 6

    # --- Initial guess and bounds setup ---
    min_loss_obs = np.min(y)
    max_loss_obs = np.max(y)
    
    # Initial heuristic for L_min: slightly below the minimum observed loss.
    # Clamped to be positive and strictly less than min_loss_obs.
    l_min_heuristic_val = np.clip(min_loss_obs - 0.05 * (max_loss_obs - min_loss_obs), 
                                  0.001, min_loss_obs - 1e-4)

    # Define a robust range for L_min candidates
    l_min_low_bound = max(0.001, l_min_heuristic_val * 0.9)
    l_min_high_bound = min(min_loss_obs - 1e-5, l_min_heuristic_val * 1.05)

    if l_min_low_bound &gt;= l_min_high_bound:
        # Fallback if the calculated range is invalid or too small
        l_min_candidates = np.array([l_min_heuristic_val])
        # If heuristic is also invalid, use a safe default
        if l_min_candidates[0] &gt;= min_loss_obs - 1e-6:
             l_min_candidates = np.array([max(0.001, min_loss_obs * 0.9)])
    else:
        l_min_candidates = np.linspace(l_min_low_bound, l_min_high_bound, num=15)
    
    # Ensure all candidates are strictly less than min_loss_obs after generation
    l_min_candidates = l_min_candidates[l_min_candidates &lt; min_loss_obs - 1e-6]
    # Final check if filtering left it empty
    if len(l_min_candidates) == 0:
        l_min_candidates = np.array([max(0.001, min_loss_obs * 0.9)])


    best_lr_mse = np.inf
    best_lr_params = None

    # Use log-linear regression to find initial A, alpha_param, alpha_exp for each L_min candidate.
    # This approximates the model log(Y - L_min) = log(A) + alpha_param * log(P_norm) + alpha_exp * log(E_norm).
    
    # Normalize features for the initial linear regression calculation using the same refs as scaling_law_func
    P_ref = 2.8284271247e8 
    E_ref = 8.0
    log_normalized_dense_params = np.log(np.maximum(X[:, 1] / P_ref, 1e-9))
    log_normalized_num_experts = np.log(np.maximum(X[:, 0] / E_ref, 1e-9))
    
    # Design matrix for linear regression: [1 (intercept), log(normalized_dense_params), log(normalized_num_experts)]
    A_lr_base = np.vstack([np.ones(len(X)), log_normalized_dense_params, log_normalized_num_experts]).T

    for l_min_candidate in l_min_candidates:
        # Ensure (y - l_min_candidate) is strictly positive for log
        y_adjusted = np.maximum(y - l_min_candidate, 1e-6) 
        log_y_adjusted = np.log(y_adjusted)

        # Filter out any NaN/Inf values that could result from log transformations
        valid_indices = np.all(np.isfinite(A_lr_base), axis=1) &amp; np.isfinite(log_y_adjusted)

        current_init_A, current_init_alpha_param, current_init_alpha_exp = 1.0, -0.1, -0.1 # Default fallback values

        # Perform linear regression if enough valid data points exist
        if np.sum(valid_indices) &gt;= 3: # Need at least 3 points for 3 coefficients
            try:
                coeffs_lr, _, _, _ = np.linalg.lstsq(A_lr_base[valid_indices], log_y_adjusted[valid_indices], rcond=None)
                current_init_A = np.exp(coeffs_lr[0]) # From log(A)
                current_init_alpha_param = coeffs_lr[1]
                current_init_alpha_exp = coeffs_lr[2]
                
                # Clip initial estimates to reasonable ranges to assist the optimizer
                current_init_A = np.clip(current_init_A, 1e-3, 1e3) 
                current_init_alpha_param = np.clip(current_init_alpha_param, -1.0, 0.0) # Exponents typically negative or zero
                current_init_alpha_exp = np.clip(current_init_alpha_exp, -1.0, 0.0)
            except np.linalg.LinAlgError:
                pass # Fallback to default values if linear regression fails

        # For the linear regression evaluation, interaction terms are zero
        current_initial_candidate_params = np.array([
            current_init_A,
            current_init_alpha_param,
            current_init_alpha_exp,
            0.0, # beta_param
            0.0, # beta_exp
            l_min_candidate
        ])
        
        # Evaluate MSE for this initial guess using the 6-parameter scaling_law_func
        # This gives us the best 4-parameter fit (with zero interaction terms) with L_min chosen from candidates
        pred_initial = scaling_law_func(X, current_initial_candidate_params)
        current_mse = np.mean((pred_initial - y) ** 2)

        if current_mse &lt; best_lr_mse:
            best_lr_mse = current_mse
            best_lr_params = current_initial_candidate_params
    
    # If no valid L_min candidate produced a good fit, fall back to a safe default.
    if best_lr_params is None:
        best_lr_params = np.array([1.0, -0.1, -0.1, 0.0, 0.0, l_min_heuristic_val]) # Default for 6 params
    
    # Define bounds for parameters: [A, alpha_p, alpha_e, beta_p, beta_e, L_min]
    bounds = [
        (1e-6, 1e6),    # A_coeff: Must be positive, can range widely
        (-2.0, 0.0),    # alpha_param: Typically negative or zero for loss reduction
        (-2.0, 0.0),    # alpha_exp: Typically negative or zero for loss reduction
        (-1.0, 1.0),    # beta_param: Interaction coeff for alpha_param, can be positive or negative
        (-1.0, 1.0),    # beta_exp: Interaction coeff for alpha_exp, can be positive or negative
        (0.001, min_loss_obs - 1e-5) # L_min: Strictly positive, and strictly less than min observed loss
    ]
    # Ensure L_min upper bound is valid, especially if min_loss_obs is very small.
    if bounds[5][0] &gt;= bounds[5][1]: # Check for L_min bounds (index 5)
        bounds[5] = (0.001, min_loss_obs * 0.999 if min_loss_obs &gt; 0.001 else 0.001)

    def objective(params_local):
        &quot;&quot;&quot;Calculates the Mean Squared Error for the given parameters.&quot;&quot;&quot;
        pred = scaling_law_func(X, params_local)
        mse = np.mean((pred - y) ** 2)
        return mse

    # --- Multi-start optimization for beta_p and beta_e ---
    best_overall_mse = np.inf
    best_overall_params = np.copy(best_lr_params) # Start with the best 4-param fit as a fallback

    # Perturb initial beta_p and beta_e around 0, based on the best 4-param fit
    beta_p_starts = np.linspace(bounds[3][0], bounds[3][1], 3) # Use bounds for start points
    beta_e_starts = np.linspace(bounds[4][0], bounds[4][1], 3) 
    
    # Add 0.0 if not already included due to linspace range/num.
    # This ensures the best_lr_params (beta=0) start is always considered.
    if 0.0 not in beta_p_starts: beta_p_starts = np.sort(np.append(beta_p_starts, 0.0))
    if 0.0 not in beta_e_starts: beta_e_starts = np.sort(np.append(beta_e_starts, 0.0))

    for bp_start in beta_p_starts:
        for be_start in beta_e_starts:
            # Create an initial guess by taking the best values from LR for A, alpha_p, alpha_e, L_min
            # and combining them with current beta_p_start, beta_e_start
            current_initial_guess = np.copy(best_lr_params)
            current_initial_guess[3] = bp_start # beta_param
            current_initial_guess[4] = be_start # beta_exp
            
            # Ensure the initial guess respects the bounds
            for i in range(num_params):
                current_initial_guess[i] = np.clip(current_initial_guess[i], bounds[i][0], bounds[i][1])

            result = minimize(objective, current_initial_guess, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)
            
            if result.success and result.fun &lt; best_overall_mse:
                best_overall_mse = result.fun
                best_overall_params = result.x
    
    # If optimization failed across all starts (e.g. no result.success), return the best_lr_params
    if not np.isfinite(best_overall_mse):
        return best_lr_params

    return best_overall_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.872454
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts validation loss based on a combined power law, irreducible loss (L_min),
    and logarithmic linear terms for expert count and parameter count.
    The model is: loss = A * (dense_parameter_count ^ alpha_param) * (num_experts ^ alpha_expert) + L_min + D * log(dense_parameter_count) + F * log(num_experts)

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
                                  N is the number of data points.
        params (np.ndarray): Array of parameters. Can be (P,) or (T, P) where P=6.
                             P = [A_coeff, alpha_param, alpha_expert, L_min_bias, D_log_param, F_log_expert]

    Returns:
        np.ndarray: Predicted validation loss values (N,) or (N, T).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points)) # Shape (N, 2)
    
    # Ensure params is 2D (T, P) for consistent broadcasting
    params_arr = np.asarray(params)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :] # Make it (1, P) if 1D

    # Unpack parameters for each T set
    A_coeffs = params_arr[:, 0]
    alpha_params = params_arr[:, 1]
    alpha_experts = params_arr[:, 2]
    L_mins = params_arr[:, 3]
    D_log_params = params_arr[:, 4] # Coefficient for log(dense_parameter_count)
    F_log_experts = params_arr[:, 5] # Coefficient for log(num_experts)

    # Extract features, ensuring they are strictly positive for power function stability
    # Use np.float64 for intermediate calculations to prevent potential overflow/underflow issues
    # All input values (num_experts, dense_parameter_count) are strictly positive,
    # so `1e-9` as a minimum clip is mostly for theoretical robustness and avoids log(0).
    num_experts = np.maximum(X[:, 0].astype(np.float64), 1e-9) # (N,)
    dense_param_count = np.maximum(X[:, 1].astype(np.float64), 1e-9) # (N,)
    
    # Calculate power law terms using broadcasting for (N, T) result
    # np.power can handle negative exponents and fractional exponents
    term_dense_pow = np.power(dense_param_count[:, None], alpha_params[None, :]) # (N, T)
    term_experts_pow = np.power(num_experts[:, None], alpha_experts[None, :]) # (N, T)
    
    # Combined power law component
    power_law_component = A_coeffs[None, :] * term_dense_pow * term_experts_pow # (N, T)
    
    # Logarithmic linear components
    log_param_component = D_log_params[None, :] * np.log(dense_param_count[:, None]) # (N, T)
    log_expert_component = F_log_experts[None, :] * np.log(num_experts[:, None]) # (N, T)
    
    # Combine all terms
    pred = power_law_component + L_mins[None, :] + log_param_component + log_expert_component
    
    # Return (N,) if T=1, otherwise (N, T)
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the combined power law, irreducible loss (L_min), and logarithmic linear terms
    scaling function to the provided data.
    The model is: loss = A * (dense_parameter_count ^ alpha_param) * (num_experts ^ alpha_expert) + L_min + D * log(dense_parameter_count) + F * log(num_experts)

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
        loss_values (np.ndarray): (N,) array of corresponding validation loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha_P, alpha_E, L_min, D_log_P, F_log_E] (1D array).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    num_params = 6 # [A_coeff, alpha_param, alpha_expert, L_min_bias, D_log_param, F_log_expert]

    min_loss_obs = np.min(y)
    
    # --- Refined Initial guess strategy using iterative L_min search for log-linear fit ---
    # This aims to find a better L_min initial guess by evaluating its impact on a simplified log-linear model.
    best_r_squared = -np.inf
    best_init_coeffs = None
    
    # Candidates for L_min, ensuring it&#x27;s positive and strictly less than min_loss_obs
    # Start with a range from a small positive number up to just below the minimum observed loss.
    l_min_candidates = np.linspace(0.001, np.maximum(0.0011, min_loss_obs * 0.999 - 1e-5), 20)
    # Ensure all candidates are strictly less than min_loss_obs to avoid log(0) or log(negative) in y_adjusted
    l_min_candidates = l_min_candidates[l_min_candidates &lt; (min_loss_obs - 1e-5)] 
    
    # Fallback if the valid range for candidates is too small or empty
    if len(l_min_candidates) == 0:
        l_min_candidates = [np.clip(min_loss_obs * 0.9, 0.001, min_loss_obs - 1e-4)]

    # Log transform features for the power law part (and for initializing log-linear terms)
    # Use np.maximum with a small epsilon to handle potential zero values gracefully before log.
    log_num_experts_orig = np.log(np.maximum(X[:, 0], 1e-9))
    log_dense_params_orig = np.log(np.maximum(X[:, 1], 1e-9))

    for l_min_c in l_min_candidates:
        # Adjust loss values and take log, ensuring positivity
        y_adjusted = np.maximum(y - l_min_c, 1e-6) 
        log_y_adjusted = np.log(y_adjusted)

        # Design matrix for log-linear regression: [intercept, log(dense_param), log(num_experts)]
        # This part of the initial guess primarily handles the A * P^alpha_P * E^alpha_E + L_min component.
        A_lr = np.vstack([np.ones(len(X)), log_dense_params_orig, log_num_experts_orig]).T
        
        # Filter out any NaN/Inf that might arise from log operations, to ensure a robust linear regression
        valid_indices = np.all(np.isfinite(A_lr), axis=1) &amp; np.isfinite(log_y_adjusted)
        
        if np.sum(valid_indices) &gt;= 3: # Need at least 3 points for 3 coefficients (intercept, alpha_P, alpha_E)
            try:
                # Solve for coefficients [log(A_coeff), alpha_param, alpha_expert]
                coeffs_lr, _, _, _ = np.linalg.lstsq(A_lr[valid_indices], log_y_adjusted[valid_indices], rcond=None)
                
                # Calculate R-squared for this specific L_min to find the best one
                pred_log_y_adjusted = A_lr[valid_indices] @ coeffs_lr
                ss_total = np.sum((log_y_adjusted[valid_indices] - np.mean(log_y_adjusted[valid_indices]))**2)
                
                # Avoid division by zero for R-squared calculation if target variance is negligible
                if ss_total &gt; 1e-9: 
                    r_squared = 1 - np.sum((log_y_adjusted[valid_indices] - pred_log_y_adjusted)**2) / ss_total
                else:
                    r_squared = -np.inf # If no variance, R-squared is not well-defined, assign a low value.
                
                if r_squared &gt; best_r_squared:
                    best_r_squared = r_squared
                    # Store A_coeff (exp(intercept)), alpha_param, alpha_expert, and the current L_min candidate
                    best_init_coeffs = (np.exp(coeffs_lr[0]), coeffs_lr[1], coeffs_lr[2], l_min_c)
            except np.linalg.LinAlgError:
                # If linear regression fails (e.g., singular matrix), this L_min candidate is skipped.
                pass 

    # Fallback if no valid R-squared was found or issues during iteration
    if best_init_coeffs is None:
        init_A = 10.0 # Default reasonable starting point for A
        init_alpha_param = -0.1 # Exponents are typically negative for loss reduction
        init_alpha_expert = -0.1
        # Fallback L_min heuristic
        l_min_init_val = np.clip(min_loss_obs * 0.9, 0.001, min_loss_obs - 1e-4) 
    else:
        init_A, init_alpha_param, init_alpha_expert, l_min_init_val = best_init_coeffs
        # Clip initial estimates to reasonable ranges to assist the optimizer
        init_A = np.clip(init_A, 1e-3, 1e3) 
        init_alpha_param = np.clip(init_alpha_param, -1.0, 0.0) # Standard exponent range for loss reduction
        init_alpha_expert = np.clip(init_alpha_expert, -1.0, 0.0) # Standard exponent range for loss reduction

    # Initial guesses for the new logarithmic linear terms (start neutral at 0.0)
    init_D_log_param = 0.0 
    init_F_log_expert = 0.0 
    
    initial_params = np.array([init_A, init_alpha_param, init_alpha_expert, l_min_init_val, init_D_log_param, init_F_log_expert])
    
    # Define bounds for parameters: [A, alpha_P, alpha_E, L_min, D_log_P, F_log_E]
    bounds = [
        (1e-6, 1e6),    # A_coeff: Must be positive, can range widely.
        (-1.0, 0.0),    # alpha_param: Typically negative or zero for loss reduction.
        (-1.0, 0.0),    # alpha_expert: Typically negative or zero for loss reduction.
        (0.001, min_loss_obs - 1e-5), # L_min: Strictly positive, and strictly less than min observed loss.
        (-1.0, 1.0),    # D_log_param: Coefficient for log(dense_parameter_count), allowing both positive and negative impact.
                        #   log(1e8) is ~18.4, log(8e8) is ~20.5. A coefficient of 1.0 would add ~18-20 to loss.
                        #   This range allows for significant contribution, but could be adjusted if too wide/narrow.
        (-1.0, 1.0)     # F_log_expert: Coefficient for log(num_experts), allowing both positive and negative impact.
                        #   log(1) is 0, log(64) is ~4.15. A coefficient of 1.0 would add ~0-4.15 to loss.
    ]
    # Ensure L_min upper bound is valid (i.e., upper bound &gt; lower bound).
    if bounds[3][0] &gt;= bounds[3][1]:
        # Fallback to a tiny valid range if min_loss_obs is too low to provide a meaningful range
        bounds[3] = (0.001, min_loss_obs * 0.999 if min_loss_obs &gt; 0.001 else 0.001)
        if bounds[3][0] &gt;= bounds[3][1]: 
             bounds[3] = (0.001, 0.002) # A last resort small valid range to prevent optimizer crash

    def objective(params_local):
        &quot;&quot;&quot;Calculates the Mean Squared Error for the given parameters.&quot;&quot;&quot;
        pred = scaling_law_func(X, params_local)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Use &#x27;L-BFGS-B&#x27; for bounded optimization as it&#x27;s more robust than BFGS for constraints.
    # Added options for maxiter and ftol for better control over convergence and precision.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                      options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-6})

    # Return optimized parameters if successful, otherwise return the robust initial guess
    params_opt = result.x if result.success else initial_params
    
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.819750
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts validation loss based on an extended multiplicative inverse power law with a bias.
    The model is: loss = A / (P^beta_P * E^beta_E) + C / (P * E) + D / E + L_min
    where P is dense_parameter_count, E is num_experts.
    beta_P and beta_E are positive exponents. This model uses 6 parameters.

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
                                  N is the number of data points.
        params (np.ndarray): Array of parameters. Can be (P,) or (T, P) where P=6.
                             P = [A_coeff, beta_P_exponent, beta_E_exponent, C_coeff, D_coeff, L_min_bias]

    Returns:
        np.ndarray: Predicted validation loss values (N,) or (N, T).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points)) # Shape (N, 2)
    
    # Ensure params is 2D (T, P) for consistent broadcasting
    params_arr = np.asarray(params)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :] # Make it (1, P) if 1D

    # Unpack parameters for each T set
    # P must be 6 for our model: [A, beta_P, beta_E, C, D, L_min]
    A_coeffs = params_arr[:, 0]
    beta_P_exponents = params_arr[:, 1] # Positive exponents for dense_parameter_count
    beta_E_exponents = params_arr[:, 2] # Positive exponents for num_experts
    C_coeffs = params_arr[:, 3] # Coefficient for the additive P*E inverse term
    D_coeffs = params_arr[:, 4] # Coefficient for the additive E inverse term
    L_mins = params_arr[:, 5]

    # Extract features, ensuring they are strictly positive for power function stability
    num_experts = np.maximum(X[:, 0], 1e-9) # (N,)
    dense_param_count = np.maximum(X[:, 1], 1e-9) # (N,)
    
    # Calculate first term: A / (P^beta_P * E^beta_E)
    term_dense_denom_1 = np.power(dense_param_count[:, None], beta_P_exponents[None, :]) # (N, T)
    term_experts_denom_1 = np.power(num_experts[:, None], beta_E_exponents[None, :]) # (N, T)
    denominator_1 = term_dense_denom_1 * term_experts_denom_1
    denominator_1 = np.maximum(denominator_1, 1e-30) # Prevent division by near zero
    term1_pred = A_coeffs[None, :] / denominator_1

    # Calculate second term: C / (P * E)
    denominator_2 = dense_param_count[:, None] * num_experts[:, None]
    denominator_2 = np.maximum(denominator_2, 1e-30)
    term2_pred = C_coeffs[None, :] / denominator_2
    
    # Calculate third term: D / E
    denominator_3 = num_experts[:, None]
    denominator_3 = np.maximum(denominator_3, 1e-30)
    term3_pred = D_coeffs[None, :] / denominator_3
    
    # Combine terms to get final predictions (N, T)
    pred = term1_pred + term2_pred + term3_pred + L_mins[None, :]
    
    # Return (N,) if T=1, otherwise (N, T)
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the extended scaling function to the provided data.
    The function uses a 6-parameter model:
    loss = A / (P^beta_P * E^beta_E) + C / (P * E) + D / E + L_min

    It employs a robust initialization strategy involving a grid search for L_min,
    followed by an approximate log-linear fit for A, beta_P, beta_E, and finally
    a bounded L-BFGS-B optimization for all 6 parameters.

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
        loss_values (np.ndarray): (N,) array of corresponding validation loss values.

    Returns:
        np.ndarray: Optimized parameters [A_coeff, beta_P_exponent, beta_E_exponent, C_coeff, D_coeff, L_min_bias] (1D array).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # The scaling law now uses 6 parameters: [A, beta_P, beta_E, C, D, L_min]
    num_params = 6

    # --- Initial guess and bounds setup ---
    min_loss_obs = np.min(y)
    
    # Define a range of L_min candidates for a more robust initial guess
    L_min_lower_bound = 1e-6 # Ensure L_min is strictly positive
    # Upper bound for L_min must be strictly less than min_loss_obs to allow calculation of residuals for fitting
    L_min_upper_bound = np.maximum(L_min_lower_bound + 1e-5, min_loss_obs - 1e-5) 
    
    # Generate candidates for L_min; use a single point if range is too small
    if L_min_upper_bound &lt;= L_min_lower_bound:
        l_min_candidates = np.array([L_min_lower_bound])
    else:
        # Increased number of candidates for L_min to explore the range more thoroughly
        l_min_candidates = np.linspace(L_min_lower_bound, L_min_upper_bound, 15)

    best_mse = np.inf
    best_initial_params = None

    # Pre-calculate log features for efficiency, used for the log-linear part of initialization
    log_num_experts = np.log(np.maximum(X[:, 0], 1e-9))
    log_dense_params = np.log(np.maximum(X[:, 1], 1e-9))

    # Iterate through L_min candidates to find the best initial guess
    for l_min_try in l_min_candidates:
        # For initialization of A, beta_P, beta_E, we approximate using the dominant power law term:
        # log(Y - L_min) approx log(A) - beta_P * log(P) - beta_E * log(E)
        # We start with C and D as small values and let the optimizer refine them.
        y_adjusted = np.maximum(y - l_min_try, 1e-6) # Ensure strictly positive for log
        log_y_adjusted = np.log(y_adjusted)

        # Design matrix for linear regression: [1 (intercept, log A), -log(dense_params), -log(num_experts)]
        A_lr = np.vstack([np.ones(len(X)), -log_dense_params, -log_num_experts]).T
        
        # Filter out any NaN/Inf values that could result from log transformations
        valid_indices = np.all(np.isfinite(A_lr), axis=1) &amp; np.isfinite(log_y_adjusted)

        current_init_A, current_init_beta_P, current_init_beta_E = 1.0, 0.1, 0.1 # Default fallback values
        current_init_C = 1e-8 # Initial guess for the C parameter, a small positive value
        current_init_D = 1e-8 # Initial guess for the D parameter, a small positive value

        if np.sum(valid_indices) &gt;= 3: # Need at least 3 points for 3 coefficients
            try:
                coeffs_lr, _, _, _ = np.linalg.lstsq(A_lr[valid_indices], log_y_adjusted[valid_indices], rcond=None)
                current_init_A = np.exp(coeffs_lr[0])
                # beta_P and beta_E correspond to coeffs_lr[1] and coeffs_lr[2] because of the -log terms in A_lr
                current_init_beta_P = coeffs_lr[1] 
                current_init_beta_E = coeffs_lr[2]
                
                # Clip initial estimates to reasonable ranges
                current_init_A = np.clip(current_init_A, 1e-3, 1e3) 
                current_init_beta_P = np.clip(current_init_beta_P, 0.01, 2.0) 
                current_init_beta_E = np.clip(current_init_beta_E, 0.01, 2.0)
            except np.linalg.LinAlgError:
                pass # Fallback to default in case of singular matrix

        temp_initial_params = np.array([current_init_A, current_init_beta_P, current_init_beta_E, 
                                        current_init_C, current_init_D, l_min_try])
        
        # Evaluate MSE using the full non-linear function
        temp_pred = scaling_law_func(X, temp_initial_params)
        temp_mse = np.mean((temp_pred - y) ** 2)

        if temp_mse &lt; best_mse:
            best_mse = temp_mse
            best_initial_params = temp_initial_params

    # If no valid initial params were found, use a safe default
    if best_initial_params is None:
        best_initial_params = np.array([1.0, 0.1, 0.1, 1e-8, 1e-8, L_min_lower_bound]) # Default with C=1e-8, D=1e-8

    # Define bounds for parameters: [A, beta_P, beta_E, C, D, L_min]
    bounds = [
        (1e-8, 1e8),        # A_coeff: Can range widely, must be positive
        (0.001, 5.0),       # beta_P_exponent: Positive, reasonable max
        (0.001, 5.0),       # beta_E_exponent: Positive, reasonable max
        (1e-8, 1e8),        # C_coeff: Must be positive, can range widely
        (1e-8, 1e8),        # D_coeff: Must be positive, can range widely
        (L_min_lower_bound, L_min_upper_bound) # L_min: Strictly positive, and strictly less than min observed loss
    ]
    
    def objective(params_local):
        &quot;&quot;&quot;Calculates the Mean Squared Error for the given parameters.&quot;&quot;&quot;
        pred = scaling_law_func(X, params_local)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Use a bounded optimization method like &#x27;L-BFGS-B&#x27; for better stability and constraint handling
    result = minimize(objective, best_initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)

    # Return optimized parameters if successful, otherwise return the best initial guess
    params_opt = result.x if result.success else best_initial_params
    
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.383469
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts validation loss based on MoE architecture parameters using a 
    mixed log-polynomial and inverse-expert count model.

    Parameters:
    data_points: (N,2) array with columns [num_experts, dense_parameter_count]
                 num_experts: Array of expert counts
                 dense_parameter_count: Array of dense parameter counts
    params: Array of 6 parameters [p0, p1, p2, p3, p4, p5]
            Loss = p0 + p1*log(Np) + p2*log(Ne) + p3*log(Np)^2 + p4*(1/Ne) + p5*log(Np)*log(Ne)

    Returns:
    Predicted validation loss values (N,)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    num_experts = X[:, 0]
    dense_parameter_count = X[:, 1]

    # Log transform inputs for numerical stability and to align with common scaling law forms.
    # Using np.maximum to ensure inputs to log are positive and 1/X terms don&#x27;t divide by zero.
    log_dense_parameter_count = np.log(np.maximum(dense_parameter_count, 1e-9))
    
    # log(Ne) term remains for general expert scaling behavior.
    log_num_experts = np.log(np.maximum(num_experts, 1e-9))
    
    # New term: 1/Ne to capture inverse scaling or diminishing returns with experts.
    inverse_num_experts = 1.0 / np.maximum(num_experts, 1e-9)

    # Ensure params are correctly shaped for a single prediction set.
    # The optimizer passes a 1D array of parameters.
    params_flat = np.asarray(params).flatten()

    # Unpack the 6 parameters: p0, p1, p2, p3, p4, p5
    # Loss = p0 + p1*Lp + p2*Le + p3*Lp^2 + p4*(1/Ne) + p5*Lp*Le
    # where Lp = log_dense_parameter_count, Le = log_num_experts
    p0, p1, p2, p3, p4, p5 = params_flat[:6]

    predicted_loss = p0 \
                   + p1 * log_dense_parameter_count \
                   + p2 * log_num_experts \
                   + p3 * (log_dense_parameter_count ** 2) \
                   + p4 * inverse_num_experts \
                   + p5 * (log_dense_parameter_count * log_num_experts)

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function to the provided data. This function uses 
    Ordinary Least Squares (OLS) to derive an optimal initial guess for the
    parameters, followed by L-BFGS-B optimization to refine them with bounds,
    providing the analytical Jacobian for improved convergence and theoretical stability.

    The model is linear in its parameters after feature transformations,
    making OLS highly effective for parameter estimation.

    Parameters:
    data_points: (N,2) array with columns [num_experts, dense_parameter_count]
    loss_values: Array of corresponding validation loss values (N,)

    Returns:
    Optimized parameters (6 parameters) for the scaling_law_func.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    N = len(y) # Number of data points

    # Transform inputs for constructing the design matrix for OLS.
    # Consistent robust transformation as in scaling_law_func.
    log_dense_parameter_count = np.log(np.maximum(X[:, 1], 1e-9))
    log_num_experts = np.log(np.maximum(X[:, 0], 1e-9))
    inverse_num_experts = 1.0 / np.maximum(X[:, 0], 1e-9)

    # Construct the design matrix for linear regression (OLS).
    # Each column corresponds to a term in the scaling_law_func:
    # [1, log(Np), log(Ne), log(Np)^2, 1/Ne, log(Np)*log(Ne)]
    design_matrix = np.column_stack([
        np.ones(N),                                         # p0 (intercept)
        log_dense_parameter_count,                         # p1 * log(Np)
        log_num_experts,                                   # p2 * log(Ne)
        log_dense_parameter_count ** 2,                    # p3 * log(Np)^2
        inverse_num_experts,                               # p4 * (1/Ne)
        log_dense_parameter_count * log_num_experts        # p5 * log(Np)*log(Ne)
    ])

    P_model = 6 # The number of parameters for our model

    initial_params = np.zeros(P_model)
    if N &gt; 0:
        initial_params[0] = np.mean(y) # Default intercept guess

    try:
        # Perform OLS if enough data points exist to solve for parameters.
        # rcond=None ensures modern default behavior for singular values.
        if N &gt;= P_model:
            ols_params, _, _, _ = np.linalg.lstsq(design_matrix, y, rcond=None)
            
            # Ensure the OLS result has P_model elements.
            if len(ols_params) &lt; P_model:
                initial_params = np.pad(ols_params, (0, P_model - len(ols_params)), &#x27;constant&#x27;)
            elif len(ols_params) &gt; P_model:
                initial_params = ols_params[:P_model]
            else:
                initial_params = ols_params
    except np.linalg.LinAlgError:
        # Fallback to default initial_params if OLS encounters an error.
        pass

    def objective_with_grad(flat_params):
        &quot;&quot;&quot;
        Calculates the Mean Squared Error (MSE) and its analytical Jacobian.
        &quot;&quot;&quot;
        # The scaling_law_func handles the internal log-transformation of data_points.
        pred = scaling_law_func(data_points, flat_params)
        residuals = pred - y
        mse = np.mean(residuals ** 2)
        
        # The gradient of MSE for a linear model (Y = X @ p) is (2/N) * X.T @ (X @ p - y).
        # Here, `design_matrix` is X, `flat_params` is p, and `residuals` is (X @ p - y).
        grad = (2.0 / N) * np.dot(design_matrix.T, residuals)
        
        return mse, grad

    # Define bounds for parameters to ensure theoretical stability and physical plausibility.
    # These bounds are chosen based on typical scaling law behaviors for loss:
    # - p1 (log(Np)): Should be negative or zero, as increasing model size usually reduces or plateaus loss.
    # - p3 (log(Np)^2): Should be positive or zero, to model diminishing returns (loss curve bends upwards on log-log plot).
    # - p4 (1/Ne): Should be positive or zero. As Ne increases, 1/Ne decreases. A positive p4 means this term reduces loss as Ne increases.
    # Other parameters (p0, p2, p5) are left unbounded as their signs can be context-dependent or absorb other effects.
    bounds = [
        (None, None),   # p0: intercept - no strong theoretical bound
        (-100.0, 0.0),  # p1: log_dense_parameter_count coefficient (loss decreases with model size)
        (None, None),   # p2: log_num_experts coefficient - can be complex with 1/Ne present
        (0.0, None),    # p3: (log_dense_parameter_count)^2 coefficient (diminishing returns)
        (0.0, None),    # p4: 1/num_experts coefficient (loss decreases as Ne increases)
        (None, None)    # p5: interaction term - no strong theoretical bound
    ]

    # Use the L-BFGS-B optimizer, which supports bounds and analytical Jacobian.
    result = minimize(objective_with_grad, initial_params, method=&#x27;L-BFGS-B&#x27;, jac=True, bounds=bounds)

    # Return the optimized parameters if the optimization was successful,
    # otherwise fall back to the initial OLS guess (which is generally good).
    params_opt = result.x if result.success else initial_params

    return params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.056532
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts validation loss based on an additive inverse power law model, extended with an expert offset.
    The model is: Loss = L_min + A_param / (dense_parameter_count ^ alpha_param) + A_expert / ((num_experts + C_expert_offset) ^ alpha_expert)

    This formulation uses 6 parameters to capture the independent scaling effects of
    dense parameter count and number of experts, plus an irreducible minimum loss and
    an offset for the expert count that can better model MoE behavior at low expert counts.

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
                                  N is the number of data points.
        params (np.ndarray): Array of 6 parameters. Can be (P,) or (T, P) where P=6.
                             P = [L_min, A_param, alpha_param, A_expert, alpha_expert, C_expert_offset]

    Returns:
        np.ndarray: Predicted validation loss values (N,) or (N, T).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    
    # Ensure params is 2D (T, P) for consistent broadcasting
    params_arr = np.asarray(params)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :]

    # Unpack parameters (6 parameters total)
    L_mins = params_arr[:, 0]          # Irreducible minimum loss
    A_params = params_arr[:, 1]        # Coefficient for dense parameter scaling term
    alpha_params = params_arr[:, 2]    # Exponent for dense parameter scaling term (positive for inverse power)
    A_experts = params_arr[:, 3]       # Coefficient for expert count scaling term
    alpha_experts = params_arr[:, 4]   # Exponent for expert count scaling term (positive for inverse power)
    C_expert_offsets = params_arr[:, 5] # Offset added to num_experts, useful for small expert counts

    # Extract features
    num_experts_raw = X[:, 0]
    dense_param_count_raw = X[:, 1]

    # Apply a small epsilon to ensure positivity for power functions and avoid division by zero
    dense_param_count = np.maximum(dense_param_count_raw, 1e-9)
    
    # Apply expert offset and ensure positivity
    num_experts_adjusted = np.maximum(num_experts_raw[:, None] + C_expert_offsets[None, :], 1e-9)
    
    # Ensure exponents are positive for the inverse power law formulation (as in 1/x^alpha)
    safe_alpha_params = np.maximum(alpha_params, 1e-9)
    safe_alpha_experts = np.maximum(alpha_experts, 1e-9)

    # Calculate the two additive inverse power law terms using broadcasting
    term_param = A_params[None, :] / np.power(dense_param_count[:, None], safe_alpha_params[None, :])
    term_expert = A_experts[None, :] / np.power(num_experts_adjusted, safe_alpha_experts[None, :])
    
    # Sum the terms with the minimum loss to get the final prediction
    pred = L_mins[None, :] + term_param + term_expert
    
    # Return (N,) if T=1 (single set of parameters), otherwise (N, T)
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the additive inverse power law scaling function to the provided data using bounded optimization.

    Args:
        data_points (np.ndarray): (N, 2) array with columns [num_experts, dense_parameter_count].
        loss_values (np.ndarray): (N,) array of corresponding validation loss values.

    Returns:
        np.ndarray: Optimized parameters [L_min, A_param, alpha_param, A_expert, alpha_expert, C_expert_offset] (1D array).
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # The scaling law uses 6 parameters
    # P = [L_min, A_param, alpha_param, A_expert, alpha_expert, C_expert_offset]

    # --- Initial guess and bounds setup ---
    min_loss_obs = np.min(y)
    max_loss_obs = np.max(y)
    
    # Initial guess for L_min: a positive value strictly below the minimum observed loss.
    # This ensures that the power law terms are positive contributions to the loss above L_min.
    l_min_init_val = np.maximum(0.01, min_loss_obs - 0.05 * (max_loss_obs - min_loss_obs))
    l_min_init_val = np.clip(l_min_init_val, 0.001, min_loss_obs * 0.999 if min_loss_obs &gt; 0.001 else 0.001)

    # Heuristic initial guesses for coefficients and exponents.
    # The variable part of the loss is (max_loss_obs - l_min_init_val).
    # We can distribute this potential reduction between the two power law terms.
    total_variable_loss_range = max_loss_obs - l_min_init_val
    # A_init takes half of the variable loss range as a starting point, ensuring it&#x27;s positive.
    A_init = np.maximum(0.1, total_variable_loss_range / 2.0) 

    initial_params = np.array([
        l_min_init_val,  # L_min
        A_init,          # A_param (coefficient for dense_parameter_count term)
        0.1,             # alpha_param (exponent for dense_parameter_count term, common scaling value)
        A_init,          # A_expert (coefficient for num_experts term)
        0.1,             # alpha_expert (exponent for num_experts term, common scaling value)
        1.0              # C_expert_offset (small positive offset for num_experts)
    ])
    
    # Define bounds for parameters to ensure stability and physical realism
    bounds = [
        (0.001, min_loss_obs - 1e-5), # L_min: Must be positive, strictly less than min observed loss.
        (1e-6, max_loss_obs * 5),     # A_param: Positive, upper bound allows for a significant portion of max_loss_obs.
        (1e-6, 2.0),                  # alpha_param: Positive exponent (e.g., 0.0 to 2.0).
        (1e-6, max_loss_obs * 5),     # A_expert: Positive, upper bound allows for a significant portion of max_loss_obs.
        (1e-6, 2.0),                  # alpha_expert: Positive exponent (e.g., 0.0 to 2.0).
        (0.0, 64.0)                   # C_expert_offset: Non-negative, up to the maximum observed experts.
    ]
    
    # Re-check and adjust L_min upper bound and initial guess if min_loss_obs is very small,
    # ensuring the upper bound is always greater than the lower bound.
    if bounds[0][0] &gt;= bounds[0][1]:
        bounds[0] = (0.001, min_loss_obs * 0.999 if min_loss_obs &gt; 0.001 else 0.001)
        # If L_min bound changes, adjust initial_params[0] to be within new valid range
        if initial_params[0] &gt;= bounds[0][1]:
            initial_params[0] = bounds[0][1] - 1e-6 
        elif initial_params[0] &lt; bounds[0][0]:
             initial_params[0] = bounds[0][0] # ensure it&#x27;s not below lower bound

    def objective(params_local):
        &quot;&quot;&quot;Calculates the Mean Squared Error for the given parameters.&quot;&quot;&quot;
        pred = scaling_law_func(X, params_local)
        
        # If predictions result in NaN or Inf (due to numerical instability, e.g., during intermediate steps),
        # return an extremely large value to guide the optimizer away from invalid regions.
        if not np.all(np.isfinite(pred)):
            return np.inf
        
        mse = np.mean((pred - y) ** 2)
        return mse

    # Use &#x27;L-BFGS-B&#x27; for bounded optimization, which is robust and handles constraints well.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)

    # Return the optimized parameters if the optimization was successful;
    # otherwise, return the robust initial guess to avoid returning potentially bad/NaN results.
    params_opt = result.x if result.success else initial_params
    
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>