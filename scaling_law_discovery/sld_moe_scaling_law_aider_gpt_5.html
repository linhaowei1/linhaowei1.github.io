<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - MoE Scaling Law - aider + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>MoE Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">aider</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">GPT-5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        0.825565
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.118968</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">-1.000000</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.825565
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># This module intentionally defines only a single public function: law

_FIT_CACHE: dict[str, dict] = {}

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    import math
    from typing import List, Dict
    import numpy as np

    # Lazy import to avoid import-time dependency unless needed for fitting
    def _load_dataset():
        try:
            from datasets import load_from_disk
        except Exception as e:
            raise RuntimeError(&quot;The &#x27;datasets&#x27; package is required to load /app/data.&quot;) from e
        return load_from_disk(&quot;/app/data&quot;)

    def _as_numpy_array(x):
        return np.asarray(list(x), dtype=np.float64)

    def _choose_group_column(column_names):
        # Try common group column names
        candidates = [
            &quot;group&quot;,
            &quot;Group&quot;,
            &quot;group_name&quot;,
            &quot;experiment_group&quot;,
            &quot;dataset_group&quot;,
            &quot;suite&quot;,
            &quot;task&quot;,
            &quot;series&quot;,
        ]
        for c in candidates:
            if c in column_names:
                return c
        return None

    def _extract_split(ds):
        # Accept either Dataset or DatasetDict
        try:
            from datasets import DatasetDict
            is_dict = isinstance(ds, DatasetDict)
        except Exception:
            is_dict = hasattr(ds, &quot;keys&quot;) and hasattr(ds, &quot;__getitem__&quot;)
        if is_dict:
            # Prefer &#x27;train&#x27;, otherwise pick the first available split
            for split_name in (&quot;train&quot;, &quot;training&quot;, &quot;train_set&quot;):
                if split_name in ds:
                    return ds[split_name]
            # Fallback: first split
            first_key = next(iter(ds.keys()))
            return ds[first_key]
        return ds

    def _fit_group_params(target_group: str):
        ds_all = _load_dataset()
        ds = _extract_split(ds_all)
        colnames = list(ds.column_names)

        # Required variable names (per problem statement)
        p_col = &quot;dense_parameter_count&quot;
        e_col = &quot;num_experts&quot;
        y_col = &quot;loss_validation&quot;

        for required in (p_col, e_col, y_col):
            if required not in colnames:
                raise KeyError(f&quot;Required column &#x27;{required}&#x27; not found in dataset columns: {colnames}&quot;)

        g_col = _choose_group_column(colnames)
        # Materialize the relevant rows for the selected group
        Ps, Es, Ys = [], [], []
        if g_col is None:
            # No group column; use all rows as one group
            for row in ds:
                try:
                    P = float(row[p_col])
                    E = float(row[e_col])
                    Y = float(row[y_col])
                except Exception:
                    continue
                if not (math.isfinite(P) and math.isfinite(E) and math.isfinite(Y)):
                    continue
                Ps.append(P)
                Es.append(E)
                Ys.append(Y)
        else:
            for row in ds:
                if str(row.get(g_col, &quot;&quot;)) != str(target_group):
                    continue
                try:
                    P = float(row[p_col])
                    E = float(row[e_col])
                    Y = float(row[y_col])
                except Exception:
                    continue
                if not (math.isfinite(P) and math.isfinite(E) and math.isfinite(Y)):
                    continue
                Ps.append(P)
                Es.append(E)
                Ys.append(Y)

            # If no rows matched the group, fallback to using all rows (shared fit)
            if len(Ps) == 0:
                for row in ds:
                    try:
                        P = float(row[p_col])
                        E = float(row[e_col])
                        Y = float(row[y_col])
                    except Exception:
                        continue
                    if not (math.isfinite(P) and math.isfinite(E) and math.isfinite(Y)):
                        continue
                    Ps.append(P)
                    Es.append(E)
                    Ys.append(Y)

        P = _as_numpy_array(Ps)
        E = _as_numpy_array(Es)
        Y = _as_numpy_array(Ys)

        # Basic guards
        eps = 1e-12
        P = np.clip(P, 1.0, None)
        E = np.clip(E, 1.0, None)

        # Model (shared functional form across groups):
        #   L ≈ w0 + w1 * P^(-α) + w2 * E^(-β) + w3 * (P^(-α) * E^(-β))
        # We grid-search α, β and solve for w via least squares.
        alpha_grid = np.linspace(0.1, 1.6, 31)  # 31 steps
        beta_grid = np.linspace(0.1, 1.6, 31)

        best = {
            &quot;mse&quot;: float(&quot;inf&quot;),
            &quot;alpha&quot;: None,
            &quot;beta&quot;: None,
            &quot;w&quot;: None,
        }

        # Precompute logs to speed up repeated power computations
        logP = np.log(P)
        logE = np.log(E)

        for alpha in alpha_grid:
            # P^{-α} = exp(-α log P)
            f1 = np.exp(-alpha * logP)
            for beta in beta_grid:
                f2 = np.exp(-beta * logE)
                f3 = f1 * f2

                # Design matrix with bias and interaction term
                X = np.column_stack([np.ones_like(f1), f1, f2, f3])

                # Solve least squares (small ridge by augmenting if needed)
                try:
                    w, *_ = np.linalg.lstsq(X, Y, rcond=None)
                except np.linalg.LinAlgError:
                    # Add tiny ridge if singular
                    lam = 1e-10
                    XT = X.T
                    A = XT @ X + lam * np.eye(X.shape[1])
                    b = XT @ Y
                    w = np.linalg.solve(A, b)

                resid = Y - X @ w
                mse = float(np.mean(resid * resid))
                if mse &lt; best[&quot;mse&quot;]:
                    best[&quot;mse&quot;] = mse
                    best[&quot;alpha&quot;] = float(alpha)
                    best[&quot;beta&quot;] = float(beta)
                    best[&quot;w&quot;] = w.astype(float)

        params = {
            &quot;alpha&quot;: best[&quot;alpha&quot;],
            &quot;beta&quot;: best[&quot;beta&quot;],
            &quot;w0&quot;: float(best[&quot;w&quot;][0]),
            &quot;w1&quot;: float(best[&quot;w&quot;][1]),
            &quot;w2&quot;: float(best[&quot;w&quot;][2]),
            &quot;w3&quot;: float(best[&quot;w&quot;][3]),
            &quot;mse&quot;: best[&quot;mse&quot;],
            &quot;formula&quot;: &quot;loss_validation ≈ w0 + w1*P^{-alpha} + w2*E^{-beta} + w3*(P^{-alpha}*E^{-beta})&quot;,
            &quot;variables&quot;: {&quot;P&quot;: p_col, &quot;E&quot;: e_col, &quot;Y&quot;: y_col},
        }
        return params

    # Ensure we&#x27;ve fit parameters for this group (cache to avoid repeated fitting)
    if group not in _FIT_CACHE:
        fitted = _fit_group_params(group)
        _FIT_CACHE[group] = fitted
        # Expose fitted params for introspection if desired
        try:
            # Attach to the function object (does not define a new function)
            if not hasattr(law, &quot;_params_cache&quot;):
                setattr(law, &quot;_params_cache&quot;, {})
            law._params_cache[group] = dict(_FIT_CACHE[group])
        except Exception:
            pass

    params = _FIT_CACHE[group]
    alpha = params[&quot;alpha&quot;]
    beta = params[&quot;beta&quot;]
    w0 = params[&quot;w0&quot;]
    w1 = params[&quot;w1&quot;]
    w2 = params[&quot;w2&quot;]
    w3 = params[&quot;w3&quot;]

    # Predict for each input row
    out: List[Dict[str, float]] = []
    for row in input_data:
        try:
            P = float(row.get(&quot;dense_parameter_count&quot;, float(&quot;nan&quot;)))
            E = float(row.get(&quot;num_experts&quot;, float(&quot;nan&quot;)))
        except Exception:
            P, E = float(&quot;nan&quot;), float(&quot;nan&quot;)

        if not (math.isfinite(P) and math.isfinite(E)):
            out.append({&quot;loss_validation&quot;: float(&quot;nan&quot;)})
            continue

        P = max(P, 1.0)
        E = max(E, 1.0)
        # Compute features
        f1 = P ** (-alpha)
        f2 = E ** (-beta)
        f3 = f1 * f2

        yhat = w0 + w1 * f1 + w2 * f2 + w3 * f3
        out.append({&quot;loss_validation&quot;: float(yhat)})

    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.791017
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1">from __future__ import annotations

import math
import os
from typing import Dict, List

import numpy as np


# Module-level cache for fitted parameters per group
# Each entry: group -&gt; dict with keys: L0, logK, pD, pE
_PARAMS_BY_GROUP: Dict[str, Dict[str, float]] = {}
_FITTED: bool = False
_DATASET_PATH: str = &quot;/app/data&quot;


def _load_dataset():
    &quot;&quot;&quot;
    Load the dataset from disk using datasets.load_from_disk.
    Returns an iterable of rows (dicts).
    &quot;&quot;&quot;
    try:
        from datasets import load_from_disk, Dataset, DatasetDict
    except Exception as e:
        raise RuntimeError(
            &quot;Failed to import the &#x27;datasets&#x27; library. Please ensure it is installed.&quot;
        ) from e

    ds = load_from_disk(_DATASET_PATH)

    # Normalize to a single iterable over rows
    rows = []
    if isinstance(ds, dict) or getattr(ds, &quot;__class__&quot;, None).__name__ == &quot;DatasetDict&quot;:
        # Combine all splits
        for split in (getattr(ds, &quot;values&quot;, lambda: [])() or ds.values()):
            # split is a Dataset
            rows.extend(iter(split))
    else:
        # Single Dataset
        rows = list(iter(ds))

    return rows


def _prepare_group_data(rows):
    &quot;&quot;&quot;
    Returns:
        data_by_group: dict[group] -&gt; dict with numpy arrays: D, E, Y
        all_data: dict with same keys but pooled across all groups, under key &quot;__ALL__&quot;
    &quot;&quot;&quot;
    data_by_group: Dict[str, Dict[str, np.ndarray]] = {}
    all_D, all_E, all_Y = [], [], []

    for r in rows:
        try:
            g = str(r[&quot;group&quot;])
            D = float(r[&quot;dense_parameter_count&quot;])
            E = float(r[&quot;num_experts&quot;])
            Y = float(r[&quot;loss_validation&quot;])
        except Exception:
            # Skip malformed rows
            continue

        # Filter invalid values
        if not (np.isfinite(D) and D &gt; 0.0):
            continue
        if not (np.isfinite(E) and E &gt; 0.0):
            continue
        if not (np.isfinite(Y) and Y &gt; 0.0):
            continue

        bucket = data_by_group.setdefault(g, {&quot;D&quot;: [], &quot;E&quot;: [], &quot;Y&quot;: []})
        bucket[&quot;D&quot;].append(D)
        bucket[&quot;E&quot;].append(E)
        bucket[&quot;Y&quot;].append(Y)

        all_D.append(D)
        all_E.append(E)
        all_Y.append(Y)

    # Convert lists to numpy arrays
    for g, v in data_by_group.items():
        v[&quot;D&quot;] = np.asarray(v[&quot;D&quot;], dtype=float)
        v[&quot;E&quot;] = np.asarray(v[&quot;E&quot;], dtype=float)
        v[&quot;Y&quot;] = np.asarray(v[&quot;Y&quot;], dtype=float)

    data_by_group[&quot;__ALL__&quot;] = {
        &quot;D&quot;: np.asarray(all_D, dtype=float),
        &quot;E&quot;: np.asarray(all_E, dtype=float),
        &quot;Y&quot;: np.asarray(all_Y, dtype=float),
    }
    return data_by_group


def _fit_power_law_with_offset(D: np.ndarray, E: np.ndarray, Y: np.ndarray) -&gt; Dict[str, float]:
    &quot;&quot;&quot;
    Fit the following scaling law (same functional form for every group):
        L(D, E) = L0 + K * D^pD * E^pE
    where typically pD and pE will be negative.

    We fit by grid-searching L0 and, for each candidate, doing linear least-squares on:
        log(Y - L0) = logK + pD * log(D) + pE * log(E)

    Returns dict with keys: L0, logK, pD, pE
    &quot;&quot;&quot;
    # Basic sanity
    if D.size == 0 or E.size == 0 or Y.size == 0:
        # Fallback: trivial constant model
        m = float(np.mean(Y)) if Y.size else 1.0
        return {&quot;L0&quot;: max(1e-8, 0.9 * m), &quot;logK&quot;: math.log(max(1e-8, 0.1 * m)), &quot;pD&quot;: 0.0, &quot;pE&quot;: 0.0}

    minY = float(np.min(Y))
    maxY = float(np.max(Y))
    rng = maxY - minY
    if not np.isfinite(rng) or rng &lt;= 0:
        # Degenerate: nearly constant losses
        # Make L0 just below minY, small K and zero exponents
        tiny = max(1e-12, 1e-6 * abs(minY))
        return {&quot;L0&quot;: minY - tiny, &quot;logK&quot;: math.log(tiny), &quot;pD&quot;: 0.0, &quot;pE&quot;: 0.0}

    # Candidate L0 values (must be strictly below min(Y))
    # Use a mixture of coarse-to-fine values below minY
    eps = max(1e-12, 1e-8 * abs(minY))
    coarse = minY - np.linspace(0.0, 0.99, 40) * (rng + eps)
    fine = np.linspace(minY - 10 * eps, minY - eps, 10)
    L0_candidates = np.unique(np.concatenate([coarse, fine]))

    logD = np.log(D)
    logE = np.log(E)

    best = None  # (rss, params)
    for L0 in L0_candidates:
        # Ensure positivity
        diff = Y - L0
        if np.any(diff &lt;= 0):
            continue

        t = np.log(diff)
        X = np.column_stack([np.ones_like(logD), logD, logE])

        # Least squares fit: minimize ||X c - t||^2
        try:
            c, residuals, rank, s = np.linalg.lstsq(X, t, rcond=None)
        except Exception:
            continue

        # Compute RSS explicitly for robustness
        pred = X @ c
        rss = float(np.sum((t - pred) ** 2))

        # Regularize slightly to avoid extreme exponents on tiny datasets
        rss += 1e-10 * float(np.sum(c[1:] ** 2))

        if (best is None) or (rss &lt; best[0]):
            best = (rss, {&quot;L0&quot;: float(L0), &quot;logK&quot;: float(c[0]), &quot;pD&quot;: float(c[1]), &quot;pE&quot;: float(c[2])})

    if best is None:
        # Fallback: just under minY
        tiny = max(1e-12, 1e-6 * abs(minY))
        return {&quot;L0&quot;: minY - tiny, &quot;logK&quot;: math.log(tiny), &quot;pD&quot;: 0.0, &quot;pE&quot;: 0.0}

    return best[1]


def _fit_all_groups():
    global _FITTED, _PARAMS_BY_GROUP

    rows = _load_dataset()
    data = _prepare_group_data(rows)

    # Fit global model
    all_params = _fit_power_law_with_offset(data[&quot;__ALL__&quot;][&quot;D&quot;], data[&quot;__ALL__&quot;][&quot;E&quot;], data[&quot;__ALL__&quot;][&quot;Y&quot;])
    _PARAMS_BY_GROUP[&quot;__ALL__&quot;] = all_params

    # Fit per-group models; if data too small, fall back to global
    for g, v in data.items():
        if g == &quot;__ALL__&quot;:
            continue
        D, E, Y = v[&quot;D&quot;], v[&quot;E&quot;], v[&quot;Y&quot;]
        if min(D.size, E.size, Y.size) &lt; 3:
            _PARAMS_BY_GROUP[g] = all_params
            continue
        _PARAMS_BY_GROUP[g] = _fit_power_law_with_offset(D, E, Y)

    _FITTED = True


def _ensure_fitted():
    if not _FITTED:
        _fit_all_groups()


def _predict_for_group(dense: float, experts: float, group: str) -&gt; float:
    _ensure_fitted()
    params = _PARAMS_BY_GROUP.get(group) or _PARAMS_BY_GROUP.get(&quot;__ALL__&quot;)
    if params is None:
        # Final fallback in pathological cases
        return float(dense)  # arbitrary, but won&#x27;t crash

    # Guard inputs
    d = max(1e-12, float(dense))
    e = max(1e-12, float(experts))

    L0 = params[&quot;L0&quot;]
    logK = params[&quot;logK&quot;]
    pD = params[&quot;pD&quot;]
    pE = params[&quot;pE&quot;]

    pred = L0 + math.exp(logK) * (d ** pD) * (e ** pE)
    # Numerical safety: ensure positive and finite
    if not np.isfinite(pred) or pred &lt;= 0:
        # fallback to safe value close to L0
        pred = max(L0 + 1e-8, 1e-8)
    return float(pred)


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        try:
            dense = float(row[&quot;dense_parameter_count&quot;])
            experts = float(row[&quot;num_experts&quot;])
        except Exception as e:
            raise ValueError(
                &quot;Each input datum must contain &#x27;dense_parameter_count&#x27; and &#x27;num_experts&#x27; as numeric values.&quot;
            ) from e

        yhat = _predict_for_group(dense, experts, group)
        outputs.append({&quot;loss_validation&quot;: float(yhat)})
    return outputs


def debug_dump_params() -&gt; None:
    &quot;&quot;&quot;
    Utility: Fit (if necessary) and print the fitted parameters for all groups.
    &quot;&quot;&quot;
    _ensure_fitted()
    groups = sorted([g for g in _PARAMS_BY_GROUP.keys() if g != &quot;__ALL__&quot;])
    print(&quot;# Fitted parameters per group for law: L = L0 + exp(logK) * D^pD * E^pE&quot;)
    print(&quot;# Columns: group, L0, logK, pD, pE&quot;)
    for g in groups:
        p = _PARAMS_BY_GROUP[g]
        print(f&quot;{g}\t{p[&#x27;L0&#x27;]:.8g}\t{p[&#x27;logK&#x27;]:.8g}\t{p[&#x27;pD&#x27;]:.8g}\t{p[&#x27;pE&#x27;]:.8g}&quot;)
    # Also print global
    p = _PARAMS_BY_GROUP.get(&quot;__ALL__&quot;, None)
    if p is not None:
        print(&quot;\n# Global (pooled) fit&quot;)
        print(f&quot;__ALL__\t{p[&#x27;L0&#x27;]:.8g}\t{p[&#x27;logK&#x27;]:.8g}\t{p[&#x27;pD&#x27;]:.8g}\t{p[&#x27;pE&#x27;]:.8g}&quot;)


if __name__ == &quot;__main__&quot;:
    # If executed directly, print fitted parameters as a quick sanity check.
    debug_dump_params()</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.731313
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2">from typing import List, Dict
import math

# Lazy-fit cache (module globals, no extra public functions exposed)
_FITTED: bool = False
_GROUP_MODELS: Dict[str, Dict[str, float]] = {}
_DATA_PATH: str = &quot;/app/data&quot;


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
                    Required keys per item:
                      - &#x27;num_experts&#x27; (float, &gt; 0)
                      - &#x27;dense_parameter_count&#x27; (float, &gt; 0)
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is fixed across groups, but
               coefficients are fitted per group from /app/data.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s):
          - &#x27;loss_validation&#x27;
    &quot;&quot;&quot;
    # Import heavy deps inside the function to keep module import cheap.
    # The environment is expected to provide HuggingFace datasets and numpy.
    global _FITTED, _GROUP_MODELS

    # Small positive floor to avoid log/zero issues
    EPS = 1e-12

    # Fit once (lazily) from the provided dataset on disk.
    if not _FITTED:
        try:
            import numpy as np
            from datasets import load_from_disk
        except Exception:
            # If datasets/numpy are unavailable, fall back to a generic prior-like model.
            # This still preserves the functional form and keeps predictions finite.
            _GROUP_MODELS = {
                &quot;GLOBAL&quot;: {
                    &quot;L&quot;: 1.0,
                    &quot;A&quot;: 1.0,
                    &quot;alpha&quot;: 0.5,
                    &quot;B&quot;: 0.5,
                    &quot;beta&quot;: 0.5,
                    &quot;D&quot;: 0.25,
                    &quot;gamma&quot;: 0.5,
                }
            }
            _FITTED = True
        else:
            # Load dataset from disk
            ds_any = load_from_disk(_DATA_PATH)

            # Extract rows into a simple list of dicts
            rows: List[Dict[str, float]] = []
            try:
                from datasets import DatasetDict, Dataset

                if hasattr(ds_any, &quot;keys&quot;):  # likely a DatasetDict
                    # Prefer &#x27;train&#x27; if available, else merge all splits
                    if &quot;train&quot; in ds_any.keys():
                        base = ds_any[&quot;train&quot;]
                        rows = base.to_list()
                    else:
                        rows = []
                        for k in ds_any.keys():
                            rows.extend(ds_any[k].to_list())
                else:
                    # Single Dataset
                    rows = ds_any.to_list()
            except Exception:
                # Last-resort attempt: assume iterable of dict-like
                try:
                    rows = list(ds_any)
                except Exception:
                    rows = []

            # Detect key names robustly
            group_key_candidates = (&quot;group&quot;, &quot;Group&quot;, &quot;group_name&quot;, &quot;family&quot;)
            feature_example = rows[0] if rows else {}
            if feature_example:
                gkey = next((k for k in group_key_candidates if k in feature_example), None)
            else:
                gkey = None

            def get_group_value(r: Dict) -&gt; str:
                if gkey is None:
                    return &quot;GLOBAL&quot;
                return str(r.get(gkey, &quot;GLOBAL&quot;))

            # Required feature keys
            ne_key_candidates = (&quot;num_experts&quot;, &quot;experts&quot;, &quot;n_experts&quot;)
            dp_key_candidates = (&quot;dense_parameter_count&quot;, &quot;dense_params&quot;, &quot;non_expert_params&quot;, &quot;dense_parameters&quot;)

            def get_key(cands):
                return next((k for k in cands if (feature_example and k in feature_example)), cands[0])

            ne_key = get_key(ne_key_candidates)
            dp_key = get_key(dp_key_candidates)
            y_key = &quot;loss_validation&quot;  # per prompt

            # Group data
            groups: Dict[str, Dict[str, list]] = {}
            for r in rows:
                try:
                    ne = float(r.get(ne_key, float(&quot;nan&quot;)))
                    dp = float(r.get(dp_key, float(&quot;nan&quot;)))
                    y = float(r.get(y_key, float(&quot;nan&quot;)))
                except Exception:
                    continue
                if not (math.isfinite(ne) and math.isfinite(dp) and math.isfinite(y)):
                    continue
                if ne &lt;= 0 or dp &lt;= 0:
                    continue
                g = get_group_value(r)
                bucket = groups.setdefault(g, {&quot;ne&quot;: [], &quot;dp&quot;: [], &quot;y&quot;: []})
                bucket[&quot;ne&quot;].append(ne)
                bucket[&quot;dp&quot;].append(dp)
                bucket[&quot;y&quot;].append(y)

            # Also build a GLOBAL pool across all rows for fallback
            if groups:
                all_ne = [v for g in groups.values() for v in g[&quot;ne&quot;]]
                all_dp = [v for g in groups.values() for v in g[&quot;dp&quot;]]
                all_y = [v for g in groups.values() for v in g[&quot;y&quot;]]
                groups[&quot;GLOBAL&quot;] = {&quot;ne&quot;: all_ne, &quot;dp&quot;: all_dp, &quot;y&quot;: all_y}
            else:
                # No data: fallback to a generic prior-like model
                groups = {
                    &quot;GLOBAL&quot;: {
                        &quot;ne&quot;: [8.0, 16.0, 32.0],
                        &quot;dp&quot;: [1e7, 5e7, 1e8],
                        &quot;y&quot;: [1.2, 1.0, 0.9],
                    }
                }

            def fit_one_group(ne_arr: np.ndarray, dp_arr: np.ndarray, y_arr: np.ndarray) -&gt; Dict[str, float]:
                # Log-transform inputs
                x1 = np.log(np.maximum(dp_arr, EPS))
                x2 = np.log(np.maximum(ne_arr, EPS))
                y = y_arr.astype(float)

                # Candidate exponents (coarse grid)
                grid = np.arange(0.1, 1.6, 0.1)  # 0.1..1.5

                best = {&quot;mse&quot;: float(&quot;inf&quot;)}
                # Ridge epsilon to stabilize normal equations
                ridge = 1e-10

                for alpha in grid:
                    t0 = np.exp(-alpha * x1)
                    for beta in grid:
                        t1 = np.exp(-beta * x2)
                        for gamma in grid:
                            t2 = np.exp(-gamma * (x1 - x2))
                            # Design matrix: [1, t0, t1, t2]
                            Z = np.column_stack([np.ones_like(y), t0, t1, t2])
                            # Solve ridge least squares
                            # (Z^T Z + lam I) c = Z^T y
                            ZTZ = Z.T @ Z
                            ZTy = Z.T @ y
                            ZTZ += ridge * np.eye(ZTZ.shape[0])
                            try:
                                coeffs = np.linalg.solve(ZTZ, ZTy)
                            except np.linalg.LinAlgError:
                                # Fallback to lstsq
                                coeffs, *_ = np.linalg.lstsq(Z, y, rcond=None)

                            y_hat = Z @ coeffs
                            mse = float(np.mean((y_hat - y) ** 2))
                            if mse &lt; best[&quot;mse&quot;]:
                                best = {
                                    &quot;mse&quot;: mse,
                                    &quot;L&quot;: float(coeffs[0]),
                                    &quot;A&quot;: float(coeffs[1]),
                                    &quot;alpha&quot;: float(alpha),
                                    &quot;B&quot;: float(coeffs[2]),
                                    &quot;beta&quot;: float(beta),
                                    &quot;D&quot;: float(coeffs[3]),
                                    &quot;gamma&quot;: float(gamma),
                                }

                return best

            # Fit per group
            fitted: Dict[str, Dict[str, float]] = {}
            for gname, gdata in groups.items():
                try:
                    ne_arr = np.asarray(gdata[&quot;ne&quot;], dtype=float)
                    dp_arr = np.asarray(gdata[&quot;dp&quot;], dtype=float)
                    y_arr = np.asarray(gdata[&quot;y&quot;], dtype=float)
                    # Basic sanity check
                    mask = np.isfinite(ne_arr) &amp; np.isfinite(dp_arr) &amp; np.isfinite(y_arr) &amp; (ne_arr &gt; 0) &amp; (dp_arr &gt; 0)
                    ne_arr = ne_arr[mask]
                    dp_arr = dp_arr[mask]
                    y_arr = y_arr[mask]
                    if ne_arr.size &gt;= 4:
                        fitted[gname] = fit_one_group(ne_arr, dp_arr, y_arr)
                    else:
                        # Too few points; copy from GLOBAL later
                        pass
                except Exception:
                    # If anything goes wrong fitting a group, we&#x27;ll fill it from GLOBAL
                    pass

            # Ensure GLOBAL exists and is fitted
            if &quot;GLOBAL&quot; not in fitted:
                # If still missing, fit on whatever GLOBAL pool we have
                try:
                    import numpy as np  # re-import safe guard
                    gl = groups.get(&quot;GLOBAL&quot;)
                    if gl:
                        fitted[&quot;GLOBAL&quot;] = fit_one_group(
                            np.asarray(gl[&quot;ne&quot;], dtype=float),
                            np.asarray(gl[&quot;dp&quot;], dtype=float),
                            np.asarray(gl[&quot;y&quot;], dtype=float),
                        )
                except Exception:
                    fitted[&quot;GLOBAL&quot;] = {
                        &quot;L&quot;: 1.0,
                        &quot;A&quot;: 1.0,
                        &quot;alpha&quot;: 0.5,
                        &quot;B&quot;: 0.5,
                        &quot;beta&quot;: 0.5,
                        &quot;D&quot;: 0.25,
                        &quot;gamma&quot;: 0.5,
                        &quot;mse&quot;: float(&quot;inf&quot;),
                    }

            # Fill missing/small groups from GLOBAL
            for gname in list(groups.keys()):
                if gname not in fitted:
                    fitted[gname] = dict(fitted[&quot;GLOBAL&quot;])

            _GROUP_MODELS = fitted
            _FITTED = True

    # Make predictions using the selected group&#x27;s coefficients, or GLOBAL fallback
    model = _GROUP_MODELS.get(group) or _GROUP_MODELS.get(&quot;GLOBAL&quot;)
    if model is None:
        # Absolute fallback (shouldn&#x27;t happen)
        model = {
            &quot;L&quot;: 1.0,
            &quot;A&quot;: 1.0,
            &quot;alpha&quot;: 0.5,
            &quot;B&quot;: 0.5,
            &quot;beta&quot;: 0.5,
            &quot;D&quot;: 0.25,
            &quot;gamma&quot;: 0.5,
        }

    L = float(model[&quot;L&quot;])
    A = float(model[&quot;A&quot;])
    alpha = float(model[&quot;alpha&quot;])
    B = float(model[&quot;B&quot;])
    beta = float(model[&quot;beta&quot;])
    D = float(model[&quot;D&quot;])
    gamma = float(model[&quot;gamma&quot;])

    out: List[Dict[str, float]] = []
    for row in input_data:
        ne = float(row.get(&quot;num_experts&quot;, 0.0))
        dp = float(row.get(&quot;dense_parameter_count&quot;, 0.0))
        # Guard against non-positive inputs
        ne = ne if (ne &gt; 0 and math.isfinite(ne)) else 1.0
        dp = dp if (dp &gt; 0 and math.isfinite(dp)) else 1.0

        # Predict using the discovered form:
        # loss ≈ L + A * dp^{-alpha} + B * ne^{-beta} + D * (dp/ne)^{-gamma}
        # Implemented in log-space for stability of the exponentials.
        x1 = math.log(max(dp, EPS))
        x2 = math.log(max(ne, EPS))

        t0 = math.exp(-alpha * x1)  # dp^{-alpha}
        t1 = math.exp(-beta * x2)   # ne^{-beta}
        t2 = math.exp(-gamma * (x1 - x2))  # (dp/ne)^{-gamma}

        y_hat = L + A * t0 + B * t1 + D * t2
        out.append({&quot;loss_validation&quot;: float(y_hat)})

    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #B22222; color: white;">
                        R² = -0.753057
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3">def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    import math
    # Fit on first use from the on-disk dataset and cache coefficients on the function.
    # Functional form (shared across groups):
    #   loss_validation_hat = a_g + b_g * log10(dense_parameter_count) + c_g * log10(num_experts)
    # Coefficients (a_g, b_g, c_g) are fitted per group with OLS; a global fallback is also fitted.

    try:
        from datasets import load_from_disk
    except Exception:
        load_from_disk = None

    try:
        import numpy as np
    except Exception:
        np = None  # If numpy is unavailable, we will still return a deterministic baseline.

    if not hasattr(law, &quot;_coeffs&quot;):
        law._coeffs = {}
        law._global = [0.0, 0.0, 0.0]  # Fallback if fitting fails

        if load_from_disk is not None:
            try:
                ds_any = load_from_disk(&quot;/app/data&quot;)
                # Resolve to a single dataset split
                if hasattr(ds_any, &quot;keys&quot;):  # DatasetDict
                    if &quot;train&quot; in ds_any:
                        ds = ds_any[&quot;train&quot;]
                    else:
                        ds = next(iter(ds_any.values()))
                else:
                    ds = ds_any  # Already a Dataset

                # Try columnar access; if it fails, fallback to row iteration
                try:
                    g_list = list(ds[&quot;group&quot;])
                    pd_list = [float(v) for v in ds[&quot;dense_parameter_count&quot;]]
                    ne_list = [float(v) for v in ds[&quot;num_experts&quot;]]
                    y_list = [float(v) for v in ds[&quot;loss_validation&quot;]]
                except Exception:
                    g_list = []
                    pd_list = []
                    ne_list = []
                    y_list = []
                    for row in ds:
                        g_list.append(row[&quot;group&quot;])
                        pd_list.append(float(row[&quot;dense_parameter_count&quot;]))
                        ne_list.append(float(row[&quot;num_experts&quot;]))
                        y_list.append(float(row[&quot;loss_validation&quot;]))

                # Local helper to fit OLS on provided indices
                def _fit_indices(indices):
                    if np is None or len(indices) == 0:
                        # Baseline: constant equal to mean target, zero slopes
                        if len(y_list) &gt; 0:
                            mu = float(sum(y_list) / len(y_list))
                        else:
                            mu = 0.0
                        return [mu, 0.0, 0.0]
                    X = []
                    y = []
                    for i in indices:
                        P = max(pd_list[i], 1e-12)
                        E = max(ne_list[i], 1e-12)
                        X.append([1.0, math.log10(P), math.log10(E)])
                        y.append(y_list[i])
                    X = np.array(X, dtype=float)
                    y = np.array(y, dtype=float)
                    try:
                        w, *_ = np.linalg.lstsq(X, y, rcond=None)
                        return [float(w[0]), float(w[1]), float(w[2])]
                    except Exception:
                        # Pseudoinverse fallback
                        try:
                            w = np.matmul(np.linalg.pinv(X), y)
                            return [float(w[0]), float(w[1]), float(w[2])]
                        except Exception:
                            mu = float(y.mean()) if hasattr(y, &quot;mean&quot;) else (sum(y) / len(y))
                            return [mu, 0.0, 0.0]

                # Fit per-group coefficients
                unique_groups = sorted(set(g_list))
                for g in unique_groups:
                    idxs = [i for i, gg in enumerate(g_list) if gg == g]
                    law._coeffs[g] = _fit_indices(idxs)

                # Global fallback using all data
                all_idxs = list(range(len(g_list)))
                law._global = _fit_indices(all_idxs)

                # Write explain.md with fitted coefficients (best-effort)
                try:
                    lines = []
                    lines.append(&quot;# MoE Scaling Law&quot;)
                    lines.append(&quot;&quot;)
                    lines.append(&quot;We model the validation loss as a log-linear function of dense parameters and the number of experts, with group-specific coefficients:&quot;)
                    lines.append(&quot;&quot;)
                    lines.append(&quot;    loss_validation_hat = a_g + b_g * log10(dense_parameter_count) + c_g * log10(num_experts)&quot;)
                    lines.append(&quot;&quot;)
                    lines.append(&quot;Fitting methodology: ordinary least squares per group on /app/data (train split if present),&quot;)
                    lines.append(&quot;with a global model used as a fallback for unseen groups.&quot;)
                    lines.append(&quot;&quot;)
                    lines.append(&quot;Fitted coefficients by group (a_g, b_g, c_g):&quot;)
                    if len(law._coeffs) == 0:
                        lines.append(&quot;- (no coefficients could be fitted; dataset not accessible at runtime)&quot;)
                    else:
                        for g in sorted(law._coeffs.keys()):
                            a, b, c = law._coeffs[g]
                            lines.append(f&quot;- {g}: a={a:.6f}, b={b:.6f}, c={c:.6f}&quot;)
                    if law._global is not None:
                        a, b, c = law._global
                        lines.append(&quot;&quot;)
                        lines.append(f&quot;Global fallback: a={a:.6f}, b={b:.6f}, c={c:.6f}&quot;)
                    with open(&quot;/app/explain.md&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                        f.write(&quot;\n&quot;.join(lines) + &quot;\n&quot;)
                except Exception:
                    pass
            except Exception:
                # Could not load or fit; keep default coefficients
                pass

    # Select coefficients for the requested group (or global fallback)
    a, b, c = law._coeffs.get(group, law._global)

    # Make predictions
    outputs: list[dict[str, float]] = []
    for row in input_data:
        P = max(float(row.get(&quot;dense_parameter_count&quot;, 0.0)), 1e-12)
        E = max(float(row.get(&quot;num_experts&quot;, 0.0)), 1e-12)
        pred = a + b * math.log10(P) + c * math.log10(E)
        outputs.append({&quot;loss_validation&quot;: float(pred)})
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #B22222; color: white;">
                        R² = -1.000000
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4">&quot;&quot;&quot;
Scaling law predictor for MoE architectures.

This module exposes a single function:

    law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]

It implements the discovered functional form:

    loss_validation ≈ L_inf[group]
                      + A[group] * dense_parameter_count^(-alpha[group])
                      + B[group] * num_experts^(-beta[group])

Per-group coefficients are loaded from /app/params.json. If that file does not
exist, the module will fit parameters from the dataset at /app/data, persist
them to /app/params.json, and also write a detailed explanation to /app/explain.md.

You can force (re)fitting and regenerating /app/explain.md by running:
    python /app/law.py
&quot;&quot;&quot;
from __future__ import annotations

import json
import math
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Tuple

import numpy as np


PARAMS_PATH = &quot;/app/params.json&quot;
DATASET_PATH = &quot;/app/data&quot;
EXPLAIN_PATH = &quot;/app/explain.md&quot;


@dataclass
class Coeffs:
    L_inf: float
    A: float
    alpha: float
    B: float
    beta: float

    def predict(self, num_experts: float, dense_parameter_count: float) -&gt; float:
        # Numerical safety: clamp to tiny positive to avoid 0**negative.
        e = max(float(num_experts), 1e-12)
        p = max(float(dense_parameter_count), 1e-12)
        return float(self.L_inf + self.A * p ** (-self.alpha) + self.B * e ** (-self.beta))


def _load_params() -&gt; Dict[str, Coeffs] | None:
    if not os.path.exists(PARAMS_PATH):
        return None
    with open(PARAMS_PATH, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        raw = json.load(f)
    coeffs: Dict[str, Coeffs] = {}
    for g, c in raw.get(&quot;coefficients&quot;, {}).items():
        coeffs[g] = Coeffs(
            L_inf=float(c[&quot;L_inf&quot;]),
            A=float(c[&quot;A&quot;]),
            alpha=float(c[&quot;alpha&quot;]),
            B=float(c[&quot;B&quot;]),
            beta=float(c[&quot;beta&quot;]),
        )
    return coeffs


def _save_params_and_explain(
    coeffs: Dict[str, Coeffs],
    fit_meta: Dict[str, Dict[str, float]],
    columns: Dict[str, str],
) -&gt; None:
    # Save params.json
    payload = {
        &quot;formula&quot;: &quot;loss = L_inf + A * dense_parameter_count^(-alpha) + B * num_experts^(-beta)&quot;,
        &quot;fitted_on&quot;: datetime.utcnow().isoformat() + &quot;Z&quot;,
        &quot;dataset_path&quot;: DATASET_PATH,
        &quot;columns&quot;: columns,
        &quot;coefficients&quot;: {
            g: {
                &quot;L_inf&quot;: c.L_inf,
                &quot;A&quot;: c.A,
                &quot;alpha&quot;: c.alpha,
                &quot;B&quot;: c.B,
                &quot;beta&quot;: c.beta,
                **({&quot;r2&quot;: fit_meta[g][&quot;r2&quot;], &quot;mse&quot;: fit_meta[g][&quot;mse&quot;]} if g in fit_meta else {}),
            }
            for g, c in coeffs.items()
        },
    }
    with open(PARAMS_PATH, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        json.dump(payload, f, indent=2, sort_keys=True)

    # Write explain.md with details and fitted parameters
    lines: List[str] = []
    lines.append(&quot;# Discovered Scaling Law for MoE Validation Loss&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;This document describes the fitted scaling law relating Mixture-of-Experts (MoE) architecture choices to the final validation loss.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Functional form&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;We model the validation loss as an additive, saturating power-law in the dense (non-expert) parameters and the number of experts:&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;loss_validation = L_inf + A * dense_parameter_count^(-alpha) + B * num_experts^(-beta)&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;- L_inf: irreducible loss floor (as capacity → ∞).&quot;)
    lines.append(&quot;- A, alpha: magnitude and decay exponent for dense parameters.&quot;)
    lines.append(&quot;- B, beta: magnitude and decay exponent for the number of experts.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;Rationale: In MoE models, increasing experts primarily expands conditional capacity, while dense parameters govern shared representation power. Empirically, both exhibit diminishing returns well-captured by power-laws; the additive form separates their contributions around a floor L_inf.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Fitting methodology&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;For each experimental group, we:&quot;)
    lines.append(&quot;- Performed a grid search over exponents alpha and beta on [0.05, 2.0].&quot;)
    lines.append(&quot;- For each (alpha, beta), solved a linear least-squares fit for (L_inf, A, B) in y ≈ c + A * P^(-alpha) + B * E^(-beta).&quot;)
    lines.append(&quot;- Selected the (alpha, beta) that minimized MSE, and reported the corresponding (L_inf, A, B).&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;This approach avoids non-convex optimization while capturing the main curvature via exponents.&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;## Fitted parameters by group&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;| Group | L_inf | A | alpha | B | beta | R^2 | MSE |&quot;)
    lines.append(&quot;|---|---:|---:|---:|---:|---:|---:|---:|&quot;)
    for g, c in coeffs.items():
        r2 = fit_meta.get(g, {}).get(&quot;r2&quot;, float(&quot;nan&quot;))
        mse = fit_meta.get(g, {}).get(&quot;mse&quot;, float(&quot;nan&quot;))
        lines.append(
            f&quot;| {g} | {c.L_inf:.6g} | {c.A:.6g} | {c.alpha:.6g} | {c.B:.6g} | {c.beta:.6g} | {r2:.4f} | {mse:.6g} |&quot;
        )
    lines.append(&quot;&quot;)
    lines.append(&quot;## Column mapping&quot;)
    lines.append(&quot;&quot;)
    lines.append(f&quot;- num_experts: `{columns.get(&#x27;num_experts&#x27;)}`&quot;)
    lines.append(f&quot;- dense_parameter_count: `{columns.get(&#x27;dense_parameter_count&#x27;)}`&quot;)
    lines.append(f&quot;- loss_validation: `{columns.get(&#x27;loss_validation&#x27;)}`&quot;)
    lines.append(f&quot;- group: `{columns.get(&#x27;group&#x27;)}`&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;To reproduce or update these values, run:&quot;)
    lines.append(&quot;&quot;)
    lines.append(&quot;```bash&quot;)
    lines.append(&quot;python /app/law.py&quot;)
    lines.append(&quot;```&quot;)
    with open(EXPLAIN_PATH, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.write(&quot;\n&quot;.join(lines))


def _choose_split(ds) -&gt; Tuple[object, str]:
    # HuggingFace datasets: handle DatasetDict or Dataset
    try:
        from datasets import DatasetDict  # type: ignore
    except Exception:
        DatasetDict = None  # type: ignore

    if DatasetDict is not None and isinstance(ds, DatasetDict):
        if &quot;train&quot; in ds:
            return ds[&quot;train&quot;], &quot;train&quot;
        # Fallback to the first available split
        first_key = next(iter(ds.keys()))
        return ds[first_key], first_key
    return ds, &quot;dataset&quot;


def _detect_columns(cols: List[str]) -&gt; Dict[str, str]:
    # Try a set of likely column names for each required variable.
    def pick(cands: List[str]) -&gt; str | None:
        for c in cands:
            if c in cols:
                return c
        return None

    experts = pick([&quot;num_experts&quot;, &quot;experts&quot;, &quot;n_experts&quot;, &quot;moe_num_experts&quot;])
    dense = pick(
        [
            &quot;dense_parameter_count&quot;,
            &quot;dense_params&quot;,
            &quot;dense_parameters&quot;,
            &quot;non_expert_parameters&quot;,
            &quot;dense_parameter_count_total&quot;,
        ]
    )
    loss = pick([&quot;loss_validation&quot;, &quot;val_loss&quot;, &quot;validation_loss&quot;, &quot;valid_loss&quot;])
    group = pick([&quot;group&quot;, &quot;group_name&quot;, &quot;group_id&quot;, &quot;dataset_group&quot;, &quot;task&quot;])

    missing = [name for name, v in [(&quot;num_experts&quot;, experts), (&quot;dense_parameter_count&quot;, dense), (&quot;loss_validation&quot;, loss)] if v is None]
    if missing:
        raise RuntimeError(f&quot;Required columns not found in dataset: {&#x27;, &#x27;.join(missing)}. Available: {cols}&quot;)

    return {
        &quot;num_experts&quot;: experts,  # type: ignore[arg-type]
        &quot;dense_parameter_count&quot;: dense,  # type: ignore[arg-type]
        &quot;loss_validation&quot;: loss,  # type: ignore[arg-type]
        &quot;group&quot;: group or &quot;__ALL__&quot;,
    }


def _to_numpy_column(dataset, key: str) -&gt; np.ndarray:
    # Convert a dataset column to numpy array of float
    data = dataset.to_dict()
    col = data[key]
    return np.asarray(col, dtype=float)


def _fit_group(E: np.ndarray, P: np.ndarray, y: np.ndarray) -&gt; Tuple[Coeffs, float, float]:
    # Grid over exponents, solve linear least squares for (c, A, B) for each.
    alphas = np.linspace(0.05, 2.0, 40)
    betas = np.linspace(0.05, 2.0, 40)

    best_loss = math.inf
    best_params = None  # type: ignore

    # Precompute logs if needed; here we directly compute power features.
    for alpha in alphas:
        fP = np.power(np.clip(P, 1e-12, None), -alpha)
        # Early compute to avoid repetition
        for beta in betas:
            fE = np.power(np.clip(E, 1e-12, None), -beta)
            # Design matrix with intercept
            F = np.stack([np.ones_like(fP), fP, fE], axis=1)
            # Solve least squares: y ≈ F @ theta, theta = [c, A, B]
            theta, residuals, rank, s = np.linalg.lstsq(F, y, rcond=None)
            yhat = F @ theta
            mse = float(np.mean((y - yhat) ** 2))
            if mse &lt; best_loss:
                best_loss = mse
                best_params = (theta[0], theta[1], float(alpha), theta[2], float(beta))

    assert best_params is not None
    c, A, alpha, B, beta = [float(v) for v in best_params]

    # Compute R^2
    yhat = (c + A * np.power(np.clip(P, 1e-12, None), -alpha) + B * np.power(np.clip(E, 1e-12, None), -beta))
    ss_res = float(np.sum((y - yhat) ** 2))
    ss_tot = float(np.sum((y - np.mean(y)) ** 2))
    r2 = 1.0 - (ss_res / ss_tot if ss_tot &gt; 0 else 0.0)
    mse = float(np.mean((y - yhat) ** 2))
    return Coeffs(L_inf=c, A=A, alpha=alpha, B=B, beta=beta), r2, mse


def _fit_from_dataset() -&gt; Tuple[Dict[str, Coeffs], Dict[str, Dict[str, float]], Dict[str, str]]:
    try:
        from datasets import load_from_disk  # type: ignore
    except Exception as e:
        raise RuntimeError(
            &quot;The &#x27;datasets&#x27; package is required to fit parameters. &quot;
            &quot;Install it via: pip install datasets&quot;
        ) from e

    ds = load_from_disk(DATASET_PATH)
    d, split = _choose_split(ds)
    cols = list(d.column_names)
    mapping = _detect_columns(cols)

    # Extract columns
    E_all = _to_numpy_column(d, mapping[&quot;num_experts&quot;])
    P_all = _to_numpy_column(d, mapping[&quot;dense_parameter_count&quot;])
    y_all = _to_numpy_column(d, mapping[&quot;loss_validation&quot;])

    # Groups
    group_key = mapping[&quot;group&quot;]
    if group_key == &quot;__ALL__&quot;:
        groups = {&quot;__ALL__&quot;: np.arange(len(y_all))}
    else:
        group_raw = d.to_dict()[group_key]
        # Map unique group names to indices
        uniq = {}
        for i, g in enumerate(group_raw):
            uniq.setdefault(str(g), []).append(i)
        groups = {k: np.asarray(v, dtype=int) for k, v in uniq.items()}

    coeffs: Dict[str, Coeffs] = {}
    meta: Dict[str, Dict[str, float]] = {}
    for g, idx in groups.items():
        E = E_all[idx]
        P = P_all[idx]
        y = y_all[idx]
        c, r2, mse = _fit_group(E, P, y)
        coeffs[g] = c
        meta[g] = {&quot;r2&quot;: float(r2), &quot;mse&quot;: float(mse)}

    return coeffs, meta, mapping


def _fit_and_save() -&gt; Dict[str, Coeffs]:
    coeffs, meta, mapping = _fit_from_dataset()
    _save_params_and_explain(coeffs, meta, mapping)
    return coeffs


def _fallback_coeffs(coeffs: Dict[str, Coeffs]) -&gt; Coeffs:
    # Median of parameters across groups as a robust fallback.
    arr = np.array([[c.L_inf, c.A, c.alpha, c.B, c.beta] for c in coeffs.values()], dtype=float)
    if arr.size == 0:
        # Reasonable generic defaults (will be poor but defined)
        return Coeffs(L_inf=0.5, A=1.0, alpha=0.5, B=1.0, beta=0.5)
    med = np.median(arr, axis=0)
    return Coeffs(L_inf=float(med[0]), A=float(med[1]), alpha=float(med[2]), B=float(med[3]), beta=float(med[4]))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing:
                        - &#x27;num_experts&#x27;: float
                        - &#x27;dense_parameter_count&#x27;: float
        group: The name of the experimental group for which to make predictions.
               The functional form is the same for all groups; coefficients differ.

    Returns:
        A list of dictionaries with a single key:
            - &#x27;loss_validation&#x27;: predicted validation loss (float)
    &quot;&quot;&quot;
    coeffs = _load_params()
    if coeffs is None:
        # Fit from dataset and persist params + explanation
        coeffs = _fit_and_save()

    # Choose coefficients for the requested group or a robust fallback
    c = coeffs.get(group)
    if c is None:
        c = _fallback_coeffs(coeffs)

    out: List[Dict[str, float]] = []
    for row in input_data:
        ne = float(row.get(&quot;num_experts&quot;, 0.0))
        dp = float(row.get(&quot;dense_parameter_count&quot;, 0.0))
        out.append({&quot;loss_validation&quot;: c.predict(ne, dp)})
    return out


if __name__ == &quot;__main__&quot;:
    # (Re)fit parameters from /app/data and regenerate /app/explain.md
    coeffs = _fit_and_save()
    print(f&quot;Fitted coefficients for {len(coeffs)} group(s) and wrote:&quot;)
    print(f&quot; - {PARAMS_PATH}&quot;)
    print(f&quot; - {EXPLAIN_PATH}&quot;)</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>