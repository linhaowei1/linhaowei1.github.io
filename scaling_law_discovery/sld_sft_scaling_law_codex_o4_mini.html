<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - SFT Scaling Law - codex + o4-mini</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>SFT Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">codex</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">o4-mini</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.948364 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.882920</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.787239</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.948364 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">import math

# Precomputed parameters for each experimental group (intercept a, slope b)
PARAMS: dict[str, tuple[float, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: (5.988266, -0.285828),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: (4.737756, -0.263382),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: (3.391898, -0.138080),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: (4.783598, -0.212231),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: (4.204573, -0.238074),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: (2.652870, -0.096028),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: (3.460631, -0.121686),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: (3.820356, -0.190610),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: (2.950335, -0.102045),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: (4.210632, -0.144671),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: (4.218448, -0.225641),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: (3.928450, -0.166944),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: (5.722812, -0.278850),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: (5.410756, -0.334725),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: (4.492634, -0.241355),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: (4.453519, -0.198143),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: (5.242536, -0.333921),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: (2.719349, -0.108131),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: (3.003527, -0.102269),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: (4.067384, -0.225039),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: (2.227610, -0.067383),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: (4.096427, -0.156777),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: (4.789217, -0.284161),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: (2.969051, -0.109319),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: (2.206064, -0.035974),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: (2.162636, -0.027858),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: (1.962319, -0.051533),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: (4.098808, -0.165621),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (3.335554, -0.082349),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (3.861648, -0.194937),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: (3.228232, -0.115502),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: (3.414668, -0.095339),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: (3.353100, -0.152914),
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: (6.179866, -0.296177),
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: (4.817200, -0.280875),
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: (3.493378, -0.147500),
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: (3.285517, -0.117172),
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (1.480251, -0.069629),
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (2.132301, -0.067850),
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: (3.752075, -0.134443),
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: (1.608933, -0.072299),
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: (2.594400, -0.090536),
}

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form is the same for all groups, but parameters
               differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) (sft_loss).
    &quot;&quot;&quot;
    if group not in PARAMS:
        raise ValueError(f&quot;Unknown group: {group}&quot;)
    a, b = PARAMS[group]
    predictions: list[dict[str, float]] = []
    for entry in input_data:
        x = entry[&#x27;sft_data_size&#x27;]
        y = a + b * math.log(x)
        predictions.append({&#x27;sft_loss&#x27;: y})
    return predictions</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.893000 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">&quot;&quot;&quot;
Scaling law function for predicting SFT loss based on data size.
&quot;&quot;&quot;
from typing import List, Dict

def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the coefficients differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) as &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    # Fitted power-law parameters per group: sft_loss = a * (sft_data_size ** b)
    _params = {
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 7.465597659576905, &#x27;b&#x27;: -0.08938336391209896},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 6.969626540790517, &#x27;b&#x27;: -0.12488797569428042},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.8453873317979776, &#x27;b&#x27;: -0.06600249911783039},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 5.713040837756609, &#x27;b&#x27;: -0.07829225824344388},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 6.293121529736756, &#x27;b&#x27;: -0.12918481189914824},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.8960123299335456, &#x27;b&#x27;: -0.05447544270876587},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.763368646463403, &#x27;b&#x27;: -0.05245645350446021},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 4.8571354747287865, &#x27;b&#x27;: -0.09575030044686328},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.1982974522931946, &#x27;b&#x27;: -0.05122695523416853},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.582026537858244, &#x27;b&#x27;: -0.05133628273093655},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 5.787318731177163, &#x27;b&#x27;: -0.11196737789112267},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 4.508753213335996, &#x27;b&#x27;: -0.0704142021738492},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 7.161143632325512, &#x27;b&#x27;: -0.09164738334291277},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 9.296091035384052, &#x27;b&#x27;: -0.1581352715189432},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 5.82331151864561, &#x27;b&#x27;: -0.10583613940732714},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 5.239565986675214, &#x27;b&#x27;: -0.07673442669563073},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 9.506911791025203, &#x27;b&#x27;: -0.16937059578718577},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.022916994149884, &#x27;b&#x27;: -0.06221765532278332},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.242895597483687, &#x27;b&#x27;: -0.049961389646430265},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 5.693457761695671, &#x27;b&#x27;: -0.11822788318303022},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.352359108086735, &#x27;b&#x27;: -0.04191838265818277},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.585856367154473, &#x27;b&#x27;: -0.06062077349418788},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 7.476891875498453, &#x27;b&#x27;: -0.1403389747361835},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.250071982062783, &#x27;b&#x27;: -0.05575365697201477},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 2.2344284064674027, &#x27;b&#x27;: -0.019171706023120263},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 2.180823669815048, &#x27;b&#x27;: -0.014692781292003678},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.0379653707032412, &#x27;b&#x27;: -0.034542865866457675},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.621169816481478, &#x27;b&#x27;: -0.06486990722366648},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 3.4542367430363625, &#x27;b&#x27;: -0.032132757710130744},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 4.880269855685557, &#x27;b&#x27;: -0.09617770187230981},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.5193429654174015, &#x27;b&#x27;: -0.05367728546091697},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 3.5889984641635846, &#x27;b&#x27;: -0.038154542411887996},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.9537507864011348, &#x27;b&#x27;: -0.07896963709680072},
        &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 7.743248668603319, &#x27;b&#x27;: -0.09034898764189149},
        &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 7.448123349328367, &#x27;b&#x27;: -0.1368351251564653},
        &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 4.005381596620439, &#x27;b&#x27;: -0.06983526356123387},
        &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.5927162123671015, &#x27;b&#x27;: -0.05385301204930485},
        &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 1.7460064580179184, &#x27;b&#x27;: -0.08169520675626497},
        &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.2734524247637107, &#x27;b&#x27;: -0.04525490715783883},
        &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.10316214565362, &#x27;b&#x27;: -0.054114550232136904},
        &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 1.8635831534568412, &#x27;b&#x27;: -0.07576270233096472},
        &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.8159260594072437, &#x27;b&#x27;: -0.05183604484401076},
    }
    if group not in _params:
        raise ValueError(f&quot;Unknown group &#x27;{group}&#x27;. Available groups: {list(_params.keys())}&quot;)
    a = _params[group][&#x27;a&#x27;]
    b = _params[group][&#x27;b&#x27;]
    results: List[Dict[str, float]] = []
    for entry in input_data:
        n = entry.get(&#x27;sft_data_size&#x27;)
        if n is None:
            raise KeyError(&quot;Each input_data dict must contain &#x27;sft_data_size&#x27;.&quot;)
        pred = a * (n ** b)
        results.append({&#x27;sft_loss&#x27;: pred})
    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.893000 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">import math

_PARAMS = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.465597659576905, &quot;m&quot;: -0.08938336391209896},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 6.969626540790517, &quot;m&quot;: -0.12488797569428042},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.8453873317979776, &quot;m&quot;: -0.06600249911783039},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 5.713040837756609, &quot;m&quot;: -0.07829225824344388},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 6.293121529736756, &quot;m&quot;: -0.12918481189914824},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.8960123299335456, &quot;m&quot;: -0.05447544270876587},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.763368646463403, &quot;m&quot;: -0.05245645350446021},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 4.8571354747287865, &quot;m&quot;: -0.09575030044686328},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.1982974522931946, &quot;m&quot;: -0.05122695523416853},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.582026537858244, &quot;m&quot;: -0.05133628273093655},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 5.787318731177163, &quot;m&quot;: -0.11196737789112267},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 4.508753213335996, &quot;m&quot;: -0.0704142021738492},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.161143632325512, &quot;m&quot;: -0.09164738334291277},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 9.296091035384052, &quot;m&quot;: -0.1581352715189432},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 5.82331151864561, &quot;m&quot;: -0.10583613940732714},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 5.239565986675214, &quot;m&quot;: -0.07673442669563073},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 9.506911791025203, &quot;m&quot;: -0.16937059578718577},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.022916994149884, &quot;m&quot;: -0.06221765532278332},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.242895597483687, &quot;m&quot;: -0.049961389646430265},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 5.693457761695671, &quot;m&quot;: -0.11822788318303022},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.352359108086735, &quot;m&quot;: -0.04191838265818277},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.585856367154473, &quot;m&quot;: -0.06062077349418788},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 7.476891875498453, &quot;m&quot;: -0.1403389747361835},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.250071982062783, &quot;m&quot;: -0.05575365697201477},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 2.2344284064674027, &quot;m&quot;: -0.019171706023120263},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 2.180823669815048, &quot;m&quot;: -0.014692781292003678},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.0379653707032412, &quot;m&quot;: -0.034542865866457675},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.621169816481478, &quot;m&quot;: -0.06486990722366648},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 3.4542367430363625, &quot;m&quot;: -0.032132757710130744},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 4.880269855685557, &quot;m&quot;: -0.09617770187230981},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.5193429654174015, &quot;m&quot;: -0.05367728546091697},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 3.5889984641635846, &quot;m&quot;: -0.038154542411887996},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.9537507864011348, &quot;m&quot;: -0.07896963709680072},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.743248668603319, &quot;m&quot;: -0.09034898764189149},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 7.448123349328367, &quot;m&quot;: -0.1368351251564653},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 4.005381596620439, &quot;m&quot;: -0.06983526356123387},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.5927162123671015, &quot;m&quot;: -0.05385301204930485},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 1.7460064580179184, &quot;m&quot;: -0.08169520675626497},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.2734524247637107, &quot;m&quot;: -0.04525490715783883},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.10316214565362, &quot;m&quot;: -0.054114550232136904},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 1.8635831534568412, &quot;m&quot;: -0.07576270233096472},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.8159260594072437, &quot;m&quot;: -0.05183604484401076},
}

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts sft_loss based on sft_data_size using a power-law scaling:   
        sft_loss = A * (sft_data_size ** m)   
    with parameters A and m specific to each group.

    Args:
        input_data: List of dicts with key &#x27;sft_data_size&#x27;.
        group: Experimental group identifier matching dataset groups.

    Returns:
        List of dicts with predicted &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    if group not in _PARAMS:
        raise ValueError(f&quot;Unknown group: {group}&quot;)
    A = _PARAMS[group][&quot;A&quot;]
    m = _PARAMS[group][&quot;m&quot;]
    results = []
    for datapoint in input_data:
        x = datapoint.get(&#x27;sft_data_size&#x27;)
        if x is None:
            raise KeyError(&quot;Input data must contain &#x27;sft_data_size&#x27;.&quot;)
        y_pred = A * (x ** m)
        results.append({&quot;sft_loss&quot;: y_pred})
    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.892997 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">&quot;&quot;&quot;
Scaling law predictor for Supervised Fine-Tuning (SFT) loss.
&quot;&quot;&quot;
# Discovered power-law form: loss = a * (data_size ** b), per group
_params = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 7.465598, &quot;b&quot;: -0.089383},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 6.969627, &quot;b&quot;: -0.124888},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 3.845387, &quot;b&quot;: -0.066002},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 5.713041, &quot;b&quot;: -0.078292},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 6.293122, &quot;b&quot;: -0.129185},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 2.896012, &quot;b&quot;: -0.054475},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 3.763369, &quot;b&quot;: -0.052456},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 4.857135, &quot;b&quot;: -0.095750},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 3.198297, &quot;b&quot;: -0.051227},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 4.582027, &quot;b&quot;: -0.051336},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 5.787319, &quot;b&quot;: -0.111967},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 4.508753, &quot;b&quot;: -0.070414},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 7.161144, &quot;b&quot;: -0.091647},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 9.296091, &quot;b&quot;: -0.158135},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 5.823312, &quot;b&quot;: -0.105836},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 5.239566, &quot;b&quot;: -0.076734},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 9.506912, &quot;b&quot;: -0.169371},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 3.022917, &quot;b&quot;: -0.062218},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 3.242896, &quot;b&quot;: -0.049961},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 5.693458, &quot;b&quot;: -0.118228},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 2.352359, &quot;b&quot;: -0.041918},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 4.585856, &quot;b&quot;: -0.060621},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 7.476892, &quot;b&quot;: -0.140339},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 3.250072, &quot;b&quot;: -0.055754},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 2.234428, &quot;b&quot;: -0.019172},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 2.180824, &quot;b&quot;: -0.014693},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 2.037965, &quot;b&quot;: -0.034543},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 4.621170, &quot;b&quot;: -0.064870},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 3.454237, &quot;b&quot;: -0.032133},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 4.880270, &quot;b&quot;: -0.096178},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 3.519343, &quot;b&quot;: -0.053677},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 3.588998, &quot;b&quot;: -0.038155},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 3.953751, &quot;b&quot;: -0.078970},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 7.743249, &quot;b&quot;: -0.090349},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 7.448123, &quot;b&quot;: -0.136835},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 4.005382, &quot;b&quot;: -0.069835},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 3.592716, &quot;b&quot;: -0.053853},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 1.746006, &quot;b&quot;: -0.081695},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 2.273452, &quot;b&quot;: -0.045255},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 4.103162, &quot;b&quot;: -0.054115},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 1.863583, &quot;b&quot;: -0.075763},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 2.815926, &quot;b&quot;: -0.051836},
}

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts SFT loss based on data size using a power-law scaling law.

    Args:
        input_data: List of data points, each with key &#x27;sft_data_size&#x27;.
        group: Experimental group specifying which coefficients to use.

    Returns:
        List of dictionaries with predicted &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    if group not in _params:
        raise ValueError(f&quot;Unknown group: {group}&quot;)
    coeffs = _params[group]
    a = coeffs[&quot;a&quot;]
    b = coeffs[&quot;b&quot;]
    predictions: list[dict[str, float]] = []
    for point in input_data:
        n = point.get(&quot;sft_data_size&quot;)
        loss = a * (n ** b)
        predictions.append({&quot;sft_loss&quot;: loss})
    return predictions</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.787239 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">&quot;&quot;&quot;
Module implementing the discovered scaling law for supervised fine-tuning loss.
&quot;&quot;&quot;

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
               The functional form of the law is the same for all groups,
               but the constant parameters/coefficients differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s) under key &#x27;sft_loss&#x27;.
    &quot;&quot;&quot;
    # Parameters per group: L_inf + a * N^{-alpha}
    params = {
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.88891522852415e-13, &#x27;a&#x27;: 6.973889417936588, &#x27;alpha&#x27;: 0.08134160664016567},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2549475345759155e-15, &#x27;a&#x27;: 5.923742908062525, &#x27;alpha&#x27;: 0.10520859443461071},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.73534819503915e-22, &#x27;a&#x27;: 3.7815959443424148, &#x27;alpha&#x27;: 0.06407311292143066},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.0986536858492428e-15, &#x27;a&#x27;: 5.416194447728044, &#x27;alpha&#x27;: 0.07203582303793636},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.5568069794903192e-15, &#x27;a&#x27;: 5.350693782433369, &#x27;alpha&#x27;: 0.10952387398189671},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.5817591620080455e-18, &#x27;a&#x27;: 2.8815067646791404, &#x27;alpha&#x27;: 0.05390255281176658},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.660261124868628e-19, &#x27;a&#x27;: 3.72668747103668, &#x27;alpha&#x27;: 0.05133622852901864},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.4572929400202255e-21, &#x27;a&#x27;: 4.601802095259647, &#x27;alpha&#x27;: 0.08937942187812241},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.166756221644035e-20, &#x27;a&#x27;: 3.1670522055347803, &#x27;alpha&#x27;: 0.05010482255812582},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.681581108258014e-16, &#x27;a&#x27;: 4.497775771102446, &#x27;alpha&#x27;: 0.049197297920243487},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.792914647146667e-14, &#x27;a&#x27;: 5.237428498166979, &#x27;alpha&#x27;: 0.10004126991906952},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.3546553850129447e-24, &#x27;a&#x27;: 4.456019626802189, &#x27;alpha&#x27;: 0.06905741523821077},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.485165927406458e-21, &#x27;a&#x27;: 6.796594796854344, &#x27;alpha&#x27;: 0.08550170315119676},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.8011504261522693e-13, &#x27;a&#x27;: 7.594455935790706, &#x27;alpha&#x27;: 0.13330962892884293},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.2505157704866666, &#x27;a&#x27;: 5.861961168558081, &#x27;alpha&#x27;: 0.12018562279904178},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.90097148338901e-13, &#x27;a&#x27;: 5.121314574164879, &#x27;alpha&#x27;: 0.07407830522184079},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.348996681476381e-17, &#x27;a&#x27;: 7.683922260497716, &#x27;alpha&#x27;: 0.14310663430953338},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7814633219457406, &#x27;a&#x27;: 2.620750838514362, &#x27;alpha&#x27;: 0.11520371495174195},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.451184733271898e-25, &#x27;a&#x27;: 3.2171035320467642, &#x27;alpha&#x27;: 0.049050713966180944},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.3610823050173863e-20, &#x27;a&#x27;: 5.229994584305667, &#x27;alpha&#x27;: 0.10805297945869073},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.9031538655738537e-16, &#x27;a&#x27;: 2.3491291175103908, &#x27;alpha&#x27;: 0.041762877333812815},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.937175363375485e-15, &#x27;a&#x27;: 4.468174390456382, &#x27;alpha&#x27;: 0.057611663183111395},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.294599340497756e-16, &#x27;a&#x27;: 6.499202040847061, &#x27;alpha&#x27;: 0.12331848582705905},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.338143776217542e-10, &#x27;a&#x27;: 3.2418388198175885, &#x27;alpha&#x27;: 0.055463997801027023},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.14082711544948e-10, &#x27;a&#x27;: 2.2339677911390265, &#x27;alpha&#x27;: 0.019148365255527804},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.4850563005254914e-21, &#x27;a&#x27;: 2.1770799489607238, &#x27;alpha&#x27;: 0.014498526959777614},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.26936689227561367, &#x27;a&#x27;: 1.788070219787539, &#x27;alpha&#x27;: 0.04220960709323751},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.526590900644073e-12, &#x27;a&#x27;: 4.571149010287784, &#x27;alpha&#x27;: 0.06361718985883816},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.114373774345398e-18, &#x27;a&#x27;: 3.439603746623132, &#x27;alpha&#x27;: 0.03165062217213554},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.10877531839249e-14, &#x27;a&#x27;: 4.734453353182449, &#x27;alpha&#x27;: 0.09261262630404997},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.672745382307123e-19, &#x27;a&#x27;: 3.492798565293053, &#x27;alpha&#x27;: 0.05281181441669174},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.873110659782422e-19, &#x27;a&#x27;: 3.549257797168276, &#x27;alpha&#x27;: 0.03688126478621837},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.632774773255506e-16, &#x27;a&#x27;: 3.926297389250783, &#x27;alpha&#x27;: 0.07816268985536855},
        &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.700579218298801e-13, &#x27;a&#x27;: 7.1983961879890614, &#x27;alpha&#x27;: 0.08172041645085604},
        &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.449791195262194e-16, &#x27;a&#x27;: 6.339048122442959, &#x27;alpha&#x27;: 0.11725592134070295},
        &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.929261588051236e-25, &#x27;a&#x27;: 3.946267218928482, &#x27;alpha&#x27;: 0.06811657162760475},
        &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.841398974031212e-21, &#x27;a&#x27;: 3.543052166590739, &#x27;alpha&#x27;: 0.052255117939287866},
        &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.41674098898230477, &#x27;a&#x27;: 1.8233793996455916, &#x27;alpha&#x27;: 0.16745997442401883},
        &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.264923914746759e-22, &#x27;a&#x27;: 2.2598720880828873, &#x27;alpha&#x27;: 0.04457386963843646},
        &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.731082823765475e-18, &#x27;a&#x27;: 4.0524929725844165, &#x27;alpha&#x27;: 0.05269102972550974},
        &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.40091815004312686, &#x27;a&#x27;: 1.7757420094751855, &#x27;alpha&#x27;: 0.13439805928221435},
        &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.5765442089801293e-20, &#x27;a&#x27;: 2.7905737195023983, &#x27;alpha&#x27;: 0.0507995309961754},
    }
    if group not in params:
        raise ValueError(f&quot;Unknown group: {group}&quot;)
    p = params[group]
    results = []
    for d in input_data:
        N = d.get(&#x27;sft_data_size&#x27;)
        # Apply scaling law: loss = L_inf + a * N^{-alpha}
        loss = p[&#x27;L_inf&#x27;] + p[&#x27;a&#x27;] * (N ** (-p[&#x27;alpha&#x27;]))
        results.append({&#x27;sft_loss&#x27;: loss})
    return results</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
