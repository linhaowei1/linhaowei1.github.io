<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - SFT Scaling Law - codex + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>SFT Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">codex</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.980828 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.854753</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.708780</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.980828 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">from __future__ import annotations

from typing import Dict, List


# Common functional form across all groups:
#   sft_loss(N) = L_inf + A * (N + N0) ** (-alpha)
# where N is `sft_data_size` and parameters (L_inf, A, alpha, N0) vary by `group`.

PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.865671228941949e-17, &#x27;A&#x27;: 12.637678561446139, &#x27;alpha&#x27;: 0.13564240302792172, &#x27;N0&#x27;: 3172.8349547867774},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.6238498800785734, &#x27;A&#x27;: 99.99999999999999, &#x27;alpha&#x27;: 0.3976078224298724, &#x27;N0&#x27;: 11558.491067541954},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.063925056378967e-15, &#x27;A&#x27;: 4.23348975814935, &#x27;alpha&#x27;: 0.07460412111294643, &#x27;N0&#x27;: 436.6866783168706},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.10578307107794e-19, &#x27;A&#x27;: 8.92224005340684, &#x27;alpha&#x27;: 0.11739594489898576, &#x27;N0&#x27;: 3069.407127001592},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.49159473249601304, &#x27;A&#x27;: 53.72380342046997, &#x27;alpha&#x27;: 0.35384913142132957, &#x27;N0&#x27;: 8208.078352425737},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2593622265895715e-07, &#x27;A&#x27;: 2.9896484714861424, &#x27;alpha&#x27;: 0.05735309742066091, &#x27;N0&#x27;: 140.71022280680234},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1318279234338576e-18, &#x27;A&#x27;: 4.062878459420533, &#x27;alpha&#x27;: 0.05934500761506854, &#x27;N0&#x27;: 426.03416018746805},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.3927299190951424e-21, &#x27;A&#x27;: 6.336196488869017, &#x27;alpha&#x27;: 0.1192014399950389, &#x27;N0&#x27;: 1084.1232942926576},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.220157697788088e-20, &#x27;A&#x27;: 3.4101310251993695, &#x27;alpha&#x27;: 0.056959891171103054, &#x27;N0&#x27;: 363.70595860114133},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.315142285734426e-22, &#x27;A&#x27;: 5.319365673488941, &#x27;alpha&#x27;: 0.06450038022371594, &#x27;N0&#x27;: 1162.8602806462848},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.01404483163551e-14, &#x27;A&#x27;: 10.7925493382822, &#x27;alpha&#x27;: 0.16678612735396567, &#x27;N0&#x27;: 2909.744466994437},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.6675446178776843e-19, &#x27;A&#x27;: 4.756329316561344, &#x27;alpha&#x27;: 0.07520633987592405, &#x27;N0&#x27;: 197.06918542242636},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.4024201572921337e-19, &#x27;A&#x27;: 9.466998707085137, &#x27;alpha&#x27;: 0.1163303938056367, &#x27;N0&#x27;: 1218.0973728427302},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.569762177448528, &#x27;A&#x27;: 99.9999998931003, &#x27;alpha&#x27;: 0.4093787678794461, &#x27;N0&#x27;: 6244.642597957972},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2241220338350347, &#x27;A&#x27;: 14.5986109269959, &#x27;alpha&#x27;: 0.29682572736757595, &#x27;N0&#x27;: 550.5089063545902},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1510770614726884e-17, &#x27;A&#x27;: 5.611410375270038, &#x27;alpha&#x27;: 0.0826931330009914, &#x27;N0&#x27;: 269.42264002978004},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4362137935788755, &#x27;A&#x27;: 61.030730385354516, &#x27;alpha&#x27;: 0.36845048401898794, &#x27;N0&#x27;: 4178.040636016911},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7814640933847434, &#x27;A&#x27;: 2.6207508142290563, &#x27;alpha&#x27;: 0.11520380951312024, &#x27;N0&#x27;: 7.81642673397928e-11},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.0801677443604495e-12, &#x27;A&#x27;: 3.437167448516286, &#x27;alpha&#x27;: 0.05519316053319131, &#x27;N0&#x27;: 323.52209754302413},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.303325908352575, &#x27;A&#x27;: 10.781976113943252, &#x27;alpha&#x27;: 0.19556325386045303, &#x27;N0&#x27;: 1844.5375817073113},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.849950839473154e-13, &#x27;A&#x27;: 2.371033103373906, &#x27;alpha&#x27;: 0.0426291470638209, &#x27;N0&#x27;: 42.52989450771065},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.0439604500936763e-21, &#x27;A&#x27;: 5.627459766341364, &#x27;alpha&#x27;: 0.07868013844546193, &#x27;N0&#x27;: 1427.3103646514269},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.3170636936481373, &#x27;A&#x27;: 21.45165635532035, &#x27;alpha&#x27;: 0.2548387620240815, &#x27;N0&#x27;: 2967.5199471791925},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.435230193328557e-18, &#x27;A&#x27;: 3.257815231163734, &#x27;alpha&#x27;: 0.055927008001466834, &#x27;N0&#x27;: 15.872263908578827},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.7464718748726132e-09, &#x27;A&#x27;: 2.239883130424448, &#x27;alpha&#x27;: 0.019392187633072766, &#x27;N0&#x27;: 27.45074276948627},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.6339485409447412, &#x27;A&#x27;: 1.852648048421951, &#x27;alpha&#x27;: 0.19215155224167946, &#x27;N0&#x27;: 5578.387896916012},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.8797137795917916, &#x27;A&#x27;: 1.3801606877059753, &#x27;alpha&#x27;: 0.09031133705708351, &#x27;N0&#x27;: 150.715299869533},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.0369199206942231e-16, &#x27;A&#x27;: 4.936124715180229, &#x27;alpha&#x27;: 0.07082556462170639, &#x27;N0&#x27;: 268.26577139087215},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.993452899495181e-20, &#x27;A&#x27;: 3.657207489347324, &#x27;alpha&#x27;: 0.037261242285363386, &#x27;N0&#x27;: 549.53675753708},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.178379339595827e-17, &#x27;A&#x27;: 5.558671475357777, &#x27;alpha&#x27;: 0.10787587390263807, &#x27;N0&#x27;: 388.26365277913226},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.0351395227477922e-12, &#x27;A&#x27;: 3.7361033461210726, &#x27;alpha&#x27;: 0.059085413639518115, &#x27;N0&#x27;: 296.79195384894666},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.7285593853941235e-20, &#x27;A&#x27;: 4.301700747580993, &#x27;alpha&#x27;: 0.054175354177230746, &#x27;N0&#x27;: 2255.1405984927396},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.385679168082505e-20, &#x27;A&#x27;: 4.070364998240034, &#x27;alpha&#x27;: 0.08159499527141832, &#x27;N0&#x27;: 84.73574253453266},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.015340729288829e-15, &#x27;A&#x27;: 14.34124705639774, &#x27;alpha&#x27;: 0.14433103359274688, &#x27;N0&#x27;: 3987.9597869657864},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.47259432236704396, &#x27;A&#x27;: 41.02611997599669, &#x27;alpha&#x27;: 0.3190891985857585, &#x27;N0&#x27;: 5570.9100030079235},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.8048515222229452e-22, &#x27;A&#x27;: 4.388935478788763, &#x27;alpha&#x27;: 0.0780881235127983, &#x27;N0&#x27;: 365.9997120466946},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.028632468328126e-18, &#x27;A&#x27;: 3.8842431380507225, &#x27;alpha&#x27;: 0.06076610065498959, &#x27;N0&#x27;: 454.6947810694002},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.416740982867647, &#x27;A&#x27;: 1.8233793878342568, &#x27;alpha&#x27;: 0.16745997189052603, &#x27;N0&#x27;: 7.833754721840204e-09},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.7554230404581307e-14, &#x27;A&#x27;: 2.3917550734396555, &#x27;alpha&#x27;: 0.049831240630992694, &#x27;N0&#x27;: 303.9980112153414},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.006136285290001e-21, &#x27;A&#x27;: 4.428866599546551, &#x27;alpha&#x27;: 0.06092213223812954, &#x27;N0&#x27;: 428.3855452155827},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.5585503700141525, &#x27;A&#x27;: 2.424820616633481, &#x27;alpha&#x27;: 0.20909831661111153, &#x27;N0&#x27;: 173.82806592094695},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.0419309334128787e-12, &#x27;A&#x27;: 3.0054681324705808, &#x27;alpha&#x27;: 0.057697528448694614, &#x27;N0&#x27;: 352.6602281915071},
}


# Fallback parameters (robust median-ish typical behavior) if an unknown group is requested.
FALLBACK = {&#x27;L_inf&#x27;: 0.5, &#x27;A&#x27;: 5.0, &#x27;alpha&#x27;: 0.10, &#x27;N0&#x27;: 300.0}


def _predict_loss(n: float, p: Dict[str, float]) -&gt; float:
    # Guard against negative or pathological inputs
    n = max(0.0, float(n))
    L_inf = float(p[&#x27;L_inf&#x27;])
    A = float(p[&#x27;A&#x27;])
    alpha = float(p[&#x27;alpha&#x27;])
    N0 = float(p[&#x27;N0&#x27;])
    return L_inf + A * (n + N0) ** (-alpha)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;

    params = PARAMS.get(group, FALLBACK)
    out: List[Dict[str, float]] = []
    for row in input_data:
        n = row.get(&#x27;sft_data_size&#x27;)
        if n is None:
            raise KeyError(&quot;Each input row must include &#x27;sft_data_size&#x27;.&quot;)
        pred = _predict_loss(n, params)
        out.append({&#x27;sft_loss&#x27;: float(pred)})
    return out</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.893894 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1">from __future__ import annotations

import math
from typing import Dict, List


# Discovered scaling law:
#   sft_loss(N) = c_g + A_g * N^{-alpha_g}
# where N is `sft_data_size` and parameters (A_g, alpha_g, c_g) depend on the group `g`.
# Parameters were fitted from /app/data using a grid search over c and closed-form
# linear regression in log-space for (A, alpha). See explain.md for details.


_PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.433658255860093, &quot;alpha&quot;: 0.09226000464368624, &quot;c&quot;: 0.09675148370440967},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 6.874107269901793, &quot;alpha&quot;: 0.0961185010987388, &quot;c&quot;: -0.6048987107551289},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.515099444896067, &quot;alpha&quot;: 0.09118112442159486, &quot;c&quot;: 0.5658052043085156},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 5.571301521167711, &quot;alpha&quot;: 0.08866702222735884, &quot;c&quot;: 0.308757393835946},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 6.165131768239635, &quot;alpha&quot;: 0.09804084906491561, &quot;c&quot;: -0.5602691056924662},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.4143756266192393, &quot;alpha&quot;: 0.09291530407343383, &quot;c&quot;: 0.7169066913563793},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.0922612285131654, &quot;alpha&quot;: 0.0926145433550617, &quot;c&quot;: 0.9894025703690386},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 4.8983902032674775, &quot;alpha&quot;: 0.08980259295448803, &quot;c&quot;: -0.12785715270495857},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.6082145614016587, &quot;alpha&quot;: 0.09040044252353252, &quot;c&quot;: 0.849221081874973},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.8360560761338975, &quot;alpha&quot;: 0.08283987366395605, &quot;c&quot;: 1.0527230568498172},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 5.82483780086157, &quot;alpha&quot;: 0.09288397507637496, &quot;c&quot;: -0.3990739992461749},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 4.203731485262738, &quot;alpha&quot;: 0.09241947304680805, &quot;c&quot;: 0.5521166281945702},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.166252744001162, &quot;alpha&quot;: 0.09118887577361773, &quot;c&quot;: -0.014849184533570536},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 8.530694977189771, &quot;alpha&quot;: 0.09957563721804477, &quot;c&quot;: -1.1823316026689035},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 5.831048007260207, &quot;alpha&quot;: 0.12181764753304936, &quot;c&quot;: 0.2875148200417713},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 5.055545188910447, &quot;alpha&quot;: 0.08972030157576516, &quot;c&quot;: 0.3642928048427696},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 8.440715433300968, &quot;alpha&quot;: 0.10128919982674446, &quot;c&quot;: -1.2542028866650223},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.625901988771753, &quot;alpha&quot;: 0.11450881725727033, &quot;c&quot;: 0.773990348139355},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 2.6019395932949694, &quot;alpha&quot;: 0.09128396055228089, &quot;c&quot;: 0.912092996343137},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 5.698463179207566, &quot;alpha&quot;: 0.09684355526954083, &quot;c&quot;: -0.404887663369363},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 1.6913415499558018, &quot;alpha&quot;: 0.0926469766458569, &quot;c&quot;: 0.8686594562819331},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.114510224476879, &quot;alpha&quot;: 0.08609102442015397, &quot;c&quot;: 0.7495118335285937},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 7.215778308055833, &quot;alpha&quot;: 0.09785084183895248, &quot;c&quot;: -0.8406868812657662},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.747043184816001, &quot;alpha&quot;: 0.09075858027212512, &quot;c&quot;: 0.743147348569997},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 0.9105549945441712, &quot;alpha&quot;: 0.08988730230965163, &quot;c&quot;: 1.467552846893482},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 0.7268311599516257, &quot;alpha&quot;: 0.09370785253710144, &quot;c&quot;: 1.5912280173978794},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 1.2863909658621537, &quot;alpha&quot;: 0.09301795411148002, &quot;c&quot;: 0.9279196588592447},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 4.169395463541406, &quot;alpha&quot;: 0.09377290515999764, &quot;c&quot;: 0.7709238331604022},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 2.1192744195133595, &quot;alpha&quot;: 0.08928573438345472, &quot;c&quot;: 1.6240943680061208},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 4.892420916516959, &quot;alpha&quot;: 0.09430230139026845, &quot;c&quot;: -0.039111613116963984},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 2.9138444990850156, &quot;alpha&quot;: 0.09375204525332917, &quot;c&quot;: 0.9044896404314862},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 2.4904411567913773, &quot;alpha&quot;: 0.0908186613029444, &quot;c&quot;: 1.430872600935274},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.810771690226976, &quot;alpha&quot;: 0.09346933670550471, &quot;c&quot;: 0.2930155180254166},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 7.7293097116637774, &quot;alpha&quot;: 0.09159395725105893, &quot;c&quot;: 0.04323131903587285},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 7.210656946072294, &quot;alpha&quot;: 0.09789730975284584, &quot;c&quot;: -0.7803905818129324},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 3.7187595959817026, &quot;alpha&quot;: 0.09361665473439741, &quot;c&quot;: 0.524992541691645},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.038007165514026, &quot;alpha&quot;: 0.08652520067841116, &quot;c&quot;: 0.8070610562780735},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 1.8251540531547072, &quot;alpha&quot;: 0.16771846337021848, &quot;c&quot;: 0.4170609196163503},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 1.72449969723365, &quot;alpha&quot;: 0.0914230741844482, &quot;c&quot;: 0.7464464112092959},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;A&quot;: 3.4550037153996023, &quot;alpha&quot;: 0.08889329646715426, &quot;c&quot;: 0.9551825336351996},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;A&quot;: 1.773215194224546, &quot;alpha&quot;: 0.1355972851826645, &quot;c&quot;: 0.40697723824523435},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;A&quot;: 2.302773376251765, &quot;alpha&quot;: 0.09181930287533102, &quot;c&quot;: 0.7478278999059439},
}

# Global fallback (simple mean across groups) in case an unseen group is requested.
_FALLBACK = {&quot;A&quot;: 4.148897722449893, &quot;alpha&quot;: 0.09644804195199079, &quot;c&quot;: 0.41145739856516367}


def _get_params_for_group(group: str) -&gt; Dict[str, float]:
    # Exact match first; otherwise, use fallback.
    if group in _PARAMS:
        return _PARAMS[group]
    # Light normalization attempt for common formatting differences.
    g = group.strip()
    if g in _PARAMS:
        return _PARAMS[g]
    return _FALLBACK


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = _get_params_for_group(group)
    A = float(params[&quot;A&quot;])  # amplitude
    alpha = float(params[&quot;alpha&quot;])  # power-law exponent
    c = float(params[&quot;c&quot;])  # irreducible loss floor

    out: List[Dict[str, float]] = []
    for row in input_data:
        N = float(row.get(&quot;sft_data_size&quot;, 0.0))
        # Guard against non-positive sizes; use a tiny positive number to avoid
        # division-by-zero while keeping monotonic behavior.
        if not math.isfinite(N) or N &lt;= 0.0:
            N = 1.0
        pred = c + A * (N ** (-alpha))
        out.append({&quot;sft_loss&quot;: float(pred)})
    return out</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.893354 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2">from __future__ import annotations

from typing import Dict, List


# Discovered scaling law (shared functional form across all groups):
#   sft_loss(N) = L_inf + A * N**(-alpha)
# where N is `sft_data_size`, and (L_inf, A, alpha) depend on `group`.


PARAMS: Dict[str, Dict[str, float]] = {
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 7.465597659576915,
    &quot;alpha&quot;: 0.08938336391209903
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 5.713040837756619,
    &quot;alpha&quot;: 0.078292258243444
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.582026537858248,
    &quot;alpha&quot;: 0.051336282730936586
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.7633686464634066,
    &quot;alpha&quot;: 0.05245645350446028
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 7.161143632325522,
    &quot;alpha&quot;: 0.09164738334291282
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 5.2395659866752196,
    &quot;alpha&quot;: 0.0767344266956308
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.2428955974836873,
    &quot;alpha&quot;: 0.04996138964643028
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.5858563671544825,
    &quot;alpha&quot;: 0.060620773494188006
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.2344284064674045,
    &quot;alpha&quot;: 0.019171706023120298
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 7.743248668603332,
    &quot;alpha&quot;: 0.09034898764189159
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.5927162123671033,
    &quot;alpha&quot;: 0.05385301204930487
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.103162145653623,
    &quot;alpha&quot;: 0.05411455023213695
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.621169816481482,
    &quot;alpha&quot;: 0.06486990722366653
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.5193429654174033,
    &quot;alpha&quot;: 0.053677285460916976
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 6.969626540790526,
    &quot;alpha&quot;: 0.12488797569428049
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 6.293121529736768,
    &quot;alpha&quot;: 0.12918481189914838
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 5.7873187311771686,
    &quot;alpha&quot;: 0.11196737789112272
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.857135474728793,
    &quot;alpha&quot;: 0.09575030044686336
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 9.29609103538407,
    &quot;alpha&quot;: 0.1581352715189433
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 9.506911791025216,
    &quot;alpha&quot;: 0.16937059578718588
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 5.693457761695681,
    &quot;alpha&quot;: 0.1182278831830303
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 7.476891875498462,
    &quot;alpha&quot;: 0.1403389747361836
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.180823669815048,
    &quot;alpha&quot;: 0.014692781292003692
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 7.448123349328373,
    &quot;alpha&quot;: 0.13683512515646537
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.41614442341441643,
    &quot;A&quot;: 1.8229587560850902,
    &quot;alpha&quot;: 0.16731161211612708
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.4048895734279312,
    &quot;A&quot;: 1.771686134476739,
    &quot;alpha&quot;: 0.13503543127027648
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.4542367430363656,
    &quot;alpha&quot;: 0.032132757710130806
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.5889984641635824,
    &quot;alpha&quot;: 0.03815454241188795
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.8453873317979803,
    &quot;alpha&quot;: 0.06600249911783039
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.8960123299335487,
    &quot;alpha&quot;: 0.054475442708765934
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.508753213336002,
    &quot;alpha&quot;: 0.07041420217384924
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.1982974522931986,
    &quot;alpha&quot;: 0.0512269552341686
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.2863502829285129,
    &quot;A&quot;: 5.830753430036486,
    &quot;alpha&quot;: 0.12174274538311074
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.7756306965839549,
    &quot;A&quot;: 2.6260572549578582,
    &quot;alpha&quot;: 0.11471754747211897
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.3523591080867368,
    &quot;alpha&quot;: 0.04191838265818281
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.250071982062786,
    &quot;alpha&quot;: 0.05575365697201484
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.26802688506172534,
    &quot;A&quot;: 1.7891072019745622,
    &quot;alpha&quot;: 0.04215249152496872
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.005381596620445,
    &quot;alpha&quot;: 0.06983526356123393
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.273452424763712,
    &quot;alpha&quot;: 0.04525490715783886
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 2.8159260594072455,
    &quot;alpha&quot;: 0.05183604484401081
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.880269855685565,
    &quot;alpha&quot;: 0.09617770187230991
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 3.953750786401135,
    &quot;alpha&quot;: 0.07896963709680072
  }
}


# Fallback parameters (median-like) if an unknown group is requested
_FALLBACK = {
    &quot;L_inf&quot;: 0.0,
    &quot;A&quot;: 4.5,
    &quot;alpha&quot;: 0.08,
}


def _predict_one(n: float, p: Dict[str, float]) -&gt; float:
    # Guard against degenerate or non-positive sizes
    n = float(n)
    if n &lt;= 0:
        return float(&quot;nan&quot;)
    return float(p[&quot;L_inf&quot;] + p[&quot;A&quot;] * (n ** (-p[&quot;alpha&quot;])) )


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # Fetch parameters for the requested group, with a safe fallback
    params = PARAMS.get(group, _FALLBACK)

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = row.get(&quot;sft_data_size&quot;)
        y = _predict_one(n, params)
        outputs.append({&quot;sft_loss&quot;: y})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.796907 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations

from typing import Dict, List


# Discovered scaling law (common functional form across all groups):
#     sft_loss(N) = c + a * N^{-b}
# where N = sft_data_size, and (c, a, b) are group-specific constants.

COEFFICIENTS: Dict[str, tuple[float, float, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: (1.731987, 10.331897, 0.214956),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.839163, 10.071592, 0.224204),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: (1.384851, 4.809672, 0.217954),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: (1.564914, 7.408989, 0.204933),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.71613, 9.159209, 0.228164),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: (1.277986, 3.356375, 0.223473),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: (1.699213, 4.308025, 0.221376),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: (1.021228, 6.648891, 0.214838),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: (1.46071, 3.561491, 0.216802),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: (1.974765, 4.782859, 0.1925),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.896973, 8.176252, 0.218907),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: (1.533293, 5.816801, 0.222374),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: (1.629405, 9.870861, 0.216781),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.610121, 12.890388, 0.235353),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: (1.197579, 8.704848, 0.245301),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: (1.549392, 6.772108, 0.212899),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: (0.505826, 12.967178, 0.24014),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: (1.190723, 3.589003, 0.221618),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: (1.516717, 3.56025, 0.217815),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: (0.842355, 8.301704, 0.230019),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: (1.266395, 2.360779, 0.225048),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: (1.71605, 5.336651, 0.201391),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: (0.718749, 10.705091, 0.232876),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: (1.404271, 3.745513, 0.221097),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: (1.685592, 1.221022, 0.216304),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: (1.747063, 1.022884, 0.216527),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: (1.229994, 1.780851, 0.224694),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: (1.724393, 5.843416, 0.224196),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (2.118312, 2.823209, 0.210612),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (1.083449, 6.917795, 0.226978),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: (1.570841, 4.096099, 0.224528),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: (1.986351, 3.419825, 0.21213),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: (1.188915, 5.346282, 0.227568),
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: (1.75414, 10.695338, 0.213789),
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: (0.740268, 10.674409, 0.22961),
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: (1.378322, 5.215325, 0.224403),
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: (1.529924, 3.927784, 0.203483),
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.516131, 2.333481, 0.230664),
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (1.145011, 2.344419, 0.216588),
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: (1.768963, 4.609379, 0.211197),
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: (0.609782, 2.497339, 0.235035),
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: (1.271723, 3.125673, 0.215205),
}

# Fallback (used if an unknown group string is passed)
FALLBACK: tuple[float, float, float] = (1.3166183154612168, 5.836451283719312, 0.2205792477344024)


def _predict_single(n_examples: float, params: tuple[float, float, float]) -&gt; float:
    c, a, b = params
    # Guard against degenerate inputs
    n = max(float(n_examples), 1e-9)
    return c + a * (n ** (-b))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = COEFFICIENTS.get(group, FALLBACK)
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&quot;sft_data_size&quot;, 0.0))
        yhat = _predict_single(n, params)
        outputs.append({&quot;sft_loss&quot;: float(yhat)})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.708780 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4">from __future__ import annotations

from typing import Dict, List


# Common functional form across all groups:
#   sft_loss(n) = L + A * n^{-alpha}
# Coefficients below were fit on the provided dataset for each group.
COEFS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.151278, &#x27;A&#x27;: 6.387290, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.209436, &#x27;A&#x27;: 6.069774, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.594863, &#x27;A&#x27;: 3.404022, &#x27;alpha&#x27;: 0.089655},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.887005, &#x27;A&#x27;: 4.744217, &#x27;alpha&#x27;: 0.099569},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.043412, &#x27;A&#x27;: 5.242398, &#x27;alpha&#x27;: 0.109483},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.421853, &#x27;A&#x27;: 2.527688, &#x27;alpha&#x27;: 0.069828},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.881216, &#x27;A&#x27;: 2.990184, &#x27;alpha&#x27;: 0.079741},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.357477, &#x27;A&#x27;: 4.524004, &#x27;alpha&#x27;: 0.109483},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.617501, &#x27;A&#x27;: 2.616905, &#x27;alpha&#x27;: 0.069828},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.211186, &#x27;A&#x27;: 3.422374, &#x27;alpha&#x27;: 0.079741},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.229292, &#x27;A&#x27;: 5.082541, &#x27;alpha&#x27;: 0.109483},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.784877, &#x27;A&#x27;: 3.914892, &#x27;alpha&#x27;: 0.099569},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.051009, &#x27;A&#x27;: 6.602574, &#x27;alpha&#x27;: 0.129310},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.069724, &#x27;A&#x27;: 7.734551, &#x27;alpha&#x27;: 0.139224},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.547840, &#x27;A&#x27;: 5.883053, &#x27;alpha&#x27;: 0.139224},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.853263, &#x27;A&#x27;: 4.705743, &#x27;alpha&#x27;: 0.109483},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: -0.042885, &#x27;A&#x27;: 7.589665, &#x27;alpha&#x27;: 0.139224},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.818466, &#x27;A&#x27;: 2.610666, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.671749, &#x27;A&#x27;: 2.611523, &#x27;alpha&#x27;: 0.069828},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.162161, &#x27;A&#x27;: 5.289383, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.484885, &#x27;A&#x27;: 1.928485, &#x27;alpha&#x27;: 0.059914},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.963881, &#x27;A&#x27;: 3.772039, &#x27;alpha&#x27;: 0.089655},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.118605, &#x27;A&#x27;: 6.472091, &#x27;alpha&#x27;: 0.129310},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.598750, &#x27;A&#x27;: 2.790886, &#x27;alpha&#x27;: 0.079741},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.157440, &#x27;A&#x27;: 1.128705, &#x27;alpha&#x27;: 0.050000},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 1.360993, &#x27;A&#x27;: 0.857867, &#x27;alpha&#x27;: 0.050000},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.457253, &#x27;A&#x27;: 1.621418, &#x27;alpha&#x27;: 0.050000},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.968873, &#x27;A&#x27;: 3.909269, &#x27;alpha&#x27;: 0.099569},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 1.245034, &#x27;A&#x27;: 2.290756, &#x27;alpha&#x27;: 0.059914},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.371285, &#x27;A&#x27;: 4.503793, &#x27;alpha&#x27;: 0.109483},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.741700, &#x27;A&#x27;: 2.913317, &#x27;alpha&#x27;: 0.079741},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 1.195306, &#x27;A&#x27;: 2.516514, &#x27;alpha&#x27;: 0.069828},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.418632, &#x27;A&#x27;: 3.712393, &#x27;alpha&#x27;: 0.099569},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.192834, &#x27;A&#x27;: 6.551524, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.130181, &#x27;A&#x27;: 6.102760, &#x27;alpha&#x27;: 0.119397},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.672005, &#x27;A&#x27;: 3.558654, &#x27;alpha&#x27;: 0.099569},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 0.756869, &#x27;A&#x27;: 2.967052, &#x27;alpha&#x27;: 0.079741},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.416953, &#x27;A&#x27;: 1.843005, &#x27;alpha&#x27;: 0.168966},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.380374, &#x27;A&#x27;: 1.936891, &#x27;alpha&#x27;: 0.059914},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L&#x27;: 1.035842, &#x27;A&#x27;: 3.299010, &#x27;alpha&#x27;: 0.089655},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L&#x27;: 0.382809, &#x27;A&#x27;: 1.758617, &#x27;alpha&#x27;: 0.129310},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L&#x27;: 0.472628, &#x27;A&#x27;: 2.416158, &#x27;alpha&#x27;: 0.069828},
}

# Pooled fallback coefficients (used if an unknown group is requested)
FALLBACK = {&#x27;L&#x27;: -0.374218, &#x27;A&#x27;: 4.399888, &#x27;alpha&#x27;: 0.059914}


def _predict_one(n: float, params: Dict[str, float]) -&gt; float:
    if n is None:
        raise ValueError(&quot;Missing &#x27;sft_data_size&#x27; in input_data item&quot;)
    if n &lt;= 0:
        # For non-positive sizes, return the asymptote as a conservative estimate.
        return float(params[&#x27;L&#x27;])
    return float(params[&#x27;L&#x27;] + params[&#x27;A&#x27;] * (n ** (-params[&#x27;alpha&#x27;])))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values. Must include key &#x27;sft_data_size&#x27;.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law is the same for all groups,
                but the constant parameters/coefficients differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s): {&#x27;sft_loss&#x27;: float}.
    &quot;&quot;&quot;
    params = COEFS.get(group, FALLBACK)
    preds: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&#x27;sft_data_size&#x27;)) if &#x27;sft_data_size&#x27; in row else None
        y = _predict_one(n, params)
        preds.append({&#x27;sft_loss&#x27;: y})
    return preds</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
