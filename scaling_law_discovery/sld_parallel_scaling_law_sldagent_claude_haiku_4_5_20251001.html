<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Parallel Scaling Law - SLDAgent + Claude Haiku 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Parallel Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Haiku 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.999952 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.999916</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.999893</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">

        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999952 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law for LLM parallel training with enhanced optimization
Uses 4-parameter model: loss = a * N^b / P^c + d
- N = num_params, P = parallel_size
- Captures both parameter scaling and parallel benefit with superior fitting
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: loss = a * num_params^b / parallel_size^c + d
    
    params: [a, b, c, d] (exactly 4 parameters)
    - a: coefficient (&gt; 0)
    - b: exponent for num_params (typically -0.1 to 0.2)
    - c: exponent for parallel_size (typically 0 to 0.5)
    - d: baseline loss offset
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    a = params[0]
    b = params[1]
    c = params[2]
    d = params[3]
    
    # Numerically stable computation with epsilon guards
    eps = 1e-10
    num_params_safe = np.maximum(num_params, eps)
    parallel_size_safe = np.maximum(parallel_size, eps)
    
    # Compute: a * N^b / P^c + d
    numerator = a * np.power(num_params_safe, b)
    denominator = np.power(parallel_size_safe, c)
    
    loss = numerator / (denominator + eps) + d
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit 4-parameter scaling law using three-phase optimization:
    Phase 1: Smart local optimization from data-driven initialization
    Phase 2: Global optimization if needed with fine convergence
    Phase 3: Local refinement on best solution found
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    # Data statistics
    n_min, n_max = num_params.min(), num_params.max()
    p_min, p_max = parallel_size.min(), parallel_size.max()
    y_min, y_max = y.min(), y.max()
    y_range = y_max - y_min
    
    def objective(params):
        &quot;&quot;&quot;Objective function with robustness checks&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            mse = np.mean((pred - y) ** 2)
            return max(float(mse), 0)
        except:
            return 1e10
    
    # Data-driven bounds based on observations
    bounds = [
        (1e-6, 1e3),            # a: coefficient (positive)
        (-0.2, 0.2),            # b: num_params exponent (small)
        (0.0, 0.5),             # c: parallel_size exponent (small positive)
        (y_min - 0.5, y_max + 0.5)  # d: baseline offset
    ]
    
    # Enhanced smart initialization from data characteristics
    y_span = y_range if y_range &gt; 1e-6 else 1.0
    a_init = y_span / np.power(np.maximum(n_max, 1), 0.05)
    b_init = -0.05
    c_init = 0.15
    d_init = y_min - 0.05 * y_span
    
    x0 = np.array([a_init, b_init, c_init, d_init])
    x0 = np.clip(x0, [b[0] for b in bounds], [b[1] for b in bounds])
    
    # Phase 1: Local optimization from smart initialization with tight convergence
    result_local = minimize(
        objective,
        x0,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
    )
    
    best_params = result_local.x if result_local.success else x0
    best_loss = objective(best_params)
    
    # Phase 2: Global optimization if local result is suboptimal
    if best_loss &gt; 0.005:
        result_global = differential_evolution(
            objective,
            bounds,
            maxiter=500,
            popsize=20,
            seed=42,
            atol=1e-11,
            tol=1e-11,
            workers=1,
            updating=&#x27;deferred&#x27;,
            strategy=&#x27;best1bin&#x27;
        )
        
        if result_global.fun &lt; best_loss:
            best_params = result_global.x
            best_loss = result_global.fun
            
            # Phase 3: Local refinement on global solution
            result_local2 = minimize(
                objective,
                best_params,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
            )
            
            if result_local2.fun &lt; best_loss:
                best_params = result_local2.x
    else:
        # Fine-tune already-good local result
        result_local_refined = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10}
        )
        
        if result_local_refined.fun &lt; best_loss:
            best_params = result_local_refined.x
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999949 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for LLM parallel training
Multiplicative inverse power law with high-precision fitting
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: loss = a + b / (num_params^c * parallel_size^d)
    
    Multiplicative form in denominator captures synergistic scaling effects between
    model size and parallel aggregation.
    
    params[0] = a (baseline loss)
    params[1] = b (scaling coefficient)
    params[2] = c (parameter exponent)
    params[3] = d (parallel exponent)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    p = np.atleast_1d(np.asarray(params, dtype=np.float64))
    
    # Extract features with numerical stability bounds
    num_params = np.maximum(X[:, 0], 1e6)
    parallel_size = np.maximum(X[:, 1], 1.0)
    
    # Multiplicative inverse scaling law
    return p[0] + p[1] / (np.power(num_params, p[2]) * np.power(parallel_size, p[3]))


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law using L-BFGS-B with smart data-driven initialization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.atleast_1d(np.asarray(loss_values, dtype=np.float64))
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            mse = np.mean((pred - y) ** 2)
            return mse if np.isfinite(mse) else 1e10
        except:
            return 1e10
    
    # Data-driven initialization
    y_min = np.min(y)
    y_max = np.max(y)
    y_range = y_max - y_min
    
    # Baseline loss: slightly below minimum observed loss
    a_init = y_min * 0.91
    
    # Scaling coefficient: captures dynamic range of variation
    b_init = y_range * 0.78
    
    # Exponents: based on empirical scaling law literature
    # Parameter scaling is more dominant than parallel scaling
    c_init = 0.08
    d_init = 0.11
    
    x0 = np.array([a_init, b_init, c_init, d_init])
    
    # Physical bounds ensuring stability and validity
    bounds = [
        (0.001, y_max * 1.5),      # a: baseline loss (positive, reasonable)
        (1e-10, y_range * 100),    # b: scaling strength (positive, wide)
        (0.001, 0.5),              # c: parameter exponent (small positive)
        (0.001, 0.5),              # d: parallel exponent (small positive)
    ]
    
    # Single-stage L-BFGS-B optimization with tight convergence
    result = minimize(
        objective,
        x0,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={
            &#x27;ftol&#x27;: 1e-13,
            &#x27;gtol&#x27;: 1e-10,
            &#x27;maxiter&#x27;: 4000,
            &#x27;maxfun&#x27;: 4000,
            &#x27;maxcor&#x27;: 25
        }
    )
    
    return result.x if result.success else x0

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999894 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM parallel training scenarios
Uses a 4-parameter model: base scaling with parallel efficiency
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: loss = a * (num_params^-b) * (1 + c / parallel_size^d)
    
    This models:
    - Power law decrease with model size (num_params^-b)
    - Degradation with parallelization (1 + c / parallel_size^d term)
    - Parameters: [a, b, c, d] (exactly 4)
    
    Physical interpretation:
    - a: base loss constant
    - b: scaling exponent with model size (typically 0.05-0.15)
    - c: parallel efficiency loss magnitude (typically 0.01-0.1)
    - d: parallel efficiency decay rate (typically 0.5-2.0)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if len(params.shape) == 1:
        params = params.reshape(1, -1)
    
    # Extract the first row of parameters (single model)
    p = params[0]
    
    if len(p) != 4:
        raise ValueError(f&quot;Expected 4 parameters, got {len(p)}&quot;)
    
    a, b, c, d = p
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    # Clip parameters for numerical stability
    b = np.clip(b, 0.001, 0.5)
    d = np.clip(d, 0.01, 5.0)
    c = np.clip(c, -0.5, 2.0)
    a = np.clip(a, 0.1, 10.0)
    
    # Base power law component
    base_loss = a * np.power(num_params, -b)
    
    # Parallel efficiency factor: loss increases slightly with parallelization
    # When parallel_size=1: factor=1, when parallel_size=4: factor=1+c/4^d
    parallel_factor = 1.0 + c / np.power(parallel_size, d)
    
    # Combined prediction
    loss = base_loss * parallel_factor
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit 4-parameter scaling law using global optimization for better convergence.
    Uses differential_evolution for global search, then local refinement.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            mse = np.mean((pred - y) ** 2)
            if np.isnan(mse) or np.isinf(mse):
                return 1e10
            return mse
        except:
            return 1e10
    
    # Bounds for global optimization
    bounds = [
        (0.1, 10.0),      # a: base constant
        (0.001, 0.5),     # b: power law exponent
        (-0.5, 2.0),      # c: parallel loss magnitude
        (0.01, 5.0),      # d: parallel decay rate
    ]
    
    # Try global optimization first
    result_global = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=300,
        atol=1e-8,
        tol=1e-8,
        workers=1,
        updating=&#x27;deferred&#x27;
    )
    
    # Local refinement
    result_local = minimize(
        objective,
        result_global.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-9, &#x27;maxiter&#x27;: 500}
    )
    
    # Return best result
    if result_local.fun &lt; result_global.fun:
        return result_local.x
    else:
        return result_global.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999893 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law: loss = a/(N^b) + c/(P^d)
Simplified 4-parameter inverse power law with streamlined optimization.
Removes redundant stages and complexity while maintaining high accuracy.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: loss = a / (num_params ^ b) + c / (parallel_size ^ d)
    params: [a, b, c, d] - 4 parameters
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    a, b, c, d = params[0], params[1], params[2], params[3]
    
    eps = 1e-10
    loss = a / (np.maximum(num_params, eps) ** b) + c / (np.maximum(parallel_size, eps) ** d)
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit 4-parameter scaling law with direct optimization.
    Single-stage approach using differential evolution with local refinement.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    # Data-driven initialization
    y_mean = np.mean(y)
    N_min = np.min(num_params)
    
    # Stratify data for parameter estimation
    p1_idx = parallel_size &lt; 1.5
    p4_idx = parallel_size &gt; 3.0
    small_n_idx = num_params &lt; np.percentile(num_params, 33)
    large_n_idx = num_params &gt; np.percentile(num_params, 67)
    
    # Estimate parameters from data stratification
    if np.sum(p1_idx) &gt; 0 and np.sum(p4_idx) &gt; 0:
        c_est = (np.mean(y[p1_idx]) - np.mean(y[p4_idx])) * 0.5
    else:
        c_est = y_mean * 0.05
    
    if np.sum(small_n_idx) &gt; 0 and np.sum(large_n_idx) &gt; 0:
        a_est = (np.mean(y[small_n_idx]) - np.mean(y[large_n_idx])) * N_min ** 0.1
    else:
        a_est = y_mean * N_min ** 0.1
    
    c_est = max(0.0001, c_est)
    a_est = max(0.01, a_est)
    
    def objective(params):
        &quot;&quot;&quot;MSE objective with safety.&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            return np.mean((pred - y) ** 2)
        except:
            return 1e10
    
    # Bounds for parameters
    bounds = [
        (max(0.01, a_est * 0.1), min(1000, a_est * 10)),
        (0.01, 2.0),
        (max(0.0001, c_est * 0.1), min(100, c_est * 10)),
        (0.01, 2.0)
    ]
    
    best_params = None
    best_loss = float(&#x27;inf&#x27;)
    
    # Global optimization with differential evolution
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=300,
            popsize=20,
            tol=1e-9,
            atol=1e-11,
            workers=1,
            polish=True
        )
        best_loss = result_de.fun
        best_params = result_de.x
    except:
        best_params = np.array([a_est, 0.08, c_est, 0.25])
        best_loss = objective(best_params)
    
    # Single local refinement pass from best solution
    if best_params is not None:
        try:
            result_local = minimize(
                objective,
                best_params,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 500}
            )
            if result_local.fun &lt; best_loss:
                best_params = result_local.x
        except:
            pass
    
    if best_params is None:
        best_params = np.array([a_est, 0.08, c_est, 0.25])
    
    return np.clip(best_params, [b[0] for b in bounds], [b[1] for b in bounds])

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999893 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for parallel LLM training.
Additive inverse power law: L(N, P) = a / N^alpha + b / P^beta
Streamlined with efficient initialization and direct optimization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;Predict loss using additive inverse power law.&quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    a = np.abs(params[0]) + 1e-10
    alpha = np.clip(params[1], 0.02, 0.25)
    b = np.abs(params[2]) + 1e-10
    beta = np.clip(params[3], 0.2, 1.2)
    
    return a / np.power(num_params, alpha) + b / np.power(parallel_size, beta)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;Fit scaling law with smart initialization and hybrid optimization.&quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    num_params = X[:, 0]
    parallel_size = X[:, 1]
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            return np.mean((pred - y) ** 2) if np.all(np.isfinite(pred)) else 1e10
        except:
            return 1e10
    
    # Smart initialization via log-log analysis
    log_np = np.log(num_params)
    log_ps = np.log(parallel_size)
    log_y = np.log(np.clip(y, 1e-8, None))
    
    # Estimate exponents from median-based slope estimation
    def get_exp(log_var, log_loss):
        split = log_var &gt; np.median(log_var)
        if np.sum(split) &gt; 1 and np.sum(~split) &gt; 1:
            slope = (np.mean(log_loss[~split]) - np.mean(log_loss[split])) / (np.mean(log_var[~split]) - np.mean(log_var[split]))
            return -slope
        return 0.0
    
    alpha_init = np.clip(get_exp(log_np, log_y) or 0.075, 0.02, 0.25)
    beta_init = np.clip(get_exp(log_ps, log_y) or 0.5, 0.2, 1.2)
    
    # Estimate coefficients from median point
    mid = len(y) // 2
    a_init = np.clip(np.abs(y[mid] * (num_params[mid] ** alpha_init)) + 1e-8, 1e-6, 1e4)
    b_init = np.clip(np.abs(y[mid] * (parallel_size[mid] ** beta_init)) + 1e-8, 1e-6, 1e2)
    
    bounds = [(1e-8, 1e4), (0.02, 0.25), (1e-8, 1e2), (0.2, 1.2)]
    
    # Two-phase optimization: global then local
    res_de = differential_evolution(
        objective, bounds, seed=42, maxiter=400, popsize=20,
        atol=1e-10, tol=1e-10, polish=True
    )
    
    # Local refinement with strong convergence criteria
    res_final = minimize(
        objective, res_de.x, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
        options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-13, &#x27;gtol&#x27;: 1e-11}
    )
    
    return res_final.x if res_final.fun &lt; res_de.fun else res_de.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>