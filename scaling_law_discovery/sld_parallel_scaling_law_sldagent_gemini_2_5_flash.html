<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Parallel Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Parallel Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Gemini 2.5 Flash</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.999695 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.999666</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.999658</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999695 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts language modeling loss based on model parameters and parallel size.

    The scaling law model used is:
    Loss = A * num_params^(alpha + beta * log(parallel_size)) + C

    This model suggests that parallel_size modifies the scaling exponent of num_params,
    capturing an interaction effect where increased parallelism (via log-scaling)
    makes num_params scale more effectively (if beta is negative). This form allows
    the effectiveness of parallel scaling to itself scale with model size, which can
    be a more nuanced fit than a purely multiplicative power law, especially for data
    where the parallel effect is not simply a constant multiplicative factor.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        params (np.ndarray): Array of 4 parameters [A, alpha, beta, C].
            - A (coefficient): A positive scaling factor for the power-law term.
            - alpha (base exponent for num_params): Typically negative, indicating loss decreases as num_params increases.
            - beta (parallel_size interaction factor): Typically negative. If negative, increasing `parallel_size` makes the effective exponent more negative, leading to lower loss.
            - C (irreducible loss): A positive baseline loss component that cannot be reduced by scaling N or P.

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    num_params = X[:, 0]
    parallel_size = X[:, 1]

    A, alpha, beta, C = params[0], params[1], params[2], params[3]

    # Use logarithmic transformation for numerical stability.
    # This avoids potential issues with direct power computation for very large bases
    # or small/negative fractional exponents. np.log(1) is correctly evaluated as 0,
    # ensuring the model behaves as a standard power law when parallel_size=1.
    log_num_params = np.log(num_params)
    log_parallel_size = np.log(parallel_size)

    # Calculate the effective exponent for num_params. This exponent dynamically changes
    # with parallel_size. If `beta` is negative, an increase in `parallel_size` (and thus
    # `log_parallel_size`) will make the `effective_exponent` more negative, leading to
    # a smaller power-law term and thus lower predicted loss, consistent with observations.
    effective_exponent = alpha + beta * log_parallel_size

    # Reconstruct the power law term using exp(exponent * log(base)) for numerical stability.
    term_N_effective_exponent = np.exp(effective_exponent * log_num_params)

    # Combine terms to get the predicted loss.
    predicted_loss = A * term_N_effective_exponent + C

    # Ensure predictions are non-negative. Loss values cannot be negative, so this clipping
    # maintains physical plausibility and prevents potential numerical artifacts.
    return np.maximum(predicted_loss, 0.0)

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function to the provided data using scipy.optimize.least_squares.
    This method is generally more robust for non-linear least squares problems with
    bounds compared to general-purpose minimizers like L-BFGS-B, especially with the
    Trust Region Reflective (&#x27;trf&#x27;) algorithm.

    Initial guesses and bounds are carefully chosen to guide the optimizer towards
    physically plausible and accurate solutions for the
    `A * num_params^(alpha + beta * log(parallel_size)) + C` model.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        loss_values (np.ndarray): Array of corresponding actual loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha, beta, C].
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)

    # Refined initial guesses for the 4 parameters [A, alpha, beta, C].
    # - A: Coefficient. Initialized to a reasonable positive value.
    # - alpha: Base exponent for num_params. Negative, typical for LLM scaling.
    # - beta: Parallel_size interaction factor. Initialized to a value that reflects
    #   an observed negative impact of parallelism on loss.
    # - C: Irreducible loss. Initialized to 50% of the minimum observed loss. This
    #   allows the power-law term ample room to model the reducible portion of the loss,
    #   which can lead to a more accurate fit for the scaling behavior.
    initial_params = np.array([10.0, -0.08, -0.03, np.min(y) * 0.5])

    # Refined bounds for the parameters. These bounds are crucial for guiding the
    # optimizer towards physically meaningful solutions and preventing divergence.
    # - A: (0.01, 200.0) - Must be positive and covers a broad range of scaling factors.
    # - alpha: (-0.5, -0.001) - Must be negative to reflect loss reduction with increasing parameters.
    # - beta: (-0.3, -0.001) - Must be negative, ensuring increasing parallel_size decreases loss
    #   by making the effective exponent more negative. The range is chosen to allow
    #   meaningful interaction effects without being excessively broad.
    # - C: (0.001, np.min(y) * 0.95) - Must be positive. The upper bound ensures that &#x27;C&#x27; is
    #   always less than the minimum observed loss, forcing the power-law term to contribute
    #   positively to explaining the observed loss.
    bounds = [
        (0.01, 200.0),                  # A
        (-0.5, -0.001),                 # alpha
        (-0.3, -0.001),                 # beta
        (0.001, np.min(y) * 0.95)       # C
    ]

    # The objective function for least_squares returns the residuals (predicted - actual).
    def objective_residuals(params):
        pred = scaling_law_func(X, params)
        # scaling_law_func already ensures pred is non-negative.
        return pred - y

    # Perform optimization using least_squares with the &#x27;trf&#x27; method.
    # &#x27;trf&#x27; (Trust Region Reflective) is generally robust for bounded non-linear least squares.
    # &#x27;verbose=0&#x27; suppresses output from the optimizer.
    # &#x27;max_nfev&#x27; (maximum number of function evaluations) is increased to allow for better convergence likelihood.
    result = least_squares(objective_residuals, initial_params, bounds=np.array(bounds).T,
                           method=&#x27;trf&#x27;, verbose=0, max_nfev=5000)

    # Check for successful convergence. `result.success` or `result.status &gt; 0`
    # indicates successful termination or reaching tolerance.
    if result.success or result.status &gt; 0:
        optimized_params = result.x
    else:
        # If optimization fails or does not converge, return the initial parameters
        # as a fallback to ensure a valid array is always returned.
        optimized_params = initial_params
        # Optionally log a warning for debugging purposes:
        # print(f&quot;Warning: least_squares optimization failed with status: {result.status} ({result.message}). Returning initial parameters.&quot;)

    return optimized_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999662 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Proposed scaling law function: L(N, S) = A * N^alpha * S^beta + C
    where N is num_params and S is parallel_size.
    Uses exactly 4 parameters: [A, alpha, beta, C].

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        params (np.ndarray): Array of 4 parameters [A, alpha, beta, C].

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    # Extract num_params and parallel_size
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]

    # Extract parameters
    A, alpha, beta, C = params

    # Apply log transform to base values for numerical stability with power law.
    # num_params and parallel_size are guaranteed to be positive in the problem description 
    # (large numbers for num_params, [1, 2, 4] for parallel_size).
    # Thus, direct np.log is safe and no epsilon offset is needed to avoid log(0).
    log_num_params = np.log(num_params)
    log_parallel_size = np.log(parallel_size)

    # Compute terms using exp(exponent * log(base)) for robustness.
    # This is numerically stable and equivalent to base^exponent.
    term_num_params = np.exp(alpha * log_num_params)
    term_parallel_size = np.exp(beta * log_parallel_size)

    # Combine terms according to the proposed scaling law
    predicted_loss = A * term_num_params * term_parallel_size + C

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func using L-BFGS-B.
    This version includes adaptive initial parameter guessing via linear regression
    for A, alpha, and beta, and refined bounds for robustness and physical plausibility.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        loss_values (np.ndarray): Array of corresponding loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha, beta, C].
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)

    # Handle empty data: return sensible default initial parameters if no data to fit.
    if len(y) == 0:
        return np.array([10.0, -0.1, -0.1, 0.5]) 
    
    min_observed_loss = np.min(y)
    max_observed_loss = np.max(y) # Used for fallback C bound

    # Initial guess for C, adapted to the observed minimum loss.
    # A factor around 0.7-0.8 provides a good balance, giving room for the power law component
    # to explain variation above the irreducible loss.
    initial_c_guess = min_observed_loss * 0.7 
    if initial_c_guess &lt; 0: initial_c_guess = 0.0 # C (irreducible loss) cannot be negative.

    # Default initial guesses for parameters [A, alpha, beta, C]
    initial_params = np.array([
        10.0,                     # A: Scale factor, a reasonable starting point for typical loss ranges.
        -0.1,                     # alpha: Typical LLM parameter scaling exponent (negative).
        -0.05,                    # beta: Expected to be negative, usually smaller in magnitude than alpha.
        initial_c_guess           # C: Initialized adaptively below the minimum observed loss.
    ])

    # Bounds for parameters to guide the optimizer and ensure physical plausibility.
    bounds = [
        (1e-6, 100.0),    # A: Must be positive, with a reasonable upper bound for typical losses.
        (-0.3, -1e-6),    # alpha: Negative exponent. Common LLM scaling range is -0.05 to -0.2. (-0.3, -1e-6) provides sufficient flexibility.
        (-0.15, -1e-6),   # beta: Negative exponent. The effect of parallel_size is usually less pronounced than num_params. (-0.15, -1e-6) is a plausible range.
    ]

    # Robust C bound: C should ideally be strictly less than the minimum observed loss,
    # as it represents an asymptotic lower bound. We apply a small margin (0.99).
    c_upper_bound = min_observed_loss * 0.99 
    if c_upper_bound &lt; 0.001: 
        # Fallback to ensure a minimum positive upper bound if min_loss is very small.
        c_upper_bound = max(0.001, min_observed_loss * 0.9) 
    bounds.append((0.0, c_upper_bound)) 

    # Ensure initial C is within its bounds (especially the new tighter upper bound).
    initial_params[3] = np.clip(initial_params[3], bounds[3][0], bounds[3][1])

    # --- Improved Initial Parameter Guessing using Linear Regression ---
    # This step linearizes L = A * N^alpha * S^beta + C to log(L - C_guess) = log(A) + alpha * log(N) + beta * log(S).
    # This provides a more data-driven starting point for A, alpha, and beta.
    if len(y) &gt;= 4: # Need at least 3 points for a 3-parameter linear fit (alpha, beta, log(A)), 4 for safety.
        C_guess_for_linear_fit = initial_params[3] # Use the current, bounded initial C guess.
        
        # Calculate transformed_y. Add a very small epsilon to y to ensure
        # that (y - C_guess_for_linear_fit) is strictly positive, crucial for log transformation.
        transformed_y = (y + 1e-12) - C_guess_for_linear_fit 
        
        # Filter data points where `transformed_y` is not strictly positive.
        valid_indices = transformed_y &gt; 1e-9 

        if np.sum(valid_indices) &gt;= 3: # Need at least 3 valid points for linear regression
            # num_params and parallel_size are positive, so direct log is numerically stable.
            log_num_params_valid = np.log(X[valid_indices, 0]) 
            log_parallel_size_valid = np.log(X[valid_indices, 1])
            log_transformed_y_valid = np.log(transformed_y[valid_indices])

            # Design matrix for linear regression: [log(N), log(S), intercept for log(A)]
            A_matrix = np.column_stack([log_num_params_valid, log_parallel_size_valid, np.ones_like(log_num_params_valid)])
            
            try:
                # Solve for [alpha, beta, log(A)] using least squares.
                coefficients, _, _, _ = np.linalg.lstsq(A_matrix, log_transformed_y_valid, rcond=None)
                
                alpha_lr = coefficients[0]
                beta_lr = coefficients[1]
                log_A_lr = coefficients[2]
                A_lr = np.exp(log_A_lr)

                # Update initial_params with these new estimates, clipping them to their respective bounds.
                initial_params[0] = np.clip(A_lr, bounds[0][0], bounds[0][1])
                initial_params[1] = np.clip(alpha_lr, bounds[1][0], bounds[1][1])
                initial_params[2] = np.clip(beta_lr, bounds[2][0], bounds[2][1])
                # initial_params[3] (C) is already from the initial guess and within its bounds, no update needed here.

            except (np.linalg.LinAlgError, OverflowError, ValueError):
                # If linear regression fails (e.g., singular matrix, numerical overflow),
                # fall back to the default initial_params set earlier.
                pass 

    def objective(params):
        &quot;&quot;&quot;
        Calculates the Mean Squared Error (MSE) between predicted and actual loss values.
        Used as the objective function for minimization.
        &quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)

        # Check for invalid predictions (NaN, Inf) and return a very high error.
        # This guides the optimizer away from problematic parameter regions.
        if not np.all(np.isfinite(predicted_loss)):
            return np.inf
        
        # Losses cannot be negative. Apply a high penalty if the model predicts negative loss,
        # which is a strong physical constraint for loss functions.
        if np.any(predicted_loss &lt; 0):
            return np.inf

        # Compute Mean Squared Error (MSE)
        mse = np.mean((predicted_loss - y) ** 2)
        return mse

    # Perform the optimization using the L-BFGS-B algorithm, which efficiently supports bounds.
    # Increased &#x27;maxiter&#x27; and tightened &#x27;ftol&#x27;, &#x27;gtol&#x27; for better convergence control,
    # helping the optimizer find a precise minimum for highly accurate fitting.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds, 
                      options={&#x27;maxiter&#x27;: 7500, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-12}) # Tighter tolerances and more iterations

    # Return optimized parameters if the optimization was successful, otherwise return the (potentially refined) initial guess.
    params_opt = result.x if result.success else initial_params
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999658 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Proposed scaling law function: L(N, S) = A * N^alpha * S^beta + C
    where N is num_params and S is parallel_size.
    Uses exactly 4 parameters: [A, alpha, beta, C].

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
            num_params: Model parameter counts.
            parallel_size: Parallel copies for input augmentation.
        params (np.ndarray): Array of 4 parameters [A, alpha, beta, C].

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    # Ensure data_points is a 2D array
    data_points = np.atleast_2d(data_points)

    # Extract num_params and parallel_size
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]

    # Extract parameters A, alpha, beta, C
    A, alpha, beta, C = params

    # Apply log transform to base values for numerical stability with power law.
    # num_params and parallel_size are guaranteed to be positive, making np.log safe.
    log_num_params = np.log(num_params)
    log_parallel_size = np.log(parallel_size)

    # Compute terms using exp(exponent * log(base)) for robustness.
    # This is mathematically equivalent to base^exponent but more stable
    # for very large/small bases or exponents, avoiding potential overflow/underflow
    # or precision issues with direct power operations.
    term_num_params = np.exp(alpha * log_num_params)
    term_parallel_size = np.exp(beta * log_parallel_size)

    # Combine terms according to the proposed scaling law L = A * N^alpha * S^beta + C
    predicted_loss = A * term_num_params * term_parallel_size + C

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func using scipy.optimize.least_squares.
    This method is chosen for its robustness, ability to handle bounds, and support for
    robust loss functions.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
            num_params: Array of model parameter counts.
            parallel_size: Array of parallel copies for input augmentation.
        loss_values (np.ndarray): Array of corresponding loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha, beta, C].
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)

    # Handle empty data: return sensible default initial parameters if no data to fit.
    if len(y) == 0:
        return np.array([5.0, -0.1, -0.1, 0.5]) 
    
    min_observed_loss = np.min(y)
    max_observed_loss = np.max(y)
    mean_observed_loss = np.mean(y)

    # --- Initial Guess for Parameters [A, alpha, beta, C] ---
    # C: Irreducible loss/bias term. Should be non-negative and generally lower than observed losses.
    # A data-driven initial C helps in faster convergence. A small offset to ensure it&#x27;s above zero.
    initial_C = max(0.01, min_observed_loss * 0.75) 
    
    # alpha: Negative exponent for num_params scaling (loss decreases with more parameters).
    initial_alpha = -0.1 # Typical LLM scaling exponent
    
    # beta: Negative exponent for parallel_size scaling (loss decreases with more parallelism).
    initial_beta = -0.1 # Expect similar negative scaling behavior

    # A: Scale factor. A more principled initialization using average values.
    # Calculate the average contribution of the power law terms N^alpha * S^beta.
    # This term should represent the portion of loss *above* the irreducible loss C.
    log_N_values = np.log(X[:, 0])
    log_S_values = np.log(X[:, 1])
    
    initial_power_num_params = np.exp(initial_alpha * log_N_values)
    initial_power_parallel_size = np.exp(initial_beta * log_S_values)
    
    mean_multiplicative_term = np.mean(initial_power_num_params * initial_power_parallel_size)
    
    # Ensure mean_multiplicative_term is sufficiently positive to avoid division by zero or inf.
    # Since alpha, beta are negative and N, S &gt;= 1, N^alpha and S^beta are in (0, 1].
    # So their product and thus their mean will be &gt; 0. A small minimum is for extreme numerical cases.
    mean_multiplicative_term = max(mean_multiplicative_term, 1e-12) 
    
    # Initial A should scale the multiplicative term to roughly explain (mean_observed_loss - initial_C).
    # Ensure the numerator is positive to keep A positive, as required by its bounds.
    numerator_for_A = max(1e-6, mean_observed_loss - initial_C) 
    initial_A = numerator_for_A / mean_multiplicative_term
    
    # Clip initial_A to ensure it&#x27;s within plausible bounds for losses around 1-2.
    initial_A = np.clip(initial_A, 1e-3, 100.0) 

    initial_params = np.array([initial_A, initial_alpha, initial_beta, initial_C])

    # --- Define Robust Bounds for Parameters ---
    # Bounds are crucial for guiding the optimizer, ensuring physical plausibility,
    # and preventing divergence into non-sensical parameter spaces.
    bounds_lower = np.array([
        1e-6,   # A: Must be positive. Small lower bound for numerical stability.
        -0.5,   # alpha: Negative exponent for num_params scaling. Allowing a broader but realistic range.
        -0.5,   # beta: Negative exponent for parallel_size scaling. Allowing a broader but realistic range.
        0.0     # C: Must be non-negative, representing a floor for the loss.
    ])
    bounds_upper = np.array([
        200.0,  # A: Generous upper bound for scale factor, adjusted to be more realistic for typical loss ranges.
        -1e-6,  # alpha: Strictly negative (loss must decrease with more parameters).
        -1e-6,  # beta: Strictly negative (loss must decrease with more parallelization).
    ])

    # Robust C upper bound: C (irreducible loss) should ideally be strictly less than
    # the minimum observed loss to allow the power-law component to be positive.
    if min_observed_loss &lt;= 1e-6: # Fallback if min_observed_loss is tiny or zero.
        bounds_upper = np.append(bounds_upper, max_observed_loss + 0.1) 
    else:
        # A very tight upper bound for C ensures the power-law component is always active.
        bounds_upper = np.append(bounds_upper, min_observed_loss * 0.9999) 

    # Clip initial parameters to ensure they are strictly within the defined bounds.
    # This prevents issues if initial guesses fall exactly on or outside boundaries.
    initial_params = np.clip(initial_params, bounds_lower, bounds_upper)

    def residuals(params):
        &quot;&quot;&quot;
        Calculates the residuals (predicted_loss - actual_loss) for least_squares.
        &quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)

        # Check for invalid predictions (NaN, Inf) and return a very large residual
        # to guide the optimizer away from problematic parameter regions.
        if not np.all(np.isfinite(predicted_loss)):
            # Return a large array of residuals to effectively penalize this parameter set.
            # Using np.finfo(float).max / 2 to prevent overflow during internal squaring by least_squares.
            return np.full_like(y, np.finfo(float).max / 2)

        return predicted_loss - y

    # Use scipy.optimize.least_squares for robust non-linear least squares optimization.
    # &#x27;trf&#x27; (Trust Region Reflective) method is generally robust and handles bounds well.
    # &#x27;soft_l1&#x27; loss function provides robustness against potential outliers in the data.
    result = least_squares(
        residuals,
        initial_params,
        bounds=(bounds_lower, bounds_upper),
        method=&#x27;trf&#x27;,      # Trust Region Reflective algorithm, good for bounded problems
        loss=&#x27;soft_l1&#x27;,    # Robust loss function for outlier resilience
        verbose=0,         # Suppress verbose output from the optimizer
        ftol=1e-10,        # Increased function tolerance for termination
        xtol=1e-10,        # Increased step tolerance for termination
        gtol=1e-10         # Increased gradient tolerance for termination
    )

    # least_squares returns &#x27;status&#x27; to indicate success (1, 2, 3, 4 are successful completion codes).
    # If optimization fails, return the initial parameters as a fallback.
    params_opt = result.x if result.status &gt; 0 else initial_params
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999658 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Evolved scaling law function: L(N, S) = A * N^alpha * S^beta + C
    where N is num_params and S is parallel_size.
    Uses exactly 4 parameters: [A, alpha, beta, C].

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        params (np.ndarray): Array of 4 parameters [A, alpha, beta, C].

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    # Extract num_params and parallel_size from data_points
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]

    # Extract parameters [A, alpha, beta, C]
    A, alpha, beta, C = params

    # Apply log transform to base values for numerical stability with power law.
    # num_params and parallel_size are guaranteed positive in this context.
    # The exponential function then ensures calculation robustness for powers.
    # Clip num_params and parallel_size to a minimum to avoid log(0) if any values are problematic,
    # though problem description implies they are positive.
    num_params_stable = np.maximum(num_params, 1e-12) # Use a very small positive number
    parallel_size_stable = np.maximum(parallel_size, 1e-12) # Use a very small positive number

    term_num_params = np.exp(alpha * np.log(num_params_stable))
    term_parallel_size = np.exp(beta * np.log(parallel_size_stable))

    # Combine terms according to the proposed scaling law
    predicted_loss = A * term_num_params * term_parallel_size + C

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func using scipy.optimize.least_squares.
    This improved implementation incorporates:
    1. Robust initial parameter guesses derived from a linear regression on log-transformed data,
       providing a better starting point for the non-linear optimizer.
    2. Physically plausible and adaptive bounds for parameters, especially for &#x27;C&#x27; (irreducible loss).
    3. The &#x27;trf&#x27; (Trust Region Reflective) method for robust handling of bounds.
    4. &#x27;soft_l1&#x27; loss function for robustness against potential outliers.
    5. Tighter tolerances and increased max_nfev for potentially better convergence.
    6. Minor refinement in initial C guess and added clipping for log inputs in scaling_law_func.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        loss_values (np.ndarray): Array of corresponding loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha, beta, C].
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)

    # Handle empty data by returning a sensible default set of parameters
    if len(y) == 0:
        return np.array([10.0, -0.1, -0.1, 0.5]) 
    
    min_observed_loss = np.min(y)
    max_observed_loss = np.max(y)
    mean_observed_loss = np.mean(y)

    # 1. Initial guess for C (irreducible loss term)
    # C should be non-negative and is typically a fraction of the minimum observed loss.
    # Increased the multiplier for min_observed_loss from 0.5 to 0.75, assuming C might be closer to min_loss.
    initial_C = max(1e-6, min_observed_loss * 0.75) 
    
    # 2. Derive initial guesses for A, alpha, beta using linear regression on log-transformed data
    # The scaling law can be linearized as log(L - C) = log(A) + alpha * log(N) + beta * log(S).
    
    # Ensure (y - initial_C) is strictly positive for log-transformation.
    # Filter out points where this condition is not met or where the values are extremely small.
    valid_indices = (y - initial_C) &gt; 1e-6 
    
    # Also ensure num_params and parallel_size are strictly positive for log.
    # This is handled by clipping in scaling_law_func, but for initial regression, ensure positive
    # (though problem spec implies they are positive).
    num_params_for_reg = np.maximum(X[valid_indices, 0], 1e-12)
    parallel_size_for_reg = np.maximum(X[valid_indices, 1], 1e-12)

    if np.sum(valid_indices) &lt; 3: 
        # Fallback to heuristic initial guesses if not enough valid points for regression
        # (needs at least 3 points for 3 parameters: log(A), alpha, beta)
        initial_A_reg = max(1e-3, mean_observed_loss - initial_C)
        initial_alpha_reg = -0.1
        initial_beta_reg = -0.05 # Heuristic for parallel_size effect, often less steep
    else:
        Y_reg = np.log(y[valid_indices] - initial_C)
        N_reg = np.log(num_params_for_reg)
        S_reg = np.log(parallel_size_for_reg)
        
        # Construct the design matrix for linear regression: [intercept_term, log(N)_term, log(S)_term]
        design_matrix = np.column_stack([np.ones_like(N_reg), N_reg, S_reg])
        
        try:
            # Perform linear least squares to find [log(A), alpha, beta]
            coeffs, _, _, _ = np.linalg.lstsq(design_matrix, Y_reg, rcond=None)
            initial_log_A_reg, initial_alpha_reg, initial_beta_reg = coeffs
            initial_A_reg = np.exp(initial_log_A_reg)

            # Post-process regression results to ensure physical plausibility
            # Alpha and beta should be negative. Ensure they are not positive (e.g., due to noise)
            # and within a reasonable negative range.
            initial_alpha_reg = max(-1.0, initial_alpha_reg)
            initial_alpha_reg = min(-1e-6, initial_alpha_reg) if initial_alpha_reg &gt; 0 else initial_alpha_reg

            initial_beta_reg = max(-1.0, initial_beta_reg)
            initial_beta_reg = min(-1e-6, initial_beta_reg) if initial_beta_reg &gt; 0 else initial_beta_reg

            # Ensure initial_A_reg is positive and reasonable
            initial_A_reg = max(1e-3, initial_A_reg)
            
        except np.linalg.LinAlgError:
            # Fallback if linear regression fails (e.g., singular matrix)
            initial_A_reg = max(1e-3, mean_observed_loss - initial_C)
            initial_alpha_reg = -0.1
            initial_beta_reg = -0.05

    # Combine all initial guesses
    initial_params = np.array([initial_A_reg, initial_alpha_reg, initial_beta_reg, initial_C])

    # Define robust bounds for parameters to guide the optimizer and ensure physical plausibility
    bounds_lower = np.array([
        1e-8,   # A: Must be positive, very small lower bound for stability.
        -1.0,   # alpha: Negative exponent (loss decreases with num_params), a common range for LLMs.
        -1.0,   # beta: Negative exponent (loss decreases with parallel_size), more flexible.
        0.0     # C: Must be non-negative.
    ])
    
    # C should ideally be strictly less than the minimum observed loss (asymptotic floor).
    C_upper_bound = min_observed_loss * 0.999 if min_observed_loss &gt; 1e-6 else max_observed_loss + 0.1

    bounds_upper = np.array([
        1000.0,          # A: Generous upper bound for scale factor.
        -1e-8,           # alpha: Strictly negative (loss must decrease with num_params).
        -1e-8,           # beta: Strictly negative (loss must decrease with parallel_size).
        C_upper_bound    # C: Upper bound derived from observed data.
    ])

    # Clip initial parameters to ensure they are within the defined bounds before optimization.
    initial_params = np.clip(initial_params, bounds_lower, bounds_upper)

    def residuals(params):
        &quot;&quot;&quot;
        Calculates the residuals (predicted_loss - actual_loss) for least_squares.
        &quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)

        # Check for invalid predictions (NaN, Inf) and return a very large residual
        # to guide the optimizer away from problematic parameter regions.
        if not np.all(np.isfinite(predicted_loss)):
            # Return max_float / 2 to prevent overflow during internal squaring by least_squares
            return np.full_like(y, np.finfo(float).max / 2) 

        return predicted_loss - y

    # Use scipy.optimize.least_squares for robust non-linear least squares optimization.
    # &#x27;trf&#x27; (Trust Region Reflective) method is well-suited for bounded problems.
    # &#x27;soft_l1&#x27; loss provides robustness against outliers.
    # Tighter tolerances (`ftol`, `xtol`, `gtol`) and increased `max_nfev` can help
    # converge to a more precise solution, especially for complex surfaces.
    result = least_squares(
        residuals,
        initial_params,
        bounds=(bounds_lower, bounds_upper),
        method=&#x27;trf&#x27;, 
        loss=&#x27;soft_l1&#x27;, 
        ftol=1e-10,  # Function tolerance (controls stopping based on change in cost)
        xtol=1e-10,  # Step tolerance (controls stopping based on change in parameters)
        gtol=1e-10,  # Gradient tolerance (controls stopping based on gradient magnitude)
        max_nfev=20000, # Increased max number of function evaluations
        verbose=0 # Suppress verbose output
    )

    # If optimization was successful (status &gt; 0 indicates success), return the optimized parameters;
    # otherwise, return the (clipped) initial parameters to avoid returning invalid results.
    params_opt = result.x if result.status &gt; 0 else initial_params
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.999658 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Proposed scaling law function: L(N, S) = A * N^alpha * S^beta + C
    where N is num_params and S is parallel_size.
    Uses exactly 4 parameters: [A, alpha, beta, C].

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        params (np.ndarray): Array of 4 parameters [A, alpha, beta, C].

    Returns:
        np.ndarray: Predicted loss values for each data point.
    &quot;&quot;&quot;
    # Extract num_params and parallel_size
    num_params = data_points[:, 0]
    parallel_size = data_points[:, 1]

    # Extract parameters
    A, alpha, beta, C = params

    # Compute terms using np.exp(exponent * np.log(base)) for improved numerical stability.
    # This approach is generally more robust for power laws, especially with large base values
    # (like num_params, which can be billions) and potentially small or negative exponents,
    # compared to direct np.power. num_params and parallel_size are guaranteed positive based
    # on the problem description, so directly using np.log is safe and efficient.
    log_num_params = np.log(num_params)
    log_parallel_size = np.log(parallel_size)

    # Compute the product of power terms using exp(sum of logs) for robustness and efficiency.
    # This is equivalent to (N^alpha) * (S^beta) but more stable for very large/small bases or exponents.
    predicted_term = np.exp(alpha * log_num_params + beta * log_parallel_size)

    # Combine terms according to the proposed scaling law
    predicted_loss = A * predicted_term + C

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func using L-BFGS-B.
    This version incorporates several improvements:
    1. Adaptive initial parameter guesses based on data characteristics.
    2. Tighter and more informed bounds for exponents (alpha, beta) reflecting LLM literature.
    3. Clipping of initial parameters to their bounds for improved optimizer stability.
    4. Enforces a theoretical constraint within the objective function (predicted_loss &gt;= C)
       to ensure physical plausibility of the loss predictions.

    Args:
        data_points (np.ndarray): (N,2) array with columns [num_params, parallel_size].
        loss_values (np.ndarray): Array of corresponding loss values.

    Returns:
        np.ndarray: Optimized parameters [A, alpha, beta, C].
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)

    # Handle empty data by returning sensible default initial parameters
    if len(y) == 0:
        return np.array([10.0, -0.1, -0.1, 0.5])

    num_params_data = X[:, 0]
    parallel_size_data = X[:, 1]
    min_observed_loss = np.min(y)
    max_observed_loss = np.max(y)
    mean_observed_loss = np.mean(y)

    # --- Bounds for Parameters ---
    # Define bounds first, as initial guesses for alpha and C are derived from them,
    # ensuring initial guesses are within their allowed ranges.
    # A: Scale factor, must be positive.
    # alpha: Exponent for num_params. Refined to a common range for LLM parameter scaling (-0.2 to -0.05).
    # beta: Exponent for parallel_size. Allows for potentially stronger inverse scaling (-1.0 to -1e-6).
    bounds = [
        (1e-6, 100.0),    # A: Scale factor, must be positive.
        (-0.2, -0.05),    # alpha: Tighter, more informed range based on LLM literature.
        (-1.0, -1e-6),    # beta: Allows for significant parallel scaling benefits.
    ]

    # C bound: Non-negative, and strictly less than the minimum observed loss
    # (as C represents an irreducible asymptotic loss).
    # Ensures C is physically plausible and acts as a lower bound for loss.
    c_upper_bound = max(0.0, min_observed_loss * 0.99)
    if min_observed_loss &lt;= 1e-6: # Fallback for extremely small or zero observed loss
        bounds.append((0.0, max_observed_loss + 0.1))
    else:
        bounds.append((0.0, c_upper_bound))

    # --- Adaptive Initial Guess for Parameters ---
    # alpha: Set to the midpoint of its refined bounds for a balanced starting point.
    initial_alpha = (bounds[1][0] + bounds[1][1]) / 2.0
    # beta: Set to the midpoint of its refined bounds for a balanced starting point.
    initial_beta = (bounds[2][0] + bounds[2][1]) / 2.0

    # C: Irreducible loss term. Initialized adaptively as a smaller fraction of the minimum observed loss.
    # This helps anchor C in a plausible range relative to the data and ensures it&#x27;s positive.
    initial_C = max(1e-3, min_observed_loss * 0.3)
    
    # A: Scale factor. Estimated from the average loss, given initial guesses for other parameters.
    # Uses geometric means (via log-transform) for a more consistent initial estimate in power law models.
    # num_params_data and parallel_size_data are guaranteed positive.
    log_num_params_mean = np.mean(np.log(num_params_data)) 
    log_parallel_size_mean = np.mean(np.log(parallel_size_data))
    
    # Calculate denominator term: exp(alpha * mean(log(N)) + beta * mean(log(S)))
    denom = np.exp(initial_alpha * log_num_params_mean + initial_beta * log_parallel_size_mean)
    
    # Handle cases where the denominator is extremely small to prevent large initial_A.
    if denom &lt; 1e-10:
        initial_A = 10.0 # Fallback to a default A
    else:
        # Calculate A based on (mean_loss - initial_C) / denominator.
        # Ensure the numerator (mean_observed_loss - initial_C) is positive, as A must be positive.
        numerator_A = max(1e-6, mean_observed_loss - initial_C)
        initial_A = numerator_A / denom
        # Clip A to a reasonable range to prevent extreme initial values that could destabilize optimization.
        initial_A = np.clip(initial_A, 0.1, 50.0) 

    initial_params = np.array([initial_A, initial_alpha, initial_beta, initial_C])

    # Crucial step: Clip initial_params to ensure they are within the defined bounds *before* optimization.
    # This prevents the optimizer from starting in an invalid region, improving convergence stability.
    for i in range(len(initial_params)):
        initial_params[i] = np.clip(initial_params[i], bounds[i][0], bounds[i][1])


    def objective(params):
        predicted_loss = scaling_law_func(X, params)

        # Check for invalid predictions (NaN, Inf) and return a very high error
        # to guide the optimizer away from problematic parameter regions.
        if not np.all(np.isfinite(predicted_loss)):
            return np.inf

        # The model L = A * N^alpha * S^beta + C implies L &gt;= C if A &gt; 0, N, S &gt; 0, alpha, beta &lt; 0.
        # Ensure predicted loss respects this theoretical lower bound (C).
        # We use max(0.0, params[3]) to ensure C itself is non-negative, as it&#x27;s a loss.
        # The bounds for C already ensure it&#x27;s &gt;= 0, so using params[3] directly is fine.
        predicted_loss = np.maximum(predicted_loss, params[3]) # params[3] is C

        # Compute Mean Squared Error (MSE) as the objective function.
        mse = np.mean((predicted_loss - y) ** 2)
        return mse

    # Use L-BFGS-B optimizer, which efficiently supports bounds for parameters.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)

    # Return optimized parameters if the optimization was successful, otherwise return
    # the (clipped) initial guess as a fallback to ensure a valid return value.
    params_opt = result.x if result.success else initial_params
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
