<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - Domain Mixture Scaling Law - SLDAgent + Gemini 3 Pro Preview</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>Domain Mixture Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 3 Pro Preview</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.989908
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.989641</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.989378</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989908
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts multi-domain loss using an Effective Data Mixture Scaling Law.
    Model: L_i = bias_i + (sum_j (w_ij^2 * x_j))^(-exp(alpha_i))
    
    This function applies transformations to parameters to ensure physical validity:
    - Weights are squared to be non-negative.
    - Alpha is exponentiated to be strictly positive.
    - Bias is linear (fitted to be effectively the asymptotic loss).
    
    Args:
        data_points: (N, 5) array of domain mixture proportions.
        params: Flat array of 35 parameters (7 per domain: 1 bias, 1 log_alpha, 5 weights).
        
    Returns:
        Predicted losses: (N, 5) array.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params)
    
    # Constants
    n_domains = 5
    n_params_per = 7  # 1 bias, 1 log_alpha, 5 weights
    total_len = n_domains * n_params_per
    
    # Robust parameter reshaping
    if params.size != total_len:
        flat = params.ravel()
        if flat.size &lt; total_len:
            flat = np.pad(flat, (0, total_len - flat.size), constant_values=0.1)
        params = flat[:total_len]
        
    P = params.reshape(n_domains, n_params_per)
    
    # Extract and transform parameters
    bias = P[:, 0]
    # Clip log_alpha to valid range [~0.006, ~20.0] to prevent overflow/underflow
    log_alpha = np.clip(P[:, 1], -5.0, 3.0) 
    weights = P[:, 2:] ** 2
    
    # Calculate Effective Data: D_eff = X @ W.T
    # Maps mixture proportions to an effective dataset size for each domain
    D_eff = np.dot(X, weights.T)
    D_eff = np.maximum(D_eff, 1e-10) # Numerical stability
    
    # Power Law Term: D^(-alpha)
    term = D_eff ** (-np.exp(log_alpha)[None, :])
    
    # Prediction: Bias + Term
    return bias[None, :] + term

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law using a robust hybrid strategy:
    1. Generates multiple weight priors (Correlation-based, Diagonal, Uniform).
    2. Performs a coarse grid search for Bias and Alpha for each weight prior.
    3. Optimizes the best candidate using BFGS to find the global minimum.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    Y = np.atleast_2d(np.asarray(loss_values))
    
    # Align shapes
    if Y.shape[0] != X.shape[0] and Y.shape[1] == X.shape[0]:
        Y = Y.T
        
    n_domains = 5
    n_feat = 5
    final_params = []
    
    # --- 1. Pre-compute Correlation Priors ---
    # Determine which features correlate with performance (negative correlation with loss)
    w_corrs_all = []
    for t in range(n_domains):
        y = Y[:, t]
        corrs = []
        for j in range(n_feat):
            if np.std(X[:, j]) &gt; 1e-9:
                corrs.append(np.corrcoef(X[:, j], y)[0, 1])
            else:
                corrs.append(0.0)
        # Convert correlation to weight: Strong neg correlation -&gt; High weight
        w_c = np.sqrt(np.maximum(-np.array(corrs), 0) + 0.05)
        w_c /= np.max(w_c) # Normalize
        w_corrs_all.append(w_c)

    # --- 2. Fit Each Domain ---
    for t in range(n_domains):
        y_tgt = Y[:, t]
        y_min = np.min(y_tgt)
        
        # Weight Candidates
        # A. Correlation: Data-driven guess
        # B. Diagonal: Self-data is most important
        w_diag = np.full(n_feat, np.sqrt(0.01))
        if t &lt; n_feat: w_diag[t] = 1.0
        # C. Uniform: All data helps equally
        w_uni = np.full(n_feat, np.sqrt(0.2))
        
        candidates = [w_corrs_all[t], w_diag, w_uni]
        
        # Grid Search for Basins
        best_mse = np.inf
        best_p0 = None
        
        # Bias: Asymptotic loss is usually bounded by observed min loss
        bias_grid = [max(0, y_min - 0.01), max(0, y_min - 0.5), 0.0]
        # Alpha: Standard scaling exponents
        alpha_grid = [0.1, 0.5, 1.0]
        
        for w_base in candidates:
            # Pre-calc effective data for this weight vector
            w_sq = w_base**2
            d_eff = np.dot(X, w_sq)
            d_eff = np.maximum(d_eff, 1e-10)
            
            for b_val in bias_grid:
                for a_val in alpha_grid:
                    pred = b_val + d_eff**(-a_val)
                    mse = np.mean((pred - y_tgt)**2)
                    
                    if mse &lt; best_mse:
                        best_mse = mse
                        best_p0 = np.concatenate(([b_val, np.log(a_val)], w_base))
                        
        # --- 3. Optimization ---
        def objective(p):
            b = p[0]
            log_a = np.clip(p[1], -5.0, 3.0)
            a = np.exp(log_a)
            w = p[2:]**2
            
            d = np.dot(X, w)
            d = np.maximum(d, 1e-10)
            pred = b + d**(-a)
            
            mse = np.mean((pred - y_tgt)**2)
            
            # Penalties / Priors
            pen = 0.0
            # Soft constraint: Bias should be effectively positive
            if b &lt; 0: pen += 100.0 * b**2
            # Soft constraint: Bias shouldn&#x27;t exceed min observed loss too much
            if b &gt; y_min: pen += 10.0 * (b - y_min)**2
            
            # L2 Regularization on weights to improve convexity
            reg = 1e-7 * np.sum(w**2)
            
            return mse + pen + reg

        try:
            # BFGS is efficient and robust for this smooth problem
            res = minimize(objective, best_p0, method=&#x27;BFGS&#x27;, tol=1e-6)
            final_params.append(res.x)
        except Exception:
            final_params.append(best_p0)
            
    return np.concatenate(final_params)
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989802
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Implements an Effective Data Mixture scaling law with L1-regularized weights.
Model: L_i = c_i + ( sum_j (w_{ij}^2) * x_j ) ^ -exp(log_alpha_i)
Improvements:
- L1 regularization on effective weights (sparsity inducing)
- Soft penalty for negative bias to maintain physical plausibility
- Multi-start optimization with BFGS to avoid local minima
Total params: 5 domains * 7 params/domain = 35 params.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params)
    
    # Ensure parameter array size matches expectation
    expected_size = 35
    if params.size &lt; expected_size:
        params = np.pad(params, (0, expected_size - params.size), &#x27;constant&#x27;)
    elif params.size &gt; expected_size:
        params = params[:expected_size]
        
    P = params.reshape(5, 7)
    
    # Parameters
    # c: bias
    # alpha: power law exponent (enforced positive via exp)
    # W: mixing weights (enforced positive via square)
    
    c = P[:, 0]
    alpha = np.exp(P[:, 1])
    W = P[:, 2:]**2
    
    # Effective data: D = X @ W.T
    # D shape: (N, 5)
    D_eff = X @ W.T
    
    # Power law term: D^-alpha
    # Numerical stability: max(D, 1e-10)
    term = np.maximum(D_eff, 1e-10) ** (-alpha[None, :])
    
    pred = c[None, :] + term
    
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points))
    Y = np.atleast_2d(np.asarray(loss_values))
    if Y.ndim == 1: Y = Y[:, None]
    if Y.shape[0] != X.shape[0]: Y = Y.T
    
    _, n_dims = X.shape
    _, n_domains = Y.shape
    
    final_params = []
    
    for t in range(n_domains):
        y_t = Y[:, t]
        y_min = np.min(y_t)
        
        # Objective function
        def objective(p):
            c = p[0]
            log_a = p[1]
            w_raw = p[2:]
            
            # Derived parameters
            alpha = np.exp(log_a)
            w = w_raw**2
            
            # Prediction
            d = X @ w
            pred = c + np.maximum(d, 1e-10)**(-alpha)
            
            mse = np.mean((pred - y_t)**2)
            
            # Regularization
            # 1. Sparsity on effective weights (L1 on W &lt;=&gt; L2 on w_raw)
            # This encourages selecting only relevant domains
            reg_w = 1e-6 * np.sum(w)
            
            # 2. Soft penalty for negative bias
            # Allows transient negative exploration but penalizes final solution
            pen_c = 0.0
            if c &lt; 0:
                pen_c = 100.0 * c**2
            
            # 3. Soft penalty for extreme alpha
            # log_alpha should be in reasonable range [-5, 3]
            pen_a = 0.0
            if log_a &lt; -5: pen_a += (log_a + 5)**2
            if log_a &gt; 3: pen_a += (log_a - 3)**2
                
            return mse + reg_w + pen_c + pen_a

        # Initialization Strategies
        candidates = []
        
        # Strategy 1: &quot;Standard&quot; 
        # Bias near minimum, modest diagonal dominance
        c1 = max(0.0, y_min - 0.2)
        w1 = np.full(n_dims, 0.2)
        if t &lt; n_dims: w1[t] = 1.0 # Self-weight 1.0, others 0.04
        p1 = np.concatenate(([c1, 0.0], np.sqrt(w1)))
        candidates.append(p1)
        
        # Strategy 2: &quot;Low Bias&quot;
        # Bias very low, relying more on power law (good for steep curves)
        c2 = max(0.0, y_min - 1.0)
        w2 = np.full(n_dims, 0.3)
        if t &lt; n_dims: w2[t] = 1.2
        p2 = np.concatenate(([c2, np.log(0.5)], np.sqrt(w2)))
        candidates.append(p2)
        
        # Optimization
        best_p = None
        best_loss = np.inf
        
        for p0 in candidates:
            try:
                res = minimize(objective, p0, method=&#x27;BFGS&#x27;, tol=1e-6, options={&#x27;maxiter&#x27;: 500})
                if res.fun &lt; best_loss:
                    best_loss = res.fun
                    best_p = res.x
            except Exception:
                continue
                
        if best_p is None:
            best_p = candidates[0]
            
        final_params.append(best_p)
        
    return np.concatenate(final_params)
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989715
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Improved model using Effective Data Mixture formulation.
Models loss as L_t = bias_t + (sum_j (w_{tj}^2 * x_j))^(-alpha_t)
Uses 7 parameters per domain * 5 domains = 35 parameters total.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 5) domain proportions
    # params: (35,) flat array representing parameters for 5 domains
    
    X = np.atleast_2d(np.asarray(data_points, dtype=float))
    params = np.asarray(params, dtype=float)
    
    n_domains = 5
    n_params_per_domain = 7
    expected_size = n_domains * n_params_per_domain
    
    # Robustly handle parameter shape
    # If parameters are passed from a different sized model or messed up, we handle it
    if params.size != expected_size:
        flat_params = params.ravel()
        if flat_params.size &lt; expected_size:
            # Pad with sensible defaults if too small
            # bias=2.0, alpha=0.5, weights=0.1
            padding = np.tile(np.array([2.0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1]), n_domains)
            flat_params = np.concatenate([flat_params, padding])[:expected_size]
        else:
            flat_params = flat_params[:expected_size]
        P = flat_params.reshape(n_domains, n_params_per_domain)
    else:
        P = params.reshape(n_domains, n_params_per_domain)
        
    bias = P[:, 0]          # (5,)
    alpha = P[:, 1]         # (5,)
    weights = P[:, 2:]      # (5, 5)
    
    # Weights squared for positivity (learns magnitude of contribution)
    W = weights ** 2        # (5, 5)
    # Alpha positive for power law decay
    A = np.abs(alpha)       # (5,)
    
    # Compute effective data: D_eff[n, t] = sum_j (X[n, j] * W[t, j])
    # This represents the &quot;equivalent dataset size&quot; for domain t derived from the mixture
    D_eff = np.dot(X, W.T)
    
    # Stability: prevent division by zero or effectively zero data
    D_eff = np.maximum(D_eff, 1e-6)
    
    # Power law: D_eff ^ -alpha
    # Broadcasting: (N, 5) ^ (1, 5)
    power_term = D_eff ** (-A[None, :])
    
    pred = bias[None, :] + power_term
    
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points, dtype=float))
    Y = np.asarray(loss_values, dtype=float)
    
    if Y.ndim == 1:
        Y = Y[:, None]
        
    n_domains = 5
    n_features = X.shape[1]
    
    all_params = []
    
    # Fit each domain independently for stability
    for t in range(n_domains):
        y_target = Y[:, t]
        min_y = np.min(y_target)
        
        # Objective: MSE
        def objective(p):
            b = p[0]
            a = p[1]
            w = p[2:]
            
            w_sq = w ** 2
            d_eff = np.dot(X, w_sq)
            d_eff = np.maximum(d_eff, 1e-6)
            
            pred = b + d_eff ** (-a)
            return np.mean((pred - y_target)**2)
            
        # Initialization
        # Bias is the asymptotic floor, likely slightly below min observed loss
        bias_init = min_y - 0.1
        # Alpha is the scaling exponent, typically 0.1-1.0
        alpha_init = 0.5
        # Weights: assume diagonal dominance (domain helps itself most)
        w_init = np.full(n_features, np.sqrt(0.1))
        if t &lt; n_features:
            w_init[t] = 1.0
            
        p0 = np.concatenate(([bias_init, alpha_init], w_init))
        
        # Bounds: 
        # Bias &lt; min_y to prevent unphysical negative power terms
        # Alpha &gt; 0
        bounds = [(None, min_y - 1e-4), (1e-2, 5.0)] + [(None, None)] * n_features
        
        try:
            res = minimize(objective, p0, method=&#x27;L-BFGS-B&#x27;, bounds=bounds)
            p_opt = res.x
        except:
            # Fallback to initialization if optimization fails
            p_opt = p0
            
        all_params.append(p_opt)
        
    # Concatenate all parameters into a flat array of 35 (5 domains * 7 params)
    return np.concatenate(all_params)
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989400
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Implements an Effective Data Mixture scaling law.
Model: Loss_t = Bias_t + (Sum_j (W_tj^2 * x_j))^(-exp(alpha_t))
This captures how data from different domains contributes to the effective training size for a target domain.
Uses exactly 35 parameters (7 per domain * 5 domains).
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 5) array of domain proportions
    # params: (35,) flat array or (5, 7) array
    
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params)
    
    # Parameter unpacking and shape handling
    # We use 7 parameters per domain: 1 bias, 1 log_alpha, 5 weights
    params_per_domain = 7
    
    if params.ndim == 1:
        # Infer number of domains from parameter size
        # Expected size is 35 for 5 domains
        if params.size % params_per_domain == 0:
            T = params.size // params_per_domain
            P = params.reshape(T, params_per_domain)
        else:
            # Fallback for mismatched size, possibly just return bias or crash safely
            # Assuming 5 domains if size is weird but we want to try
            T = 5
            # Resize or truncate would be dangerous, just let reshape fail if needed
            P = params.reshape(T, params_per_domain) 
    else:
        P = params
        T = P.shape[0]

    # Extract parameters
    # P structure: [bias, log_alpha, w0_sqrt, w1_sqrt, w2_sqrt, w3_sqrt, w4_sqrt]
    bias = P[:, 0]              # (T,)
    log_alpha = P[:, 1]         # (T,)
    w_sqrt = P[:, 2:]           # (T, F) - F is number of weight params, should match X.shape[1]
    
    # Enforce constraints via transforms
    # Alpha must be positive. exp() ensures this. Clip to avoid overflow.
    alpha = np.exp(np.clip(log_alpha, -5.0, 4.0)) 
    # Weights must be positive. Square of parameter ensures this.
    weights = w_sqrt ** 2       
    
    # Compute Effective Data for each target domain
    # D_eff = X @ W.T
    # X: (N, 5), Weights: (T, 5) -&gt; D_eff: (N, T)
    # If dimensions mismatch (e.g. testing with fewer features), handle gracefully
    if X.shape[1] != weights.shape[1]:
        F_in = min(X.shape[1], weights.shape[1])
        D_eff = X[:, :F_in] @ weights[:, :F_in].T
    else:
        D_eff = X @ weights.T
    
    # Numerical stability: Effective data cannot be 0 or negative
    D_eff = np.maximum(D_eff, 1e-8)
    
    # Power law term: (D_eff) ^ (-alpha)
    term = D_eff ** (-alpha[None, :])
    
    # Final prediction: bias + term
    pred = bias[None, :] + term
    
    # Return shape handling
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points))
    Y = np.asarray(loss_values)
    if Y.ndim == 1:
        Y = Y[:, None]
        
    N, F = X.shape
    _, T = Y.shape
    
    # Model uses 7 params per output domain: bias (1), log_alpha (1), weights (5)
    # Total params = 7 * T. For T=5, Total=35.
    
    all_params = []
    
    for t in range(T):
        y_target = Y[:, t]
        
        # Heuristic Initialization
        # Bias: Irreducible loss. Start near minimum observed loss.
        bias_init = np.min(y_target) - 0.2
        
        # Alpha: Scaling exponent. Start with typical value ~0.5 (log_alpha ~ -0.7)
        log_alpha_init = -0.7
        
        # Weights: Start with diagonal dominance (self-transfer is best)
        # sqrt(1.0) = 1.0, sqrt(0.1) ~ 0.316
        w_init = np.full(F, 0.32)
        if t &lt; F:
            w_init[t] = 1.0
            
        p0 = np.concatenate(([bias_init, log_alpha_init], w_init))
        
        def objective(p):
            # Decode params
            b = p[0]
            log_a = np.clip(p[1], -5.0, 4.0)
            a = np.exp(log_a)
            w = p[2:] ** 2
            
            # Compute effective data
            d = X @ w
            d = np.maximum(d, 1e-8)
            
            # Predict
            pred = b + d**(-a)
            
            # MSE Loss
            return np.mean((pred - y_target)**2)
        
        # Optimize
        # BFGS is robust enough for this unconstrained formulation (due to transforms)
        try:
            res = minimize(objective, p0, method=&#x27;BFGS&#x27;)
            best_p = res.x
        except Exception:
            # Fallback to init if optimization crashes
            best_p = p0
            
        all_params.append(best_p)
    
    # Return flattened parameters array (size 35)
    return np.concatenate(all_params)
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989378
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Implements an Effective Data Mixture scaling law:
L_i = bias_i + (sum_j (w_{ij}^2 * x_j))^(-exp(alpha_i))
This models how mixing data from different domains contributes to reducing loss in a specific domain.
Uses exactly 35 parameters (7 per domain * 5 domains).
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 5) array of domain proportions
    # params: (35,) array of parameters
    
    # Ensure inputs are correct type and shape
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).ravel()
    
    N, F = X.shape
    num_domains = 5
    params_per_domain = 7
    
    # Reshape parameters
    # Expected size is 35 (5 domains * 7 params).
    expected = num_domains * params_per_domain
    if params.size &lt; expected:
        params = np.pad(params, (0, expected - params.size), &#x27;constant&#x27;)
    elif params.size &gt; expected:
        params = params[:expected]
        
    P = params.reshape(num_domains, params_per_domain)
    
    # Extract parameter components
    # col 0: bias
    # col 1: log_alpha (exponent)
    # col 2-6: raw weights for the 5 input features
    
    bias = P[:, 0]              # (5,)
    alpha = np.exp(P[:, 1])     # (5,) - ensure positive exponent
    weights = P[:, 2:] ** 2     # (5, 5) - ensure positive contribution
    
    # Calculate Effective Data
    # D_eff_i = sum_j (w_ij^2 * x_j)
    # X: (N, 5), W^T: (5, 5) -&gt; D_eff: (N, 5)
    D_eff = X @ weights.T
    
    # Avoid numerical instability with power law near 0
    D_eff = np.maximum(D_eff, 1e-6)
    
    # Calculate Power Law Term
    # term_i = D_eff_i ^ (-alpha_i)
    # Broadcasting: (N, 5) ^ (1, 5) -&gt; (N, 5)
    power_term = D_eff ** (-alpha[None, :])
    
    # Final Prediction
    pred = bias[None, :] + power_term
    
    # Match output requirements
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred


def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    Y = np.asarray(loss_values, dtype=np.float64)
    if Y.ndim == 1:
        Y = Y[:, None]
        
    N, F = X.shape
    _, num_domains = Y.shape
    
    all_params = []
    
    # Fit each domain independently
    for t in range(num_domains):
        y_tgt = Y[:, t]
        
        # Heuristic Initialization
        min_y = np.min(y_tgt)
        bias_init = max(0.0, min_y - 0.2)
        log_alpha_init = np.log(0.5) # Start with sqrt decay
        
        # Initialize weights
        # Domain i usually learns best from data i, so diagonal should be larger
        w_init = np.full(F, np.sqrt(0.1)) 
        if t &lt; F:
            w_init[t] = 1.0 
            
        p0 = np.concatenate(([bias_init, log_alpha_init], w_init))
        
        def objective(p):
            b = p[0]
            # Constrain alpha to reasonable range to prevent overflow
            a = np.exp(np.clip(p[1], -3.0, 3.0)) 
            w = p[2:] ** 2
            
            d = X @ w
            d = np.maximum(d, 1e-6)
            pred = b + d**(-a)
            
            return np.mean((pred - y_tgt)**2)
        
        try:
            res = minimize(objective, p0, method=&#x27;BFGS&#x27;, tol=1e-5)
            best_p = res.x
        except Exception:
            best_p = p0
            
        all_params.append(best_p)
        
    return np.concatenate(all_params)
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>