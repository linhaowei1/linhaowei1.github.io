<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - Parallel Scaling Law - SLDAgent + Gemini 3 Pro Preview</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>Parallel Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 3 Pro Preview</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.999659
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.999659</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.999658</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.999659
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery with improved fitting stability and efficiency.
Model: Loss = a * (N/1e9)^(-b) * S^(-c) + d
Optimization uses grid-search initialization followed by L-BFGS-B with analytical gradient.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Computes scaling law predictions.
    Args:
        data_points: (N, 2) array [num_params, parallel_size]
        params: (4,) or (T, 4) array [a, b, c, d]
    Returns:
        Predicted loss (N,) or (N, T)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N_scaled = X[:, 0] / 1.0e9  # Scale to ~O(1)
    S = X[:, 1]
    
    p = np.asarray(params)
    is_1d = (p.ndim == 1)
    if is_1d: p = p[None, :]
    
    # Broadcast parameters against data
    # p shape: (T, 4)
    a = p[:, 0]
    b = p[:, 1]
    c = p[:, 2]
    d = p[:, 3]
    
    # Calculation: (N, 1) op (1, T) -&gt; (N, T)
    term_n = N_scaled[:, None] ** (-b[None, :])
    term_s = S[:, None] ** (-c[None, :])
    pred = a[None, :] * term_n * term_s + d[None, :]
    
    return pred[:, 0] if is_1d else pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits parameters [a, b, c, d] minimizing MSE.
    Uses analytical gradients for robust convergence.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    if y.ndim == 1: y = y[:, None]
    
    N_scaled = X[:, 0] / 1.0e9
    S = X[:, 1]
    n_samples, n_targets = y.shape
    
    # Pre-compute logarithmic features for initialization
    log_N = np.log(N_scaled)
    log_S = np.log(S)
    ones = np.ones(n_samples)
    design_matrix = np.column_stack([ones, -log_N, -log_S])
    
    final_params = []
    
    for t in range(n_targets):
        y_curr = y[:, t]
        min_y = np.min(y_curr)
        
        # 1. Grid Search for Initialization
        # We search for optimal &#x27;d&#x27; that linearizes the problem
        best_init_loss = np.inf
        # Default fallback
        best_p = np.array([1.0, 0.1, 0.1, 0.0])
        
        # Check candidates for d in [0, min_y)
        # 25 points provides good coverage
        d_candidates = np.linspace(0, min_y * 0.9999, 25)
        
        for d_val in d_candidates:
            y_shift = y_curr - d_val
            # Log requires strictly positive
            if np.any(y_shift &lt;= 1e-10): continue
            
            # Linear fit: log(y-d) ~ log(a) - b*log(N) - c*log(S)
            try:
                coeffs, _, _, _ = np.linalg.lstsq(design_matrix, np.log(y_shift), rcond=None)
                a_est, b_est, c_est = np.exp(coeffs[0]), coeffs[1], coeffs[2]
                
                # Evaluate MSE in original space
                term = a_est * (N_scaled**-b_est) * (S**-c_est)
                pred = term + d_val
                mse = np.mean((pred - y_curr)**2)
                
                if mse &lt; best_init_loss:
                    best_init_loss = mse
                    best_p = np.array([a_est, b_est, c_est, d_val])
            except:
                continue

        # 2. Fine-tuning with Analytical Gradient
        # Closure captures data
        def objective_with_grad(p):
            a, b, c, d = p
            term_n = N_scaled ** -b
            term_s = S ** -c
            term = term_n * term_s
            pred = a * term + d
            diff = pred - y_curr
            mse = np.mean(diff ** 2)
            
            # Gradients
            factor = (2.0 / n_samples) * diff
            grad_a = np.sum(factor * term)
            grad_b = np.sum(factor * a * term * (-log_N))
            grad_c = np.sum(factor * a * term * (-log_S))
            grad_d = np.sum(factor)
            
            return mse, np.array([grad_a, grad_b, grad_c, grad_d])

        # Bounds: a&gt;0 to prevent singular model, d &lt; min_y to valid range
        bounds = [(1e-8, None), (None, None), (None, None), (None, min_y - 1e-9)]
        
        # L-BFGS-B with jac=True is significantly more robust than finite differences
        res = minimize(objective_with_grad, best_p, method=&#x27;L-BFGS-B&#x27;, jac=True, bounds=bounds)
        final_params.append(res.x if res.success else best_p)
        
    final_params = np.array(final_params)
    return final_params[0] if n_targets == 1 else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.999659
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery with improved fitting stability and efficiency.
Model: Loss = a * (N/1e9)^(-b) * S^(-c) + d
Optimization uses grid-search initialization followed by L-BFGS-B with analytical gradient.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Computes scaling law predictions.
    Args:
        data_points: (N, 2) array [num_params, parallel_size]
        params: (4,) or (T, 4) array [a, b, c, d]
    Returns:
        Predicted loss (N,) or (N, T)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N_scaled = X[:, 0] / 1.0e9  # Scale to ~O(1)
    S = X[:, 1]
    
    p = np.asarray(params)
    is_1d = (p.ndim == 1)
    if is_1d: p = p[None, :]
    
    # Broadcast parameters against data
    # p shape: (T, 4)
    a = p[:, 0]
    b = p[:, 1]
    c = p[:, 2]
    d = p[:, 3]
    
    # Calculation: (N, 1) op (1, T) -&gt; (N, T)
    term_n = N_scaled[:, None] ** (-b[None, :])
    term_s = S[:, None] ** (-c[None, :])
    pred = a[None, :] * term_n * term_s + d[None, :]
    
    return pred[:, 0] if is_1d else pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits parameters [a, b, c, d] minimizing MSE.
    Uses analytical gradients for robust convergence.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    if y.ndim == 1: y = y[:, None]
    
    N_scaled = X[:, 0] / 1.0e9
    S = X[:, 1]
    n_samples, n_targets = y.shape
    
    # Pre-compute logarithmic features for initialization
    log_N = np.log(N_scaled)
    log_S = np.log(S)
    ones = np.ones(n_samples)
    design_matrix = np.column_stack([ones, -log_N, -log_S])
    
    final_params = []
    
    for t in range(n_targets):
        y_curr = y[:, t]
        min_y = np.min(y_curr)
        
        # 1. Grid Search for Initialization
        # We search for optimal &#x27;d&#x27; that linearizes the problem
        best_init_loss = np.inf
        # Default fallback
        best_p = np.array([1.0, 0.1, 0.1, 0.0])
        
        # Check candidates for d in [0, min_y)
        # 25 points provides good coverage
        d_candidates = np.linspace(0, min_y * 0.9999, 25)
        
        for d_val in d_candidates:
            y_shift = y_curr - d_val
            # Log requires strictly positive
            if np.any(y_shift &lt;= 1e-10): continue
            
            # Linear fit: log(y-d) ~ log(a) - b*log(N) - c*log(S)
            try:
                coeffs, _, _, _ = np.linalg.lstsq(design_matrix, np.log(y_shift), rcond=None)
                a_est, b_est, c_est = np.exp(coeffs[0]), coeffs[1], coeffs[2]
                
                # Evaluate MSE in original space
                term = a_est * (N_scaled**-b_est) * (S**-c_est)
                pred = term + d_val
                mse = np.mean((pred - y_curr)**2)
                
                if mse &lt; best_init_loss:
                    best_init_loss = mse
                    best_p = np.array([a_est, b_est, c_est, d_val])
            except:
                continue

        # 2. Fine-tuning with Analytical Gradient
        # Closure captures data
        def objective_with_grad(p):
            a, b, c, d = p
            term_n = N_scaled ** -b
            term_s = S ** -c
            term = term_n * term_s
            pred = a * term + d
            diff = pred - y_curr
            mse = np.mean(diff ** 2)
            
            # Gradients
            factor = (2.0 / n_samples) * diff
            grad_a = np.sum(factor * term)
            grad_b = np.sum(factor * a * term * (-log_N))
            grad_c = np.sum(factor * a * term * (-log_S))
            grad_d = np.sum(factor)
            
            return mse, np.array([grad_a, grad_b, grad_c, grad_d])

        # Bounds: a&gt;0 to prevent singular model, d &lt; min_y to valid range
        bounds = [(1e-8, None), (None, None), (None, None), (None, min_y - 1e-9)]
        
        # L-BFGS-B with jac=True is significantly more robust than finite differences
        res = minimize(objective_with_grad, best_p, method=&#x27;L-BFGS-B&#x27;, jac=True, bounds=bounds)
        final_params.append(res.x if res.success else best_p)
        
    final_params = np.array(final_params)
    return final_params[0] if n_targets == 1 else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.999659
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery with improved fitting stability and efficiency.
Model: Loss = a * (N/1e9)^(-b) * S^(-c) + d
Optimization uses grid-search initialization followed by L-BFGS-B with analytical gradient.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Computes scaling law predictions.
    Args:
        data_points: (N, 2) array [num_params, parallel_size]
        params: (4,) or (T, 4) array [a, b, c, d]
    Returns:
        Predicted loss (N,) or (N, T)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N_scaled = X[:, 0] / 1.0e9  # Scale to ~O(1)
    S = X[:, 1]
    
    p = np.asarray(params)
    is_1d = (p.ndim == 1)
    if is_1d: p = p[None, :]
    
    # Broadcast parameters against data
    # p shape: (T, 4)
    a = p[:, 0]
    b = p[:, 1]
    c = p[:, 2]
    d = p[:, 3]
    
    # Calculation: (N, 1) op (1, T) -&gt; (N, T)
    term_n = N_scaled[:, None] ** (-b[None, :])
    term_s = S[:, None] ** (-c[None, :])
    pred = a[None, :] * term_n * term_s + d[None, :]
    
    return pred[:, 0] if is_1d else pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits parameters [a, b, c, d] minimizing MSE.
    Uses analytical gradients for robust convergence.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    if y.ndim == 1: y = y[:, None]
    
    N_scaled = X[:, 0] / 1.0e9
    S = X[:, 1]
    n_samples, n_targets = y.shape
    
    # Pre-compute logarithmic features for initialization
    log_N = np.log(N_scaled)
    log_S = np.log(S)
    ones = np.ones(n_samples)
    design_matrix = np.column_stack([ones, -log_N, -log_S])
    
    final_params = []
    
    for t in range(n_targets):
        y_curr = y[:, t]
        min_y = np.min(y_curr)
        
        # 1. Grid Search for Initialization
        # We search for optimal &#x27;d&#x27; that linearizes the problem
        best_init_loss = np.inf
        # Default fallback
        best_p = np.array([1.0, 0.1, 0.1, 0.0])
        
        # Check candidates for d in [0, min_y)
        # 25 points provides good coverage
        d_candidates = np.linspace(0, min_y * 0.9999, 25)
        
        for d_val in d_candidates:
            y_shift = y_curr - d_val
            # Log requires strictly positive
            if np.any(y_shift &lt;= 1e-10): continue
            
            # Linear fit: log(y-d) ~ log(a) - b*log(N) - c*log(S)
            try:
                coeffs, _, _, _ = np.linalg.lstsq(design_matrix, np.log(y_shift), rcond=None)
                a_est, b_est, c_est = np.exp(coeffs[0]), coeffs[1], coeffs[2]
                
                # Evaluate MSE in original space
                term = a_est * (N_scaled**-b_est) * (S**-c_est)
                pred = term + d_val
                mse = np.mean((pred - y_curr)**2)
                
                if mse &lt; best_init_loss:
                    best_init_loss = mse
                    best_p = np.array([a_est, b_est, c_est, d_val])
            except:
                continue

        # 2. Fine-tuning with Analytical Gradient
        # Closure captures data
        def objective_with_grad(p):
            a, b, c, d = p
            term_n = N_scaled ** -b
            term_s = S ** -c
            term = term_n * term_s
            pred = a * term + d
            diff = pred - y_curr
            mse = np.mean(diff ** 2)
            
            # Gradients
            factor = (2.0 / n_samples) * diff
            grad_a = np.sum(factor * term)
            grad_b = np.sum(factor * a * term * (-log_N))
            grad_c = np.sum(factor * a * term * (-log_S))
            grad_d = np.sum(factor)
            
            return mse, np.array([grad_a, grad_b, grad_c, grad_d])

        # Bounds: a&gt;0 to prevent singular model, d &lt; min_y to valid range
        bounds = [(1e-8, None), (None, None), (None, None), (None, min_y - 1e-9)]
        
        # L-BFGS-B with jac=True is significantly more robust than finite differences
        res = minimize(objective_with_grad, best_p, method=&#x27;L-BFGS-B&#x27;, jac=True, bounds=bounds)
        final_params.append(res.x if res.success else best_p)
        
    final_params = np.array(final_params)
    return final_params[0] if n_targets == 1 else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.999659
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery with robust multiplicative power law model.
Model: Loss = a * (N/1e9)^(-b) * S^(-c) + d
Optimization uses 1D bounded optimization for initialization of &#x27;d&#x27;,
followed by L-BFGS-B with analytical gradients and physical constraints.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, minimize_scalar

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss using the multiplicative scaling law:
    L = a * (N/1e9)^(-b) * S^(-c) + d
    
    Args:
        data_points: (N, 2) array [num_params, parallel_size]
        params: (4,) or (T, 4) array [a, b, c, d]
    Returns:
        Predicted loss (N,) or (N, T)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N_scaled = X[:, 0] / 1.0e9 # Normalize params to ~O(1)
    S = X[:, 1]
    
    params = np.asarray(params)
    squeeze_output = (params.ndim == 1)
    if squeeze_output:
        params = params[None, :]
    
    # Extract parameters: shape (T,)
    a = params[:, 0]
    b = params[:, 1]
    c = params[:, 2]
    d = params[:, 3]
    
    # Vectorized computation
    # (N, 1) ** (1, T) -&gt; (N, T)
    term_n = N_scaled[:, None] ** (-b[None, :])
    term_s = S[:, None] ** (-c[None, :])
    
    pred = a[None, :] * term_n * term_s + d[None, :]
    
    return pred[:, 0] if squeeze_output else pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits parameters [a, b, c, d] using a robust two-stage optimization.
    Stage 1: Profile likelihood optimization for &#x27;d&#x27; via linear regression proxy.
    Stage 2: Gradient-based minimization of MSE with bounds.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    Y = np.asarray(loss_values, dtype=np.float64)
    if Y.ndim == 1:
        Y = Y[:, None]
    
    n_samples, n_targets = Y.shape
    N_scaled = X[:, 0] / 1.0e9
    S = X[:, 1]
    
    # Precompute logarithmic features for initialization
    log_N = np.log(N_scaled)
    log_S = np.log(S)
    ones = np.ones(n_samples)
    # Design matrix for linear system: log(y-d) = log(a) - b*log(N) - c*log(S)
    # Becomes: log(y-d) = [1, -logN, -logS] @ [log(a), b, c]
    A = np.column_stack([ones, -log_N, -log_S])
    
    fitted_params = []
    
    for i in range(n_targets):
        y = Y[:, i]
        min_y = np.min(y)
        
        # --- Stage 1: Initialization ---
        # Find &#x27;d&#x27; that minimizes MSE of the power law when a,b,c are optimal for that d (in log space)
        
        def init_objective(d_val):
            y_shift = y - d_val
            # Penalize invalid d values
            if np.any(y_shift &lt;= 1e-9): return 1e9
            
            # Linear fit in log space
            try:
                # sol = [log(a), b, c]
                sol, _, _, _ = np.linalg.lstsq(A, np.log(y_shift), rcond=None)
                
                # Reconstruct params
                a_est = np.exp(sol[0])
                b_est = sol[1]
                c_est = sol[2]
                
                # Calc MSE in linear space
                pred = a_est * (N_scaled ** -b_est) * (S ** -c_est) + d_val
                return np.mean((pred - y)**2)
            except:
                return 1e9

        # Bound d to [0, min_y - eps]
        # Use bounded scalar optimization which is cleaner than grid search
        limit = max(0.0, min_y - 1e-6)
        res_d = minimize_scalar(init_objective, bounds=(0.0, limit), method=&#x27;bounded&#x27;)
        
        d_init = res_d.x
        
        # Recalculate best a,b,c for this d_init
        y_shift = y - d_init
        if np.any(y_shift &lt;= 0): # Safety fallback
            d_init = 0.0
            y_shift = y
        
        sol, _, _, _ = np.linalg.lstsq(A, np.log(y_shift), rcond=None)
        # Construct initial guess [a, b, c, d]
        # Clamp exponents to be non-negative for physical plausibility
        p_init = np.array([np.exp(sol[0]), max(sol[1], 0.01), max(sol[2], 0.01), d_init])
        
        # --- Stage 2: Fine-tuning ---
        # L-BFGS-B with analytical Jacobian
        
        def objective_with_grad(p):
            a, b, c, d = p
            
            # Forward
            term_n = N_scaled ** -b
            term_s = S ** -c
            term = term_n * term_s
            pred = a * term + d
            
            diff = pred - y
            mse = np.mean(diff ** 2)
            
            # Backward
            # dMSE/dparam = mean(2 * diff * dpred/dparam)
            factor = (2.0 / n_samples) * diff
            
            grad_a = np.sum(factor * term)
            grad_b = np.sum(factor * a * term * (-log_N))
            grad_c = np.sum(factor * a * term * (-log_S))
            grad_d = np.sum(factor)
            
            return mse, np.array([grad_a, grad_b, grad_c, grad_d])
        
        bounds = [
            (1e-10, None),         # a &gt; 0
            (0.0, None),           # b &gt;= 0
            (0.0, None),           # c &gt;= 0
            (None, min_y - 1e-9)   # d &lt; min_y
        ]
        
        res = minimize(objective_with_grad, p_init, method=&#x27;L-BFGS-B&#x27;, jac=True, bounds=bounds)
        fitted_params.append(res.x if res.success else p_init)
        
    fitted_params = np.array(fitted_params)
    return fitted_params[0] if n_targets == 1 else fitted_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.999658
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced Scaling Law Discovery: L = a * N^-b * S^-c + d
Features robust initialization via dense grid search near the irreducible loss boundary
and fine-tuning with analytic gradients.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts loss using the multiplicative power law: L = a * (N/1e9)^-b * S^-c + d
    Supports both single and batched parameter sets.
    &quot;&quot;&quot;
    X = np.atleast_2d(data_points)
    # Normalize N by 1B for numerical conditioning
    N_scaled = X[:, 0] / 1e9
    S = X[:, 1]
    
    # Ensure params is at least 2D (Targets, 4)
    p = np.asarray(params)
    is_1d = (p.ndim == 1)
    if is_1d:
        p = p[None, :]
        
    # Extract parameters [a, b, c, d]
    a, b, c, d = p[:, 0], p[:, 1], p[:, 2], p[:, 3]
    
    # Compute terms with broadcasting: (Samples, 1) op (1, Targets)
    # L = a * N^-b * S^-c + d
    term_N = N_scaled[:, None] ** -b[None, :]
    term_S = S[:, None] ** -c[None, :]
    
    pred = a[None, :] * term_N * term_S + d[None, :]
    
    return pred.flatten() if is_1d else pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits parameters [a, b, c, d] for one or multiple targets.
    Method:
    1. Grid search for &#x27;d&#x27; (irreducible loss) to linearize the power law.
    2. Linear least squares to estimate a, b, c for each d.
    3. Select best initialization and refine with nonlinear least squares (TRF)
       using analytic Jacobian and soft_l1 loss for robustness.
    &quot;&quot;&quot;
    X = np.atleast_2d(data_points)
    Y = np.atleast_2d(loss_values)
    if Y.shape[0] != X.shape[0]: Y = Y.T # Ensure (Samples, Targets)
    
    N_scaled = X[:, 0] / 1e9
    S = X[:, 1]
    log_N = np.log(N_scaled)
    log_S = np.log(S)
    
    # Matrix for linear projection: log(L-d) = log(a) - b*logN - c*logS
    # Columns: [Intercept, -logN, -logS]
    DesignMat = np.column_stack((np.ones_like(log_N), -log_N, -log_S))
    
    fitted_params = []
    
    for t in range(Y.shape[1]):
        y = Y[:, t]
        y_min = np.min(y)
        
        # 1. Initialization Strategy
        # Search d in [0, y_min). Concentrate points near y_min where sensitivity is high.
        d_grid = np.concatenate([
            np.linspace(0, y_min * 0.9, 15),
            np.linspace(y_min * 0.9, y_min * 0.9999, 35)
        ])
        
        # Calculate shifted targets for all d candidates: (Samples, Candidates)
        y_shifts = y[:, None] - d_grid[None, :]
        
        # Valid columns are strictly positive
        valid = np.all(y_shifts &gt; 1e-10, axis=0)
        
        if not np.any(valid):
            # Fallback
            p0 = [1.0, 0.5, 0.5, 0.0]
        else:
            d_valid = d_grid[valid]
            log_y_shifts = np.log(y_shifts[:, valid])
            
            # Solve linear systems simultaneously
            # coeffs: (3, NumValid) -&gt; [log(a), b, c]
            coeffs, _, _, _ = np.linalg.lstsq(DesignMat, log_y_shifts, rcond=None)
            
            a_cand = np.exp(coeffs[0])
            b_cand = coeffs[1]
            c_cand = coeffs[2]
            
            # Evaluate MSE for each candidate in linear space
            preds = (a_cand[None, :] * 
                     (N_scaled[:, None] ** -b_cand[None, :]) * 
                     (S[:, None] ** -c_cand[None, :]) + 
                     d_valid[None, :])
            
            mse = np.mean((preds - y[:, None])**2, axis=0)
            best_idx = np.argmin(mse)
            
            p0 = [a_cand[best_idx], b_cand[best_idx], c_cand[best_idx], d_valid[best_idx]]
            
            # Enforce positive exponents for physical plausibility
            p0[1] = max(p0[1], 1e-3)
            p0[2] = max(p0[2], 1e-3)

        # 2. Refinement with Analytic Jacobian
        def residuals(p):
            return (p[0] * (N_scaled**-p[1]) * (S**-p[2]) + p[3]) - y
            
        def jacobian(p):
            # p = [a, b, c, d]
            base_term = (N_scaled**-p[1]) * (S**-p[2])
            val = p[0] * base_term
            
            # df/da = base_term
            # df/db = a * base_term * (-log N) = -val * log_N
            # df/dc = a * base_term * (-log S) = -val * log_S
            # df/dd = 1
            return np.column_stack((base_term, -val * log_N, -val * log_S, np.ones_like(val)))
        
        # Bounds: Non-negative coefficients, d &lt; min_y
        bounds = ([0.0, 0.0, 0.0, 0.0], [np.inf, np.inf, np.inf, y_min - 1e-10])
        
        try:
            res = least_squares(
                residuals, p0, jac=jacobian, bounds=bounds,
                method=&#x27;trf&#x27;, loss=&#x27;soft_l1&#x27;, f_scale=0.1, ftol=1e-10
            )
            fitted_params.append(res.x)
        except Exception:
            fitted_params.append(p0)
            
    return np.array(fitted_params) if len(fitted_params) &gt; 1 else fitted_params[0]
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>