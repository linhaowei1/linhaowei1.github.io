{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.998807, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses using a domain-specific power-law mix\n    plus linear cross-domain coupling. Total params = 35:\n      - params[0:20]: off-diagonal coupling weights (5x5 minus diag)\n      - params[20:25]: own-domain weights\n      - params[25:30]: input exponents\n      - params[30:35]: per-domain biases\n    preds[n,j] = w[j]*(X[n,j]**e[j]) + sum_{i!=j} W[j,i]*X[n,i] + b[j]\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    N, F = X.shape\n    assert F == 5, \"Expected 5 mixture proportions\"\n    p = np.asarray(params, dtype=float).ravel()\n\n    # unpack off-diagonal weights into a 5x5 matrix with zeros on the diagonal\n    mask = np.eye(5, dtype=bool)\n    W_off = np.zeros((5, 5), dtype=float)\n    W_off[~mask] = p[:20]\n\n    # own-domain weights, exponents, and biases\n    w_own = p[20:25]    # length-5\n    e      = p[25:30]   # length-5\n    b      = p[30:35]   # length-5\n\n    # compute own-domain power-law contributions\n    X_pow = np.power(X, e)        # shape (N,5) \n    own   = X_pow * w_own         # broadcast multiply each column\n\n    # compute cross-domain linear contributions\n    cross = X.dot(W_off.T)        # shape (N,5)\n\n    # final prediction\n    return own + cross + b\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35 parameters to minimize MSE between predictions and true losses.\n    Uses multi-start L-BFGS-B with exponent bounds [0.1, 5.0].\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float)\n    if y.ndim == 1:\n        y = y[:, None]\n    N, F = X.shape\n    assert F == 5 and y.shape == (N, 5), \"Expected shapes (N,5)\"\n\n    P = 35\n    # default initialization\n    p0 = np.zeros(P, dtype=float)\n    # own-domain weights initialized negative (higher mix lowers loss)\n    p0[20:25] = -1.0\n    # exponents initialized to linear\n    p0[25:30] = 1.0\n    # biases initialized to mean per-domain loss\n    p0[30:35] = y.mean(axis=0)\n\n    # bounds: exponents in [0.1,5], others unbounded\n    bounds = [(None, None)] * 20 + [(None, None)] * 5 + [(0.1, 5.0)] * 5 + [(None, None)] * 5\n\n    # objective: MSE\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        return np.mean((pred - y) ** 2)\n\n    # multi-start optimization for robustness\n    best_p, best_val = p0.copy(), np.inf\n    for seed in (0, 1, 2):\n        if seed == 0:\n            init = p0\n        else:\n            rng = np.random.RandomState(seed)\n            init = p0 + rng.randn(P) * 0.1\n        res = minimize(objective, init,\n                       method='L-BFGS-B',\n                       bounds=bounds,\n                       options={'maxiter': 1000, 'ftol': 1e-9})\n        if res.success and res.fun < best_val:\n            best_val, best_p = res.fun, res.x\n\n    return best_p"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.998727, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nRefined per-domain scaling law with robust cross-domain modeling\nUses 35 parameters: 7 per domain with enhanced numerical stability and fitting\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Per-domain scaling law with comprehensive modeling.\n    Each of 5 domains uses 7 parameters (35 total):\n    - scale: power law coefficient\n    - exponent: power law shape (clipped for stability)\n    - bias: baseline loss offset\n    - quad: quadratic self-interaction\n    - cross1, cross2, cross3: top 3 cross-domain linear effects\n    \n    Model: loss_d = scale * prop_d^exp + quad * prop_d^2 + \n                    sum(cross_i * prop_other_i) + bias\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params).flatten()\n    \n    # Ensure exactly 35 parameters\n    if len(params) < 35:\n        params = np.pad(params, (0, 35 - len(params)), constant_values=0.0)\n    params = params[:35]\n    \n    # Numerical stability with safe clipping\n    X_safe = np.clip(X, 1e-9, 1.0)\n    \n    predictions = np.zeros((N, F))\n    \n    for d in range(F):\n        idx = d * 7\n        \n        # Extract domain parameters\n        scale = params[idx]\n        exponent = params[idx + 1]\n        bias = params[idx + 2]\n        quad = params[idx + 3]\n        cross_weights = params[idx + 4:idx + 7]\n        \n        # Clip exponent for stability\n        exponent = np.clip(exponent, 0.1, 2.3)\n        \n        # Power law: main self-effect\n        power_term = scale * (X_safe[:, d] ** exponent)\n        \n        # Quadratic: concentration effects\n        quad_term = quad * (X_safe[:, d] ** 2)\n        \n        # Cross-domain: systematic other domain selection\n        cross_term = 0.0\n        other_domains = [i for i in range(F) if i != d]\n        for w_idx in range(min(3, len(other_domains))):\n            cross_term += cross_weights[w_idx] * X_safe[:, other_domains[w_idx]]\n        \n        # Combine terms\n        predictions[:, d] = power_term + quad_term + cross_term + bias\n    \n    return predictions\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Robust fitting with enhanced initialization and progressive optimization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.atleast_2d(np.asarray(loss_values))\n    \n    N, F = X.shape\n    \n    # Shape correction\n    if y.shape[1] != F:\n        if y.shape[0] == F and y.shape[1] == N:\n            y = y.T\n        elif y.shape[1] == 1:\n            y = np.tile(y, (1, F))\n    \n    # Enhanced data-driven initialization\n    init = np.zeros(35)\n    \n    for d in range(F):\n        idx = d * 7\n        y_d = y[:, d]\n        x_d = X[:, d]\n        \n        # Robust statistics using percentiles\n        q10, q25, q50, q75, q90 = np.percentile(y_d, [10, 25, 50, 75, 90])\n        iqr = q75 - q25\n        \n        # Estimate exponent via log-space regression on filtered data\n        mask = (x_d > 0.01) & (y_d > q10)\n        if np.sum(mask) > 3:\n            log_x = np.log(x_d[mask] + 1e-7)\n            log_y = np.log(np.maximum(y_d[mask] - q10 + 0.05, 0.05))\n            \n            if np.std(log_x) > 1e-5:\n                # Robust slope estimation\n                slope = np.polyfit(log_x, log_y, 1)[0]\n                init[idx + 1] = np.clip(slope, 0.4, 1.1)\n            else:\n                init[idx + 1] = 0.65\n        else:\n            init[idx + 1] = 0.65\n        \n        # Scale: based on IQR\n        init[idx] = iqr * 0.5\n        \n        # Bias: lower percentile baseline with median contribution\n        init[idx + 2] = q10 * 0.8 + q50 * 0.2\n        \n        # Quadratic: small stabilizing term\n        init[idx + 3] = iqr * 0.02\n        \n        # Cross-domain: proportional to residual with variation\n        init[idx + 4:idx + 7] = iqr * 0.015 * (1.0 + 0.08 * np.random.randn(3))\n    \n    def objective(params):\n        try:\n            pred = scaling_law_func(X, params)\n            \n            # Adaptive domain weighting: inverse variance\n            domain_vars = np.var(y, axis=0)\n            weights = 1.0 / (domain_vars + 0.03)\n            weights = F * weights / np.sum(weights)\n            \n            mse = np.mean(((pred - y) ** 2) * weights[None, :])\n            \n            # Hierarchical regularization\n            # Lightest on main effects\n            r_scale = 0.00002 * np.sum(params[::7] ** 2)\n            r_bias = 0.00001 * np.sum(params[2::7] ** 2)\n            \n            # Moderate on shape parameters\n            r_exp = 0.00007 * np.sum((params[1::7] - 0.7) ** 2)\n            r_quad = 0.00009 * np.sum(params[3::7] ** 2)\n            \n            # Stronger on cross-domain to prevent overfitting\n            r_cross = 0.00028 * (np.sum(params[4::7] ** 2) + \n                                  np.sum(params[5::7] ** 2) + \n                                  np.sum(params[6::7] ** 2))\n            \n            return mse + r_scale + r_bias + r_exp + r_quad + r_cross\n        except:\n            return 1e10\n    \n    # Carefully tuned bounds\n    bounds = []\n    for _ in range(F):\n        bounds.extend([\n            (-9, 13),      # scale\n            (0.1, 2.3),    # exponent\n            (-7, 11),      # bias\n            (-3, 3),       # quad\n            (-4, 4),       # cross1\n            (-4, 4),       # cross2\n            (-4, 4)        # cross3\n        ])\n    \n    # Progressive multi-stage optimization\n    best_params = init\n    best_loss = objective(init)\n    \n    # Stage 1: Exploration with moderate tolerance\n    res1 = minimize(objective, init, method='L-BFGS-B', bounds=bounds,\n                    options={'maxiter': 1400, 'ftol': 1e-9, 'gtol': 1e-8})\n    \n    if res1.success and res1.fun < best_loss:\n        best_params = res1.x\n        best_loss = res1.fun\n    \n    # Stage 2: Refinement with tighter tolerance\n    res2 = minimize(objective, best_params, method='L-BFGS-B', bounds=bounds,\n                    options={'maxiter': 1200, 'ftol': 1e-11, 'gtol': 1e-9})\n    \n    if res2.success and res2.fun < best_loss:\n        best_params = res2.x\n        best_loss = res2.fun\n    \n    # Stage 3: Final polishing\n    res3 = minimize(objective, best_params, method='L-BFGS-B', bounds=bounds,\n                    options={'maxiter': 600, 'ftol': 1e-12})\n    \n    if res3.success and res3.fun < best_loss:\n        best_params = res3.x\n    \n    return best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.998312, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points, float))\n    N, F = X.shape\n    p = np.asarray(params, float).ravel()\n    L = p.size\n    Xc = np.clip(X, 1e-12, 1.0)\n    # Preferred: per-output alpha, layout L = T*(F+2) => [b(T), W(T*F), a(T)]\n    if L > 0 and L % (F + 2) == 0:\n        T = L // (F + 2)\n        b = p[:T]\n        W = p[T:T + T * F].reshape(T, F)\n        a = p[-T:]\n        Z = Xc[:, None, :] ** a[None, :, None]\n        return (Z * W[None]).sum(2) + b\n    # Shared alpha: L = (F+1)*T + 1\n    if L > 1 and (L - 1) % (F + 1) == 0:\n        T = (L - 1) // (F + 1)\n        b = p[:T]; W = p[T:T + T * F].reshape(T, F); a = float(p[-1])\n        return (Xc ** a) @ W.T + b\n    # Linear fallback: L = (F+1)*T\n    if L > 0 and L % (F + 1) == 0:\n        T = L // (F + 1)\n        B = p.reshape(T, F + 1)\n        ZA = np.concatenate([np.ones((N, 1)), X], 1)\n        return ZA @ B.T\n    T = min(5, L if L > 0 else 1)\n    return np.tile(p[:T][None, :], (N, 1))\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, float))\n    Y = np.asarray(loss_values, float)\n    if Y.ndim == 1: Y = Y[:, None]\n    N, F = X.shape; T = Y.shape[1]\n    Xc = np.clip(X, 1e-12, 1.0)\n    lam_b = 1e-8\n    lam_grid = np.array([1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4], float)\n    a_min, a_max = 0.2, 3.0\n    phi = (1 + 5 ** 0.5) / 2\n\n    def nmse_cols(y, yhat):\n        v = np.var(y, axis=0, ddof=0) + 1e-12\n        return np.mean(((yhat - y) ** 2) / v)\n\n    def solve_scaled(Z, Ymat, lam):\n        s = np.sqrt((Z ** 2).mean(0)) + 1e-12\n        Zs = Z / s\n        ZA = np.concatenate([np.ones((N, 1)), Zs], 1)\n        A = ZA.T @ ZA\n        A[0, 0] += lam_b\n        A[1:, 1:] += lam * np.eye(F)\n        try:\n            B = np.linalg.solve(A, ZA.T @ Ymat)\n        except np.linalg.LinAlgError:\n            B = np.linalg.lstsq(A + 1e-12 * np.eye(F + 1), ZA.T @ Ymat, rcond=None)[0]\n        b = B[0, :]\n        W = (B[1:, :].T) / s\n        return W, b\n\n    def best_ridge(Z, Ymat):\n        best, Wb = np.inf, None\n        for lam in lam_grid:\n            W, b = solve_scaled(Z, Ymat, lam)\n            val = nmse_cols(Ymat, Z @ W.T + b)\n            if val < best:\n                best, Wb = val, (W, b)\n        return best, Wb\n\n    def shared_obj(a):\n        Z = Xc ** float(a)\n        v, _ = best_ridge(Z, Y)\n        return v\n\n    # Shared alpha (multi-output) via coarse grid + golden section\n    grid = np.linspace(0.3, 2.5, 9)\n    a_shared = grid[np.argmin([shared_obj(a) for a in grid])]\n    Lb, Ub = max(a_min, a_shared - 0.7), min(a_max, a_shared + 0.7)\n    c = Ub - (Ub - Lb) / phi\n    d = Lb + (Ub - Lb) / phi\n    vc, vd = shared_obj(c), shared_obj(d)\n    for _ in range(22):\n        if vc < vd:\n            Ub, d, vd = d, c, vc\n            c = Ub - (Ub - Lb) / phi\n            vc = shared_obj(c)\n        else:\n            Lb, c, vc = c, d, vd\n            d = Lb + (Ub - Lb) / phi\n            vd = shared_obj(d)\n    a_shared = c if vc < vd else d\n\n    # Per-output refinement with coupling penalty to a_shared and lam selection\n    rho = 1e-3\n    bs, Ws, alphas = [], [], []\n\n    def fit_for_alpha(y, a):\n        Z = Xc ** float(a)\n        best, Wb = np.inf, None\n        for lam in lam_grid:\n            W, b = solve_scaled(Z, y, lam)\n            pred = Z @ W.T + b\n            v = float(np.var(y, ddof=0) + 1e-12)\n            nm = float(np.mean((pred - y) ** 2) / v)\n            if nm < best:\n                best, Wb = nm, (b[0], W[0])\n        return best, Wb\n\n    for t in range(T):\n        y = Y[:, t:t+1]\n        # coarse search\n        vals = [fit_for_alpha(y, a) for a in grid]\n        idx = np.argmin([nm + rho * (a - a_shared) ** 2 for (nm, _), a in zip(vals, grid)])\n        a0 = grid[idx]\n        # refine by golden section\n        Lb, Ub = max(a_min, a0 - 0.7), min(a_max, a0 + 0.7)\n        c = Ub - (Ub - Lb) / phi; d = Lb + (Ub - Lb) / phi\n        nc, Bc = fit_for_alpha(y, c); vc = nc + rho * (c - a_shared) ** 2\n        nd, Bd = fit_for_alpha(y, d); vd = nd + rho * (d - a_shared) ** 2\n        for _ in range(22):\n            if vc < vd:\n                Ub, d, vd, Bd = d, c, vc, Bc\n                c = Ub - (Ub - Lb) / phi\n                nc, Bc = fit_for_alpha(y, c); vc = nc + rho * (c - a_shared) ** 2\n            else:\n                Lb, c, vc, Bc = c, d, vd, Bd\n                d = Lb + (Ub - Lb) / phi\n                nd, Bd = fit_for_alpha(y, d); vd = nd + rho * (d - a_shared) ** 2\n        a_opt, B_opt = (c, Bc) if vc < vd else (d, Bd)\n        alphas.append(a_opt); bs.append(B_opt[0]); Ws.append(B_opt[1])\n\n    b = np.asarray(bs)\n    W = np.vstack(Ws)\n    a = np.asarray(alphas)\n    return np.concatenate([b, W.ravel(), a])\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.998312, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nThis evolved program introduces a new functional form for the scaling law and\nenhances the optimization strategy to improve accuracy and robustness.\n\nKey Improvements:\n1.  **Scaling Law Functional Form:** The `scaling_law_func` now models each output\n    domain's loss (L_j) based on the sum of power-law contributions from all input\n    domain proportions (x_k), where the exponent (e_j) is specific to the *output*\n    domain j, rather than specific to the input domain k.\n    New form: `L_j = b_j + sum_{k=1 to F} (c_jk * x_k^e_j)`\n    -   `b_j`: Bias for output domain j.\n    -   `c_jk`: Coefficient for influence of input domain k on output domain j.\n    -   `e_j`: Exponent for output domain j, applied to all input proportions affecting L_j.\n    This maintains the 35-parameter limit (F biases + F*F coefficients + F exponents = 5 + 25 + 5 = 35).\n    This structure allows each output domain to exhibit its own scaling behavior with respect\n    to the mixture proportions, which might better capture cross-domain generalization effects.\n\n2.  **Optimization Robustness:** The `fit_scaling_law` function now employs:\n    -   **Multiple Random Initializations:** In addition to a deterministic initial guess,\n        it performs `N_TRIALS` (increased to 30) optimizations from randomly generated\n        initial parameters within their defined bounds. This helps to explore the\n        non-convex parameter space more thoroughly and mitigate getting stuck in local minima.\n    -   **Refined Exponent Initialization Range:** For the random initializations,\n        the exponents are initialized within a narrower, more common range (0.0 to 2.0)\n        to guide the search towards plausible scaling behaviors, while the broader\n        L-BFGS-B bounds (0.0 to 5.0) still allow the optimizer to explore further if beneficial.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions using a generalized power law.\n\n    The model for each domain's loss L_j is:\n    L_j = b_j + sum_{k=1 to F} (c_jk * x_k^e_j)\n\n    Where:\n    - F is the number of domains (5).\n    - x_k is the proportion of domain k.\n    - b_j is the bias term for output domain j.\n    - c_jk is the coefficient representing the influence of input proportion x_k on output loss L_j.\n    - e_j is the exponent for output domain j, shared across all input proportions affecting L_j.\n\n    Total parameters: F (biases) + F*F (coefficients) + F (exponents) = 5 + 25 + 5 = 35.\n\n    Args:\n        data_points (np.ndarray): (N, F) array with domain proportions for F domains.\n        params (np.ndarray): 1D array of 35 parameters.\n\n    Returns:\n        np.ndarray: Predicted multi-domain loss values (N, F).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    N, F = X.shape # F = 5 (number of domains)\n\n    # Unpack parameters: [b_1..b_F, c_11..c_FF, e_1..e_F]\n    # The parameters are ordered as: F biases, F*F coefficients, F exponents\n    biases = params[:F] # (F,) - bias for each output domain L_j\n    coeffs_flat = params[F : F + F*F] # (F*F,) - flattened coefficients\n    coeffs = coeffs_flat.reshape(F, F) # (F, F) - c_jk where c_jk is coeffs[j, k]\n                                       # coeffs[j, k] means the influence of input domain k on output domain j\n    # In this model, exponents are specific to the output domain j, so e_j\n    exponents = params[F + F*F : F + F*F + F] # (F,) - exponent for each output domain L_j\n\n    predicted_losses = np.zeros_like(X, dtype=float) # (N, F)\n\n    # Calculate predictions for each output domain L_j\n    for j in range(F):\n        # Get the exponent specific to this output domain j\n        ej = exponents[j]\n\n        # Calculate X_k^ej for all input domains k and all data points N\n        # np.where handles 0^e = 0, which is desired for proportions where 0 means absence.\n        # The bounds ensure exponents are non-negative, so 0^0 = 1 behavior is avoided for X=0.\n        X_powered_by_ej = np.where(X > 0, np.power(X, ej), 0.0) # (N, F)\n\n        # Multiply by coefficients c_jk for this specific output domain j\n        # coeffs[j, :] gives the (F,) array of coefficients [c_j1, c_j2, ..., c_jF]\n        # [None, :] broadcasts it to (1, F) for element-wise multiplication with (N, F)\n        contributions_for_j = X_powered_by_ej * coeffs[j, :][None, :] # (N, F)\n\n        # Sum contributions from all input domains k for each data point\n        sum_contributions_for_j = np.sum(contributions_for_j, axis=1) # (N,)\n\n        # Add bias for this output domain j\n        predicted_losses[:, j] = biases[j] + sum_contributions_for_j\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes the parameters for the scaling_law_func using L-BFGS-B with multiple initializations.\n\n    Args:\n        data_points (np.ndarray): (N, F) array with domain proportions for F domains.\n        loss_values (np.ndarray): Corresponding multi-domain losses (N, F).\n\n    Returns:\n        np.ndarray: Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values) # (N, F)\n    N, F = X.shape # F = 5 (number of domains)\n\n    # Number of parameters for the multi-output model: F biases + F*F coefficients + F exponents\n    num_params = F + F*F + F # 5 + 25 + 5 = 35\n\n    # Bounds for parameters to ensure numerical stability and reasonable values\n    # Biases: Loss values are typically positive. Range [0, 5] covers 1.8-4.2 well.\n    bias_bounds = [(0.0, 5.0)] * F\n    # Coefficients: Can be positive or negative, allowing for various interaction types. Range [-10, 10].\n    coeff_bounds = [(-10.0, 10.0)] * (F*F)\n    # Exponents: Must be non-negative. Common scaling law exponents are often between 0 and 2.\n    # Allowing up to 5 provides flexibility for the optimizer, but initial random search will be tighter.\n    exponent_bounds_optimizer = [(0.0, 5.0)] * F\n\n    # Combined bounds for the L-BFGS-B optimizer\n    bounds = bias_bounds + coeff_bounds + exponent_bounds_optimizer\n\n    def objective(params_flat):\n        \"\"\"Calculates the Mean Squared Error between predictions and actual loss values.\"\"\"\n        pred = scaling_law_func(X, params_flat) # (N, F)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    best_mse = np.inf\n    best_params = None\n\n    # --- Deterministic initial guess ---\n    # Biases: Mean loss for each domain is a good starting point.\n    init_biases_det = np.mean(y, axis=0) # (F,)\n    # Coefficients: Initialize to zeros. The optimizer will find the interactions.\n    init_coeffs_det = np.zeros((F, F)) # (F, F)\n    # Exponents: Initialize to 1.0 (linear scaling) for all output domains.\n    init_exponents_det = np.ones(F) * 1.0 # (F,)\n\n    # Combine initial parameters into a single 1D array\n    initial_params_det = np.concatenate([init_biases_det, init_coeffs_det.ravel(), init_exponents_det]) # (num_params,)\n\n    # 1. Run optimization with the deterministic initial guess\n    result_det = minimize(objective, initial_params_det, method='L-BFGS-B', bounds=bounds)\n    if result_det.success and result_det.fun < best_mse:\n        best_mse = result_det.fun\n        best_params = result_det.x\n\n    # --- Multiple Random Initializations ---\n    N_TRIALS = 30 # Number of random initializations to try for better global optimum search\n\n    # Tighter random initialization bounds for exponents (more typical range)\n    exponent_random_init_range = (0.0, 2.0)\n\n    for _ in range(N_TRIALS):\n        # Generate random initial parameters within their respective bounds\n        random_biases = np.random.uniform(bias_bounds[0][0], bias_bounds[0][1], F)\n        random_coeffs = np.random.uniform(coeff_bounds[0][0], coeff_bounds[0][1], F*F)\n        random_exponents = np.random.uniform(exponent_random_init_range[0], exponent_random_init_range[1], F)\n        \n        random_initial_params = np.concatenate([random_biases, random_coeffs, random_exponents])\n\n        result_rand = minimize(objective, random_initial_params, method='L-BFGS-B', bounds=bounds)\n        if result_rand.success and result_rand.fun < best_mse:\n            best_mse = result_rand.fun\n            best_params = result_rand.x\n            \n    # Fallback: if no successful optimization was found, return the deterministic initial guess.\n    # This scenario should be rare with good bounds and initial guesses.\n    if best_params is None:\n        return initial_params_det\n\n    return best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.998086, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law for multi-domain LLM finetuning\nOptimized with smart initialization, adaptive regularization, and refined optimization strategy\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Multi-domain loss prediction using power law basis\n    \n    Model: loss[d] = sum_f(c[d,f] * x[f]^a[f]) + b[d]\n    \n    params layout (35 total):\n    [0-4]: shared exponents for input features\n    [5-9]: domain-specific bias terms\n    [10-34]: coefficients (5×5 matrix, one row per output domain)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    params = np.asarray(params, dtype=np.float64).ravel()\n    \n    # Parse parameters with numerical stability\n    shared_exp = np.clip(params[:5], -2.0, 2.0)\n    domain_bias = params[5:10]\n    coeffs = params[10:35].reshape(5, 5)\n    \n    # Safe input preparation for power operations\n    X_safe = np.clip(X, 1e-8, 1.0)\n    \n    # Vectorized computation\n    X_powered = X_safe ** shared_exp[None, :]  # (N, 5)\n    pred = X_powered @ coeffs.T + domain_bias[None, :]  # (N, 5)\n    \n    # Clip to valid loss range\n    return np.clip(pred, 1.0, 5.0)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit scaling law with advanced initialization and adaptive regularization\n    \n    Key improvements:\n    1. Per-domain least-squares coefficient initialization\n    2. Adaptive exponent initialization based on domain variance\n    3. Centered/scaled features for better initialization numerics\n    4. Adaptive regularization scaled by data statistics\n    5. Two-stage optimization with progressive refinement\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N, F = X.shape\n    D = y.shape[1]\n    \n    # Compute data statistics for adaptive initialization\n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0)\n    \n    # Initialize parameters\n    init_params = np.zeros(35)\n    \n    # Adaptive exponent initialization based on domain variance\n    y_var_normalized = y_std / (np.max(y_std) + 1e-8)\n    init_params[0:5] = 0.3 + 0.3 * y_var_normalized  # Range [0.3, 0.6]\n    \n    # Initialize biases from per-domain means\n    init_params[5:10] = y_mean[:5]\n    \n    # Improved coefficient initialization using centered/scaled features\n    X_safe = np.clip(X, 1e-8, 1.0)\n    X_centered = X_safe - np.mean(X_safe, axis=0, keepdims=True)\n    X_std = np.std(X_safe, axis=0, keepdims=True) + 1e-8\n    X_scaled = X_centered / X_std\n    \n    # Fit coefficients per domain using scaled features\n    for d in range(D):\n        try:\n            y_d = y[:, d] - np.mean(y[:, d])\n            # Solve least-squares with scaled features for better numerics\n            c_d = np.linalg.lstsq(X_scaled, y_d, rcond=None)[0]\n            # Normalize to prevent extreme initialization\n            c_d_norm = np.linalg.norm(c_d) + 1e-8\n            c_d_normalized = c_d / c_d_norm\n            init_params[10 + d*5:10 + (d+1)*5] = np.clip(c_d_normalized, -1.0, 1.0)\n        except:\n            init_params[10 + d*5:10 + (d+1)*5] = 0.02\n    \n    # Objective function with adaptive regularization\n    def objective(flat_params):\n        try:\n            pred = scaling_law_func(X, flat_params)\n            \n            if pred.shape != y.shape:\n                return 1e10\n            \n            # Main MSE loss\n            mse = np.mean((pred - y) ** 2)\n            \n            # Adaptive coefficient regularization\n            coeffs = flat_params[10:35]\n            coeff_reg = 0.0006 * np.mean(coeffs ** 2)\n            \n            # Exponent regularization: keep close to initialized adaptive values\n            exp_deviation = flat_params[0:5] - init_params[0:5]\n            exp_reg = 0.00008 * np.sum(exp_deviation ** 2)\n            \n            # Bias regularization: keep biases anchored to data mean\n            bias_deviation = flat_params[5:10] - y_mean[:5]\n            bias_reg = 0.00005 * np.sum(bias_deviation ** 2)\n            \n            return mse + coeff_reg + exp_reg + bias_reg\n        except:\n            return 1e10\n    \n    # Adaptive bounds based on data statistics\n    loss_min, loss_max = np.min(y), np.max(y)\n    \n    bounds = [\n        *[(-2.0, 2.0)] * 5,                          # exponents\n        *[(loss_min - 0.3, loss_max + 0.3)] * 5,     # biases\n        *[(-1.5, 1.5)] * 25                           # coefficients\n    ]\n    \n    # Primary optimization with balanced settings\n    result = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={\n            'maxiter': 1500,\n            'ftol': 1e-9,\n            'gtol': 1e-8,\n            'maxcor': 15\n        }\n    )\n    \n    # Secondary fine-tuning for improved convergence\n    if result.success:\n        result2 = minimize(\n            objective,\n            result.x,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={\n                'maxiter': 500,\n                'ftol': 1e-10,\n                'gtol': 1e-9,\n                'maxcor': 20\n            }\n        )\n        # Use better result\n        if result2.fun < result.fun:\n            return result2.x.ravel()\n        return result.x.ravel()\n    else:\n        return init_params.ravel()\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.996946, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nImproved with a \"Non-linear Signed Rank-1 Transfer\" Model (35 parameters).\nThis model generalizes the successful Rank-1 approach by allowing the\ntransfer impact to scale non-linearly with donor data size (tau parameter),\nwhile maintaining signed transfer (interference) and pre-training offsets.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Non-linear Signed Rank-1 Transfer Scaling Law.\n    \n    Model:\n    Loss_i = A_i * (D_total_i)^(-alpha_i) + E_i\n    D_total_i = D_pre_i + x_i + beta_i * sum_{j != i} (gamma_j * x_j^(tau_j))\n    \n    Parameters (35):\n    0-4:   log_A (Amplitude)\n    5-9:   log_alpha (Scaling exponent)\n    10-14: log_E (Irreducible loss)\n    15-19: logit_beta (Receptivity)\n    20-24: raw_gamma (Transferability strength, signed)\n    25-29: log_D_pre (Pre-training offset)\n    30-34: log_tau (Transfer non-linearity)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).flatten()\n    \n    # Parameter Extraction\n    log_A = params[0:5]\n    log_alpha = params[5:10]\n    log_E = params[10:15]\n    logit_beta = params[15:20]\n    raw_gamma = params[20:25]\n    log_D_pre = params[25:30]\n    log_tau = params[30:35]\n    \n    # Transformation to physical space\n    A = np.exp(log_A)\n    alpha = np.exp(log_alpha)\n    E = np.exp(log_E)\n    beta = 1.0 / (1.0 + np.exp(-logit_beta))\n    gamma = np.tanh(raw_gamma)\n    D_pre = np.exp(log_D_pre)\n    tau = np.exp(log_tau)\n    \n    # Compute transformed inputs for transfer\n    # Use small epsilon for numerical stability of gradients (log(0))\n    # X_trans_j = x_j ^ tau_j\n    X_safe = X + 1e-9\n    X_trans = X_safe ** tau[None, :]\n    \n    # Weighted transfer components\n    # Gamma_X: (N, 5)\n    Gamma_X = X_trans * gamma[None, :]\n    \n    # Sum of all transfer sources (N, 1)\n    Sum_Gamma_X = np.sum(Gamma_X, axis=1, keepdims=True)\n    \n    # Transfer to i from others: Sum - Self\n    # Subtract own contribution: gamma_i * x_i^tau_i\n    Transfer_Input = Sum_Gamma_X - Gamma_X\n    \n    # Apply receptivity\n    # beta is (5,) -> (1, 5)\n    Transfer_Term = beta[None, :] * Transfer_Input\n    \n    # Total Effective Data\n    # D_total = D_pre + x_i + Transfer\n    # Note: Target domain data x_i is treated linearly (standard scaling)\n    D_total = D_pre[None, :] + X + Transfer_Term\n    \n    # Numerical stability\n    D_total = np.maximum(D_total, 1e-8)\n    \n    # Predictions\n    pred = A[None, :] * (D_total ** -alpha[None, :]) + E[None, :]\n    \n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    n_domains = 5\n    \n    # --- Stage 1: Robust Independent Fit ---\n    # Fit A, alpha, E, D_pre assuming no transfer.\n    # This establishes the baseline scaling curve for each domain.\n    \n    init_params_list = []\n    \n    for i in range(n_domains):\n        x_i = X[:, i]\n        y_i = Y[:, i]\n        y_min, y_max = np.min(y_i), np.max(y_i)\n        \n        # Grid search for initialization to avoid local minima\n        best_mse = float('inf')\n        best_p = None\n        \n        # Heuristic grid\n        # alpha: slope, d_guess: offset\n        for d_guess in [1e-4, 1e-2, 0.1]:\n            for alpha_guess in [0.3, 0.7, 1.5]:\n                # Estimate A based on endpoints\n                # y_range ~ A * (d^-alpha - (1+d)^-alpha)\n                denom = (d_guess**-alpha_guess) - ((1.0+d_guess)**-alpha_guess)\n                if denom < 1e-6: denom = 1e-6\n                A_guess = max((y_max - y_min) / denom, 1e-4)\n                \n                p0_local = [\n                    np.log(A_guess),\n                    np.log(alpha_guess),\n                    np.log(max(y_min - 0.05, 1e-5)),\n                    np.log(d_guess)\n                ]\n                \n                def obj_local(p):\n                    A = np.exp(p[0])\n                    al = np.exp(p[1])\n                    E = np.exp(p[2])\n                    d = np.exp(p[3])\n                    pred = A * ((x_i + d)**-al) + E\n                    return np.mean((pred - y_i)**2)\n                \n                try:\n                    # L-BFGS-B with bounds\n                    # alpha in [0.05, 7], E < y_min, D_pre in [1e-6, 1]\n                    bounds = [(None, None), (-3, 2), (None, np.log(y_min)), (-14, 0)]\n                    res = minimize(obj_local, p0_local, method='L-BFGS-B', bounds=bounds)\n                    if res.fun < best_mse:\n                        best_mse = res.fun\n                        best_p = res.x\n                except:\n                    pass\n        \n        if best_p is None:\n            # Fallback\n            best_p = np.array([0.0, -0.5, np.log(max(y_min-0.1, 1e-5)), -4.6])\n            \n        init_params_list.append(best_p)\n        \n    init_params_list = np.array(init_params_list)\n    \n    # --- Stage 2: Joint Optimization ---\n    # Initialize full model parameters.\n    # Start with tau = 1.0 (log_tau = 0) -> Linear transfer initially.\n    # Start with beta small, gamma neutral.\n    \n    p0_joint = np.concatenate([\n        init_params_list[:, 0], # log_A\n        init_params_list[:, 1], # log_alpha\n        init_params_list[:, 2], # log_E\n        np.full(5, -3.0),       # logit_beta (sigmoid(-3) ~ 0.05)\n        np.full(5, 0.0),        # raw_gamma (tanh(0) = 0)\n        init_params_list[:, 3], # log_D_pre\n        np.full(5, 0.0)         # log_tau (tau = 1)\n    ])\n    \n    def obj_joint(p):\n        preds = scaling_law_func(X, p)\n        mse = np.mean((preds - Y)**2)\n        \n        # Regularization\n        \n        # 1. Weak L2 on all params to prevent drift\n        reg_l2 = 1e-7 * np.sum(p**2)\n        \n        # 2. Sparsity on gamma (prefer simpler transfer models)\n        # indices 20-25 are raw_gamma\n        reg_gamma = 1e-5 * np.sum(p[20:25]**2)\n        \n        # 3. Regularize tau towards 1.0 (log_tau towards 0)\n        # indices 30-35 are log_tau\n        # We prefer linear transfer unless data strongly suggests otherwise\n        reg_tau = 1e-4 * np.sum(p[30:35]**2)\n        \n        # 4. Stability for D_pre (indices 25-30)\n        # Penalize if log_d < -12 (d < 6e-6) to avoid numerical instability\n        log_d = p[25:30]\n        pen_d = 1e-3 * np.sum(np.maximum(0, -12.0 - log_d)**2)\n        \n        return mse + reg_l2 + reg_gamma + reg_tau + pen_d\n        \n    try:\n        # BFGS is efficient for 35 parameters\n        res = minimize(obj_joint, p0_joint, method='BFGS', \n                      options={'maxiter': 2000, 'gtol': 1e-6})\n        best_p = res.x\n    except:\n        best_p = p0_joint\n        \n    return best_p\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.994208, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with adaptive power basis and hierarchical interactions.\nUses 35 parameters: power exponents (5), linear weights (5), quadratic weights (5),\npairwise interactions (10), cubic terms (5), global scale (3), bias (2).\nFocuses on numerical stability and cross-domain generalization.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    N, F = X.shape\n    params = np.asarray(params)\n    \n    if params.ndim == 1:\n        params = params[None, :]\n    T, P = params.shape\n    \n    # Parameter allocation (35 total)\n    alpha = params[:, :F]           # (T, F) - power law exponents\n    w_linear = params[:, F:2*F]     # (T, F) - linear weights\n    w_quad = params[:, 2*F:3*F]     # (T, F) - quadratic weights\n    w_cross = params[:, 3*F:3*F+10] # (T, 10) - pairwise interactions\n    w_cubic = params[:, 3*F+10:3*F+15] # (T, 5) - cubic terms all domains\n    scale_main = params[:, -5]      # (T,) - main output scale\n    scale_quad = params[:, -4]      # (T,) - quadratic feature scale\n    scale_cubic = params[:, -3]     # (T,) - cubic feature scale\n    bias = params[:, -2]            # (T,) - constant bias\n    bias_mix = params[:, -1]        # (T,) - mixture-dependent bias\n    \n    X_safe = np.clip(X, 1e-10, 1.0)\n    \n    # Adaptive power law features\n    power_features = X_safe[:, None, :] ** alpha[None, :, :]\n    pred = (w_linear[None, :, :] * power_features).sum(axis=2)\n    \n    # Quadratic self-interactions with dedicated scaling\n    quad_features = X_safe ** 2\n    pred += scale_quad[None, :] * (w_quad[None, :, :] * quad_features[:, None, :]).sum(axis=2)\n    \n    # Pairwise cross-domain interactions\n    cross_terms = np.zeros((N, 10))\n    idx = 0\n    for i in range(F):\n        for j in range(i+1, F):\n            cross_terms[:, idx] = X_safe[:, i] * X_safe[:, j]\n            idx += 1\n    pred += (w_cross[None, :, :] * cross_terms[:, None, :]).sum(axis=2)\n    \n    # Cubic terms for all domains with dedicated scaling\n    cubic_features = X_safe ** 3\n    pred += scale_cubic[None, :] * (w_cubic[None, :, :] * cubic_features[:, None, :]).sum(axis=2)\n    \n    # Global scaling and adaptive bias\n    X_entropy = -np.sum(X_safe * np.log(X_safe + 1e-10), axis=1, keepdims=True)\n    pred = scale_main[None, :] * pred + bias[None, :] + bias_mix[None, :] * X_entropy\n    \n    return pred[:, 0] if pred.shape[1] == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    P = 35  # All 35 parameters\n    \n    y2d = y[:, None] if y.ndim == 1 else y\n    T = y2d.shape[1]\n    \n    np.random.seed(42)\n    y_mean = np.mean(y2d, axis=0)\n    y_std = np.std(y2d, axis=0) + 1e-8\n    X_mean = np.mean(X, axis=0) + 1e-8\n    \n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        try:\n            pred = scaling_law_func(X, params)\n            mse = np.mean((pred - y2d) ** 2)\n            \n            # Hierarchical regularization\n            alpha_reg = 1e-5 * np.sum((params[:, :F] - 0.9) ** 2)  # Slightly sub-linear\n            linear_reg = 1e-6 * np.sum(params[:, F:2*F] ** 2)\n            quad_reg = 1e-6 * np.sum(params[:, 2*F:3*F] ** 2)\n            cross_reg = 1e-6 * np.sum(params[:, 3*F:3*F+10] ** 2)\n            cubic_reg = 1e-5 * np.sum(params[:, 3*F+10:3*F+15] ** 2)  # Higher penalty\n            scale_reg = 1e-5 * np.sum((params[:, -5:-3] - 1.0) ** 2)\n            bias_reg = 1e-7 * np.sum(params[:, -2:] ** 2)\n            \n            return mse + alpha_reg + linear_reg + quad_reg + cross_reg + cubic_reg + scale_reg + bias_reg\n        except:\n            return 1e10\n    \n    best_params = None\n    best_loss = float('inf')\n    \n    # Strategy 1: Data-informed initialization\n    init1 = np.random.randn(T, P) * 0.015\n    init1[:, :F] = 0.85 + np.random.randn(T, F) * 0.12\n    init1[:, F:2*F] = (y_mean / X_mean)[:, None].T * 0.4 + np.random.randn(T, F) * 0.08\n    init1[:, 2*F:3*F] = np.random.randn(T, F) * 0.04\n    init1[:, 3*F:3*F+10] = np.random.randn(T, 10) * 0.03\n    init1[:, 3*F+10:3*F+15] = np.random.randn(T, F) * 0.02\n    init1[:, -5] = 1.0\n    init1[:, -4] = 0.3\n    init1[:, -3] = 0.1\n    init1[:, -2] = y_mean\n    init1[:, -1] = 0.0\n    \n    # Strategy 2: Conservative near-linear\n    init2 = np.zeros((T, P))\n    init2[:, :F] = 1.0\n    init2[:, F:2*F] = (y_mean / X_mean)[:, None].T * 0.6\n    init2[:, 2*F:3*F] = np.random.randn(T, F) * 0.05\n    init2[:, -5] = 1.0\n    init2[:, -4] = 0.2\n    init2[:, -3] = 0.05\n    init2[:, -2] = y_mean\n    init2[:, -1] = 0.0\n    \n    # Strategy 3: Moderate exploration\n    init3 = np.random.randn(T, P) * 0.06\n    init3[:, :F] = 0.7 + np.random.randn(T, F) * 0.25\n    init3[:, F:2*F] = np.random.randn(T, F) * 0.35\n    init3[:, 2*F:3*F] = np.random.randn(T, F) * 0.12\n    init3[:, 3*F:3*F+10] = np.random.randn(T, 10) * 0.08\n    init3[:, 3*F+10:3*F+15] = np.random.randn(T, F) * 0.05\n    init3[:, -5] = 0.9 + np.random.randn(T) * 0.15\n    init3[:, -4] = 0.4 + np.random.randn(T) * 0.1\n    init3[:, -3] = 0.15 + np.random.randn(T) * 0.05\n    init3[:, -2] = y_mean + np.random.randn(T) * 0.1\n    init3[:, -1] = np.random.randn(T) * 0.05\n    \n    # Strategy 4: Wide exploration\n    init4 = np.random.randn(T, P) * 0.1\n    init4[:, :F] = 0.5 + np.random.randn(T, F) * 0.35\n    init4[:, F:2*F] = np.random.randn(T, F) * 0.5\n    init4[:, -5] = 1.0\n    init4[:, -4] = 0.5\n    init4[:, -3] = 0.2\n    init4[:, -2] = y_mean\n    \n    for init in [init1, init2, init3, init4]:\n        try:\n            result = minimize(\n                objective,\n                init.ravel(),\n                method='L-BFGS-B',\n                options={'maxiter': 1800, 'ftol': 1e-11, 'gtol': 1e-9}\n            )\n            \n            if result.fun < best_loss:\n                best_loss = result.fun\n                best_params = result.x.reshape(T, P)\n        except:\n            continue\n    \n    # Global search if needed\n    if best_params is None or best_loss > 0.04:\n        try:\n            bounds = [(-3, 3)] * (T * P)\n            result = differential_evolution(\n                objective,\n                bounds,\n                maxiter=180,\n                seed=42,\n                atol=1e-8,\n                tol=1e-8,\n                popsize=15\n            )\n            if result.fun < best_loss:\n                best_params = result.x.reshape(T, P)\n                best_loss = result.fun\n        except:\n            pass\n    \n    # Final polish\n    if best_params is not None:\n        try:\n            result = minimize(\n                objective,\n                best_params.ravel(),\n                method='L-BFGS-B',\n                options={'maxiter': 600, 'ftol': 1e-12, 'gtol': 1e-10}\n            )\n            if result.fun < best_loss:\n                best_params = result.x.reshape(T, P)\n        except:\n            pass\n    \n    if best_params is None:\n        best_params = init1\n    \n    return best_params[0] if T == 1 else best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.9918, "solution": "import numpy as np\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict 5-domain losses from mixture proportions using 7 features per domain\n    (35 parameters total):\n      1) bias\n      2) log(p_j + ε)\n      3) p_j\n      4) sqrt(p_j + ε)\n      5) p_j * (1 - p_j)\n      6) mixture entropy H\n      7) H^2\n    This replaces the p_j^2 feature with sqrt(p_j) for better mid‐range modeling\n    and retains entropy‐based cross‐domain coupling.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    N, D = X.shape\n    assert D == 5, \"Expected input shape (N,5)\"\n    \n    p = np.asarray(params, dtype=float).ravel()\n    assert p.size == 35, \"Parameter vector must have length 35\"\n    W = p.reshape(5, 7)  # weights: one row per domain, one column per feature\n    \n    eps = 1e-8\n    # feature precomputations\n    logX   = np.log(X + eps)                             # (N,5)\n    H      = -np.sum(X * logX, axis=1, keepdims=True)    # (N,1) entropy\n    H2     = H * H                                       # (N,1)\n    sqrtX  = np.sqrt(X + eps)                            # (N,5)\n    crossX = X * (1.0 - X)                                # (N,5) p_j*(1-p_j)\n    \n    # build predictions\n    preds = np.zeros((N, 5), dtype=float)\n    for j in range(5):\n        # slice out domain‐specific features\n        pj_log   = logX[:,   j:j+1]  # (N,1)\n        pj       = X[:,      j:j+1]  # (N,1)\n        pj_sqrt  = sqrtX[:,   j:j+1]  # (N,1)\n        pj_cross = crossX[:,    j:j+1]  # (N,1)\n        \n        # assemble feature matrix F_j: (N,7)\n        F = np.hstack([\n            np.ones((N, 1)),    # bias\n            pj_log,             # log p_j\n            pj,                 # p_j\n            pj_sqrt,            # sqrt(p_j)\n            pj_cross,           # p_j*(1-p_j)\n            H,                  # entropy\n            H2                  # entropy^2\n        ])\n        preds[:, j] = F.dot(W[j])\n    \n    return preds\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35 parameters by per‐domain ridge regression in closed form.\n    Uses the same feature set as in scaling_law_func.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float)\n    N, D = X.shape\n    assert D == 5, \"Expected input shape (N,5)\"\n    \n    # ensure y is (N,5)\n    if y.ndim == 1:\n        y = y[:, None]\n    assert y.shape == (N, 5), \"Expected loss_values shape (N,5)\"\n    \n    eps = 1e-8\n    # regularization (reduced slightly for tighter fit)\n    reg = 1e-4\n    \n    # precompute shared features\n    logX   = np.log(X + eps)                             # (N,5)\n    H      = -np.sum(X * logX, axis=1, keepdims=True)    # (N,1)\n    H2     = H * H                                       # (N,1)\n    sqrtX  = np.sqrt(X + eps)                            # (N,5)\n    crossX = X * (1.0 - X)                                # (N,5)\n    \n    # allocate weights\n    W = np.zeros((5, 7), dtype=float)\n    I7 = np.eye(7, dtype=float)\n    \n    # solve per domain\n    for j in range(5):\n        pj_log   = logX[:,   j:j+1]  # (N,1)\n        pj       = X[:,      j:j+1]  # (N,1)\n        pj_sqrt  = sqrtX[:,   j:j+1]  # (N,1)\n        pj_cross = crossX[:,    j:j+1]  # (N,1)\n        \n        F = np.hstack([\n            np.ones((N, 1)),    # bias\n            pj_log,             # log p_j\n            pj,                 # p_j\n            pj_sqrt,            # sqrt(p_j)\n            pj_cross,           # p_j*(1-p_j)\n            H,                  # entropy\n            H2                  # entropy^2\n        ])  # (N,7)\n        \n        # ridge regression: (F^T F + reg I) w = F^T y\n        A = F.T.dot(F) + reg * I7\n        b = F.T.dot(y[:, j])\n        W[j] = np.linalg.solve(A, b)\n    \n    return W.ravel()  # length 35"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.991126, "solution": "import numpy as n\n\ndef scaling_law_func(X,p):\n p=p.ravel();e=n.log(X+1e-6);d=n.log(1-X+1e-6);h=-n.sum(X*e,1);m=len(X)\n O=n.empty((m,5))\n for j in range(5):\n  l=e[:,j]\n  F=n.c_[n.ones(m),l,d[:,j],h,l**2,h**2]\n  O[:,j]=F.dot(p[j*6:(j+1)*6])\n return O\n\ndef fit_scaling_law(X,Y):\n e=n.log(X+1e-6);d=n.log(1-X+1e-6);h=-n.sum(X*e,1);m=len(X);P=n.zeros(30)\n for j in range(5):\n  l=e[:,j]\n  F=n.c_[n.ones(m),l,d[:,j],h,l**2,h**2]\n  P[j*6:(j+1)*6]=n.linalg.solve(F.T@F+1e-6*n.eye(6),F.T@Y[:,j])\n return P"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.99046, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nThis version refines the scaling law function and optimization to\naccurately model multi-domain loss with a strict parameter limit and\nmulti-output predictions. It aims for mathematical accuracy across\ndifferent model sizes by fitting a unified parameter set to all data.\n\nThe model structure `L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)`\nis designed for expressiveness within the 35-parameter budget. It allows\nfor non-linear contributions from each input domain proportion (X_i^e_i),\ncross-domain influences (c_ji), and baseline losses (Bias_j).\n\nThe `fit_scaling_law` optimization algorithm employs a robust strategy:\n1.  **Informed Initialization**:\n    -   `init_biases` are set to the average observed loss for each output domain, providing a strong baseline.\n    -   `init_exponents` start at 1.0, treating the initial state as a linear combination for the LSTSQ step.\n    -   `init_coeffs` are determined using a Least Squares (lstsq) solution. This provides an analytically\n        derived, optimal starting point for coefficients given the initial exponents and biases, which is\n        crucial for rapid convergence and reaching better minima in the subsequent non-linear optimization.\n2.  **Perturbation**: A small random perturbation is added to the informed initial parameters. This helps\n    the optimizer escape shallow local minima around the initial guess and explore the immediate vicinity\n    for a potentially better starting point for the non-linear search, without deviating too far from the\n    analytically derived initial values.\n3.  **Bounded Optimization**: The L-BFGS-B algorithm is used, which is efficient and handles parameter bounds\n    effectively. Robust bounds are applied to exponents (non-negative for stability), coefficients (broad range\n    for diverse influences), and biases (within realistic loss ranges) to ensure numerical stability and\n    guide the search to meaningful solutions, preventing unphysical parameter values.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions.\n\n    The model form is: L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)\n    where:\n    - L_j is the predicted loss for output domain j.\n    - X_i is the proportion of input domain i.\n    - e_i are shared exponents for each input proportion (X_i).\n    - c_{ji} are coefficients influencing output domain j from input proportion i.\n    - Bias_j is the bias term for output domain j.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions for F domains.\n                   F=5 in this problem.\n    - params: 1D array of parameters (total 35 for F=5).\n              Structure: [exponents (F), coeffs (F*F), biases (F)]\n\n    Returns:\n    - Predicted multi-domain loss values (N, F).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # Parameter extraction based on fixed structure\n    # Total params = F (exponents) + F*F (coefficients) + F (biases)\n    # For F=5: 5 + 25 + 5 = 35 parameters\n\n    exponents = params[0:F] # Shape (F,)\n    coeffs_flat = params[F : F + F*F]\n    coeffs = coeffs_flat.reshape(F, F) # Shape (F_output, F_input)\n    biases = params[F + F*F : F + F*F + F] # Shape (F,)\n\n    # Calculate the power terms for each input dimension: X_ni ^ e_i\n    # np.power correctly handles 0^positive_exp = 0 and 0^0 = 1, ensuring stability.\n    # Exponents are bounded to be non-negative during fitting.\n    X_powered = np.power(X, exponents[None, :]) # Element-wise power, shape (N, F)\n\n    # Calculate predicted losses:\n    # predicted_losses[n, j] = Bias_j + sum_i (coeffs[j, i] * X_powered[n, i])\n    # This is efficiently computed via matrix multiplication: (N, F) @ (F, F).T + (1, F)\n    predicted_losses = np.dot(X_powered, coeffs.T) + biases[None, :]\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling_law_func parameters to the given data.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions.\n    - loss_values: (N, F) array of corresponding multi-domain losses.\n\n    Returns:\n    - Optimized parameters (1D array, up to 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values) # (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    P_total = F + F*F + F # Total parameters: 5 + 25 + 5 = 35\n\n    # --- Initial parameter guess ---\n    # 1. Exponents: Start with 1.0 for a linear-like initial approximation,\n    #    which simplifies the initial LSTSQ for coefficients.\n    init_exponents = np.ones(F) * 1.0\n    \n    # 2. Biases: Initialize with the average loss for each output domain.\n    #    This provides a strong, data-driven baseline for the loss values.\n    init_biases = np.mean(y, axis=0) # Shape (F,)\n\n    # 3. Coefficients: Use Least Squares to find an informed initial guess.\n    #    We approximate the problem as linear for coefficients, given initial exponents and biases.\n    #    The `X` values are powered by the initial exponents (currently 1.0, so X_powered is X itself).\n    X_powered_for_init_coeffs = np.power(X, init_exponents[None, :])\n    \n    # The target for LSTSQ is the actual loss minus the initial bias terms.\n    target_for_coeffs = y - init_biases[None, :]\n    \n    try:\n        # Solve multi-output linear regression: X_powered @ C_transpose = target\n        # C_transpose will be (F_input, F_output). We need to transpose to get (F_output, F_input)\n        C_transpose_init = np.linalg.lstsq(X_powered_for_init_coeffs, target_for_coeffs, rcond=None)[0]\n        init_coeffs = C_transpose_init.T.flatten() # Flatten for concatenation\n    except np.linalg.LinAlgError:\n        # Fallback to random initialization if LSTSQ fails (e.g., singular matrix, highly unlikely for this data)\n        # This provides robustness in case of numerical issues with LSTSQ, although less informed.\n        init_coeffs = np.random.uniform(low=-0.5, high=0.5, size=F * F)\n\n    # Concatenate all initial parameter components\n    initial_params = np.concatenate([init_exponents, init_coeffs, init_biases])\n    \n    # Add a small random perturbation for robustness, encouraging exploration\n    # around the informed initial guess. A scale of 0.01 provides sufficient\n    # exploration while staying close to the analytical starting point.\n    perturbation_scale = 0.01 \n    initial_params += np.random.normal(loc=0, scale=perturbation_scale, size=P_total)\n\n    # --- Define objective function for minimization ---\n    def objective(flat_params):\n        pred = scaling_law_func(X, flat_params)\n        mse = np.mean((pred - y) ** 2) # Mean Squared Error (MSE) as the objective\n        return mse\n\n    # --- Define parameter bounds for L-BFGS-B optimizer ---\n    bounds = []\n    # Exponents (F parameters): Non-negative to ensure numerical stability (0^negative is undefined).\n    # Upper bound of 5.0 allows for significant non-linearity while preventing overly steep functions.\n    for _ in range(F):\n        bounds.append((0.0, 5.0))\n\n    # Coefficients (F*F parameters): Broad range to allow for both positive\n    # and negative influences from input proportions on output losses, which\n    # can span a range of values to fit cross-entropy losses (1.8-4.2).\n    for _ in range(F * F):\n        bounds.append((-10.0, 10.0))\n\n    # Biases (F parameters): Constrained to typical observed loss ranges (1.0-5.0)\n    # to maintain numerical stability and ensure realistic predicted loss values.\n    # The actual loss values are between 1.8-4.2, so this range is appropriate.\n    for _ in range(F):\n        bounds.append((1.0, 5.0))\n\n    # Ensure initial_params are clamped within bounds. This is crucial for\n    # bounded optimization algorithms like L-BFGS-B, which require the\n    # starting point to be within the feasible region, especially after random perturbation.\n    initial_params_clamped = np.array([np.clip(initial_params[i], bounds[i][0], bounds[i][1]) for i in range(P_total)])\n\n    # --- Optimization ---\n    # L-BFGS-B is chosen for its efficiency and ability to handle parameter bounds\n    # and is suitable for moderately sized non-linear optimization problems.\n    result = minimize(objective, initial_params_clamped, method='L-BFGS-B', bounds=bounds)\n    \n    # Return optimized parameters. If the minimization fails for some reason\n    # (e.g., convergence not achieved), fall back to the clamped initial parameters.\n    params_opt = result.x if result.success else initial_params_clamped\n\n    return params_opt\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.990428, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nThis version evolves the scaling law function and optimization to\nbetter model multi-domain loss with a strict parameter limit and\nmulti-output predictions, focusing on mathematical accuracy across\ndifferent model sizes implicitly through a unified parameter set.\n\nImprovements in this version build upon the multi-start optimization strategy:\n1.  **Increased Multi-start Restarts (30):** The number of optimization restarts is\n    increased from 20 to 30. This allows the optimizer to explore an even wider\n    range of initial parameter configurations, further improving the chances\n    of finding a better global minimum for the non-convex objective function.\n2.  **Expanded Initial Parameter Ranges:**\n    -   `init_exponents`: The random sampling range for exponents is slightly\n        widened from (0.5, 2.0) to (0.3, 2.5). This allows the model to capture\n        an even broader spectrum of non-linear scaling behaviors, from highly\n        sub-linear (exponent < 1) to super-linear (exponent > 1).\n    -   `init_coeffs`: The random sampling range for coefficients is slightly\n        widened from (-1.0, 1.0) to (-1.5, 1.5). This allows for larger initial\n        contributions (positive or negative) from input proportions, potentially\n        reaching optimal values more efficiently.\n    -   `init_biases`: The Gaussian noise added to the mean loss for biases is\n        increased (`scale=0.15` from `scale=0.1`). This further diversifies the\n        starting points for bias terms across restarts, helping to break symmetry\n        and explore different local optima more thoroughly.\n3.  **Explicit Initial Parameter Clipping:** Initialized parameters are explicitly\n    clipped to the defined `bounds` before being passed to the optimizer. This\n    ensures that every optimization run starts from a valid point within the\n    specified parameter constraints, enhancing stability and efficiency.\n4.  **Tighter Optimizer Tolerances & Increased Iterations:** Explicit `options` are\n    passed to the `minimize` function (`maxiter=2000`, `ftol=1e-8`, `gtol=1e-6`).\n    This increases the maximum number of iterations for each run and tightens\n    the convergence criteria for function value (`ftol`) and gradient norm (`gtol`),\n    potentially leading to more precise fits at the cost of slightly longer\n    optimization times per run.\n5.  **Robust Best Parameter Selection:** The best parameters are selected based on\n    the absolute lowest objective function value (MSE) found across all restarts,\n    *regardless* of whether the `minimize` function itself reported `success=True`.\n    This ensures that even if an optimization run did not formally \"converge\" but\n    found a lower error, that solution is still considered. A check for `np.isfinite`\n    is added to `objective` to guard against invalid loss values.\n6.  **Refined Fallback Mechanism:** The fallback logic for when all multi-start\n    optimizations fail (extremely rare) now generates a robust default initial\n    guess, clips it, and attempts one final minimization run using these parameters\n    and the tighter tolerances.\n\nThe core `scaling_law_func` model form remains unchanged as it effectively\nutilizes the 35-parameter budget with its generalized additive power-law structure,\noffering a good balance of expressiveness, interpretability, and numerical stability.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions.\n\n    The model form is: L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)\n    where:\n    - L_j is the predicted loss for output domain j.\n    - X_i is the proportion of input domain i.\n    - e_i are shared exponents for each input proportion (across all output domains).\n    - c_{ji} are coefficients for how input proportion i influences output domain j.\n    - Bias_j is the bias term for output domain j.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions for F domains.\n                   F=5 in this problem. N is the number of data samples.\n    - params: 1D array of parameters (total 35 for F=5).\n              Structure: [exponents (F), coeffs (F*F), biases (F)]\n\n    Returns:\n    - Predicted multi-domain loss values (N, F).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # Ensure X is 2D, (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # Parameter extraction based on fixed structure\n    # Total params = F (exponents) + F*F (coefficients) + F (biases)\n    # For F=5: 5 + 25 + 5 = 35 parameters\n\n    # Exponents for each input proportion X_i. Shape (F,)\n    exponents = params[0:F]\n    \n    # Coefficients for F outputs (rows) from F inputs (columns).\n    # Reshape from flat (F*F,) to (F_output, F_input)\n    coeffs_flat = params[F : F + F*F]\n    coeffs = coeffs_flat.reshape(F, F) # Shape (F, F), coeffs[j,i] corresponds to output j influenced by input i\n\n    # Bias terms for each output domain L_j. Shape (F,)\n    biases = params[F + F*F : F + F*F + F]\n\n    # Calculate the power terms for each input dimension: X_ni ^ e_i\n    # `exponents[None, :]` broadcasts exponents across N samples.\n    # np.power handles `0^0=1` and `0^positive=0` correctly.\n    # Non-negative bounds for exponents prevent `0^negative` (infinity).\n    X_powered = np.power(X, exponents[None, :]) # Element-wise power, shape (N, F)\n\n    # Calculate predicted losses:\n    # predicted_losses[n, j] = Bias_j + sum_i (coeffs[j, i] * X_powered[n, i])\n    # This is efficiently calculated using matrix multiplication:\n    # (N, F) @ (F, F).T + (1, F) -> (N, F)\n    predicted_losses = np.dot(X_powered, coeffs.T) + biases[None, :]\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling_law_func parameters to the given data using multi-start optimization.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions.\n    - loss_values: (N, F) array of corresponding multi-domain losses.\n\n    Returns:\n    - Optimized parameters (1D array, up to 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values) # (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # Total number of parameters as defined by scaling_law_func\n    P_total = F + F*F + F # 5 + 25 + 5 = 35\n\n    # --- Define objective function for minimization ---\n    def objective(flat_params):\n        # Calculate predicted losses using the current parameters\n        pred = scaling_law_func(X, flat_params) # (N, F)\n        # Compute Mean Squared Error (MSE) between predictions and actual loss values\n        mse = np.mean((pred - y) ** 2)\n        # Return a very large value if MSE is not finite (e.g., due to NaNs or Infs)\n        if not np.isfinite(mse):\n            return np.inf\n        return mse\n\n    # --- Define parameter bounds for L-BFGS-B optimizer ---\n    # These bounds are crucial for numerical stability and preventing unrealistic parameter values.\n    bounds = []\n    # Exponents (F parameters):\n    # Constrained to be non-negative (to avoid 0^negative issues) and reasonable upper limit.\n    for _ in range(F):\n        bounds.append((0.0, 5.0)) # Exponents between 0 and 5\n\n    # Coefficients (F*F parameters):\n    # A wider range allows for both positive and negative contributions from input proportions.\n    for _ in range(F * F):\n        bounds.append((-10.0, 10.0)) # Coefficients between -10 and 10\n\n    # Biases (F parameters):\n    # Constrained to be within or near the observed loss range (1.8-4.2)\n    # to maintain numerical stability and realistic loss predictions.\n    for _ in range(F):\n        bounds.append((1.0, 5.0)) # Biases within a reasonable loss range for CE loss\n\n    # --- Multi-start Optimization ---\n    num_restarts = 30 # Increased restarts for better exploration\n    best_params_overall = None\n    min_mse_overall = np.inf\n\n    for _ in range(num_restarts):\n        # Generate initial parameter guess for this restart\n        # Exponents: Randomly sample from a slightly wider range.\n        init_exponents = np.random.uniform(0.3, 2.5, F)\n        \n        # Coefficients: Initialize with small random values from a slightly wider range.\n        init_coeffs = np.random.uniform(low=-1.5, high=1.5, size=F * F)\n        \n        # Biases: Initialize around the average loss for each output domain, with increased noise.\n        init_biases = np.mean(y, axis=0) + np.random.normal(loc=0, scale=0.15, size=F)\n\n        # Concatenate all initial parameter components\n        initial_params = np.concatenate([init_exponents, init_coeffs, init_biases])\n        \n        # Clip initial parameters to ensure they are within the defined bounds.\n        # This helps L-BFGS-B start within a valid region and avoid boundary issues.\n        initial_params_clipped = np.array([np.clip(val, b[0], b[1]) for val, b in zip(initial_params, bounds)])\n\n        # Perform optimization using L-BFGS-B, with explicit options for thoroughness.\n        result = minimize(\n            objective, \n            initial_params_clipped, \n            method='L-BFGS-B', \n            bounds=bounds,\n            options={'maxiter': 2000, 'ftol': 1e-8, 'gtol': 1e-6} # Increased maxiter, tightened tolerances\n        )\n        \n        # Always check result.fun to find the true minimum MSE, regardless of result.success.\n        # It's possible for an optimization to \"fail\" to converge (e.g., hit maxiter)\n        # but still yield a lower objective value than other \"successful\" runs.\n        if np.isfinite(result.fun) and result.fun < min_mse_overall:\n            min_mse_overall = result.fun\n            best_params_overall = result.x\n\n    # Fallback: If no valid minimum was found across all restarts (highly unlikely with these settings).\n    if best_params_overall is None:\n        # Generate a robust, default initial guess within bounds\n        init_exponents_default = np.ones(F) * 1.0 # Default exponent (linear-ish behavior)\n        init_coeffs_default = np.zeros(F * F) # Zero coefficients (no cross-influence)\n        init_biases_default = np.mean(y, axis=0) # Mean observed loss for biases\n        \n        default_params = np.concatenate([init_exponents_default, init_coeffs_default, init_biases_default])\n        \n        # Clip this default to ensure it's strictly within the defined bounds\n        initial_params_fallback_clipped = np.array([np.clip(val, b[0], b[1]) for val, b in zip(default_params, bounds)])\n        \n        # As a last resort, attempt one final minimization with this default\n        result_fallback = minimize(\n            objective, \n            initial_params_fallback_clipped, # Use the clipped default as starting point\n            method='L-BFGS-B', \n            bounds=bounds,\n            options={'maxiter': 2000, 'ftol': 1e-8, 'gtol': 1e-6}\n        )\n        if np.isfinite(result_fallback.fun) and result_fallback.fun < min_mse_overall:\n            best_params_overall = result_fallback.x\n        else: # If even the fallback fails to find a better solution, return the initial clipped default\n            best_params_overall = initial_params_fallback_clipped\n\n\n    return best_params_overall\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "gpt-5", "reward_r2": 0.9904279621411324, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The law models each domain's validation loss as the sum of:\n      - a group- and domain-specific intercept a_i,\n      - a group- and domain-specific coefficient b_i times log(p_i + eps), capturing\n        diminishing returns from allocating more mixture proportion to the same domain,\n      - plus a linear combination of the proportions of the other domains (j != i),\n        with group- and domain-specific coefficients c_{i,j}.\n\n    Mathematically, for domain i in {1..5}:\n        loss_i = a_i + b_i * log(p_i + eps) + sum_{j != i} c_{i,j} * p_j\n\n    where p_k are the mixture proportions (sum_k p_k = 1), and eps is a small constant\n    to handle zero proportions inside the logarithm.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: One of the experimental groups. The same functional form is used for all\n               groups, with coefficients differing per group.\n\n    Returns:\n        A list of dicts with keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n\n    # Small constant to avoid log(0)\n    EPS = 1e-6\n\n    # Coefficients fitted per group on the provided dataset (/app/data), using the\n    # model: loss_i = a_i + b_i * log(p_i + EPS) + sum_{j != i} c_{i,j} * p_j\n    # For convenience, linear coefficients are stored as a full 5-length vector per domain\n    # with 0.0 for the self-domain (j == i) entry.\n    COEFFS = {\n        \"70M\": {\n            1: {\"a\": 2.352400, \"b\": -0.041342, \"c\": [0.000000, 0.552302, 0.679733, 0.457510, 0.478500]},\n            2: {\"a\": 3.119185, \"b\": -0.005609, \"c\": [0.733329, 0.000000, 0.567223, 0.760307, 0.571576]},\n            3: {\"a\": 1.557687, \"b\": -0.029500, \"c\": [1.776484, 1.574088, 0.000000, 1.672027, 1.590520]},\n            4: {\"a\": 1.005729, \"b\": -0.040741, \"c\": [0.682161, 0.804593, 0.768164, 0.000000, 0.680742]},\n            5: {\"a\": 3.401418, \"b\": -0.019938, \"c\": [0.282951, 0.204621, 0.280657, 0.244292, 0.000000]},\n        },\n        \"160M\": {\n            1: {\"a\": 2.084419, \"b\": -0.039436, \"c\": [0.000000, 0.515541, 0.590549, 0.410446, 0.414215]},\n            2: {\"a\": 2.848965, \"b\": -0.005760, \"c\": [0.664815, 0.000000, 0.533358, 0.698111, 0.486927]},\n            3: {\"a\": 1.375788, \"b\": -0.028472, \"c\": [1.645880, 1.472320, 0.000000, 1.592583, 1.466833]},\n            4: {\"a\": 0.822570, \"b\": -0.036176, \"c\": [0.633280, 0.747330, 0.680942, 0.000000, 0.623930]},\n            5: {\"a\": 3.044954, \"b\": -0.020112, \"c\": [0.288934, 0.234711, 0.313982, 0.265677, 0.000000]},\n        },\n        \"305M\": {\n            1: {\"a\": 1.965386, \"b\": -0.039011, \"c\": [0.000000, 0.461256, 0.591688, 0.362942, 0.378769]},\n            2: {\"a\": 2.675656, \"b\": -0.004898, \"c\": [0.681773, 0.000000, 0.558797, 0.717652, 0.506549]},\n            3: {\"a\": 1.389474, \"b\": -0.030900, \"c\": [1.455301, 1.326467, 0.000000, 1.424874, 1.288538]},\n            4: {\"a\": 0.758123, \"b\": -0.034855, \"c\": [0.586244, 0.671620, 0.645107, 0.000000, 0.580221]},\n            5: {\"a\": 2.880988, \"b\": -0.021162, \"c\": [0.278675, 0.225879, 0.321137, 0.249162, 0.000000]},\n        },\n        \"410M\": {\n            1: {\"a\": 1.904173, \"b\": -0.038724, \"c\": [0.000000, 0.497929, 0.520547, 0.389682, 0.371875]},\n            2: {\"a\": 2.648743, \"b\": -0.005145, \"c\": [0.632228, 0.000000, 0.458498, 0.688205, 0.451025]},\n            3: {\"a\": 1.311117, \"b\": -0.031575, \"c\": [1.474932, 1.346313, 0.000000, 1.429078, 1.297670]},\n            4: {\"a\": 0.726224, \"b\": -0.033638, \"c\": [0.560347, 0.717670, 0.657147, 0.000000, 0.569629]},\n            5: {\"a\": 2.802291, \"b\": -0.021963, \"c\": [0.276436, 0.261534, 0.247464, 0.274675, 0.000000]},\n        },\n    }\n\n    # Fallback: if an unknown group is provided, use the closest available group\n    # by parameterization (default to the smallest model \"70M\").\n    params_by_group = COEFFS.get(group)\n    if params_by_group is None:\n        params_by_group = COEFFS[\"70M\"]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Read proportions in a fixed order\n        p = [float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)]\n        # Normalize defensively in case inputs are not perfectly normalized\n        s = sum(p)\n        if s > 0:\n            p = [pi / s for pi in p]\n\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            par = params_by_group[i]\n            a = par[\"a\"]\n            b = par[\"b\"]\n            c = par[\"c\"]  # length-5, zero at index i-1\n            log_term = math.log(max(p[i - 1], 0.0) + EPS)\n            linear_term = sum(c[j] * p[j] for j in range(5))\n            y = a + b * log_term + linear_term\n            pred[f\"loss_domain_{i}\"] = float(y)\n\n        outputs.append(pred)\n\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.990416, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nThis version evolves the scaling law function and optimization to\nbetter model multi-domain loss with a strict parameter limit and\nmulti-output predictions, focusing on mathematical accuracy across\ndifferent model sizes implicitly through a unified parameter set.\n\nImprovements focus on enhancing the robustness and global search capability\nof the optimization process by implementing a hybrid initialization strategy\nwith multiple restarts, and fine-tuning optimization parameters. The\nscaling law function itself remains robust in handling zero proportions.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions.\n\n    The model form is a generalized linear model with power-law terms:\n    L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)\n    where:\n    - L_j is the predicted loss for output domain j.\n    - X_i is the proportion of input domain i.\n    - e_i are shared exponents for each input proportion X_i (total F exponents).\n    - c_{ji} are coefficients, linking input proportion i to output loss j (total F*F coefficients).\n    - Bias_j is the bias term specific to output domain j (total F biases).\n\n    This structure accounts for domain interactions and non-linear effects using a total of\n    F (exponents) + F*F (coefficients) + F (biases) parameters.\n    For F=5 domains, this is 5 + (5*5) + 5 = 35 parameters, strictly adhering to the limit.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions for F domains.\n                   N is the number of data points, F=5 in this problem.\n    - params: 1D array of parameters (total 35 for F=5).\n              Structure: [exponents (F), coeffs (F*F, flattened), biases (F)]\n\n    Returns:\n    - Predicted multi-domain loss values (N, F). Each row corresponds to a data point,\n      and each column corresponds to the predicted loss for one of the F output domains.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # Ensure X is a 2D array (N, F)\n    N, F = X.shape # N: number of samples, F: number of domains (5)\n\n    # --- Parameter extraction from the flat 'params' array ---\n    # The total number of parameters is fixed at 35 for F=5 as per problem constraints.\n    # Parameters are ordered as: F exponents, F*F coefficients (flattened), F biases.\n\n    # 1. Exponents for each input proportion X_i. Shape (F,)\n    # These exponents apply globally to each input domain's proportion.\n    exponents = params[0:F]\n    \n    # 2. Coefficients for each output domain (rows) from each input domain (columns).\n    # The F*F coefficients are flattened in 'params', so they need reshaping.\n    # coeffs[j, i] represents the influence of input X_i on output L_j.\n    coeffs_flat = params[F : F + F*F]\n    coeffs = coeffs_flat.reshape(F, F) # Shape (F_output, F_input)\n\n    # 3. Bias terms for each output domain L_j. Shape (F,)\n    biases = params[F + F*F : F + F*F + F]\n\n    # --- Calculation of predicted losses ---\n    # Robust handling for X_i = 0: If a proportion is 0, its transformed contribution should be 0.\n    # This prevents np.power(0,0)=1 from inappropriately adding a constant term when a domain is absent.\n    # Initialize X_powered with zeros, then fill in values for non-zero proportions.\n    X_powered = np.zeros_like(X, dtype=float)\n    non_zero_mask = X > 0 # Boolean mask for elements where X is strictly greater than 0.\n    \n    # Apply power only to strictly positive proportions.\n    # `np.where(non_zero_mask)[1]` correctly gets the column indices for each True element,\n    # mapping each non-zero X[r,c] to its corresponding exponent exponents[c].\n    X_powered[non_zero_mask] = np.power(X[non_zero_mask], exponents[np.where(non_zero_mask)[1]])\n\n    # Calculate the weighted sum of powered input proportions for each output domain\n    # This involves a matrix multiplication: (N, F) @ (F, F).T\n    # The transpose of coeffs (coeffs.T) is used to align input features (F) with coefficients\n    predicted_losses = np.dot(X_powered, coeffs.T)\n\n    # Step 3: Add bias terms\n    # biases[None, :] reshapes biases to (1, F) for broadcasting across N samples.\n    predicted_losses = predicted_losses + biases[None, :]\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling_law_func parameters to the given data using L-BFGS-B optimization.\n    Employs multiple random restarts and a hybrid initialization strategy to improve\n    the chances of finding a global optimum for the non-convex optimization problem.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions.\n                   N is the number of data points, F=5 domains.\n    - loss_values: (N, F) array of corresponding multi-domain losses.\n\n    Returns:\n    - Optimized parameters (1D array, up to 35 parameters).\n      Returns the best optimized parameters found across restarts, or a robust\n      default initial guess if all optimizations are unsuccessful.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values) # (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # Total number of parameters (F exponents + F*F coefficients + F biases = 35 for F=5)\n    P_total = F + F*F + F\n\n    # --- Objective function for minimization (defined once) ---\n    def objective(flat_params):\n        # Calculate predicted losses using the current set of parameters\n        pred = scaling_law_func(X, flat_params) # (N, F)\n        # Compute Mean Squared Error (MSE) between predictions and actual loss values.\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # --- Parameter bounds for L-BFGS-B optimizer ---\n    # Bounds are crucial for numerical stability and ensuring physically meaningful parameters.\n    bounds = []\n\n    # 1. Exponents (F parameters):\n    # Non-negative (0.0) to handle 0^exponent robustly (0^0=1, 0^positive=0, avoids 0^negative=inf).\n    # An upper bound (e.g., 5.0) prevents excessively steep power functions, aiding stability\n    # and reflecting typical scaling law exponents.\n    for _ in range(F):\n        bounds.append((0.0, 5.0)) # Exponents often fall in this range (e.g., 0.5, 1, 2)\n\n    # 2. Coefficients (F*F parameters):\n    # A wider range allows for both positive and negative contributions,\n    # capturing various interactions between domain proportions and losses effectively.\n    for _ in range(F * F):\n        bounds.append((-10.0, 10.0)) # Coefficients can be positive or negative, adjusting influence\n\n    # 3. Biases (F parameters):\n    # Constrained to be within a realistic range for loss values (1.8-4.2 is given in data characteristics).\n    # This prevents unphysical negative loss predictions or extreme baseline values.\n    # A slightly wider bound (e.g., 1.0 to 5.0) gives optimizer some wiggle room.\n    for _ in range(F):\n        bounds.append((1.0, 5.0)) # Losses are positive and typically within this range.\n\n    best_mse = np.inf\n    best_params_opt = None\n    \n    num_restarts = 20 # Increased number of random restarts for better global search\n\n    min_loss_y = np.min(y) # Used for more informed random bias initialization\n    max_loss_y = np.max(y)\n\n    for i in range(num_restarts):\n        # --- Initial parameter guess for each restart (hybrid strategy) ---\n        if i == 0: # First restart uses a more conservative, stable starting point\n            init_exponents = np.ones(F) * 1.0 # Start exponents at 1.0 (linear relationship assumption)\n            init_coeffs = np.random.uniform(low=-0.1, high=0.1, size=F * F) # Tighter range for coefficients\n            init_biases = np.mean(y, axis=0) # Mean loss per domain as a stable baseline\n            perturbation_scale = 0.01 # Small perturbation to introduce slight variations\n        else: # Subsequent restarts use wider random ranges for broader exploration\n            # Exponents: Random values in a typical scaling range, allowing for sub-linear (e.g., 0.1)\n            # to super-linear (e.g., 3.0) relationships.\n            init_exponents = np.random.uniform(low=0.1, high=3.0, size=F) \n            # Coefficients: Wider range to explore stronger positive or negative influences.\n            init_coeffs = np.random.uniform(low=-2.0, high=2.0, size=F * F)\n            # Biases: Random within a slightly extended range of observed losses for exploration.\n            init_biases = np.random.uniform(low=min_loss_y * 0.9, high=max_loss_y * 1.1, size=F)\n            perturbation_scale = 0.05 # Larger perturbation for more diverse starting points\n\n        # Concatenate all initial parameter components into a single flat array\n        initial_params = np.concatenate([init_exponents, init_coeffs, init_biases])\n        \n        # Add a small random perturbation to avoid multiple runs starting from identical points\n        initial_params += np.random.normal(loc=0, scale=perturbation_scale, size=P_total)\n\n        # Clip initial parameters to ensure they are strictly within their defined bounds\n        # This is important for L-BFGS-B, which expects the initial point to be feasible.\n        initial_params_clipped = np.array([\n            np.clip(initial_params[k], bounds[k][0], bounds[k][1]) for k in range(P_total)\n        ])\n\n        # --- Optimization using L-BFGS-B ---\n        # 'L-BFGS-B' is a quasi-Newton method that efficiently handles bounds on parameters.\n        # Added 'maxiter' and 'ftol' for better convergence control, allowing more iterations\n        # and requiring a tighter tolerance for the objective function change.\n        result = minimize(objective, initial_params_clipped, method='L-BFGS-B', bounds=bounds,\n                          options={'maxiter': 2000, 'ftol': 1e-9}) \n        \n        # Keep track of the best result found across all restarts\n        if result.success and result.fun < best_mse:\n            best_mse = result.fun\n            best_params_opt = result.x\n            \n    # Return the optimized parameters. If no successful optimization was found (unlikely\n    # with multiple restarts and robust initializations), return a robust default initial guess.\n    if best_params_opt is None:\n        # Fallback to a sensible initial guess if no optimization succeeded\n        default_exponents = np.ones(F) * 1.0\n        default_coeffs = np.zeros(F * F) # Assume zero influence as default for coefficients\n        default_biases = np.mean(y, axis=0) # Mean loss as a robust default for biases\n        best_params_opt = np.concatenate([default_exponents, default_coeffs, default_biases])\n        # Ensure fallback parameters are also within bounds before returning\n        for j, (lower, upper) in enumerate(bounds):\n            if lower is not None:\n                best_params_opt[j] = max(best_params_opt[j], lower)\n            if upper is not None:\n                best_params_opt[j] = min(best_params_opt[j], upper)\n\n    return best_params_opt\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.990398, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\n\nThis program implements a multi-output scaling law function and an optimization\nalgorithm to model the relationship between domain mixture proportions and\nmulti-domain loss values. It is designed to be mathematically accurate,\nparameter-efficient, and robust, operating within a strict parameter limit.\nThe model implicitly accounts for varying model sizes by learning a unified\nset of parameters across all provided data points, regardless of model size.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions using a\n    generalized power-law model.\n\n    The model form for each output domain j is:\n    L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)\n\n    where:\n    - L_j: Predicted loss for output domain j.\n    - X_i: Proportion of input domain i.\n    - e_i: Shared exponent for the proportion of input domain i. These exponents\n           capture the non-linear impact of each domain's proportion.\n    - c_{ji}: Coefficient representing the influence of input domain i's\n              powered proportion (X_i^e_i) on the output loss for domain j.\n              This allows for cross-domain interactions.\n    - Bias_j: An additive bias term specific to output domain j, representing\n              a baseline loss.\n\n    The total number of parameters is fixed at 35 for F=5 domains:\n    F (exponents) + F*F (coefficients) + F (biases) = 5 + 25 + 5 = 35.\n\n    Parameters:\n    - data_points: (N, F) array where N is the number of data samples and F=5\n                   is the number of domain proportions. Each row sums to 1.0.\n    - params: 1D array of 35 parameters, structured as:\n              [e_1, ..., e_F, c_11, ..., c_FF, Bias_1, ..., Bias_F].\n\n    Returns:\n    - predicted_losses: (N, F) array of predicted multi-domain loss values.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # Ensure X is a 2D array (N, F)\n    N, F = X.shape                             # F=5 (5 domains)\n\n    # Parameter extraction based on the predefined structure\n    # 1. Exponents for each input proportion X_i. Shape (F,)\n    exponents = params[0:F]\n    \n    # 2. Coefficients for F outputs from F inputs. Shape (F, F)\n    #    coeffs[j, i] means the coefficient for output domain j from input domain i.\n    coeffs_flat = params[F : F + F*F]\n    coeffs = coeffs_flat.reshape(F, F) # Reshape from flat to (F_output, F_input)\n\n    # 3. Bias terms for each output domain L_j. Shape (F,)\n    biases = params[F + F*F : F + F*F + F]\n\n    # Calculate the powered terms: X_ni ^ e_i.\n    # np.power(0, 0) is conventionally 1.0. However, for domain proportions (X_i),\n    # if a proportion X_i is 0 (meaning the domain is absent), its contribution\n    # should ideally be 0, regardless of the exponent e_i (assuming e_i >= 0).\n    # This explicit handling ensures semantic correctness: 0^e = 0 for absent domains.\n    X_powered = np.power(X, exponents[None, :]) # Element-wise power, shape (N, F)\n    X_powered[X == 0] = 0.0 # Override 0^0=1.0 to 0.0, and keeps 0^e=0 for e>0.\n    \n    # Calculate predicted losses using matrix multiplication:\n    # predicted_losses[n, j] = Bias_j + sum_i (coeffs[j, i] * X_powered[n, i])\n    # This is equivalent to: (N, F) @ (F, F).T + (1, F) broadcast\n    predicted_losses = np.dot(X_powered, coeffs.T) + biases[None, :]\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the parameters of the `scaling_law_func` to the given data using\n    bounded L-BFGS-B optimization with multiple random restarts to improve\n    robustness against local minima and parameter initialization sensitivity.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions.\n    - loss_values: (N, F) array of corresponding multi-domain losses.\n\n    Returns:\n    - Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values)                # (N, F)\n    N_samples, F = X.shape                     # F=5 (5 domains)\n\n    P_total = F + F*F + F # Total number of parameters (35 for F=5)\n\n    # --- Parameter bounds for L-BFGS-B optimizer ---\n    # Bounds help constrain parameters to physically meaningful ranges,\n    # improving optimization stability and preventing unrealistic values.\n    bounds = []\n    # Exponents (F parameters): Must be non-negative for X^e to be well-defined\n    # and to ensure 0^e behavior is consistent. An upper bound prevents extreme\n    # non-linearities and helps bound the search space.\n    for _ in range(F):\n        bounds.append((0.0, 5.0)) # Exponents often fall between 0 and 5.\n\n    # Coefficients (F*F parameters): Allow for both positive and negative\n    # contributions. The range is chosen to be wide enough to encompass\n    # typical loss scales (1.8-4.2) given proportions [0,1].\n    for _ in range(F * F):\n        bounds.append((-10.0, 10.0))\n\n    # Biases (F parameters): Constrained to be within or near the observed\n    # loss range (1.8-4.2) to maintain numerical stability and ensure\n    # realistic baseline predictions.\n    for _ in range(F):\n        bounds.append((1.0, 5.0)) # Reflects the typical loss range of 1.8-4.2.\n\n    # Objective function for minimization\n    def objective(flat_params):\n        \"\"\"Calculates the Mean Squared Error (MSE) of predictions.\"\"\"\n        pred = scaling_law_func(X, flat_params) # (N, F)\n        mse = np.mean((pred - y) ** 2)          # Compute MSE across all outputs and samples\n        return mse\n\n    best_params = None\n    min_mse = np.inf\n    num_restarts = 20 # Increased number of restarts for better exploration of the parameter space\n\n    # Pre-compute lower and upper bounds as arrays for efficient clipping\n    lower_bounds = np.array([b[0] for b in bounds])\n    upper_bounds = np.array([b[1] for b in bounds])\n\n    for i in range(num_restarts):\n        # --- Generate diverse initial parameter guesses for each restart ---\n        # Exponents: Start around 1.0 (linear-like) with a uniform perturbation.\n        # This range (0.5 to 1.5) ensures they respect the [0,5] bound.\n        init_exponents = np.ones(F) * 1.0 + np.random.uniform(-0.5, 0.5, F)\n        \n        # Coefficients: More structured initialization to reflect expected behavior.\n        # Self-influence (diagonal c_jj) often positive, cross-influence (off-diagonal c_ji) smaller.\n        init_coeffs_matrix = np.zeros((F, F))\n        for j in range(F): # For each output domain j\n            # Initialize diagonal coefficients (c_jj) to be positive, suggesting self-contribution.\n            init_coeffs_matrix[j, j] = np.random.uniform(low=0.1, high=0.5)\n            # Initialize off-diagonal coefficients (c_ji for i!=j) closer to zero,\n            # allowing for both positive and negative cross-domain effects.\n            for k in range(F):\n                if k != j:\n                    init_coeffs_matrix[j, k] = np.random.uniform(low=-0.5, high=0.5)\n        init_coeffs = init_coeffs_matrix.flatten()\n        \n        # Biases: Perturbed mean of observed losses for each output domain, providing a data-driven baseline.\n        init_biases = np.mean(y, axis=0) + np.random.uniform(-0.5, 0.5, F)\n\n        # Concatenate all initial parameter components into a single flat array\n        initial_params = np.concatenate([init_exponents, init_coeffs, init_biases])\n        \n        # Clip initial parameters to ensure they are within the defined bounds.\n        # This is critical for L-BFGS-B, which expects to start within bounds to ensure proper convergence.\n        initial_params = np.clip(initial_params, lower_bounds, upper_bounds)\n\n        # --- Perform optimization ---\n        result = minimize(objective, initial_params, method='L-BFGS-B', bounds=bounds)\n        \n        # Keep track of the best parameters found across all restarts\n        if result.success and result.fun < min_mse:\n            min_mse = result.fun\n            best_params = result.x\n\n    # Fallback mechanism: If no successful optimization was found (e.g., all restarts failed).\n    # This ensures a set of parameters is always returned, even if the optimization struggles.\n    if best_params is None:\n        # Provide a conservative, stable initial guess as a fallback.\n        fallback_exponents = np.ones(F) * 1.0\n        fallback_coeffs_matrix = np.zeros((F, F))\n        for j in range(F):\n            fallback_coeffs_matrix[j, j] = 0.2 # Small positive self-influence for fallback\n        fallback_coeffs = fallback_coeffs_matrix.flatten()\n        fallback_biases = np.mean(y, axis=0)\n        \n        best_params = np.concatenate([fallback_exponents, fallback_coeffs, fallback_biases])\n        # Ensure fallback parameters also conform to defined bounds\n        best_params = np.clip(best_params, lower_bounds, upper_bounds)\n\n    return best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-2.5-flash", "reward_r2": 0.990382, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nThis version evolves the scaling law function and optimization to\nbetter model multi-domain loss with a strict parameter limit and\nmulti-output predictions, focusing on mathematical accuracy across\ndifferent model sizes implicitly through a unified parameter set.\n\nThe core model: L_j = Bias_j + sum_{i=1 to F} (c_{ji} * X_i^e_i)\nwhere:\n- L_j is the predicted loss for output domain j.\n- X_i is the proportion of input domain i.\n- e_i are shared exponents for each input proportion (F parameters).\n- c_{ji} are coefficients for output domain j from input proportion i (F*F parameters).\n- Bias_j is the bias term for output domain j (F parameters).\nTotal parameters for F=5: 5 (exponents) + 25 (coefficients) + 5 (biases) = 35.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions for F=5 domains.\n                   N is the number of data points.\n    - params: 1D array of 35 parameters, structured as:\n              [exponents (F), coeffs (F*F), biases (F)]\n\n    Returns:\n    - Predicted multi-domain loss values (N, F).\n    \"\"\"\n    # Ensure data_points is a 2D numpy array of float type for numerical stability.\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64)) # Shape (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # --- Parameter extraction ---\n    # The total number of parameters is fixed at 35 for F=5, structured as:\n    # F exponents + F*F coefficients + F biases.\n\n    # 1. Exponents for each input proportion X_i. Shape (F,)\n    # These exponents are shared across all output loss dimensions, meaning each input\n    # domain's scaling behavior (how it contributes when raised to a power) is consistent,\n    # regardless of which output loss is being predicted.\n    exponents = params[0:F]\n    \n    # 2. Coefficients for F outputs (rows) from F inputs (columns).\n    # These coefficients (c_ji) represent the specific influence of input domain 'i' on\n    # output domain 'j'. They capture cross-domain interactions.\n    # Reshape from flat (F*F,) to (F_output, F_input).\n    coeffs_flat = params[F : F + F*F]\n    coeffs = coeffs_flat.reshape(F, F) # Shape (F, F), coeffs[j,i] corresponds to output j from input i\n\n    # 3. Bias terms for each output domain L_j. Shape (F,)\n    # These are constant offsets for each output loss dimension, representing a baseline loss\n    # independent of the proportions.\n    biases = params[F + F*F : F + F*F + F]\n\n    # --- Core calculation ---\n    # Calculate the power terms for each input dimension: X_ni ^ e_i\n    # We ensure exponents are strictly positive (>= 1e-9) during fitting via bounds.\n    # This guarantees that np.power(0, positive_exponent) evaluates to 0, which is the\n    # desired behavior: an absent domain (proportion 0) contributes nothing via its\n    # power term to the sum.\n    X_powered = np.power(X, exponents[None, :]) # Element-wise power, broadcasts exponents to (1, F)\n\n    # Calculate predicted losses using a vectorized approach (matrix multiplication).\n    # The sum part: sum_i (coeffs[j, i] * X_powered[n, i]) is equivalent to a\n    # matrix product: X_powered @ coeffs.T.\n    # predicted_losses[n, j] = Bias_j + (X_powered[n, :] @ coeffs.T[:, j])\n    # The biases[None, :] ensures broadcasting the F biases across all N data points.\n    predicted_losses = np.dot(X_powered, coeffs.T) + biases[None, :]\n\n    return predicted_losses\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling_law_func parameters to the given data using L-BFGS-B optimization.\n\n    This optimization approach is chosen for its efficiency with gradient-based methods\n    and its ability to handle parameter bounds, which is crucial for numerical stability\n    and constraining parameters to physically meaningful ranges for loss values.\n\n    Parameters:\n    - data_points: (N, F) array with domain proportions.\n    - loss_values: (N, F) array of corresponding multi-domain losses.\n\n    Returns:\n    - Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    # Ensure data is 2D numpy array of float type for consistency and numerical stability\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64)) # (N, F)\n    y = np.asarray(loss_values, dtype=np.float64) # (N, F)\n    N, F = X.shape # F=5 (5 domains)\n\n    # Total number of parameters as defined by scaling_law_func\n    P_total = F + F*F + F # For F=5: 5 (exponents) + 25 (coefficients) + 5 (biases) = 35 parameters.\n\n    # --- Initial parameter guess (heuristic-based for better convergence) ---\n    # A good initial guess can significantly speed up convergence and help avoid poor local minima.\n\n    # Exponents: Start near 1.0 (implying a linear initial relationship).\n    # This is a neutral starting point for power-law exponents.\n    init_exponents = np.ones(F) * 1.0\n    \n    # Coefficients: Small random values centered around zero.\n    # This helps break symmetry, allowing the optimizer to explore both positive and negative\n    # influences of input domains on output domains. A slightly narrower range might\n    # reflect typical coefficient magnitudes, but -0.2 to 0.2 is a safe exploration range.\n    init_coeffs = np.random.uniform(low=-0.2, high=0.2, size=F * F)\n    \n    # Biases: Initialize with the average loss for each respective output domain.\n    # This provides a very strong baseline guess, as biases often represent the intrinsic\n    # loss level when all proportions are zero or average out.\n    init_biases = np.mean(y, axis=0)\n\n    # Concatenate all initial parameter components into a single flat array\n    initial_params_raw = np.concatenate([init_exponents, init_coeffs, init_biases])\n    \n    # Add a small random perturbation. This helps the optimizer escape potential saddle points\n    # or break symmetries that could hinder initial progress, encouraging more thorough exploration\n    # around the heuristic initial guess.\n    initial_params_raw += np.random.normal(loc=0, scale=0.005, size=P_total) # Smaller scale for perturbation\n\n    # --- Define parameter bounds for L-BFGS-B optimizer ---\n    # These bounds are crucial for numerical stability, preventing nonsensical parameter values,\n    # and guiding the optimizer towards physically plausible solutions for cross-entropy loss values.\n    bounds = []\n    \n    # Exponents (F parameters):\n    # - Lower bound: `1e-9` (a very small positive number) ensures `0^e` consistently evaluates to 0\n    #   (as opposed to `0^0=1` if `e` could be exactly 0, or `0^-e=inf` if `e` could be negative).\n    # - Upper bound: `5.0` prevents excessively steep or flat power relationships, keeping the model\n    #   interpretable and numerically stable.\n    for _ in range(F):\n        bounds.append((1e-9, 5.0)) # Strictly positive lower bound for robustness with `X_i=0`\n\n    # Coefficients (F*F parameters):\n    # A sufficiently wide range `(-10.0, 10.0)` allows for diverse scaling behaviors.\n    # This enables both positive (increasing loss) and negative (decreasing loss, often due to\n    # specific data distributions or interactions) contributions from input proportions.\n    for _ in range(F * F):\n        bounds.append((-10.0, 10.0))\n\n    # Biases (F parameters):\n    # - Lower bound: `1.0` is a reasonable floor for cross-entropy losses, which are always positive\n    #   and typically above 1.0 in real-world scenarios. This ensures predicted losses remain realistic.\n    # - Upper bound: `5.0` accommodates the observed loss range (1.8-4.2) with some margin.\n    for _ in range(F):\n        bounds.append((1.0, 5.0))\n\n    # --- Clamp initial parameters to bounds ---\n    # It's good practice to ensure the initial guess itself respects the defined bounds.\n    # This prevents the optimizer from starting in an invalid region, which can sometimes\n    # lead to slower convergence or errors.\n    initial_params = np.array([\n        np.clip(initial_params_raw[i], bounds[i][0], bounds[i][1]) \n        for i in range(P_total)\n    ])\n\n    # --- Objective function for minimization ---\n    def objective(flat_params):\n        \"\"\"\n        Calculates the Mean Squared Error (MSE) between predicted and actual losses.\n        This function is minimized by the optimizer. MSE is a common and robust choice\n        for regression tasks due to its differentiability and interpretability.\n        \"\"\"\n        pred = scaling_law_func(X, flat_params)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # --- Optimization ---\n    # Use 'L-BFGS-B' method as it effectively handles parameter bounds and is efficient\n    # for medium-scale, bound-constrained optimization problems.\n    # - `maxiter`: Increased maximum iterations to allow ample time for convergence.\n    # - `ftol`: Function tolerance, controls convergence based on improvement in objective function value.\n    # - `gtol`: Gradient tolerance, controls convergence based on the magnitude of the gradient.\n    # Tighter tolerances encourage higher precision in the optimized parameters.\n    result = minimize(\n        objective, \n        initial_params, \n        method='L-BFGS-B', \n        bounds=bounds,\n        options={'maxiter': 5000, 'ftol': 1e-9, 'gtol': 1e-7}\n    )\n    \n    # Return the optimized parameters if minimization was successful.\n    # Otherwise, return the (clamped) initial parameters as a fallback\n    # to prevent errors, although L-BFGS-B is generally very robust.\n    params_opt = result.x if result.success else initial_params\n\n    return params_opt\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.989908, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss using an Effective Data Mixture Scaling Law.\n    Model: L_i = bias_i + (sum_j (w_ij^2 * x_j))^(-exp(alpha_i))\n    \n    This function applies transformations to parameters to ensure physical validity:\n    - Weights are squared to be non-negative.\n    - Alpha is exponentiated to be strictly positive.\n    - Bias is linear (fitted to be effectively the asymptotic loss).\n    \n    Args:\n        data_points: (N, 5) array of domain mixture proportions.\n        params: Flat array of 35 parameters (7 per domain: 1 bias, 1 log_alpha, 5 weights).\n        \n    Returns:\n        Predicted losses: (N, 5) array.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params)\n    \n    # Constants\n    n_domains = 5\n    n_params_per = 7  # 1 bias, 1 log_alpha, 5 weights\n    total_len = n_domains * n_params_per\n    \n    # Robust parameter reshaping\n    if params.size != total_len:\n        flat = params.ravel()\n        if flat.size < total_len:\n            flat = np.pad(flat, (0, total_len - flat.size), constant_values=0.1)\n        params = flat[:total_len]\n        \n    P = params.reshape(n_domains, n_params_per)\n    \n    # Extract and transform parameters\n    bias = P[:, 0]\n    # Clip log_alpha to valid range [~0.006, ~20.0] to prevent overflow/underflow\n    log_alpha = np.clip(P[:, 1], -5.0, 3.0) \n    weights = P[:, 2:] ** 2\n    \n    # Calculate Effective Data: D_eff = X @ W.T\n    # Maps mixture proportions to an effective dataset size for each domain\n    D_eff = np.dot(X, weights.T)\n    D_eff = np.maximum(D_eff, 1e-10) # Numerical stability\n    \n    # Power Law Term: D^(-alpha)\n    term = D_eff ** (-np.exp(log_alpha)[None, :])\n    \n    # Prediction: Bias + Term\n    return bias[None, :] + term\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling law using a robust hybrid strategy:\n    1. Generates multiple weight priors (Correlation-based, Diagonal, Uniform).\n    2. Performs a coarse grid search for Bias and Alpha for each weight prior.\n    3. Optimizes the best candidate using BFGS to find the global minimum.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    \n    # Align shapes\n    if Y.shape[0] != X.shape[0] and Y.shape[1] == X.shape[0]:\n        Y = Y.T\n        \n    n_domains = 5\n    n_feat = 5\n    final_params = []\n    \n    # --- 1. Pre-compute Correlation Priors ---\n    # Determine which features correlate with performance (negative correlation with loss)\n    w_corrs_all = []\n    for t in range(n_domains):\n        y = Y[:, t]\n        corrs = []\n        for j in range(n_feat):\n            if np.std(X[:, j]) > 1e-9:\n                corrs.append(np.corrcoef(X[:, j], y)[0, 1])\n            else:\n                corrs.append(0.0)\n        # Convert correlation to weight: Strong neg correlation -> High weight\n        w_c = np.sqrt(np.maximum(-np.array(corrs), 0) + 0.05)\n        w_c /= np.max(w_c) # Normalize\n        w_corrs_all.append(w_c)\n\n    # --- 2. Fit Each Domain ---\n    for t in range(n_domains):\n        y_tgt = Y[:, t]\n        y_min = np.min(y_tgt)\n        \n        # Weight Candidates\n        # A. Correlation: Data-driven guess\n        # B. Diagonal: Self-data is most important\n        w_diag = np.full(n_feat, np.sqrt(0.01))\n        if t < n_feat: w_diag[t] = 1.0\n        # C. Uniform: All data helps equally\n        w_uni = np.full(n_feat, np.sqrt(0.2))\n        \n        candidates = [w_corrs_all[t], w_diag, w_uni]\n        \n        # Grid Search for Basins\n        best_mse = np.inf\n        best_p0 = None\n        \n        # Bias: Asymptotic loss is usually bounded by observed min loss\n        bias_grid = [max(0, y_min - 0.01), max(0, y_min - 0.5), 0.0]\n        # Alpha: Standard scaling exponents\n        alpha_grid = [0.1, 0.5, 1.0]\n        \n        for w_base in candidates:\n            # Pre-calc effective data for this weight vector\n            w_sq = w_base**2\n            d_eff = np.dot(X, w_sq)\n            d_eff = np.maximum(d_eff, 1e-10)\n            \n            for b_val in bias_grid:\n                for a_val in alpha_grid:\n                    pred = b_val + d_eff**(-a_val)\n                    mse = np.mean((pred - y_tgt)**2)\n                    \n                    if mse < best_mse:\n                        best_mse = mse\n                        best_p0 = np.concatenate(([b_val, np.log(a_val)], w_base))\n                        \n        # --- 3. Optimization ---\n        def objective(p):\n            b = p[0]\n            log_a = np.clip(p[1], -5.0, 3.0)\n            a = np.exp(log_a)\n            w = p[2:]**2\n            \n            d = np.dot(X, w)\n            d = np.maximum(d, 1e-10)\n            pred = b + d**(-a)\n            \n            mse = np.mean((pred - y_tgt)**2)\n            \n            # Penalties / Priors\n            pen = 0.0\n            # Soft constraint: Bias should be effectively positive\n            if b < 0: pen += 100.0 * b**2\n            # Soft constraint: Bias shouldn't exceed min observed loss too much\n            if b > y_min: pen += 10.0 * (b - y_min)**2\n            \n            # L2 Regularization on weights to improve convexity\n            reg = 1e-7 * np.sum(w**2)\n            \n            return mse + pen + reg\n\n        try:\n            # BFGS is efficient and robust for this smooth problem\n            res = minimize(objective, best_p0, method='BFGS', tol=1e-6)\n            final_params.append(res.x)\n        except Exception:\n            final_params.append(best_p0)\n            \n    return np.concatenate(final_params)\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.989802, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nImplements an Effective Data Mixture scaling law with L1-regularized weights.\nModel: L_i = c_i + ( sum_j (w_{ij}^2) * x_j ) ^ -exp(log_alpha_i)\nImprovements:\n- L1 regularization on effective weights (sparsity inducing)\n- Soft penalty for negative bias to maintain physical plausibility\n- Multi-start optimization with BFGS to avoid local minima\nTotal params: 5 domains * 7 params/domain = 35 params.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params)\n    \n    # Ensure parameter array size matches expectation\n    expected_size = 35\n    if params.size < expected_size:\n        params = np.pad(params, (0, expected_size - params.size), 'constant')\n    elif params.size > expected_size:\n        params = params[:expected_size]\n        \n    P = params.reshape(5, 7)\n    \n    # Parameters\n    # c: bias\n    # alpha: power law exponent (enforced positive via exp)\n    # W: mixing weights (enforced positive via square)\n    \n    c = P[:, 0]\n    alpha = np.exp(P[:, 1])\n    W = P[:, 2:]**2\n    \n    # Effective data: D = X @ W.T\n    # D shape: (N, 5)\n    D_eff = X @ W.T\n    \n    # Power law term: D^-alpha\n    # Numerical stability: max(D, 1e-10)\n    term = np.maximum(D_eff, 1e-10) ** (-alpha[None, :])\n    \n    pred = c[None, :] + term\n    \n    if pred.shape[1] == 1:\n        return pred[:, 0]\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    if Y.ndim == 1: Y = Y[:, None]\n    if Y.shape[0] != X.shape[0]: Y = Y.T\n    \n    _, n_dims = X.shape\n    _, n_domains = Y.shape\n    \n    final_params = []\n    \n    for t in range(n_domains):\n        y_t = Y[:, t]\n        y_min = np.min(y_t)\n        \n        # Objective function\n        def objective(p):\n            c = p[0]\n            log_a = p[1]\n            w_raw = p[2:]\n            \n            # Derived parameters\n            alpha = np.exp(log_a)\n            w = w_raw**2\n            \n            # Prediction\n            d = X @ w\n            pred = c + np.maximum(d, 1e-10)**(-alpha)\n            \n            mse = np.mean((pred - y_t)**2)\n            \n            # Regularization\n            # 1. Sparsity on effective weights (L1 on W <=> L2 on w_raw)\n            # This encourages selecting only relevant domains\n            reg_w = 1e-6 * np.sum(w)\n            \n            # 2. Soft penalty for negative bias\n            # Allows transient negative exploration but penalizes final solution\n            pen_c = 0.0\n            if c < 0:\n                pen_c = 100.0 * c**2\n            \n            # 3. Soft penalty for extreme alpha\n            # log_alpha should be in reasonable range [-5, 3]\n            pen_a = 0.0\n            if log_a < -5: pen_a += (log_a + 5)**2\n            if log_a > 3: pen_a += (log_a - 3)**2\n                \n            return mse + reg_w + pen_c + pen_a\n\n        # Initialization Strategies\n        candidates = []\n        \n        # Strategy 1: \"Standard\" \n        # Bias near minimum, modest diagonal dominance\n        c1 = max(0.0, y_min - 0.2)\n        w1 = np.full(n_dims, 0.2)\n        if t < n_dims: w1[t] = 1.0 # Self-weight 1.0, others 0.04\n        p1 = np.concatenate(([c1, 0.0], np.sqrt(w1)))\n        candidates.append(p1)\n        \n        # Strategy 2: \"Low Bias\"\n        # Bias very low, relying more on power law (good for steep curves)\n        c2 = max(0.0, y_min - 1.0)\n        w2 = np.full(n_dims, 0.3)\n        if t < n_dims: w2[t] = 1.2\n        p2 = np.concatenate(([c2, np.log(0.5)], np.sqrt(w2)))\n        candidates.append(p2)\n        \n        # Optimization\n        best_p = None\n        best_loss = np.inf\n        \n        for p0 in candidates:\n            try:\n                res = minimize(objective, p0, method='BFGS', tol=1e-6, options={'maxiter': 500})\n                if res.fun < best_loss:\n                    best_loss = res.fun\n                    best_p = res.x\n            except Exception:\n                continue\n                \n        if best_p is None:\n            best_p = candidates[0]\n            \n        final_params.append(best_p)\n        \n    return np.concatenate(final_params)\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.989715, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nImproved model using Effective Data Mixture formulation.\nModels loss as L_t = bias_t + (sum_j (w_{tj}^2 * x_j))^(-alpha_t)\nUses 7 parameters per domain * 5 domains = 35 parameters total.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) domain proportions\n    # params: (35,) flat array representing parameters for 5 domains\n    \n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    params = np.asarray(params, dtype=float)\n    \n    n_domains = 5\n    n_params_per_domain = 7\n    expected_size = n_domains * n_params_per_domain\n    \n    # Robustly handle parameter shape\n    # If parameters are passed from a different sized model or messed up, we handle it\n    if params.size != expected_size:\n        flat_params = params.ravel()\n        if flat_params.size < expected_size:\n            # Pad with sensible defaults if too small\n            # bias=2.0, alpha=0.5, weights=0.1\n            padding = np.tile(np.array([2.0, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1]), n_domains)\n            flat_params = np.concatenate([flat_params, padding])[:expected_size]\n        else:\n            flat_params = flat_params[:expected_size]\n        P = flat_params.reshape(n_domains, n_params_per_domain)\n    else:\n        P = params.reshape(n_domains, n_params_per_domain)\n        \n    bias = P[:, 0]          # (5,)\n    alpha = P[:, 1]         # (5,)\n    weights = P[:, 2:]      # (5, 5)\n    \n    # Weights squared for positivity (learns magnitude of contribution)\n    W = weights ** 2        # (5, 5)\n    # Alpha positive for power law decay\n    A = np.abs(alpha)       # (5,)\n    \n    # Compute effective data: D_eff[n, t] = sum_j (X[n, j] * W[t, j])\n    # This represents the \"equivalent dataset size\" for domain t derived from the mixture\n    D_eff = np.dot(X, W.T)\n    \n    # Stability: prevent division by zero or effectively zero data\n    D_eff = np.maximum(D_eff, 1e-6)\n    \n    # Power law: D_eff ^ -alpha\n    # Broadcasting: (N, 5) ^ (1, 5)\n    power_term = D_eff ** (-A[None, :])\n    \n    pred = bias[None, :] + power_term\n    \n    if pred.shape[1] == 1:\n        return pred[:, 0]\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    Y = np.asarray(loss_values, dtype=float)\n    \n    if Y.ndim == 1:\n        Y = Y[:, None]\n        \n    n_domains = 5\n    n_features = X.shape[1]\n    \n    all_params = []\n    \n    # Fit each domain independently for stability\n    for t in range(n_domains):\n        y_target = Y[:, t]\n        min_y = np.min(y_target)\n        \n        # Objective: MSE\n        def objective(p):\n            b = p[0]\n            a = p[1]\n            w = p[2:]\n            \n            w_sq = w ** 2\n            d_eff = np.dot(X, w_sq)\n            d_eff = np.maximum(d_eff, 1e-6)\n            \n            pred = b + d_eff ** (-a)\n            return np.mean((pred - y_target)**2)\n            \n        # Initialization\n        # Bias is the asymptotic floor, likely slightly below min observed loss\n        bias_init = min_y - 0.1\n        # Alpha is the scaling exponent, typically 0.1-1.0\n        alpha_init = 0.5\n        # Weights: assume diagonal dominance (domain helps itself most)\n        w_init = np.full(n_features, np.sqrt(0.1))\n        if t < n_features:\n            w_init[t] = 1.0\n            \n        p0 = np.concatenate(([bias_init, alpha_init], w_init))\n        \n        # Bounds: \n        # Bias < min_y to prevent unphysical negative power terms\n        # Alpha > 0\n        bounds = [(None, min_y - 1e-4), (1e-2, 5.0)] + [(None, None)] * n_features\n        \n        try:\n            res = minimize(objective, p0, method='L-BFGS-B', bounds=bounds)\n            p_opt = res.x\n        except:\n            # Fallback to initialization if optimization fails\n            p_opt = p0\n            \n        all_params.append(p_opt)\n        \n    # Concatenate all parameters into a flat array of 35 (5 domains * 7 params)\n    return np.concatenate(all_params)\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-3-pro-preview", "reward_r2": 0.9894232423883015, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Fitted parameters for each group\n    # Model: L_i = A * (sum_j T_ij * p_j)^(-alpha) + C\n    PARAMS = {\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.4238451169174666,\n      \"alpha\": 0.09398513070233284,\n      \"C\": 1.1250493207278296,\n      \"T\": [\n        1.0,\n        0.0028653843430912034,\n        0.0,\n        0.006560914041313572,\n        0.007218091947616502\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.6028897145876133,\n      \"alpha\": 0.18249317423847652,\n      \"C\": 1.7098198790278034,\n      \"T\": [\n        0.13978044088307642,\n        1.0,\n        0.2888140021128257,\n        0.09238120511024654,\n        0.2702341876935682\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.3944974988308458,\n      \"alpha\": 0.07536985492525757,\n      \"C\": 1.4540031052227687,\n      \"T\": [\n        0.00042972756094378624,\n        0.004428392340713848,\n        1.0,\n        0.0015108566548276629,\n        0.004618708293329572\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.7156208007948365,\n      \"alpha\": 0.14819891555573603,\n      \"C\": 0.7188126729944071,\n      \"T\": [\n        0.004709723482930698,\n        0.0,\n        0.007159224662733469,\n        1.0,\n        0.006354517927118956\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.6903071983073346,\n      \"alpha\": 0.07574314873996338,\n      \"C\": 1.7459964167978976,\n      \"T\": [\n        0.0,\n        0.1294716976819937,\n        0.09092332659047013,\n        0.014567119409274644,\n        1.0\n      ]\n    }\n  },\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.167794250478621,\n      \"alpha\": 0.09893031045785339,\n      \"C\": 1.0967302669578558,\n      \"T\": [\n        1.0,\n        0.0,\n        0.0,\n        0.004702404391421327,\n        0.006374420575153449\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.520792165137527,\n      \"alpha\": 0.19100210139002716,\n      \"C\": 1.4717790036178258,\n      \"T\": [\n        0.14117673959388272,\n        1.0,\n        0.30984366873545993,\n        0.09323203270576641,\n        0.2930905074144338\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.1827621056082325,\n      \"alpha\": 0.08515955574666216,\n      \"C\": 1.3810689151399624,\n      \"T\": [\n        0.0004496159377778205,\n        0.005469210248664284,\n        1.0,\n        0.00014486145067144012,\n        0.005546525753692289\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.5950466323744031,\n      \"alpha\": 0.15657006642589474,\n      \"C\": 0.6274992666731508,\n      \"T\": [\n        0.0038211623746128476,\n        0.0,\n        0.004047765748103023,\n        1.0,\n        0.006932201209717277\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.5374892790861532,\n      \"alpha\": 0.08558831045269366,\n      \"C\": 1.546810504279026,\n      \"T\": [\n        0.010196510229647623,\n        0.04817027475788599,\n        0.0331566521815975,\n        0.07622206331237742,\n        1.0\n      ]\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.0636633714879822,\n      \"alpha\": 0.1022580547558815,\n      \"C\": 1.0643051121057456,\n      \"T\": [\n        1.0,\n        0.0020628543899588276,\n        0.0,\n        0.005055971810713113,\n        0.005517181906203229\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.5091813171571034,\n      \"alpha\": 0.21086460000369067,\n      \"C\": 1.317332273323408,\n      \"T\": [\n        0.1761437051777317,\n        1.0,\n        0.328313250063059,\n        0.12228845673566739,\n        0.3242993594399039\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.2930356452032414,\n      \"alpha\": 0.06407715343277973,\n      \"C\": 1.1862706116692965,\n      \"T\": [\n        0.00010662729851481288,\n        0.0016648522472207873,\n        1.0,\n        0.0,\n        0.002186394372089585\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.5311234622127226,\n      \"alpha\": 0.16396855241073996,\n      \"C\": 0.5956951080126597,\n      \"T\": [\n        0.0007484858832986562,\n        0.0,\n        0.0020986448436579866,\n        1.0,\n        0.0074776280923507365\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.4576330665204935,\n      \"alpha\": 0.08684367375738934,\n      \"C\": 1.4616723046222218,\n      \"T\": [\n        0.0,\n        0.04878929729092485,\n        0.027998535239229693,\n        0.07915718776179,\n        1.0\n      ]\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.0716651256430547,\n      \"alpha\": 0.0979215135041499,\n      \"C\": 1.0023484878829527,\n      \"T\": [\n        1.0,\n        0.0,\n        0.0,\n        0.003578328625604031,\n        0.00520215592199023\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.4082722528529894,\n      \"alpha\": 0.21260676990383326,\n      \"C\": 1.3641437773794949,\n      \"T\": [\n        0.16418103324714028,\n        1.0,\n        0.37481870992512745,\n        0.09587346431048725,\n        0.32578174634303375\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.3149711554737487,\n      \"alpha\": 0.062094311988635076,\n      \"C\": 1.1032193950712292,\n      \"T\": [\n        0.0,\n        0.001564258864235631,\n        1.0,\n        8.56970983439856e-05,\n        0.0018986297882148968\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.49842678668584145,\n      \"alpha\": 0.1778525216820956,\n      \"C\": 0.5822681110448701,\n      \"T\": [\n        0.0060231123550343905,\n        0.0007592880970631944,\n        0.0026719690465761368,\n        1.0,\n        0.0086213744689824\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 3.233311673093248,\n      \"alpha\": 0.04148885832669186,\n      \"C\": -0.39223243148328407,\n      \"T\": [\n        0.01291802719802647,\n        0.0,\n        0.3021193873339115,\n        0.0,\n        1.0\n      ]\n    }\n  }\n}\n\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n        \n    group_params = PARAMS[group]\n    predictions = []\n    \n    for item in input_data:\n        pred_item = {}\n        \n        # Extract proportions vector p\n        # Assuming keys are 'proportion_domain_1' to 'proportion_domain_5'\n        p = np.array([\n            item.get('proportion_domain_1', 0.0),\n            item.get('proportion_domain_2', 0.0),\n            item.get('proportion_domain_3', 0.0),\n            item.get('proportion_domain_4', 0.0),\n            item.get('proportion_domain_5', 0.0)\n        ])\n        \n        # Compute loss for each domain\n        for i in range(1, 6):\n            domain_key = f\"loss_domain_{i}\"\n            if domain_key in group_params:\n                params = group_params[domain_key]\n                A = params[\"A\"]\n                alpha = params[\"alpha\"]\n                C = params[\"C\"]\n                T = np.array(params[\"T\"])\n                \n                # Effective proportion\n                p_eff = np.dot(p, T)\n                # Handle effectively zero\n                p_eff = max(p_eff, 1e-9)\n                \n                loss_pred = A * (p_eff ** -alpha) + C\n                pred_item[domain_key] = loss_pred\n                \n        predictions.append(pred_item)\n        \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "opencode", "model_name": "gpt-5", "reward_r2": 0.9894230209470385, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import List, Dict\n\n# Fixed feature ordering\nPROP_COLS = [\n    \"proportion_domain_1\",\n    \"proportion_domain_2\",\n    \"proportion_domain_3\",\n    \"proportion_domain_4\",\n    \"proportion_domain_5\",\n]\nLOSS_COLS = [\n    \"loss_domain_1\",\n    \"loss_domain_2\",\n    \"loss_domain_3\",\n    \"loss_domain_4\",\n    \"loss_domain_5\",\n]\n\nEPS = 1e-6\n\n# Coefficients learned per group for the law:\n# loss_i = intercept[i] + sum_j coef_linear[i][j] * p_j + sum_j coef_log[i][j] * log(p_j + EPS)\nCOEFFS: Dict[str, Dict[str, list]] = {\n    \"160M\": {\n        \"intercept\": [\n            2.469311683337708,\n            3.3141620411008277,\n            2.5975875154705848,\n            1.3440867180535057,\n            3.2488739962835567,\n        ],\n        \"coef_linear\": [\n            [-0.39242870031019833, 0.1449840040105368, 0.20870621607378334, 0.012988956774962533, 0.02574952345091407],\n            [0.2073851738757125, -0.5034944849958123, 0.08099673956846185, 0.2119097462425706, 0.0032028253090607807],\n            [0.4222507474605135, 0.33048349239799873, -1.3184032511886987, 0.3397661471062194, 0.22590286422400102],\n            [0.1075280031010361, 0.3278752202366596, 0.018326424467131473, -0.5403846909284411, 0.08665504312361116],\n            [0.1224578633506513, -0.06992992306569604, 0.0648541522733341, 0.08654508830086936, -0.20392718085915945],\n        ],\n        \"coef_log\": [\n            [-0.039451752022555374, -0.0003854857497984469, -2.3239743517545694e-05, 9.268231255609287e-06, -0.0006293642768779598],\n            [-0.0015843455126219829, -0.00597505925571199, -0.00010878237745062993, -0.0007202157067082326, -0.0012285972839189082],\n            [-0.0009734332588850447, -0.001936822498506686, -0.027443305577813045, -0.00024645647285300213, -0.00019926772803499236],\n            [-0.0006024744943890134, -0.002147785787884586, 0.001399812773972361, -0.036472059131277504, 0.00012750772191223904],\n            [-0.001567815576140436, 0.0013055621917748808, 0.0002487312848513498, -0.0008614874408401778, -0.019870896443806487],\n        ],\n    },\n    \"305M\": {\n        \"intercept\": [\n            2.3392247012746834,\n            3.1651345666056483,\n            2.471987105632863,\n            1.2404678308980266,\n            3.0887017193916093,\n        ],\n        \"coef_linear\": [\n            [-0.3945995646360234, 0.04212797569256443, 0.3597852823915539, 0.004507385349434609, -0.011821078797535627],\n            [0.18765244585849242, -0.5607080638027755, 0.16385032928665508, 0.22772936777546302, -0.018524079117843765],\n            [0.36498559947643294, 0.36326950855260254, -1.247281529045098, 0.3544474379638183, 0.16457898305227767],\n            [0.11479489142933053, 0.2241274675743544, 0.07534052854957383, -0.4984903999878992, 0.08422751243463553],\n            [0.1034302676552572, -0.1442429588936119, 0.1542111292796102, 0.10260754874495605, -0.2160059867862149],\n        ],\n        \"coef_log\": [\n            [-0.0389843240976756, 0.0003898475662999871, -0.0012326552175473988, -0.0008170951320506675, -0.0006305864869774297],\n            [-0.0018382099319297328, -0.004966654576883016, -0.0008004862412949112, -0.0016726743862113481, -0.0014239105552697226],\n            [-0.0013605116194238868, -0.0029875971857020777, -0.029138080972677064, -0.0016317402099057068, 0.001163162472447215],\n            [-0.0011594434613557832, -0.0010890215347730992, 0.0008814829783619934, -0.035207303872518685, 0.00014797726343401387],\n            [-0.0015994149367300917, 0.002183961698325075, -0.0005510279268070304, -0.00175006550618083, -0.020723679414693993],\n        ],\n    },\n    \"410M\": {\n        \"intercept\": [\n            2.2845576475924543,\n            3.10221083581893,\n            2.4040537489237623,\n            1.2320388989073703,\n            3.0194029194493215,\n        ],\n        \"coef_linear\": [\n            [-0.40161868178180443, 0.04851096556048266, 0.37552617435827934, -0.007771674366947659, -0.014646783770016363],\n            [0.16564665878501697, -0.5418667012877614, 0.19196166965559713, 0.21461472108487065, -0.030356348237732297],\n            [0.3827566078799856, 0.34563333912754424, -1.207292188578679, 0.2962894338651403, 0.18261280770604105],\n            [0.054055822096378214, 0.18257490953749397, 0.25515869822947196, -0.5390509589695227, 0.04726152910616942],\n            [0.08444528786706501, -0.1235789613695045, 0.1680640099151793, 0.09513264245956578, -0.22406297887230905],\n        ],\n        \"coef_log\": [\n            [-0.03838578515244451, 0.0010474524802569906, -0.0020612475600514644, 0.0001902706294946067, -0.0012861227733191377],\n            [-0.0012829345286925373, -0.004688819508647834, -0.0016346691987556602, -0.0009769593878491815, -0.002091652532429498],\n            [-0.0012514981651361474, -0.0022513525226212174, -0.03034764820962916, -0.00021614146982995423, -0.00017869542804964955],\n            [-0.0010235048622236945, -7.088093693356411e-05, -0.0007898197931760238, -0.033703719578066345, -0.0007674227907133403],\n            [-0.001315884970387432, 0.0024028568720025913, -0.0013333912060313298, -0.0011109928776418308, -0.021811398074324508],\n        ],\n    },\n    \"70M\": {\n        \"intercept\": [\n            2.7857859114105317,\n            3.631804815517477,\n            2.8681805224896912,\n            1.5890762093073625,\n            3.585150303901379,\n        ],\n        \"coef_linear\": [\n            [-0.4416445868640977, 0.07076027431299382, 0.302121611856706, 0.021520016473356454, 0.04724268422103695],\n            [0.21834311045644716, -0.553392474651338, 0.04818153280501451, 0.24033451148528265, 0.04653331990458732],\n            [0.4651202418236946, 0.2657143234624336, -1.3670739924126776, 0.36683688818700816, 0.2694025389395758],\n            [0.0850252828881378, 0.2990024146623459, 0.16822113994472984, -0.6239023579571726, 0.07165352046195206],\n            [0.14123915472576296, -0.13497556494772603, 0.03364295958750099, 0.12770730941446795, -0.16761385878000656],\n        ],\n        \"coef_log\": [\n            [-0.041246477328631105, 0.0006531144880363961, -0.0006596475145338669, -0.00019599814522888677, -0.0015631188541267603],\n            [-0.0009803943328683558, -0.005672467237098692, -8.71136475631502e-05, -0.0009074144501494191, -0.0019413115294711764],\n            [-0.0006290227608540234, -0.0005120063062147314, -0.02905249764872596, -0.0007835652353273532, -0.000662736071510837],\n            [-0.0008408582373940847, -0.0019902435925866755, 0.00039188729846795716, -0.0409361614036341, -0.0005053380487605633],\n            [-0.0009055762689076869, 0.0025986071507895507, 0.00015711172405491434, -0.0011903203768379186, -0.019717110434476673],\n        ],\n    },\n}\n\n\ndef _predict_point(p: Dict[str, float], coeff: Dict[str, list]) -> Dict[str, float]:\n    # Build feature vectors in fixed order\n    P = [float(p.get(k, 0.0)) for k in PROP_COLS]\n    logP = [math.log(x + EPS) for x in P]\n\n    y = []\n    for i in range(5):\n        val = coeff[\"intercept\"][i]\n        # linear terms\n        for j in range(5):\n            val += coeff[\"coef_linear\"][i][j] * P[j]\n        # log terms\n        for j in range(5):\n            val += coeff[\"coef_log\"][i][j] * logP[j]\n        y.append(val)\n\n    return {LOSS_COLS[i]: y[i] for i in range(5)}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in COEFFS:\n        # Fallback: use the average of available groups if unknown label is passed\n        # This keeps functional form identical while remaining robust.\n        # Precompute simple average coefficients on the fly.\n        groups = list(COEFFS.values())\n        avg = {\n            \"intercept\": [sum(g[\"intercept\"][i] for g in groups) / len(groups) for i in range(5)],\n            \"coef_linear\": [\n                [sum(g[\"coef_linear\"][i][j] for g in groups) / len(groups) for j in range(5)]\n                for i in range(5)\n            ],\n            \"coef_log\": [\n                [sum(g[\"coef_log\"][i][j] for g in groups) / len(groups) for j in range(5)]\n                for i in range(5)\n            ],\n        }\n        coeff = avg\n    else:\n        coeff = COEFFS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        outputs.append(_predict_point(row, coeff))\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.9894, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nImplements an Effective Data Mixture scaling law.\nModel: Loss_t = Bias_t + (Sum_j (W_tj^2 * x_j))^(-exp(alpha_t))\nThis captures how data from different domains contributes to the effective training size for a target domain.\nUses exactly 35 parameters (7 per domain * 5 domains).\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) array of domain proportions\n    # params: (35,) flat array or (5, 7) array\n    \n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params)\n    \n    # Parameter unpacking and shape handling\n    # We use 7 parameters per domain: 1 bias, 1 log_alpha, 5 weights\n    params_per_domain = 7\n    \n    if params.ndim == 1:\n        # Infer number of domains from parameter size\n        # Expected size is 35 for 5 domains\n        if params.size % params_per_domain == 0:\n            T = params.size // params_per_domain\n            P = params.reshape(T, params_per_domain)\n        else:\n            # Fallback for mismatched size, possibly just return bias or crash safely\n            # Assuming 5 domains if size is weird but we want to try\n            T = 5\n            # Resize or truncate would be dangerous, just let reshape fail if needed\n            P = params.reshape(T, params_per_domain) \n    else:\n        P = params\n        T = P.shape[0]\n\n    # Extract parameters\n    # P structure: [bias, log_alpha, w0_sqrt, w1_sqrt, w2_sqrt, w3_sqrt, w4_sqrt]\n    bias = P[:, 0]              # (T,)\n    log_alpha = P[:, 1]         # (T,)\n    w_sqrt = P[:, 2:]           # (T, F) - F is number of weight params, should match X.shape[1]\n    \n    # Enforce constraints via transforms\n    # Alpha must be positive. exp() ensures this. Clip to avoid overflow.\n    alpha = np.exp(np.clip(log_alpha, -5.0, 4.0)) \n    # Weights must be positive. Square of parameter ensures this.\n    weights = w_sqrt ** 2       \n    \n    # Compute Effective Data for each target domain\n    # D_eff = X @ W.T\n    # X: (N, 5), Weights: (T, 5) -> D_eff: (N, T)\n    # If dimensions mismatch (e.g. testing with fewer features), handle gracefully\n    if X.shape[1] != weights.shape[1]:\n        F_in = min(X.shape[1], weights.shape[1])\n        D_eff = X[:, :F_in] @ weights[:, :F_in].T\n    else:\n        D_eff = X @ weights.T\n    \n    # Numerical stability: Effective data cannot be 0 or negative\n    D_eff = np.maximum(D_eff, 1e-8)\n    \n    # Power law term: (D_eff) ^ (-alpha)\n    term = D_eff ** (-alpha[None, :])\n    \n    # Final prediction: bias + term\n    pred = bias[None, :] + term\n    \n    # Return shape handling\n    if pred.shape[1] == 1:\n        return pred[:, 0]\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.asarray(loss_values)\n    if Y.ndim == 1:\n        Y = Y[:, None]\n        \n    N, F = X.shape\n    _, T = Y.shape\n    \n    # Model uses 7 params per output domain: bias (1), log_alpha (1), weights (5)\n    # Total params = 7 * T. For T=5, Total=35.\n    \n    all_params = []\n    \n    for t in range(T):\n        y_target = Y[:, t]\n        \n        # Heuristic Initialization\n        # Bias: Irreducible loss. Start near minimum observed loss.\n        bias_init = np.min(y_target) - 0.2\n        \n        # Alpha: Scaling exponent. Start with typical value ~0.5 (log_alpha ~ -0.7)\n        log_alpha_init = -0.7\n        \n        # Weights: Start with diagonal dominance (self-transfer is best)\n        # sqrt(1.0) = 1.0, sqrt(0.1) ~ 0.316\n        w_init = np.full(F, 0.32)\n        if t < F:\n            w_init[t] = 1.0\n            \n        p0 = np.concatenate(([bias_init, log_alpha_init], w_init))\n        \n        def objective(p):\n            # Decode params\n            b = p[0]\n            log_a = np.clip(p[1], -5.0, 4.0)\n            a = np.exp(log_a)\n            w = p[2:] ** 2\n            \n            # Compute effective data\n            d = X @ w\n            d = np.maximum(d, 1e-8)\n            \n            # Predict\n            pred = b + d**(-a)\n            \n            # MSE Loss\n            return np.mean((pred - y_target)**2)\n        \n        # Optimize\n        # BFGS is robust enough for this unconstrained formulation (due to transforms)\n        try:\n            res = minimize(objective, p0, method='BFGS')\n            best_p = res.x\n        except Exception:\n            # Fallback to init if optimization crashes\n            best_p = p0\n            \n        all_params.append(best_p)\n    \n    # Return flattened parameters array (size 35)\n    return np.concatenate(all_params)\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gemini-3-pro-preview", "reward_r2": 0.989378, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nImplements an Effective Data Mixture scaling law:\nL_i = bias_i + (sum_j (w_{ij}^2 * x_j))^(-exp(alpha_i))\nThis models how mixing data from different domains contributes to reducing loss in a specific domain.\nUses exactly 35 parameters (7 per domain * 5 domains).\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) array of domain proportions\n    # params: (35,) array of parameters\n    \n    # Ensure inputs are correct type and shape\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    params = np.asarray(params, dtype=np.float64).ravel()\n    \n    N, F = X.shape\n    num_domains = 5\n    params_per_domain = 7\n    \n    # Reshape parameters\n    # Expected size is 35 (5 domains * 7 params).\n    expected = num_domains * params_per_domain\n    if params.size < expected:\n        params = np.pad(params, (0, expected - params.size), 'constant')\n    elif params.size > expected:\n        params = params[:expected]\n        \n    P = params.reshape(num_domains, params_per_domain)\n    \n    # Extract parameter components\n    # col 0: bias\n    # col 1: log_alpha (exponent)\n    # col 2-6: raw weights for the 5 input features\n    \n    bias = P[:, 0]              # (5,)\n    alpha = np.exp(P[:, 1])     # (5,) - ensure positive exponent\n    weights = P[:, 2:] ** 2     # (5, 5) - ensure positive contribution\n    \n    # Calculate Effective Data\n    # D_eff_i = sum_j (w_ij^2 * x_j)\n    # X: (N, 5), W^T: (5, 5) -> D_eff: (N, 5)\n    D_eff = X @ weights.T\n    \n    # Avoid numerical instability with power law near 0\n    D_eff = np.maximum(D_eff, 1e-6)\n    \n    # Calculate Power Law Term\n    # term_i = D_eff_i ^ (-alpha_i)\n    # Broadcasting: (N, 5) ^ (1, 5) -> (N, 5)\n    power_term = D_eff ** (-alpha[None, :])\n    \n    # Final Prediction\n    pred = bias[None, :] + power_term\n    \n    # Match output requirements\n    if pred.shape[1] == 1:\n        return pred[:, 0]\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    Y = np.asarray(loss_values, dtype=np.float64)\n    if Y.ndim == 1:\n        Y = Y[:, None]\n        \n    N, F = X.shape\n    _, num_domains = Y.shape\n    \n    all_params = []\n    \n    # Fit each domain independently\n    for t in range(num_domains):\n        y_tgt = Y[:, t]\n        \n        # Heuristic Initialization\n        min_y = np.min(y_tgt)\n        bias_init = max(0.0, min_y - 0.2)\n        log_alpha_init = np.log(0.5) # Start with sqrt decay\n        \n        # Initialize weights\n        # Domain i usually learns best from data i, so diagonal should be larger\n        w_init = np.full(F, np.sqrt(0.1)) \n        if t < F:\n            w_init[t] = 1.0 \n            \n        p0 = np.concatenate(([bias_init, log_alpha_init], w_init))\n        \n        def objective(p):\n            b = p[0]\n            # Constrain alpha to reasonable range to prevent overflow\n            a = np.exp(np.clip(p[1], -3.0, 3.0)) \n            w = p[2:] ** 2\n            \n            d = X @ w\n            d = np.maximum(d, 1e-6)\n            pred = b + d**(-a)\n            \n            return np.mean((pred - y_tgt)**2)\n        \n        try:\n            res = minimize(objective, p0, method='BFGS', tol=1e-5)\n            best_p = res.x\n        except Exception:\n            best_p = p0\n            \n        all_params.append(best_p)\n        \n    return np.concatenate(all_params)\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.989346, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Bounded exponent mapping for numerical stability\n_LO, _HI = 0.2, 2.5\n_EPS = 1e-8\n_LN5 = np.log(5.0)\n\ndef _sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef _map_exp(q):\n    return _LO + (_HI - _LO) * _sigmoid(q)\n\ndef _inv_map_exp(x):\n    t = (float(x) - _LO) / (_HI - _LO)\n    t = np.clip(t, 1e-6, 1 - 1e-6)\n    return np.log(t / (1 - t))\n\ndef _features(X, gp, aS, bS, kap, go, T):\n    p = np.clip(np.asarray(X, float), 0.0, 1.0)\n    N, F = p.shape\n    T = int(min(T, F))\n    pj = p[:, :T]\n\n    # Cross-domain statistics (normalized for scale stability)\n    sA = (np.sum(p ** aS, axis=1, keepdims=True) / max(F, 1))  # normalized generalized Herfindahl\n    Hn = (-np.sum(p * np.log(p + _EPS), axis=1, keepdims=True) / _LN5)  # normalized entropy in [0,1+]\n\n    # Per-output feature blocks (6 per domain, param-efficient and expressive)\n    X0 = np.ones((N, T))                                        # intercept\n    X1 = pj ** gp                                               # self-mass exponent\n    X2 = np.log(pj + _EPS) - np.log(1.0 - pj + _EPS)           # logit(p_j) for edge sensitivity\n    X3 = (sA ** bS).repeat(T, axis=1)                           # global concentration (adaptive power)\n    X4 = (np.maximum(Hn, 0.0) ** kap).repeat(T, axis=1)         # global diversity (adaptive power)\n    X5 = (1.0 - pj) ** go                                       # other-mass exponent\n    return X0, X1, X2, X3, X4, X5\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points, float))\n    q = np.asarray(params, float).ravel()\n    if q.size < 5 + 6:\n        return np.full((X.shape[0], 5), 3.0)\n\n    gp = _map_exp(q[0]); aS = _map_exp(q[1]); bS = _map_exp(q[2]); kap = _map_exp(q[3]); go = _map_exp(q[4])\n    rem = q.size - 5\n    if rem % 6 != 0:\n        rem = (rem // 6) * 6\n    T = max(1, min(5, rem // 6))\n    W = q[5:5 + 6 * T].reshape(T, 6)\n\n    X0, X1, X2, X3, X4, X5 = _features(X, gp, aS, bS, kap, go, T)\n    pred = (W[:, 0][None, :] * X0 + W[:, 1][None, :] * X1 + W[:, 2][None, :] * X2 +\n            W[:, 3][None, :] * X3 + W[:, 4][None, :] * X4 + W[:, 5][None, :] * X5)\n    if pred.shape[1] < 5:\n        pred = np.concatenate([pred, np.repeat(pred[:, [-1]], 5 - pred.shape[1], axis=1)], axis=1)\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, float))\n    Y = np.asarray(loss_values, float)\n    Y2 = Y if Y.ndim == 2 else Y[:, None]\n    N, F = X.shape\n    T = min(Y2.shape[1], F, 5)\n\n    lam = 1e-4 + 1e-2 / max(N, 1)  # small adaptive ridge on linear heads\n\n    def solve_W(gp, aS, bS, kap, go):\n        X0, X1, X2, X3, X4, X5 = _features(X, gp, aS, bS, kap, go, T)\n        W = np.zeros((T, 6))\n        reg = np.array([0.0, lam, lam, lam, lam, lam])         # leave intercept unregularized\n        D = np.sqrt(reg); Dmat = np.diag(D); Dmat[0, 0] = 0.0\n        zeros = np.zeros(6)\n        for j in range(T):\n            A = np.column_stack([X0[:, j], X1[:, j], X2[:, j], X3[:, j], X4[:, j], X5[:, j]])\n            A_aug = np.vstack([A, Dmat])\n            b_aug = np.concatenate([Y2[:, j], zeros])\n            W[j], *_ = np.linalg.lstsq(A_aug, b_aug, rcond=None)\n        return W\n\n    # Gentle priors to avoid pathological exponents: center near (gp=1, aS=2, bS=1, kap=1, go=1)\n    reg_theta = 1e-4\n    def obj(theta_raw):\n        gp = _map_exp(theta_raw[0]); aS = _map_exp(theta_raw[1]); bS = _map_exp(theta_raw[2])\n        kap = _map_exp(theta_raw[3]); go = _map_exp(theta_raw[4])\n        W = solve_W(gp, aS, bS, kap, go)\n        X0, X1, X2, X3, X4, X5 = _features(X, gp, aS, bS, kap, go, T)\n        pred = (W[:, 0][None, :] * X0 + W[:, 1][None, :] * X1 + W[:, 2][None, :] * X2 +\n                W[:, 3][None, :] * X3 + W[:, 4][None, :] * X4 + W[:, 5][None, :] * X5)\n        err = pred - Y2[:, :T]\n        pen = reg_theta * ((gp - 1.0) ** 2 + (aS - 2.0) ** 2 + (bS - 1.0) ** 2 + (kap - 1.0) ** 2 + (go - 1.0) ** 2)\n        return float(np.mean(err * err) + pen)\n\n    starts_vals = [\n        (0.9, 2.0, 1.0, 1.0, 1.0),\n        (1.1, 1.8, 0.8, 0.9, 1.2),\n        (0.8, 2.2, 1.2, 1.1, 0.9),\n        (1.2, 1.6, 1.3, 0.8, 1.3),\n        (1.0, 2.0, 1.0, 1.2, 1.0),\n        (0.7, 2.3, 0.9, 1.1, 0.8),\n        (1.3, 1.7, 1.1, 0.9, 1.1)\n    ]\n    starts = [np.array([_inv_map_exp(a), _inv_map_exp(b), _inv_map_exp(c), _inv_map_exp(d), _inv_map_exp(e)])\n              for a, b, c, d, e in starts_vals]\n\n    best_x, best_val = None, np.inf\n    for s in starts:\n        try:\n            res = minimize(obj, s, method=\"L-BFGS-B\")\n            val = res.fun if res.success else obj(s)\n            if val < best_val:\n                best_val, best_x = val, (res.x if res.success else s)\n        except Exception:\n            continue\n    if best_x is None:\n        best_x = np.array([_inv_map_exp(1.0), _inv_map_exp(2.0), _inv_map_exp(1.0), _inv_map_exp(1.0), _inv_map_exp(1.0)])\n\n    gp = _map_exp(best_x[0]); aS = _map_exp(best_x[1]); bS = _map_exp(best_x[2]); kap = _map_exp(best_x[3]); go = _map_exp(best_x[4])\n    W = solve_W(gp, aS, bS, kap, go)\n    return np.concatenate([[_inv_map_exp(gp), _inv_map_exp(aS), _inv_map_exp(bS), _inv_map_exp(kap), _inv_map_exp(go)], W.reshape(-1)])\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.988685, "solution": "import numpy as np\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses from 5-d mixture proportions using:\n      y_j = sum_i W[j,i] * p_i  +  gamma[j] * sqrt(p_j)  +  b[j]\n    where we pack parameters as:\n      - W: 5×5 weight matrix       (25 params)\n      - gamma: 5-d vector          ( 5 params)\n      - b: 5-d bias                ( 5 params)\n    Total = 35 parameters.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    N, F = X.shape\n    assert F == 5, \"Expected 5 mixture proportions\"\n\n    p = np.asarray(params, dtype=float).ravel()\n    assert p.size == 35, f\"Expected 35 parameters, got {p.size}\"\n\n    # unpack\n    W_lin = p[:25].reshape(5, 5)\n    gamma = p[25:30]\n    b     = p[30:35]\n\n    # linear part\n    lin = X.dot(W_lin.T)               # (N,5)\n    # non-linear sqrt effect per domain\n    sqrt_eff = np.sqrt(X) * gamma      # broadcast to (N,5)\n\n    return lin + sqrt_eff + b\n# EVOLVE-BLOCK-END\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the above 35-param model with a two-stage procedure:\n      1) initialize by small‐ridge per-domain least squares\n      2) refine each domain via IRLS with Huber-like weights\n    This yields robustness to outliers and tighter fit.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float)\n    N, F = X.shape\n    assert F == 5, \"Expected 5 mixture proportions\"\n    if y.ndim == 1:\n        y = y[:, None]\n    assert y.shape == (N, 5), \"Expected loss_values shape (N,5)\"\n\n    # parameters to estimate\n    W_lin = np.zeros((5, 5), dtype=float)\n    gamma = np.zeros(5, dtype=float)\n    b     = np.zeros(5, dtype=float)\n\n    # ridge factor for stability\n    lam = 1e-6 * N\n    # IRLS iterations\n    max_iter = 5\n\n    # fit each domain separately\n    for j in range(5):\n        # build design: [p1..p5, sqrt(p_j), 1]\n        sqrt_col = np.sqrt(X[:, j:j+1])               # (N,1)\n        Phi = np.hstack((X, sqrt_col, np.ones((N, 1))))  # (N,7)\n        yj  = y[:, j]\n\n        # 1) initial ridge LS\n        G = Phi.T.dot(Phi) + lam * np.eye(Phi.shape[1])\n        v = Phi.T.dot(yj)\n        theta = np.linalg.solve(G, v)\n\n        # 2) IRLS refinement (Huber‐style weights)\n        for _ in range(max_iter):\n            resid = Phi.dot(theta) - yj\n            med   = np.median(resid)\n            mad   = np.median(np.abs(resid - med)) + 1e-6\n            k     = 1.345 * mad\n            # Huber weights: w=1 if |r|<=k else k/|r|\n            absr = np.abs(resid)\n            w    = np.where(absr <= k, 1.0, k / absr)\n\n            # weighted normal equations\n            Wphi = Phi * w[:, None]\n            Gw   = Wphi.T.dot(Phi) + lam * np.eye(Phi.shape[1])\n            vw   = Wphi.T.dot(yj)\n            theta = np.linalg.solve(Gw, vw)\n\n        # unpack domain params\n        W_lin[j, :] = theta[:5]\n        gamma[j]    = theta[5]\n        b[j]        = theta[6]\n\n    # flatten into length‐35 vector\n    return np.concatenate((W_lin.ravel(), gamma, b))"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.986123, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nAdvanced scaling law for multi-domain LLM finetuning\nUses shared exponent with per-domain adjustments, additive entropy regularization,\nand learned cross-domain interactions. 32 parameters with improved per-domain control.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Scaling law with shared exponent, per-domain refinements, and entropy regularization.\n    \n    Parameters (32 total):\n    - [0:5]: base coefficients (per-domain)\n    - [5:10]: exponent offsets (per-domain adjustment to shared base)\n    - [10:15]: interaction strengths (cross-domain coupling)\n    - [15:20]: domain bias terms\n    - [20:25]: per-domain quadratic regularization (smoothness)\n    - [25]: shared exponent base\n    - [26]: global scale factor\n    - [27]: entropy weight (additive contribution)\n    - [28]: entropy strength coefficient\n    - [29]: interaction regularization strength\n    - [30]: entropy modulation type (blend between additive and multiplicative)\n    - [31]: per-domain bias scaling\n    \n    Total: 32 parameters (well under 35 limit)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64).ravel()\n    \n    eps = 1e-10\n    \n    # Parse parameters\n    base_coeffs = params[0:5]        # (5,) - per-domain base coefficients\n    exp_offsets = params[5:10]       # (5,) - per-domain exponent adjustments\n    interactions = params[10:15]     # (5,) - cross-domain coupling strength\n    domain_bias = params[15:20]      # (5,) - per-domain bias\n    quad_regs = params[20:25]        # (5,) - per-domain quadratic regularization\n    shared_exp_base = params[25]     # scalar - base exponent (shared)\n    global_scale = params[26]        # scalar - global scaling factor\n    entropy_weight = params[27]      # scalar - entropy contribution weight\n    entropy_strength = params[28]    # scalar - entropy effect magnitude\n    interact_reg = params[29]        # scalar - interaction regularization\n    entropy_blend = params[30]       # scalar - blend between additive/multiplicative\n    bias_scale = params[31]          # scalar - per-domain bias scaling factor\n    \n    # Constrain shared exponent to [0.4, 1.6] via sigmoid\n    shared_exp = 0.4 + 1.2 / (1.0 + np.exp(-np.clip(shared_exp_base, -5, 5)))\n    \n    # Constrain per-domain exponent offsets to [-0.35, 0.35]\n    exp_adjust = 0.35 * np.tanh(np.clip(exp_offsets, -3, 3))\n    \n    # Per-domain exponents with controlled variation\n    exponents = np.clip(shared_exp + exp_adjust, 0.3, 2.0)  # (5,)\n    \n    # Compute entropy of mixture: Shannon entropy normalized to [0, 1]\n    X_safe = np.maximum(X, eps)\n    entropy = -np.sum(X_safe * np.log(X_safe), axis=1)  # (N,)\n    max_entropy = np.log(F)\n    entropy_norm = entropy / max_entropy  # (N,) in [0, 1]\n    \n    # Entropy modulation: blend between additive and multiplicative\n    # When blend=0: pure additive, when blend=1: pure multiplicative\n    blend = 0.5 * (1.0 + np.tanh(np.clip(entropy_blend, -2, 2)))\n    entropy_mod = blend * np.exp(entropy_strength * (entropy_norm - 0.5)) + (1.0 - blend) * 1.0\n    \n    pred = np.zeros((N, F))\n    \n    for d in range(F):\n        # Base power law term: base_coeffs[d] * X[:, d] ^ exponents[d]\n        X_safe_d = np.maximum(X[:, d], eps)\n        base_term = base_coeffs[d] * (X_safe_d ** exponents[d])\n        \n        # Cross-domain interaction: weighted pairwise interactions\n        interaction_term = np.zeros(N)\n        for other_d in range(F):\n            if other_d != d:\n                # Pairwise interaction strength scaled by both proportions\n                interaction_term += interactions[d] * X[:, other_d] * X[:, d]\n        \n        # Apply interaction regularization\n        interaction_term = interact_reg * interaction_term\n        \n        # Per-domain quadratic regularization term for smoothness\n        quad_term = quad_regs[d] * (X[:, d] ** 2)\n        \n        # Entropy contribution: additive entropy effect\n        entropy_term = entropy_weight * entropy_strength * entropy_norm\n        \n        # Combine all components with entropy modulation\n        pred[:, d] = global_scale * entropy_mod * (base_term + interaction_term + quad_term + \n                                                     entropy_term) + bias_scale * domain_bias[d]\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit scaling law using robust initialization and multi-start L-BFGS-B optimization.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))  # (N, 5)\n    \n    # Ensure y is (N, 5)\n    if y.ndim == 1:\n        y = y[:, None]\n    if y.shape[0] != X.shape[0]:\n        y = y.T\n    \n    N, F = X.shape\n    n_domains = y.shape[1]\n    n_params = 32\n    \n    # ===== Robust Initialization =====\n    init_params = np.zeros(n_params)\n    \n    # Statistics from data\n    y_mean = np.mean(y, axis=0)           # (5,)\n    y_std = np.std(y, axis=0) + 1e-8      # (5,)\n    \n    # Initialize base coefficients from log-scale regression\n    for d in range(min(n_domains, 5)):\n        y_d = y[:, d]\n        x_d_safe = np.maximum(X[:, d], 1e-8)\n        try:\n            # Log-scale fitting for better initialization\n            log_y_d = np.log(np.maximum(y_d, 1e-8))\n            log_x_d = np.log(x_d_safe)\n            A = np.column_stack([log_x_d, np.ones(N)])\n            coeffs = np.linalg.lstsq(A, log_y_d, rcond=None)[0]\n            init_params[d] = np.exp(coeffs[1])\n            init_params[5 + d] = 0.08 * (coeffs[0] - 0.8)  # Offset from shared exp\n        except:\n            init_params[d] = y_mean[d] / (np.mean(x_d_safe ** 0.7) + 1e-8)\n            init_params[5 + d] = 0.0\n    \n    # Initialize exponent offsets to small values\n    init_params[5:10] = np.clip(init_params[5:10], -0.8, 0.8)\n    \n    # Initialize interactions: small values for coupling\n    init_params[10:15] = 0.004 * np.ones(5)\n    \n    # Initialize domain biases\n    init_params[15:20] = y_mean if len(y_mean) >= 5 else np.mean(y)\n    \n    # Initialize per-domain quadratic regularization: small positive values\n    init_params[20:25] = 0.0008 * np.ones(5)\n    \n    # Initialize global parameters\n    init_params[25] = 0.3  # shared_exp_base (maps to ~0.8 via sigmoid)\n    init_params[26] = 0.2  # global_scale\n    init_params[27] = 0.05  # entropy_weight\n    init_params[28] = 0.15  # entropy_strength\n    init_params[29] = 0.4  # interact_reg (moderate coupling)\n    init_params[30] = 0.1  # entropy_blend (mostly additive initially)\n    init_params[31] = 1.0  # bias_scale\n    \n    # ===== Define Objective Function =====\n    def objective(params):\n        try:\n            pred = scaling_law_func(X, params)\n            \n            # Ensure shapes match\n            if pred.ndim == 1:\n                pred = pred[:, None]\n            \n            # MSE loss\n            mse = np.mean((pred - y) ** 2)\n            \n            # Structured L2 regularization\n            reg_coeff = 0.0007 * np.sum(params[0:5] ** 2)\n            reg_interaction = 0.0002 * np.sum(params[10:15] ** 2)\n            reg_quad = 0.0001 * np.sum(params[20:25] ** 2)\n            \n            total_loss = mse + reg_coeff + reg_interaction + reg_quad\n            \n            if not np.isfinite(total_loss):\n                return 1e10\n            \n            return total_loss\n        except (ValueError, RuntimeWarning, FloatingPointError, OverflowError):\n            return 1e10\n    \n    # ===== Define Bounds =====\n    bounds = [\n        # base_coeffs[0:5]\n        (0.05, 15.0), (0.05, 15.0), (0.05, 15.0), (0.05, 15.0), (0.05, 15.0),\n        # exp_offsets[5:10]\n        (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0),\n        # interactions[10:15]\n        (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0),\n        # domain_bias[15:20]\n        (0.2, 5.5), (0.2, 5.5), (0.2, 5.5), (0.2, 5.5), (0.2, 5.5),\n        # quad_regs[20:25]\n        (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2),\n        # shared_exp_base[25]\n        (-5.0, 5.0),\n        # global_scale[26]\n        (-5.0, 5.0),\n        # entropy_weight[27]\n        (-2.0, 2.0),\n        # entropy_strength[28]\n        (-2.0, 2.0),\n        # interact_reg[29]\n        (-2.0, 2.0),\n        # entropy_blend[30]\n        (-2.0, 2.0),\n        # bias_scale[31]\n        (0.1, 2.0),\n    ]\n    \n    # ===== Multi-Start Optimization =====\n    best_result = None\n    best_loss = float('inf')\n    \n    # Primary optimization with careful initialization\n    result = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2500, 'ftol': 1e-9, 'gtol': 1e-8}\n    )\n    \n    if result.fun < best_loss:\n        best_loss = result.fun\n        best_result = result.x\n    \n    # Secondary optimization with small perturbation\n    perturbed = init_params + 0.12 * np.random.randn(n_params)\n    for i, (lower, upper) in enumerate(bounds):\n        perturbed[i] = np.clip(perturbed[i], lower, upper)\n    \n    result2 = minimize(\n        objective,\n        perturbed,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2000, 'ftol': 1e-9, 'gtol': 1e-8}\n    )\n    \n    if result2.fun < best_loss:\n        best_loss = result2.fun\n        best_result = result2.x\n    \n    # Tertiary optimization with larger perturbation\n    perturbed2 = init_params + 0.28 * np.random.randn(n_params)\n    for i, (lower, upper) in enumerate(bounds):\n        perturbed2[i] = np.clip(perturbed2[i], lower, upper)\n    \n    result3 = minimize(\n        objective,\n        perturbed2,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 1800, 'ftol': 1e-9}\n    )\n    \n    if result3.fun < best_loss:\n        best_loss = result3.fun\n        best_result = result3.x\n    \n    return best_result if best_result is not None else init_params\n\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.982148, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nOptimized scaling law for multi-domain LLM finetuning\nRefined hierarchical mixture model with improved entropy regularization and adaptive bounds\nFocus: Better per-domain accuracy, improved numerical stability, simplified code\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses using refined hierarchical mixture model\n    \n    Model: loss_t = base_t + sum_d(w_td * x_d^exp_d) + interact_t * entropy\n    Uses shared exponents with adaptive domain selection\n    \n    Args:\n        data_points: (N, 5) array of domain proportions\n        params: Flattened array of 35 parameters\n    \n    Returns:\n        (N, 5) predictions for N mixtures across 5 domains\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    N, F = X.shape\n    params = np.atleast_1d(np.asarray(params, dtype=np.float64))\n    \n    n_exp, n_base, n_weights = 5, 5, 20\n    \n    shared_exp = np.clip(params[:n_exp], 0.3, 3.0)\n    base_losses = np.clip(params[n_exp:n_exp+n_base], 0.9, 4.9)\n    weights = np.clip(params[n_exp+n_base:n_exp+n_base+n_weights].reshape(5, 4), -2.0, 2.0)\n    interact_coeff = np.clip(params[n_exp+n_base+n_weights:], -1.2, 1.2)\n    \n    predictions = np.zeros((N, F))\n    X_safe = np.clip(X, 1e-7, 1.0 - 1e-7)\n    \n    # Precompute entropy-based diversity (improved from concentration)\n    # Use Shannon entropy proxy: -sum(x*log(x)) but simplified\n    entropy = -np.sum(X * np.log(X_safe + 1e-10), axis=1)  # (N,)\n    entropy = np.clip((entropy - 0.6) / 1.2, -1.0, 1.0)  # Normalize to ~[-1, 1]\n    \n    for t in range(F):\n        pred = base_losses[t] * np.ones(N)\n        \n        # Adaptive domain selection: prioritize self, then neighbors\n        main_domains = [t, (t+1) % 5, (t-1) % 5, (t+2) % 5]\n        \n        for idx, d in enumerate(main_domains):\n            w = weights[t, idx]\n            exp_val = shared_exp[d]\n            pred += w * (X_safe[:, d] ** exp_val)\n        \n        # Entropy-based interaction (more principled than diversity)\n        pred += interact_coeff[t] * entropy\n        \n        predictions[:, t] = np.clip(pred, 0.7, 5.3)\n    \n    return predictions\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit scaling law parameters with improved initialization and adaptive regularization\n    \n    Args:\n        data_points: (N, 5) domain proportions\n        loss_values: (N, 5) observed losses\n    \n    Returns:\n        Optimized parameter vector (35 parameters)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.asarray(loss_values, dtype=np.float64)\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N, F = X.shape\n    T = y.shape[1]\n    n_exp, n_base, n_weights, n_interact = 5, 5, 20, 5\n    total = n_exp + n_base + n_weights + n_interact\n    \n    init_params = np.zeros(total)\n    \n    # Better exponent initialization: use percentile-based log-log regression\n    for d in range(F):\n        x_d = X[:, d]\n        y_d = np.mean(y, axis=1)\n        \n        # Use robust percentile filtering\n        p10, p90 = np.percentile(x_d, 10), np.percentile(x_d, 90)\n        valid_mask = (x_d > p10) & (x_d < p90) & (y_d > 1.5)\n        \n        if np.sum(valid_mask) > 4:\n            x_log = np.log(np.clip(x_d[valid_mask], 1e-6, 1.0))\n            y_log = np.log(np.clip(y_d[valid_mask], 1.0, 5.2))\n            coeff = np.polyfit(x_log, y_log, 1)[0]\n            init_params[d] = np.clip(coeff, 0.3, 3.0)\n        else:\n            init_params[d] = 1.2\n    \n    # Initialize base losses with per-domain refinement\n    for t in range(T):\n        y_t = y[:, t]\n        # Use robust median + small correction based on variance\n        init_params[n_exp + t] = np.median(y_t) + 0.05 * np.std(y_t)\n        init_params[n_exp + t] = np.clip(init_params[n_exp + t], 0.9, 4.9)\n    \n    # Initialize weights with adaptive scaling\n    w_idx = n_exp + n_base\n    for t in range(F):\n        main_domains = [t, (t+1) % 5, (t-1) % 5, (t+2) % 5]\n        for idx, d in enumerate(main_domains):\n            if idx == 0:\n                init_params[w_idx + t*4 + idx] = 0.4\n            elif idx == 1:\n                init_params[w_idx + t*4 + idx] = 0.15\n            else:\n                init_params[w_idx + t*4 + idx] = 0.06\n    \n    # Initialize interaction coefficients based on entropy range\n    interact_idx = n_exp + n_base + n_weights\n    init_params[interact_idx:interact_idx+5] = 0.12\n    \n    def objective(flat_params):\n        \"\"\"MSE with domain-aware adaptive regularization\"\"\"\n        pred = scaling_law_func(X, flat_params)\n        mse = np.mean((pred - y) ** 2)\n        \n        # Compute per-domain MSE to guide regularization\n        mse_per_dim = np.mean((pred - y) ** 2, axis=0)\n        \n        # Higher regularization on high-error domains\n        error_weights = np.clip(mse_per_dim / np.mean(mse_per_dim), 0.5, 2.0)\n        \n        shared_exp = flat_params[:n_exp]\n        weights = flat_params[n_exp+n_base:n_exp+n_base+n_weights]\n        interact = flat_params[n_exp+n_base+n_weights:]\n        base = flat_params[n_exp:n_exp+n_base]\n        \n        # Domain-weighted regularization\n        reg_base = 0.00015 * np.sum((base - init_params[n_exp:n_exp+n_base]) ** 2 * error_weights)\n        reg_exp = 0.00008 * np.sum((shared_exp - 1.0) ** 2)\n        reg_weights = 0.00012 * np.sum(weights ** 2)\n        reg_interact = 0.0002 * np.sum(interact ** 2)\n        \n        return mse + reg_base + reg_exp + reg_weights + reg_interact\n    \n    bounds = []\n    bounds.extend([(0.3, 3.0)] * n_exp)\n    bounds.extend([(0.9, 4.9)] * n_base)\n    bounds.extend([(-2.0, 2.0)] * n_weights)\n    bounds.extend([(-1.2, 1.2)] * n_interact)\n    \n    result = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={\n            'maxiter': 2500,\n            'ftol': 1e-11,\n            'gtol': 1e-10,\n            'maxcor': 30\n        }\n    )\n    \n    return result.x if result.success else init_params\n\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "o4-mini", "reward_r2": 0.9747667014004453, "solution": "from typing import List, Dict\n\n# Parameters for scaling law: loss(p) = c - a * p**b\n_PARAMS: Dict[str, Dict[int, Dict[str, float]]] = {\n    '70M': {\n        1: {'a': 0.9228, 'b': 0.2453, 'c': 3.4149},\n        2: {'a': 0.3726, 'b': 0.5065, 'c': 3.8184},\n        3: {'a': 0.7930, 'b': 0.2212, 'c': 3.6006},\n        4: {'a': 0.9436, 'b': 0.2406, 'c': 2.2663},\n        5: {'a': 0.5175, 'b': 0.3754, 'c': 3.9317},\n    },\n    '160M': {\n        1: {'a': 0.8432, 'b': 0.2285, 'c': 3.0604},\n        2: {'a': 0.3059, 'b': 0.4616, 'c': 3.4721},\n        3: {'a': 0.7277, 'b': 0.2081, 'c': 3.2856},\n        4: {'a': 0.8371, 'b': 0.2382, 'c': 1.9631},\n        5: {'a': 0.5291, 'b': 0.3623, 'c': 3.5949},\n    },\n    '305M': {\n        1: {'a': 0.8159, 'b': 0.2234, 'c': 2.8980},\n        2: {'a': 0.4262, 'b': 0.6940, 'c': 3.3062},\n        3: {'a': 0.7023, 'b': 0.1831, 'c': 3.1556},\n        4: {'a': 0.7988, 'b': 0.2365, 'c': 1.8330},\n        5: {'a': 0.5343, 'b': 0.3516, 'c': 3.4344},\n    },\n    '410M': {\n        1: {'a': 0.7997, 'b': 0.2158, 'c': 2.8319},\n        2: {'a': 0.3518, 'b': 0.6247, 'c': 3.2303},\n        3: {'a': 0.7099, 'b': 0.1805, 'c': 3.0983},\n        4: {'a': 0.7849, 'b': 0.2413, 'c': 1.7794},\n        5: {'a': 0.5501, 'b': 0.3404, 'c': 3.3746},\n    },\n}\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups,\n               but the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) under keys\n        'loss_domain_i'.\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n    group_params = _PARAMS[group]\n    results: List[Dict[str, float]] = []\n    # Compute prediction for each data point\n    for entry in input_data:\n        preds: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = entry.get(f'proportion_domain_{i}')\n            if p is None:\n                raise KeyError(f\"Missing proportion_domain_{i} in input data\")\n            a = group_params[i]['a']\n            b = group_params[i]['b']\n            c = group_params[i]['c']\n            # scaling law: loss = c - a * p**b\n            preds[f'loss_domain_{i}'] = c - a * (p ** b)\n        results.append(preds)\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "o4-mini", "reward_r2": 0.974745, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts validation losses for each domain based on mixture proportions.\n\n    Uses a power-law decrease model per domain and group:\n        loss = L0 - alpha * proportion**beta\n\n    Args:\n        input_data: list of dicts with keys 'proportion_domain_i' for i in 1..5.\n        group: experimental group name ('70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_i' predictions.\n    \"\"\"\n    # Pre-fitted parameters per group and domain\n    params = {\n        '70M': {\n            'L0':   [3.415, 3.818, 3.601, 2.266, 3.932],\n            'alpha':[0.923, 0.373, 0.793, 0.944, 0.518],\n            'beta': [0.245, 0.506, 0.221, 0.241, 0.375],\n        },\n        '160M': {\n            'L0':   [3.060, 3.472, 3.286, 1.963, 3.595],\n            'alpha':[0.843, 0.306, 0.728, 0.837, 0.529],\n            'beta': [0.229, 0.462, 0.208, 0.238, 0.362],\n        },\n        '305M': {\n            'L0':   [2.898, 3.306, 3.156, 1.833, 3.434],\n            'alpha':[0.816, 0.426, 0.702, 0.799, 0.534],\n            'beta': [0.223, 0.694, 0.183, 0.236, 0.352],\n        },\n        '410M': {\n            'L0':   [2.832, 3.230, 3.098, 1.779, 3.375],\n            'alpha':[0.800, 0.352, 0.710, 0.785, 0.550],\n            'beta': [0.216, 0.625, 0.181, 0.241, 0.340],\n        },\n    }\n    if group not in params:\n        raise ValueError(f\"Unknown group '{group}'\")\n    gp = params[group]\n    results: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = row.get(f'proportion_domain_{i}', 0.0)\n            L0 = gp['L0'][i-1]\n            a = gp['alpha'][i-1]\n            b = gp['beta'][i-1]\n            pred[f'loss_domain_{i}'] = L0 - a * (p ** b)\n        results.append(pred)\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.974583, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses (5 outputs) from 5-domain mixture proportions\n    using a shared‐exponent power transform + per‐domain linear weights + bias.\n    Total params = 5 alphas + 5×5 weights + 5 biases = 35.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    if X.ndim == 1:\n        X = X[None, :]\n    N, F = X.shape\n    assert F == 5, f\"Expected 5 mixture proportions, got {F}\"\n    p = params.ravel()\n    # unpack\n    alphas = p[0:F]                          # (5,)\n    W_flat = p[F:F + F*F]                   # (25,)\n    bias   = p[F + F*F:F + F*F + F]         # (5,)\n    W = W_flat.reshape(5, 5)                # (5 outputs × 5 features)\n    # safe power transform (handles zeros via small epsilon)\n    eps = 1e-8\n    G = np.exp(np.log(X + eps) * alphas)    # (N,5)\n    # linear mixing + bias\n    preds = G.dot(W.T) + bias[None, :]\n    return preds\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35 parameters by minimizing a regularized MSE over all 5 domains.\n    Returns optimized flat parameter vector of length 35.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)\n    y = np.asarray(loss_values, dtype=float)\n    if X.ndim == 1:\n        X = X[None, :]\n    if y.ndim == 1:\n        y = y[None, :]\n    N, F = X.shape\n    T = y.shape[1]\n    assert F == 5 and T == 5, \"Expected 5 mixture proportions and 5 loss dims\"\n    # initialize: alphas=1, W=identity, bias=mean per-domain loss\n    alphas0 = np.ones(F, dtype=float)\n    W0 = np.eye(T, F, dtype=float)\n    bias0 = np.mean(y, axis=0)\n    init_params = np.concatenate([alphas0, W0.ravel(), bias0])\n    # bounds: alphas in [0,5], weights in [-5,5], bias free\n    bounds = [(0.0, 5.0)] * F + [(-5.0, 5.0)] * (F*F) + [(None, None)] * T\n    # small L2 regularization for stability\n    reg_alpha = 1e-3\n    reg_W     = 1e-3\n\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse  = np.mean((pred - y)**2)\n        a = p[:F]\n        w = p[F:F + F*F]\n        # encourage alphas ≈1 and weights ≈0\n        penalty = reg_alpha * np.sum((a - 1.0)**2) + reg_W * np.sum(w**2)\n        return mse + penalty\n\n    res = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2000, 'ftol': 1e-9}\n    )\n    return res.x if res.success else init_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-sonnet-4-5", "reward_r2": 0.9715978378387945, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each domain and group\n    # Formula: loss_domain_i = L_inf_i - a_i * (proportion_domain_i)^b_i\n    params = {\n        1: {\n            '70M': {'L_inf': 3.414908, 'a': 0.919426, 'b': 0.242859},\n            '160M': {'L_inf': 3.060407, 'a': 0.841478, 'b': 0.227141},\n            '305M': {'L_inf': 2.898031, 'a': 0.812498, 'b': 0.220675},\n            '410M': {'L_inf': 2.831881, 'a': 0.797224, 'b': 0.213714}\n        },\n        2: {\n            '70M': {'L_inf': 3.818429, 'a': 0.286092, 'b': 0.325881},\n            '160M': {'L_inf': 3.472137, 'a': 0.229089, 'b': 0.253344},\n            '305M': {'L_inf': 3.306184, 'a': 0.231382, 'b': 0.286684},\n            '410M': {'L_inf': 3.230276, 'a': 0.194197, 'b': 0.211294}\n        },\n        3: {\n            '70M': {'L_inf': 3.600640, 'a': 0.884553, 'b': 0.258117},\n            '160M': {'L_inf': 3.285555, 'a': 0.821255, 'b': 0.248967},\n            '305M': {'L_inf': 3.155623, 'a': 0.780708, 'b': 0.218846},\n            '410M': {'L_inf': 3.098252, 'a': 0.789583, 'b': 0.216252}\n        },\n        4: {\n            '70M': {'L_inf': 2.266335, 'a': 0.933792, 'b': 0.235431},\n            '160M': {'L_inf': 1.963058, 'a': 0.833632, 'b': 0.236056},\n            '305M': {'L_inf': 1.832974, 'a': 0.793849, 'b': 0.233302},\n            '410M': {'L_inf': 1.779367, 'a': 0.778080, 'b': 0.236914}\n        },\n        5: {\n            '70M': {'L_inf': 3.931742, 'a': 0.511339, 'b': 0.352486},\n            '160M': {'L_inf': 3.594913, 'a': 0.526879, 'b': 0.354306},\n            '305M': {'L_inf': 3.434413, 'a': 0.530724, 'b': 0.338928},\n            '410M': {'L_inf': 3.374611, 'a': 0.548299, 'b': 0.334021}\n        }\n    }\n\n    # Process each data point\n    results = []\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain_i in range(1, 6):\n            prop_key = f'proportion_domain_{domain_i}'\n            loss_key = f'loss_domain_{domain_i}'\n\n            # Get the proportion for this domain\n            proportion = data_point.get(prop_key, 0.0)\n\n            # Get parameters for this domain and group\n            domain_params = params[domain_i][group]\n            L_inf = domain_params['L_inf']\n            a = domain_params['a']\n            b = domain_params['b']\n\n            # Apply the scaling law: loss = L_inf - a * p^b\n            predicted_loss = L_inf - a * (proportion ** b)\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "o4-mini", "reward_r2": 0.971579, "solution": "from typing import List, Dict\n\n# Precomputed parameters for the scaling law per group and domain\n_PARAMS: Dict[str, Dict[str, Dict[str, float]]] = {\n    '70M': {\n        'domain_1': {'a': -0.9196, 'b': 0.2428, 'c': 3.4151},\n        'domain_2': {'a': -0.2861, 'b': 0.3259, 'c': 3.8184},\n        'domain_3': {'a': -0.8845, 'b': 0.2583, 'c': 3.6004},\n        'domain_4': {'a': -0.9339, 'b': 0.2354, 'c': 2.2665},\n        'domain_5': {'a': -0.5144, 'b': 0.3494, 'c': 3.9349},\n    },\n    '160M': {\n        'domain_1': {'a': -0.8416, 'b': 0.2271, 'c': 3.0605},\n        'domain_2': {'a': -0.2291, 'b': 0.2533, 'c': 3.4721},\n        'domain_3': {'a': -0.8212, 'b': 0.2491, 'c': 3.2853},\n        'domain_4': {'a': -0.8337, 'b': 0.2360, 'c': 1.9631},\n        'domain_5': {'a': -0.5281, 'b': 0.3531, 'c': 3.5961},\n    },\n    '305M': {\n        'domain_1': {'a': -0.8126, 'b': 0.2206, 'c': 2.8982},\n        'domain_2': {'a': -0.2314, 'b': 0.2867, 'c': 3.3062},\n        'domain_3': {'a': -0.7806, 'b': 0.2189, 'c': 3.1554},\n        'domain_4': {'a': -0.7939, 'b': 0.2333, 'c': 1.8330},\n        'domain_5': {'a': -0.5326, 'b': 0.3372, 'c': 3.4364},\n    },\n    '410M': {\n        'domain_1': {'a': -0.7973, 'b': 0.2137, 'c': 2.8320},\n        'domain_2': {'a': -0.1942, 'b': 0.2113, 'c': 3.2303},\n        'domain_3': {'a': -0.7895, 'b': 0.2163, 'c': 3.0981},\n        'domain_4': {'a': -0.7782, 'b': 0.2369, 'c': 1.7795},\n        'domain_5': {'a': -0.5493, 'b': 0.3332, 'c': 3.3756},\n    },\n}\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts per-domain validation loss based on mixture proportions using a power-law model.\n\n    The functional form is:\n        loss_domain_i = a * (proportion_domain_i ** b) + c\n\n    Args:\n        input_data: List of data points, each with keys 'proportion_domain_i'.\n        group: Experimental group key, must be one of the precomputed groups.\n\n    Returns:\n        List of dictionaries with predicted 'loss_domain_i' for each input point.\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}', available: {list(_PARAMS.keys())}\")\n    params_group = _PARAMS[group]\n    predictions: List[Dict[str, float]] = []\n    for point in input_data:\n        pred_point: Dict[str, float] = {}\n        for i in range(1, 6):\n            key_p = f'proportion_domain_{i}'\n            key_l = f'loss_domain_{i}'\n            p = point.get(key_p, 0.0)\n            a = params_group[f'domain_{i}']['a']\n            b = params_group[f'domain_{i}']['b']\n            c = params_group[f'domain_{i}']['c']\n            pred_point[key_l] = a * (p ** b) + c\n        predictions.append(pred_point)\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5", "reward_r2": 0.9714759455458236, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Law: For each domain i in {1..5},\n        loss_domain_i = a_{group,i} + b_{group,i} * (proportion_domain_i) ** alpha_i\n    where the exponent alpha_i is domain-specific but shared across groups, and\n    (a_{group,i}, b_{group,i}) are fitted per group.\n    \"\"\"\n    # Domain-specific exponents shared across groups (fitted once)\n    alphas = {1: 0.226, 2: 0.272, 3: 0.236, 4: 0.235, 5: 0.343}\n\n    # Per-group coefficients a and b for each domain (fitted from the provided dataset)\n    coeffs = {\n        \"160M\": {\n            1: {\"a\": 3.0607589078884847, \"b\": -0.8406224674207222},\n            2: {\"a\": 3.471957561424479,  \"b\": -0.23709796451470122},\n            3: {\"a\": 3.2856010648519973, \"b\": -0.7919275425273328},\n            4: {\"a\": 1.9632078046951371, \"b\": -0.8321226336323998},\n            5: {\"a\": 3.600060737641489,  \"b\": -0.5302231304455584},\n        },\n        \"305M\": {\n            1: {\"a\": 2.896951436073815,  \"b\": -0.8170959564908562},\n            2: {\"a\": 3.306317389829822,  \"b\": -0.22521283957225652},\n            3: {\"a\": 3.155092174041798,  \"b\": -0.8182930011802386},\n            4: {\"a\": 1.8328824818924194, \"b\": -0.7963908513267552},\n            5: {\"a\": 3.4340665068448346, \"b\": -0.5313252100720468},\n        },\n        \"410M\": {\n            1: {\"a\": 2.8291888357597386, \"b\": -0.8073757705491997},\n            2: {\"a\": 3.2297361776335225, \"b\": -0.21719584738930717},\n            3: {\"a\": 3.097659192469288,  \"b\": -0.8335641687702692},\n            4: {\"a\": 1.779637332326639,  \"b\": -0.775555774148788},\n            5: {\"a\": 3.371561997175875,  \"b\": -0.5469883726664775},\n        },\n        \"70M\": {\n            1: {\"a\": 3.4193040905517047, \"b\": -0.9041352514360005},\n            2: {\"a\": 3.8189889954933474, \"b\": -0.25910738407437617},\n            3: {\"a\": 3.600895922417036,  \"b\": -0.8317098214628572},\n            4: {\"a\": 2.266520379741139,  \"b\": -0.9332890679011832},\n            5: {\"a\": 3.937342662537917,  \"b\": -0.5157344418970146},\n        },\n    }\n\n    # Fallback: if an unknown group is provided, use the average coefficients across known groups\n    if group not in coeffs:\n        groups = list(coeffs.keys())\n        avg = {}\n        for i in range(1, 6):\n            a_vals = [coeffs[g][i][\"a\"] for g in groups]\n            b_vals = [coeffs[g][i][\"b\"] for g in groups]\n            avg[i] = {\"a\": sum(a_vals) / len(a_vals), \"b\": sum(b_vals) / len(b_vals)}\n        coeffs[group] = avg\n\n    out = []\n    for row in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            a = coeffs[group][i][\"a\"]\n            b = coeffs[group][i][\"b\"]\n            alpha = alphas[i]\n            pred[f\"loss_domain_{i}\"] = a + b * (p ** alpha)\n        out.append(pred)\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "gpt-5", "reward_r2": 0.971446, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Shared per-domain offset to make log well-defined at zero proportion.\n# Selected via cross-group grid search to maximize average R^2.\n_C_BY_DOMAIN: Dict[int, float] = {\n    1: 0.003125,\n    2: 0.0046875,\n    3: 0.0015625,\n    4: 0.003125,\n    5: 0.0234375,\n}\n\n# Per-group, per-domain coefficients for the law:\n#   loss_domain_i = a[g,i] + b[g,i] * log(proportion_domain_i + C[i])\n_COEFS_BY_GROUP: Dict[str, Dict[int, Dict[str, float]]] = {\n    \"160M\": {\n        1: {\"a\": 2.2424684059708717, \"b\": -0.1412039367794934},\n        2: {\"a\": 3.2541456054570035, \"b\": -0.0405858087415962},\n        3: {\"a\": 2.567113771294527, \"b\": -0.11102183824132524},\n        4: {\"a\": 1.167057464525046, \"b\": -0.1374903829439023},\n        5: {\"a\": 3.085896059150068, \"b\": -0.13693966564752397},\n    },\n    \"305M\": {\n        1: {\"a\": 2.101566717029658, \"b\": -0.137250480526824},\n        2: {\"a\": 3.0994046220334157, \"b\": -0.03855781421113323},\n        3: {\"a\": 2.4128405334920684, \"b\": -0.11468007010082179},\n        4: {\"a\": 1.0709224798061698, \"b\": -0.13158540389099785},\n        5: {\"a\": 2.918730301818813, \"b\": -0.13736039699677574},\n    },\n    \"410M\": {\n        1: {\"a\": 2.0433633841009002, \"b\": -0.1355817799554127},\n        2: {\"a\": 3.0302311023376722, \"b\": -0.03717104079844086},\n        3: {\"a\": 2.341569758388262, \"b\": -0.11681678106085443},\n        4: {\"a\": 1.0375379829435523, \"b\": -0.1281673168777551},\n        5: {\"a\": 2.841139982418687, \"b\": -0.14126939233946334},\n    },\n    \"70M\": {\n        1: {\"a\": 2.538957210154492, \"b\": -0.15195781604593908},\n        2: {\"a\": 3.5809102439444924, \"b\": -0.044369404452315006},\n        3: {\"a\": 2.8462143926943795, \"b\": -0.11662362761003567},\n        4: {\"a\": 1.3734832511282675, \"b\": -0.15423705944877764},\n        5: {\"a\": 3.4370247988143974, \"b\": -0.13346648082051366},\n    },\n}\n\n# Fallback coefficients (mean across groups) used if an unknown group is provided.\n_FALLBACK_COEFS: Dict[int, Dict[str, float]] = {\n    1: {\"a\": 2.231588929313981, \"b\": -0.1414985033269173},\n    2: {\"a\": 3.2411728934431463, \"b\": -0.040171017050871324},\n    3: {\"a\": 2.5419346139673094, \"b\": -0.11478557925325927},\n    4: {\"a\": 1.162250294600759, \"b\": -0.13787004079035822},\n    5: {\"a\": 3.0706977855504913, \"b\": -0.13725898395106917},\n}\n\n\ndef _predict_for_row(row: Dict[str, float], coefs: Dict[int, Dict[str, float]]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n        c = _C_BY_DOMAIN[i]\n        a = coefs[i][\"a\"]\n        b = coefs[i][\"b\"]\n        # Numerically safe log with small positive offset.\n        pred = a + b * math.log(max(p, 0.0) + c)\n        out[f\"loss_domain_{i}\"] = float(pred)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coefs = _COEFS_BY_GROUP.get(group, _FALLBACK_COEFS)\n    return [_predict_for_row(row, coefs) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9714345955231741, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fixed exponents (beta) for each domain, same for all groups\n    beta = {\n        1: 0.228,\n        2: 0.572,\n        3: 0.198,\n        4: 0.239,\n        5: 0.357\n    }\n\n    # Parameters for each group and domain: (baseline, A)\n    # baseline: loss when proportion = 0\n    # A: coefficient in the formula: loss = baseline - A * proportion^beta\n    params = {\n        '70M': {\n            1: (3.415, 0.896),\n            2: (3.818, 0.466),\n            3: (3.601, 0.745),\n            4: (2.266, 0.941),\n            5: (3.932, 0.512)\n        },\n        '160M': {\n            1: (3.060, 0.843),\n            2: (3.472, 0.432),\n            3: (3.286, 0.710),\n            4: (1.963, 0.839),\n            5: (3.595, 0.527)\n        },\n        '305M': {\n            1: (2.898, 0.822),\n            2: (3.306, 0.408),\n            3: (3.156, 0.736),\n            4: (1.833, 0.803),\n            5: (3.434, 0.536)\n        },\n        '410M': {\n            1: (2.832, 0.817),\n            2: (3.230, 0.398),\n            3: (3.098, 0.750),\n            4: (1.779, 0.781),\n            5: (3.375, 0.556)\n        }\n    }\n\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Supported groups: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    result = []\n    for point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            if proportion_key not in point:\n                # If proportion key is missing, we cannot predict loss for this domain\n                continue\n            p = point[proportion_key]\n            baseline, A = group_params[i]\n            b = beta[i]\n            if p == 0:\n                loss = baseline\n            else:\n                loss = baseline - A * (p ** b)\n            pred[f'loss_domain_{i}'] = loss\n        result.append(pred)\n\n    return result"}
{"task": "domain_mixture_scaling_law", "agent_name": "aider", "model_name": "gpt-5", "reward_r2": 0.9711466282348672, "solution": "from __future__ import annotations\n\nimport math\nimport os\nfrom typing import Dict, List\n\n# Public API\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fit/load coefficients once (lazy on first invocation)\n    _ensure_fitted()\n    coeffs_for_group = _COEFFS.get(group, _COEFFS.get(_GLOBAL_KEY, _default_coeffs()))\n\n    results: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for d in _DOMAINS:\n            p = float(row.get(f\"proportion_domain_{d}\", 0.0))\n            pred[f\"loss_domain_{d}\"] = _predict_single(p, coeffs_for_group[d])\n        results.append(pred)\n    return results\n\n\n# ------------------------\n# Internal implementation\n# ------------------------\n\n# Model/Formula:\n# For each domain i in {1..5}, and for any group g:\n#     loss_domain_i = a_{g,i} + b_{g,i} * log(p_i + eps) + c_{g,i} * [log(p_i + eps)]^2\n# where p_i is the mixture proportion for domain i, eps = 1e-12.\n# This \"quadratic-in-log\" model captures a wide class of power-law-like curves\n# without requiring nonlinear optimization, improving stability and extrapolation.\n\n_EPS = 1e-12\n_DOMAINS = (1, 2, 3, 4, 5)\n_GLOBAL_KEY = \"__GLOBAL__\"\n\n# Coefficients structure:\n# _COEFFS[group][domain] = (a, b, c)\n_COEFFS: Dict[str, Dict[int, tuple[float, float, float]]] = {}\n\n# R^2 scores for reporting (per group/domain)\n_R2: Dict[str, Dict[int, float]] = {}\n\n# Guard for one-time fit\n_FITTED = False\n\n\ndef _predict_single(p: float, abc: tuple[float, float, float]) -> float:\n    a, b, c = abc\n    lp = math.log(max(p, _EPS))\n    return a + b * lp + c * (lp * lp)\n\n\ndef _default_coeffs() -> Dict[int, tuple[float, float, float]]:\n    # Neutral fallback: constant ~1.0 loss if fitting is unavailable\n    return {d: (1.0, 0.0, 0.0) for d in _DOMAINS}\n\n\ndef _ensure_fitted() -> None:\n    global _FITTED\n    if _FITTED:\n        return\n    try:\n        ds = _load_dataset(\"/app/data\")\n        if ds is None:\n            # Could not load dataset; use defaults\n            _COEFFS[_GLOBAL_KEY] = _default_coeffs()\n            _FITTED = True\n            _write_explain_file()\n            return\n\n        # Determine available groups\n        groups = _collect_groups(ds)\n        if not groups:\n            groups = {_GLOBAL_KEY}\n\n        # Fit per group\n        for g in groups:\n            rows = (r for r in ds if (g == _GLOBAL_KEY or r.get(\"group\") == g))\n            coeffs_g, r2_g = _fit_group(rows)\n            _COEFFS[g] = coeffs_g\n            _R2[g] = r2_g\n\n        # Also fit global across all data for robustness/fallback\n        rows_all = (r for r in ds)\n        coeffs_global, r2_global = _fit_group(rows_all)\n        _COEFFS[_GLOBAL_KEY] = coeffs_global\n        _R2[_GLOBAL_KEY] = r2_global\n\n    except Exception:\n        # Any failure => ensure safe defaults\n        _COEFFS[_GLOBAL_KEY] = _default_coeffs()\n    finally:\n        _FITTED = True\n        # Best-effort write explanation (ignore errors)\n        try:\n            _write_explain_file()\n        except Exception:\n            pass\n\n\ndef _load_dataset(path: str):\n    try:\n        from datasets import load_from_disk  # type: ignore\n    except Exception:\n        return None\n    if not os.path.exists(path):\n        return None\n    ds = load_from_disk(path)\n    # Support DatasetDict or Dataset\n    try:\n        # DatasetDict\n        if hasattr(ds, \"keys\"):\n            if \"train\" in ds:\n                ds_split = ds[\"train\"]\n            else:\n                # Pick the first available split\n                first_key = next(iter(ds.keys()))\n                ds_split = ds[first_key]\n        else:\n            ds_split = ds\n    except Exception:\n        ds_split = ds\n    return ds_split\n\n\ndef _collect_groups(ds) -> set:\n    groups = set()\n    try:\n        for r in ds:\n            g = r.get(\"group\")\n            if g is not None:\n                groups.add(g)\n    except Exception:\n        return set()\n    return groups\n\n\ndef _fit_group(rows_iter):\n    # Linear regression (ridge-regularized normal equations) for each domain\n    # y = a*1 + b*lp + c*lp^2  with lp = log(p + eps)\n    # We accumulate X^T X and X^T y in streaming fashion to avoid extra deps.\n    coeffs: Dict[int, tuple[float, float, float]] = {}\n    r2s: Dict[int, float] = {}\n\n    # Materialize rows for reuse (single pass needed for each domain)\n    rows = list(rows_iter)\n\n    for d in _DOMAINS:\n        # Initialize 3x3 matrix and 3x1 vector\n        xtx = [[0.0, 0.0, 0.0],\n               [0.0, 0.0, 0.0],\n               [0.0, 0.0, 0.0]]\n        xty = [0.0, 0.0, 0.0]\n\n        y_vals = []\n        f_list = []\n\n        for r in rows:\n            p = float(r.get(f\"proportion_domain_{d}\", 0.0))\n            y = r.get(f\"loss_domain_{d}\")\n            if y is None:\n                continue\n            y = float(y)\n            lp = math.log(max(p, _EPS))\n            f0 = 1.0\n            f1 = lp\n            f2 = lp * lp\n            f = (f0, f1, f2)\n            # Accumulate\n            xtx[0][0] += f0 * f0; xtx[0][1] += f0 * f1; xtx[0][2] += f0 * f2\n            xtx[1][0] += f1 * f0; xtx[1][1] += f1 * f1; xtx[1][2] += f1 * f2\n            xtx[2][0] += f2 * f0; xtx[2][1] += f2 * f1; xtx[2][2] += f2 * f2\n\n            xty[0] += f0 * y; xty[1] += f1 * y; xty[2] += f2 * y\n\n            y_vals.append(y)\n            f_list.append(f)\n\n        n = len(y_vals)\n        if n == 0:\n            coeffs[d] = (1.0, 0.0, 0.0)\n            r2s[d] = 0.0\n            continue\n\n        # Ridge regularization to stabilize\n        lam = 1e-8\n        xtx[0][0] += lam\n        xtx[1][1] += lam\n        xtx[2][2] += lam\n\n        a, b, c = _solve_3x3(xtx, xty)\n\n        coeffs[d] = (a, b, c)\n\n        # Compute R^2\n        y_mean = sum(y_vals) / n\n        ss_tot = sum((yy - y_mean) ** 2 for yy in y_vals) or 1e-12\n        ss_res = 0.0\n        for (f0, f1, f2), yy in zip(f_list, y_vals):\n            yhat = a + b * f1 + c * f2\n            ss_res += (yy - yhat) ** 2\n        r2s[d] = 1.0 - (ss_res / ss_tot)\n\n    return coeffs, r2s\n\n\ndef _solve_3x3(a: List[List[float]], b: List[float]) -> tuple[float, float, float]:\n    # Gaussian elimination with partial pivoting for 3x3\n    # Solve A x = b\n    A = [row[:] for row in a]\n    x = [0.0, 0.0, 0.0]\n    rhs = b[:]\n\n    # Forward elimination\n    for i in range(3):\n        # Pivot\n        pivot = i\n        max_abs = abs(A[i][i])\n        for r in range(i + 1, 3):\n            if abs(A[r][i]) > max_abs:\n                max_abs = abs(A[r][i])\n                pivot = r\n        if max_abs < 1e-18:\n            # Ill-conditioned; fallback identity\n            return (0.0, 0.0, 0.0)\n        if pivot != i:\n            A[i], A[pivot] = A[pivot], A[i]\n            rhs[i], rhs[pivot] = rhs[pivot], rhs[i]\n\n        # Normalize and eliminate\n        piv = A[i][i]\n        for r in range(i + 1, 3):\n            if A[r][i] == 0.0:\n                continue\n            f = A[r][i] / piv\n            rhs[r] -= f * rhs[i]\n            for c in range(i, 3):\n                A[r][c] -= f * A[i][c]\n\n    # Back substitution\n    for i in reversed(range(3)):\n        s = rhs[i]\n        for c in range(i + 1, 3):\n            s -= A[i][c] * x[c]\n        if abs(A[i][i]) < 1e-18:\n            x[i] = 0.0\n        else:\n            x[i] = s / A[i][i]\n\n    return (x[0], x[1], x[2])\n\n\ndef _write_explain_file() -> None:\n    # Write a detailed explanation with fitted coefficients to /app/explain.md\n    lines: List[str] = []\n    lines.append(\"# Discovered Scaling Law for Domain Mixture\\n\")\n    lines.append(\"This document is auto-generated by /app/law.py when imported or first used.\\n\")\n    lines.append(\"## Formula\\n\")\n    lines.append(\n        \"For each domain i in {1,2,3,4,5}, and for any experimental group G, the validation loss is modeled as:\\n\"\n    )\n    lines.append(\n        \"    loss_domain_i = a_{G,i} + b_{G,i} * log(proportion_domain_i + 1e-12) + c_{G,i} * [log(proportion_domain_i + 1e-12)]^2\\n\"\n    )\n    lines.append(\n        \"This quadratic-in-log model approximates power-law behavior with a smooth curvature term and is fit via linear regression (normal equations with a small ridge regularizer).\\n\"\n    )\n    lines.append(\"\\n## Methodology\\n\")\n    lines.append(\n        \"- Loaded the dataset from /app/data using datasets.load_from_disk.\\n\"\n        \"- For each group and each domain, constructed features [1, log(p+1e-12), (log(p+1e-12))^2].\\n\"\n        \"- Solved for coefficients (a,b,c) with closed-form least squares per domain.\\n\"\n        \"- Report R² per fit to indicate goodness-of-fit. If a group is unknown at inference time, a global fit over all groups is used.\\n\"\n    )\n    lines.append(\"\\n## Fitted Coefficients by Group and Domain\\n\")\n\n    if not _COEFFS:\n        lines.append(\"\\nNo coefficients available; using defaults (1.0, 0.0, 0.0).\\n\")\n    else:\n        for g in sorted(_COEFFS.keys()):\n            lines.append(f\"\\n### Group: {g}\\n\")\n            lines.append(\"| Domain | a | b | c | R^2 |\\n\")\n            lines.append(\"|---:|---:|---:|---:|---:|\\n\")\n            for d in _DOMAINS:\n                a, b, c = _COEFFS[g][d]\n                r2 = _R2.get(g, {}).get(d, float('nan'))\n                lines.append(f\"| {d} | {a:.6f} | {b:.6f} | {c:.6f} | {r2:.4f} |\\n\")\n\n    path = \"/app/explain.md\"\n    try:\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.writelines(lines)\n    except Exception:\n        # Swallow IO errors to avoid breaking runtime\n        pass"}
{"task": "domain_mixture_scaling_law", "agent_name": "opencode", "model_name": "gpt-5", "reward_r2": 0.971145, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Log-scaling law per group and domain:\n# loss_domain_i = a + b * log(proportion_domain_i + eps)\n# The functional form is identical across groups; only (a, b, eps) differ.\n\nCOEFFS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"160M\": {\n        \"loss_domain_1\": {\"a\": 2.2532615943868906, \"b\": -0.1337057136545849, \"eps\": 0.0023901343745956435},\n        \"loss_domain_2\": {\"a\": 3.2631919363951556, \"b\": -0.03558508713341269, \"eps\": 0.0028180383737978713},\n        \"loss_domain_3\": {\"a\": 2.617689579679002, \"b\": -0.09252884608244097, \"eps\": 0.000733714154677849},\n        \"loss_domain_4\": {\"a\": 1.1948582730723922, \"b\": -0.12312317609847837, \"eps\": 0.00195132901468823},\n        \"loss_domain_5\": {\"a\": 3.0846493236033896, \"b\": -0.14154552771058287, \"eps\": 0.027307052357303876},\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\"a\": 2.1166886286314748, \"b\": -0.12677944713435335, \"eps\": 0.002106041895388586},\n        \"loss_domain_2\": {\"a\": 3.0993803822936004, \"b\": -0.0385714100210861, \"eps\": 0.00469352643305276},\n        \"loss_domain_3\": {\"a\": 2.498051621939985, \"b\": -0.08371626119596315, \"eps\": 0.0003880182070929469},\n        \"loss_domain_4\": {\"a\": 1.0998534285937238, \"b\": -0.11664800642521109, \"eps\": 0.001864178447017992},\n        \"loss_domain_5\": {\"a\": 2.918795681535315, \"b\": -0.13713177186385297, \"eps\": 0.023253743472940992},\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\"a\": 2.0637659220766347, \"b\": -0.12150735957421108, \"eps\": 0.0017973751820540681},\n        \"loss_domain_2\": {\"a\": 3.0488279748294658, \"b\": -0.027049899121057236, \"eps\": 0.0012212145195230296},\n        \"loss_domain_3\": {\"a\": 2.431248081266068, \"b\": -0.08424573100842837, \"eps\": 0.00036447069913629604},\n        \"loss_domain_4\": {\"a\": 1.0617583927188603, \"b\": -0.11564152762782738, \"eps\": 0.0020181562979846223},\n        \"loss_domain_5\": {\"a\": 2.8417472866298, \"b\": -0.13918681081923803, \"eps\": 0.02183585057099095},\n    },\n    \"70M\": {\n        \"loss_domain_1\": {\"a\": 2.536901150373465, \"b\": -0.1533997450030275, \"eps\": 0.0032688441418023727},\n        \"loss_domain_2\": {\"a\": 3.568955616082981, \"b\": -0.05120827266924541, \"eps\": 0.007660059062331851},\n        \"loss_domain_3\": {\"a\": 2.886179438825381, \"b\": -0.10197118388034185, \"eps\": 0.0009064844324661106},\n        \"loss_domain_4\": {\"a\": 1.4039456278566775, \"b\": -0.13849029661959558, \"eps\": 0.0019748899726039115},\n        \"loss_domain_5\": {\"a\": 3.435925433424547, \"b\": -0.1375046497033619, \"eps\": 0.02690444281470573},\n    },\n}\n\n_PROP_COLS = [\n    \"proportion_domain_1\",\n    \"proportion_domain_2\",\n    \"proportion_domain_3\",\n    \"proportion_domain_4\",\n    \"proportion_domain_5\",\n]\n_LOSS_COLS = [\n    \"loss_domain_1\",\n    \"loss_domain_2\",\n    \"loss_domain_3\",\n    \"loss_domain_4\",\n    \"loss_domain_5\",\n]\n\n\ndef _predict_for_group(row: Dict[str, float], group: str) -> Dict[str, float]:\n    coeffs = COEFFS.get(group)\n    if coeffs is None:\n        raise ValueError(f\"Unknown group '{group}'. Known groups: {sorted(COEFFS)}\")\n    out: Dict[str, float] = {}\n    for i, loss_key in enumerate(_LOSS_COLS):\n        prop_key = _PROP_COLS[i]\n        p = float(row.get(prop_key, 0.0))\n        a = coeffs[loss_key][\"a\"]\n        b = coeffs[loss_key][\"b\"]\n        eps = coeffs[loss_key][\"eps\"]\n        # Guard against tiny negatives from numerical issues\n        val = a + b * math.log(max(p + eps, 1e-12))\n        out[loss_key] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    return [_predict_for_group(row, group) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-sonnet-4-5", "reward_r2": 0.971145, "solution": "import math\n\n# Fitted parameters for each domain and group\n# Structure: PARAMS[domain][group] = {'a': ..., 'p0': ..., 'c': ...}\nPARAMS = {\n    'domain_1': {\n        '160M': {'a': -0.133706, 'p0': 0.002390, 'c': 2.253262},\n        '305M': {'a': -0.126779, 'p0': 0.002106, 'c': 2.116689},\n        '410M': {'a': -0.121507, 'p0': 0.001797, 'c': 2.063766},\n        '70M': {'a': -0.153400, 'p0': 0.003269, 'c': 2.536901}\n    },\n    'domain_2': {\n        '160M': {'a': -0.035585, 'p0': 0.002818, 'c': 3.263192},\n        '305M': {'a': -0.038571, 'p0': 0.004694, 'c': 3.099380},\n        '410M': {'a': -0.027050, 'p0': 0.001221, 'c': 3.048828},\n        '70M': {'a': -0.051208, 'p0': 0.007660, 'c': 3.568956}\n    },\n    'domain_3': {\n        '160M': {'a': -0.092529, 'p0': 0.000734, 'c': 2.617690},\n        '305M': {'a': -0.083716, 'p0': 0.000388, 'c': 2.498053},\n        '410M': {'a': -0.084246, 'p0': 0.000364, 'c': 2.431248},\n        '70M': {'a': -0.101971, 'p0': 0.000906, 'c': 2.886180}\n    },\n    'domain_4': {\n        '160M': {'a': -0.123123, 'p0': 0.001951, 'c': 1.194858},\n        '305M': {'a': -0.116648, 'p0': 0.001864, 'c': 1.099853},\n        '410M': {'a': -0.115642, 'p0': 0.002018, 'c': 1.061758},\n        '70M': {'a': -0.138490, 'p0': 0.001975, 'c': 1.403946}\n    },\n    'domain_5': {\n        '160M': {'a': -0.141545, 'p0': 0.027307, 'c': 3.084649},\n        '305M': {'a': -0.137132, 'p0': 0.023254, 'c': 2.918796},\n        '410M': {'a': -0.139187, 'p0': 0.021836, 'c': 2.841747},\n        '70M': {'a': -0.137505, 'p0': 0.026904, 'c': 3.435925}\n    }\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    results = []\n\n    for data_point in input_data:\n        predictions = {}\n\n        # For each domain, predict the loss using the logarithmic scaling law\n        # L = a * log(p + p0) + c\n        for domain_idx in range(1, 6):\n            prop_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n            domain_key = f'domain_{domain_idx}'\n\n            # Get the proportion for this domain\n            proportion = data_point[prop_key]\n\n            # Get the fitted parameters for this domain and group\n            params = PARAMS[domain_key][group]\n            a = params['a']\n            p0 = params['p0']\n            c = params['c']\n\n            # Apply the scaling law: L = a * log(p + p0) + c\n            predicted_loss = a * math.log(proportion + p0) + c\n\n            predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-sonnet-4-5", "reward_r2": 0.971145, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each group and domain\n    # Formula: loss_domain_i = c - a * log(proportion_domain_i + b)\n    params = {\n        \"160M\": {\n            1: {\"a\": 0.13370571385297636, \"b\": 0.002390134393468617, \"c\": 2.2532615941125935},\n            2: {\"a\": 0.03558508709506605, \"b\": 0.002818038362083297, \"c\": 3.2631919364668476},\n            3: {\"a\": 0.09252859186605873, \"b\": 0.0007337050789613676, \"c\": 2.6176902832037365},\n            4: {\"a\": 0.12312317552374129, \"b\": 0.0019513289742103528, \"c\": 1.1948582742030367},\n            5: {\"a\": 0.14154544729126772, \"b\": 0.02730698179138869, \"c\": 3.0846493439463316}\n        },\n        \"305M\": {\n            1: {\"a\": 0.12677944537739508, \"b\": 0.0021060417483921446, \"c\": 2.116688631238799},\n            2: {\"a\": 0.03857137198216823, \"b\": 0.004693509918912462, \"c\": 3.0993804511546172},\n            3: {\"a\": 0.08371624093149321, \"b\": 0.0003880177278893411, \"c\": 2.498051678487977},\n            4: {\"a\": 0.1166480072254707, \"b\": 0.0018641785034444133, \"c\": 1.0998534270479912},\n            5: {\"a\": 0.1371317714952529, \"b\": 0.02325374313900293, \"c\": 2.9187956816183247}\n        },\n        \"410M\": {\n            1: {\"a\": 0.12150735155719182, \"b\": 0.001797374580772529, \"c\": 2.0637659341196875},\n            2: {\"a\": 0.02704989730064647, \"b\": 0.0012212141601325463, \"c\": 3.0488279783306345},\n            3: {\"a\": 0.08424583253617433, \"b\": 0.00036447295242515113, \"c\": 2.4312477954997536},\n            4: {\"a\": 0.11564152660339765, \"b\": 0.0020181562198990236, \"c\": 1.0617583947352045},\n            5: {\"a\": 0.13918678570181242, \"b\": 0.021835831641187086, \"c\": 2.841747294132424}\n        },\n        \"70M\": {\n            1: {\"a\": 0.15339974058406491, \"b\": 0.0032688436937777696, \"c\": 2.536901156656863},\n            2: {\"a\": 0.05120827004111036, \"b\": 0.007660058657766675, \"c\": 3.568955621338834},\n            3: {\"a\": 0.10197068274747872, \"b\": 0.0009064652901137906, \"c\": 2.8861808195815533},\n            4: {\"a\": 0.13849029277376568, \"b\": 0.001974889729256156, \"c\": 1.4039456354099615},\n            5: {\"a\": 0.1375046566797978, \"b\": 0.02690444916275532, \"c\": 3.435925431698058}\n        }\n    }\n\n    # Get parameters for the specified group\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Available groups: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    # Generate predictions\n    predictions = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # For each domain, predict the loss\n        for domain in range(1, 6):\n            prop_key = f\"proportion_domain_{domain}\"\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get the proportion for this domain\n            if prop_key not in data_point:\n                raise ValueError(f\"Missing input variable: {prop_key}\")\n\n            proportion = data_point[prop_key]\n\n            # Get parameters for this domain\n            a = group_params[domain][\"a\"]\n            b = group_params[domain][\"b\"]\n            c = group_params[domain][\"c\"]\n\n            # Apply the scaling law: loss = c - a * log(proportion + b)\n            predicted_loss = c - a * math.log(proportion + b)\n\n            prediction[loss_key] = predicted_loss\n\n        predictions.append(prediction)\n\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "goose", "model_name": "gpt-5", "reward_r2": 0.9711402923865737, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Scaling law (same functional form for all groups and domains):\n        loss_domain_i = a_{g,i} + b_{g,i} * ln(proportion_domain_i + eps_{g,i})\n\n    If the provided group is unknown, a sensible fallback using the average\n    coefficients across known groups is used (per-domain averages of a, b, eps).\n    \"\"\"\n    import math\n\n    # Parameters per (group, domain): loss_i = a + b * ln(p_i + eps)\n    _PARAMS = {\n        '160M': {\n            1: {'a': 2.2531838390, 'b': -0.1337589930, 'eps': 0.0023949075},\n            2: {'a': 3.2636809068, 'b': -0.0353192223, 'eps': 0.0027325293},\n            3: {'a': 2.6179153875, 'b': -0.0924472969, 'eps': 0.0007308092},\n            4: {'a': 1.1944779835, 'b': -0.1233168892, 'eps': 0.0019650546},\n            5: {'a': 3.0846022090, 'b': -0.1417331522, 'eps': 0.0274723768},\n        },\n        '305M': {\n            1: {'a': 2.1168108211, 'b': -0.1266964846, 'eps': 0.0020990011},\n            2: {'a': 3.0996329693, 'b': -0.0384297964, 'eps': 0.0046309399},\n            3: {'a': 2.4992568493, 'b': -0.0832862054, 'eps': 0.0003779407},\n            4: {'a': 1.1005269460, 'b': -0.1163058210, 'eps': 0.0018396558},\n            5: {'a': 2.9190563483, 'b': -0.1362346592, 'eps': 0.0225414637},\n        },\n        '410M': {\n            1: {'a': 2.0629628567, 'b': -0.1220476872, 'eps': 0.0018396558},\n            2: {'a': 3.0486794047, 'b': -0.0271283688, 'eps': 0.0012385352},\n            3: {'a': 2.4325992818, 'b': -0.0837640232, 'eps': 0.0003538226},\n            4: {'a': 1.0631159333, 'b': -0.1149495630, 'eps': 0.0019650546},\n            5: {'a': 2.8414721314, 'b': -0.1401158009, 'eps': 0.0225414637},\n        },\n        '70M': {\n            1: {'a': 2.5360441935, 'b': -0.1540029694, 'eps': 0.0033302662},\n            2: {'a': 3.5682949842, 'b': -0.0515955155, 'eps': 0.0078482616},\n            3: {'a': 2.8873239058, 'b': -0.1015557222, 'eps': 0.0008906727},\n            4: {'a': 1.4042502135, 'b': -0.1383351096, 'eps': 0.0019650546},\n            5: {'a': 3.4357662053, 'b': -0.1381350881, 'eps': 0.0274723768},\n        },\n    }\n\n    # Build per-domain average fallback in case of unknown group\n    if group not in _PARAMS:\n        # compute averages across known groups for each domain\n        avg_params = {}\n        for i in range(1, 6):\n            a_vals = [gparams[i]['a'] for gparams in _PARAMS.values()]\n            b_vals = [gparams[i]['b'] for gparams in _PARAMS.values()]\n            eps_vals = [gparams[i]['eps'] for gparams in _PARAMS.values()]\n            avg_params[i] = {\n                'a': sum(a_vals) / len(a_vals),\n                'b': sum(b_vals) / len(b_vals),\n                'eps': sum(eps_vals) / len(eps_vals),\n            }\n        params = avg_params\n    else:\n        params = _PARAMS[group]\n\n    predictions: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            # numerical safety for log at extremely small or slightly negative due to noise\n            if p < 0.0:\n                p = 0.0\n            a = params[i]['a']\n            b = params[i]['b']\n            eps = params[i]['eps']\n            y = a + b * math.log(p + eps)\n            out[f\"loss_domain_{i}\"] = float(y)\n        predictions.append(out)\n\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "gpt-5", "reward_r2": 0.971092, "solution": "# Discovered scaling law (shared functional form across groups):\n#   loss_domain_k = a_{g,k} + b_{g,k} * (proportion_domain_k + c_{g,k}) ** d_{g,k}\n# where g indexes the experimental group and k in {1..5} indexes the domain.\n\n\n# Fitted parameters per group and per domain (k = 1..5).\n# Values were obtained by least squares fitting on the provided dataset.\n_PARAMS = {\n    \"160M\": {\n        \"domain_1\": {\"a\": 3.118296598901631, \"b\": -0.8877441483834596, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.3016069560703,   \"b\":  0.0018261061960447534, \"c\": 0.09183755102040818, \"d\": -1.9},\n        \"domain_3\": {\"a\": 3.312678979117116, \"b\": -0.8785517989680411, \"c\": 1e-06, \"d\": 0.25},\n        \"domain_4\": {\"a\": 1.0034194693515974, \"b\": 0.24243472606360897, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 4.118698802652438, \"b\": -1.040736728854194,  \"c\": 0.010205061224489796, \"d\": 0.15},\n    },\n    \"305M\": {\n        \"domain_1\": {\"a\": 2.952929420274062, \"b\": -0.8629898487399583, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.127318377480758, \"b\":  0.007202777918698585, \"c\": 0.061225367346938786, \"d\": -1.15},\n        \"domain_3\": {\"a\": 3.207800159763377, \"b\": -0.8322110176673488, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_4\": {\"a\": 0.9142458490282026, \"b\": 0.23205050091886964, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 0.2308748803753971, \"b\": 2.689570473211351,  \"c\": 0.030613183673469394, \"d\": -0.05},\n    },\n    \"410M\": {\n        \"domain_1\": {\"a\": 2.884545514793264, \"b\": -0.852803939078441,  \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.0790530221595462, \"b\": 0.001307985468446201, \"c\": 0.07142942857142857, \"d\": -1.8},\n        \"domain_3\": {\"a\": 3.151353366782936, \"b\": -0.8477484549673475, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_4\": {\"a\": 0.884948385285892, \"b\": 0.22601551215198246, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 4.3056179059111725, \"b\": -1.4673725683803218, \"c\": 0.010205061224489796, \"d\": 0.1},\n    },\n    \"70M\": {\n        \"domain_1\": {\"a\": 2.00061995681349,  \"b\": 0.5657494904538474,  \"c\": 0.010205061224489796, \"d\": -0.2},\n        \"domain_2\": {\"a\": 3.6141195606157925, \"b\": 0.00530072540335944, \"c\": 0.10204161224489797, \"d\": -1.6},\n        \"domain_3\": {\"a\": 3.6293616626237855, \"b\": -0.9227693166043902, \"c\": 1e-06, \"d\": 0.25},\n        \"domain_4\": {\"a\": 1.1898035159399545, \"b\": 0.2720101134529132, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 3.249380658214184,  \"b\": 0.2049354477567703,  \"c\": 0.11224567346938777, \"d\": -0.55},\n    },\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Select parameters for the group; if unseen, use average across known groups.\n    if group in _PARAMS:\n        params = _PARAMS[group]\n    else:\n        domains = [f\"domain_{i}\" for i in range(1, 6)]\n        params = {}\n        for d in domains:\n            acc = {\"a\": 0.0, \"b\": 0.0, \"c\": 0.0, \"d\": 0.0}\n            for g in _PARAMS.values():\n                pd = g[d]\n                acc[\"a\"] += pd[\"a\"]\n                acc[\"b\"] += pd[\"b\"]\n                acc[\"c\"] += pd[\"c\"]\n                acc[\"d\"] += pd[\"d\"]\n            n = float(len(_PARAMS))\n            params[d] = {k: v / n for k, v in acc.items()}\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        for k in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{k}\", 0.0))\n            par = params[f\"domain_{k}\"]\n            a, b, c, d = par[\"a\"], par[\"b\"], par[\"c\"], par[\"d\"]\n            # Ensure numerical stability for zero proportions\n            pred = a + b * (p + c) ** d\n            out[f\"loss_domain_{k}\"] = float(pred)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "goose", "model_name": "gpt-5", "reward_r2": 0.971072, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups, but\n                the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Discovered scaling law parameters per group and domain\n    # Model: loss_domain_i = a + b * ln(proportion_domain_i + c)\n    # Coefficients were fitted on the provided dataset.\n    COEFFS: dict[str, dict[int, dict[str, float]]] = {\n        \"160M\": {\n            1: {\"a\": 2.255463034657955, \"b\": -0.1322014387664249, \"c\": 0.0022579423766059038},\n            2: {\"a\": 3.2627500668798435, \"b\": -0.035825707556857135, \"c\": 0.002896657450965321},\n            3: {\"a\": 2.6244424847304453, \"b\": -0.09009357911542895, \"c\": 0.0006498175348279338},\n            4: {\"a\": 1.2003470092201818, \"b\": -0.12033532343430565, \"c\": 0.0017600644406102247},\n            5: {\"a\": 3.0846622790213734, \"b\": -0.14149413638165387, \"c\": 0.027261874278143278},\n        },\n        \"305M\": {\n            1: {\"a\": 2.114124254258875, \"b\": -0.12852626852708746, \"c\": 0.0022579423766059038},\n            2: {\"a\": 3.099085405020726, \"b\": -0.03873697421213334, \"c\": 0.004767225673466724},\n            3: {\"a\": 2.4972468378996573, \"b\": -0.08400352953211676, \"c\": 0.0003948415562753178},\n            4: {\"a\": 1.1027537337452233, \"b\": -0.1151761426543334, \"c\": 0.0017600644406102247},\n            5: {\"a\": 2.919559840422695, \"b\": -0.13456190779289226, \"c\": 0.021250611175238732},\n        },\n        \"410M\": {\n            1: {\"a\": 2.0644864248236448, \"b\": -0.1210234418135825, \"c\": 0.0017600644406102247},\n            2: {\"a\": 3.0475764758867196, \"b\": -0.027711888083369603, \"c\": 0.0013719689515536608},\n            3: {\"a\": 2.4275273847120293, \"b\": -0.08557331062431346, \"c\": 0.0003948415562753178},\n            4: {\"a\": 1.0558984541830227, \"b\": -0.11864017408189217, \"c\": 0.0022579423766059038},\n            5: {\"a\": 2.8419851362063593, \"b\": -0.13840170983545286, \"c\": 0.021250611175238732},\n        },\n        \"70M\": {\n            1: {\"a\": 2.5423740680906737, \"b\": -0.14957785135641832, \"c\": 0.002896657450965321},\n            2: {\"a\": 3.568303753809257, \"b\": -0.0515903682298029, \"c\": 0.007845746694759024},\n            3: {\"a\": 2.891555035928987, \"b\": -0.1000216033928399, \"c\": 0.0008336345619486921},\n            4: {\"a\": 1.4108355902865883, \"b\": -0.13498993068037493, \"c\": 0.0017600644406102247},\n            5: {\"a\": 3.435824595629432, \"b\": -0.1379023492163919, \"c\": 0.027261874278143278},\n        },\n    }\n    DEFAULT_GROUP = \"70M\"\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        pred: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            g = group if group in COEFFS else DEFAULT_GROUP\n            params = COEFFS[g][i]\n            a = params[\"a\"]\n            b = params[\"b\"]\n            c = params[\"c\"]\n            # Ensure numerical stability for very small/negative p\n            if p < 0:\n                p = 0.0\n            y = a + b * math.log(p + c)\n            pred[f\"loss_domain_{i}\"] = float(y)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "opencode", "model_name": "gpt-5", "reward_r2": 0.971, "solution": "from __future__ import annotations\nfrom typing import Dict, List\n\n# Discovered scaling law (same functional form across groups):\n#   loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + c_{g,i}) ** (-alpha_{g,i})\n# Parameters (a, b, alpha, c) are fitted per group g and domain i.\n\n_PARAMS: Dict[str, Dict[int, Dict[str, float]]] = {\n    \"160M\": {\n        1: {\"a\": -0.11147193199439992, \"b\": 2.3787691982958217, \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.278500369142649,   \"b\": 0.01306711991811282, \"alpha\": 0.8034482758620689, \"c\": 0.034895833333333334},\n        3: {\"a\": 0.8506733951924391,  \"b\": 1.7622754024333271, \"alpha\": 0.05, \"c\": 0.0015625},\n        4: {\"a\": -1.072627406051137,  \"b\": 2.2748208460279304, \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 0.3144198553952366,  \"b\": 2.772096693493694,  \"alpha\": 0.05, \"c\": 0.034895833333333334},\n    },\n    \"305M\": {\n        1: {\"a\": -0.18733739807046243, \"b\": 2.312924189576437,  \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.0583843406593463,  \"b\": 0.05867924234335276, \"alpha\": 0.3672413793103448, \"c\": 0.019791666666666666},\n        3: {\"a\": 2.037098981893403,   \"b\": 0.48486293207181885, \"alpha\": 0.1293103448275862, \"c\": 0.0015625},\n        4: {\"a\": -1.0727458525278075, \"b\": 2.177270392291824,  \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 1.465292570110724,   \"b\": 1.4569415752353443, \"alpha\": 0.0896551724137931, \"c\": 0.034895833333333334},\n    },\n    \"410M\": {\n        1: {\"a\": -0.21860503605523499, \"b\": 2.2855798194974324, \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.068706883350695,    \"b\": 0.005605907282164662, \"alpha\": 1.0017241379310344, \"c\": 0.034895833333333334},\n        3: {\"a\": 1.958790095134566,    \"b\": 0.4939182316775028,  \"alpha\": 0.1293103448275862, \"c\": 0.0015625},\n        4: {\"a\": -1.050236886559304,   \"b\": 2.1205339628269058, \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 1.3466574766335897,   \"b\": 1.4981064795012045,  \"alpha\": 0.0896551724137931, \"c\": 0.034895833333333334},\n    },\n    \"70M\": {\n        1: {\"a\": 2.1270757115611185,  \"b\": 0.4395199897525094,  \"alpha\": 0.2482758620689655, \"c\": 0.013194444444444443},\n        2: {\"a\": 3.552025532207962,   \"b\": 0.04562819235160885, \"alpha\": 0.5258620689655172, \"c\": 0.034895833333333334},\n        3: {\"a\": 1.0433037320725291,  \"b\": 1.8510839638322976,  \"alpha\": 0.05, \"c\": 0.0015625},\n        4: {\"a\": 1.3203779743559676,  \"b\": 0.16265793539818846, \"alpha\": 0.4068965517241379, \"c\": 0.013194444444444443},\n        5: {\"a\": 3.192722657529413,   \"b\": 0.2586205571225181,  \"alpha\": 0.446551724137931, \"c\": 0.09531249999999998},\n    },\n}\n\n_DOMAIN_KEYS = {\n    1: (\"proportion_domain_1\", \"loss_domain_1\"),\n    2: (\"proportion_domain_2\", \"loss_domain_2\"),\n    3: (\"proportion_domain_3\", \"loss_domain_3\"),\n    4: (\"proportion_domain_4\", \"loss_domain_4\"),\n    5: (\"proportion_domain_5\", \"loss_domain_5\"),\n}\n\n_DEF_EPS = 1e-8  # numerical floor to avoid zero to negative-power\n\n\ndef _predict_for_group_row(row: Dict[str, float], params_g: Dict[int, Dict[str, float]]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p_key, y_key = _DOMAIN_KEYS[i]\n        p = float(row.get(p_key, 0.0))\n        par = params_g[i]\n        # Apply shifted power law with small numerical floor\n        val = par[\"a\"] + par[\"b\"] * ((max(p, 0.0) + max(par[\"c\"], _DEF_EPS)) ** (-par[\"alpha\"]))\n        out[y_key] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Known: {sorted(_PARAMS.keys())}\")\n    params_g = _PARAMS[group]\n    return [_predict_for_group_row(row, params_g) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2", "reward_r2": 0.9709959950056923, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Model parameters discovered from data analysis\n    # Model: loss_i = A_i * (proportion_i + delta_i)^beta_i\n    # Where i = domain index (1-5)\n    \n    # Parameters for each domain and group\n    # Format: {group: {domain: {'A': A, 'beta': beta, 'delta': delta}}}\n    params = {\n        '70M': {\n            1: {'A': 2.544863, 'beta': -0.055547, 'delta': 0.005023},\n            2: {'A': 3.570086, 'beta': -0.014014, 'delta': 0.008241},\n            3: {'A': 2.900582, 'beta': -0.031946, 'delta': 0.001151},\n            4: {'A': 1.423500, 'beta': -0.083270, 'delta': 0.003754},\n            5: {'A': 3.437510, 'beta': -0.039108, 'delta': 0.031957}\n        },\n        '160M': {\n            1: {'A': 2.260306, 'beta': -0.054464, 'delta': 0.003835},\n            2: {'A': 3.263859, 'beta': -0.010693, 'delta': 0.003073},\n            3: {'A': 2.631247, 'beta': -0.031889, 'delta': 0.000946},\n            4: {'A': 1.213212, 'beta': -0.086238, 'delta': 0.003772},\n            5: {'A': 3.086537, 'beta': -0.044702, 'delta': 0.033190}\n        },\n        '305M': {\n            1: {'A': 2.123389, 'beta': -0.054917, 'delta': 0.003471},\n            2: {'A': 3.100167, 'beta': -0.012180, 'delta': 0.005080},\n            3: {'A': 2.510346, 'beta': -0.030294, 'delta': 0.000525},\n            4: {'A': 1.117573, 'beta': -0.088355, 'delta': 0.003698},\n            5: {'A': 2.920654, 'beta': -0.045656, 'delta': 0.028746}\n        },\n        '410M': {\n            1: {'A': 2.070173, 'beta': -0.054007, 'delta': 0.003024},\n            2: {'A': 3.049263, 'beta': -0.008725, 'delta': 0.001348},\n            3: {'A': 2.444041, 'beta': -0.031212, 'delta': 0.000501},\n            4: {'A': 1.079474, 'beta': -0.090464, 'delta': 0.003987},\n            5: {'A': 2.843735, 'beta': -0.047495, 'delta': 0.027361}\n        }\n    }\n    \n    # Check if group is valid\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(params.keys())}\")\n    \n    results = []\n    \n    for data_point in input_data:\n        # Create prediction dictionary\n        prediction = {}\n        \n        # For each domain (1-5)\n        for domain_idx in range(1, 6):\n            prop_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n            \n            # Get proportion value (default to 0 if not present)\n            proportion = data_point.get(prop_key, 0.0)\n            \n            # Get parameters for this domain and group\n            domain_params = params[group][domain_idx]\n            A = domain_params['A']\n            beta = domain_params['beta']\n            delta = domain_params['delta']\n            \n            # Apply scaling law: loss = A * (proportion + delta)^beta\n            # Handle very small values to avoid numerical issues\n            if proportion + delta <= 0:\n                # This shouldn't happen with delta > 0, but just in case\n                predicted_loss = A * (1e-10) ** beta\n            else:\n                predicted_loss = A * ((proportion + delta) ** beta)\n            \n            # Store prediction\n            prediction[loss_key] = predicted_loss\n        \n        results.append(prediction)\n    \n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-3-pro-preview", "reward_r2": 0.970854, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Parameters fitted: L = c0 + c1 * (p + c2)^(-c3)\n    # params structure: group -> domain -> [c0, c1, c2, c3]\n    params = {\n    '70M': {\n        'loss_domain_1': [9.049791064506775e-16, 2.544863255075162, 0.005023335215394096, 0.055546740330062044],\n        'loss_domain_2': [2.7486656945851666, 0.8246899154131252, 0.010080775042703558, 0.05659733424323028],\n        'loss_domain_3': [5.985319537346353e-07, 2.9005808574645533, 0.001151070479839812, 0.031946298634857095],\n        'loss_domain_4': [1.2590153991726705, 0.21220778772640178, 0.011700299738268685, 0.3501422357655192],\n        'loss_domain_5': [3.2198538792667297, 0.23277138740838701, 0.10173862328204047, 0.48926338453846824],\n    },\n    '160M': {\n        'loss_domain_1': [7.921177091134831e-11, 2.2603061159513467, 0.0038345630816795984, 0.05446415324935802],\n        'loss_domain_2': [2.4461406598589988, 0.8195058967138309, 0.0038249028744246735, 0.04037177211421319],\n        'loss_domain_3': [7.510940516965377e-07, 2.631246343363198, 0.00094573078880027, 0.03188902494410328],\n        'loss_domain_4': [8.674987985071811e-15, 1.2132115807783543, 0.003771920704982149, 0.0862384287195669],\n        'loss_domain_5': [4.666662783752345e-17, 3.0865366503919907, 0.03318950890733831, 0.04470214262740694],\n    },\n    '305M': {\n        'loss_domain_1': [4.63396739841804e-14, 2.123388567522197, 0.0034713501822073336, 0.05491740702600626],\n        'loss_domain_2': [2.3349069954135566, 0.7673802246129325, 0.006222133135954493, 0.04638693162532186],\n        'loss_domain_3': [6.593427727187525e-13, 2.510346003736797, 0.00052546156955814, 0.030293831005327836],\n        'loss_domain_4': [4.9747859075972676e-08, 1.117572915518561, 0.003698166118962751, 0.08835480936682777],\n        'loss_domain_5': [1.2908709077742297, 1.6311412445258724, 0.03310495146187288, 0.08014352579523258],\n    },\n    '410M': {\n        'loss_domain_1': [6.45159383764858e-15, 2.0701727493297715, 0.0030243803942285714, 0.05400686896572349],\n        'loss_domain_2': [2.2870399938680213, 0.7634132483462176, 0.0017345344382877009, 0.033273045391567696],\n        'loss_domain_3': [3.4557544039691014e-07, 2.4440410784149362, 0.0005010731785675117, 0.03121240344669886],\n        'loss_domain_4': [0.7411064514872984, 0.35914458865064114, 0.007467419826885272, 0.21677194222721097],\n        'loss_domain_5': [4.776331691474593e-12, 2.8437345217824648, 0.0273606303463639, 0.047494879619303806],\n    },\n}\n    \n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}\")\n        \n    group_params = params[group]\n    predictions = []\n    \n    for row in input_data:\n        pred_row = {}\n        for d in range(1, 6):\n            input_key = f\"proportion_domain_{d}\"\n            output_key = f\"loss_domain_{d}\"\n            \n            p = row.get(input_key, 0.0)\n            \n            # Retrieve parameters for this domain\n            c0, c1, c2, c3 = group_params[output_key]\n            \n            # Calculate L = c0 + c1 * (p + c2)^(-c3)\n            # Use math.pow for single scalar power\n            loss = c0 + c1 * math.pow(p + c2, -c3)\n            \n            pred_row[output_key] = loss\n            \n        predictions.append(pred_row)\n        \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gemini-3-flash-preview", "reward_r2": 0.970853986879495, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Discovered parameters\n    params = {\n      \"1\": {\n        \"k\": 0.003926356360468923,\n        \"alpha\": 0.05718810672412049,\n        \"groups\": {\n          \"70M\": {\"c\": 0.25035470891403067, \"a\": 2.3081440577674486},\n          \"160M\": {\"c\": 0.11473631324372671, \"a\": 2.145732347753247},\n          \"305M\": {\"c\": 0.032892265162742906, \"a\": 2.0860959948872986},\n          \"410M\": {\"c\": 0.0, \"a\": 2.0606024013061623}\n        }\n      },\n      \"2\": {\n        \"k\": 0.005348734990404739,\n        \"alpha\": 0.04849930034177744,\n        \"groups\": {\n          \"70M\": {\"c\": 2.788938903014082, \"a\": 0.7992339295329967},\n          \"160M\": {\"c\": 2.5294393042337133, \"a\": 0.7313132160229274},\n          \"305M\": {\"c\": 2.4110293528397078, \"a\": 0.6946680409496713},\n          \"410M\": {\"c\": 2.366352895383652, \"a\": 0.6699095227161864}\n        }\n      },\n      \"3\": {\n        \"k\": 0.0007633629218726617,\n        \"alpha\": 0.03416674736689901,\n        \"groups\": {\n          \"70M\": {\"c\": 0.5096484002350068, \"a\": 2.41908212577788},\n          \"160M\": {\"c\": 0.3422949311162433, \"a\": 2.3033031203456074},\n          \"305M\": {\"c\": 0.11331781391577726, \"a\": 2.3803966281988505},\n          \"410M\": {\"c\": 0.0, \"a\": 2.424065471843669}\n        }\n      },\n      \"4\": {\n        \"k\": 0.005278641910570982,\n        \"alpha\": 0.1450495769891357,\n        \"groups\": {\n          \"70M\": {\"c\": 0.7098001982822185, \"a\": 0.7275091490945351},\n          \"160M\": {\"c\": 0.5755162600091049, \"a\": 0.6484659339778274},\n          \"305M\": {\"c\": 0.504693250857682, \"a\": 0.6206758323529236},\n          \"410M\": {\"c\": 0.4860887527396757, \"a\": 0.6045069362922936}\n        }\n      },\n      \"5\": {\n        \"k\": 0.030736480866075007,\n        \"alpha\": 0.04878579210761627,\n        \"groups\": {\n          \"70M\": {\"c\": 0.7516912881140767, \"a\": 2.686857307527944},\n          \"160M\": {\"c\": 0.33125206249053835, \"a\": 2.756227753016214},\n          \"305M\": {\"c\": 0.15563495035933986, \"a\": 2.7646844004467073},\n          \"410M\": {\"c\": 0.0, \"a\": 2.8427953616069908}\n        }\n      }\n    }\n\n    predictions = []\n    for data_point in input_data:\n        pred_point = {}\n        for d in range(1, 6):\n            d_str = str(d)\n            p = data_point.get(f'proportion_domain_{d}', 0.0)\n            d_params = params[d_str]\n            g_params = d_params['groups'][group]\n            \n            # L_i = c_i + a_i * (p_i + k_i)^{-alpha_i}\n            loss = g_params['c'] + g_params['a'] * (p + d_params['k'])**(-d_params['alpha'])\n            pred_point[f'loss_domain_{d}'] = loss\n        predictions.append(pred_point)\n    \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-haiku-4-5", "reward_r2": 0.9708404106349446, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law discovered is a log-linear relationship:\n    loss_domain_i = a_i + b_i * log(proportion_domain_i)\n\n    where a_i and b_i are fitted coefficients that depend on the domain and group.\n\n    For zero proportions, a baseline loss value is used.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values (proportion_domain_i).\n        group: The name of the experimental group for which to make predictions.\n               Supported groups: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) (loss_domain_i).\n    \"\"\"\n    import math\n\n    # Fitted parameters: loss_domain_i = a_i + b_i * log(proportion_domain_i)\n    # Format: {domain_i: {group: {a: float, b: float}}}\n    params = {\n        'domain_1': {\n            '70M': {'a': 2.538716023062174, 'b': -0.15014064985318967},\n            '160M': {'a': 2.2544843272662405, 'b': -0.1315809083635399},\n            '305M': {'a': 2.1175968264069764, 'b': -0.12507426813350242},\n            '410M': {'a': 2.0645520693878363, 'b': -0.1200838190624766},\n        },\n        'domain_2': {\n            '70M': {'a': 3.5716664015145185, 'b': -0.04836401240653225},\n            '160M': {'a': 3.263924262467963, 'b': -0.034830492162166084},\n            '305M': {'a': 3.1006743096532867, 'b': -0.03722876491120557},\n            '410M': {'a': 3.0490737355109614, 'b': -0.02679818184395711},\n        },\n        'domain_3': {\n            '70M': {'a': 2.893348244102368, 'b': -0.09886102692916138},\n            '160M': {'a': 2.623560530670634, 'b': -0.09003128254977677},\n            '305M': {'a': 2.501230421480594, 'b': -0.08239118209742197},\n            '410M': {'a': 2.4342910737037524, 'b': -0.08297977891663602},\n        },\n        'domain_4': {\n            '70M': {'a': 1.4060660023243017, 'b': -0.1362756449783236},\n            '160M': {'a': 1.1970241661817773, 'b': -0.12103109712578866},\n            '305M': {'a': 1.101723420311251, 'b': -0.11479764855541283},\n            '410M': {'a': 1.0636521193056327, 'b': -0.11371123168518989},\n        },\n        'domain_5': {\n            '70M': {'a': 3.434597631728354, 'b': -0.12648654647382065},\n            '160M': {'a': 3.0839186234824836, 'b': -0.12887711537787372},\n            '305M': {'a': 2.9180204995169983, 'b': -0.1269094532480054},\n            '410M': {'a': 2.8412870151502942, 'b': -0.12892583253832435},\n        },\n    }\n\n    # Baseline losses at p=0 (computed from training data)\n    baseline_losses = {\n        'domain_1': {\n            '70M': 3.4149081168601354,\n            '160M': 3.060407302873726,\n            '305M': 2.8980309491789353,\n            '410M': 2.831881281851642,\n        },\n        'domain_2': {\n            '70M': 3.818428775380711,\n            '160M': 3.4721373995346867,\n            '305M': 3.3061837391603213,\n            '410M': 3.230276246563029,\n        },\n        'domain_3': {\n            '70M': 3.60063959478022,\n            '160M': 3.2855554601648356,\n            '305M': 3.1556234171102338,\n            '410M': 3.0982517116672392,\n        },\n        'domain_4': {\n            '70M': 2.266334699876238,\n            '160M': 1.9630575752887789,\n            '305M': 1.83297445467203,\n            '410M': 1.7793668136344885,\n        },\n        'domain_5': {\n            '70M': 3.931742488662131,\n            '160M': 3.594912574404762,\n            '305M': 3.4344130881519273,\n            '410M': 3.374610814377834,\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        output = {}\n\n        # Predict loss for each domain\n        for domain_idx in range(1, 6):\n            domain_key = f'domain_{domain_idx}'\n            proportion_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n\n            # Get the proportion for this domain\n            proportion = data_point.get(proportion_key, 0.0)\n\n            # Get parameters for this domain and group\n            if domain_key in params and group in params[domain_key]:\n                # For zero proportion, use baseline loss\n                if proportion == 0:\n                    if domain_key in baseline_losses and group in baseline_losses[domain_key]:\n                        loss = baseline_losses[domain_key][group]\n                    else:\n                        loss = 3.0  # fallback value\n                else:\n                    # Apply the log-linear model: loss = a + b*log(proportion)\n                    a = params[domain_key][group]['a']\n                    b = params[domain_key][group]['b']\n                    loss = a + b * math.log(proportion)\n\n                output[loss_key] = loss\n\n        results.append(output)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-sonnet-4-5", "reward_r2": 0.970491, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each group and domain\n    # Structure: params[group][domain] = {'L_0': ..., 'L_inf': ..., 'C': ..., 'alpha': ...}\n    params = {\n        '70M': {\n            1: {'L_0': 3.414908, 'L_inf': 0.100000, 'C': 2.447942, 'alpha': 0.055697},\n            2: {'L_0': 3.818429, 'L_inf': 3.293314, 'C': 0.289679, 'alpha': 0.128688},\n            3: {'L_0': 3.600640, 'L_inf': 0.100000, 'C': 2.809106, 'alpha': 0.031725},\n            4: {'L_0': 2.266335, 'L_inf': 1.184513, 'C': 0.285163, 'alpha': 0.269767},\n            5: {'L_0': 3.931742, 'L_inf': 2.131576, 'C': 1.306624, 'alpha': 0.087635},\n        },\n        '160M': {\n            1: {'L_0': 3.060407, 'L_inf': 0.100000, 'C': 2.162561, 'alpha': 0.055281},\n            2: {'L_0': 3.472137, 'L_inf': 2.968259, 'C': 0.301832, 'alpha': 0.095188},\n            3: {'L_0': 3.285555, 'L_inf': 0.100000, 'C': 2.538393, 'alpha': 0.031901},\n            4: {'L_0': 1.963058, 'L_inf': 0.100000, 'C': 1.118286, 'alpha': 0.089544},\n            5: {'L_0': 3.594913, 'L_inf': 0.100000, 'C': 2.985780, 'alpha': 0.041148},\n        },\n        '305M': {\n            1: {'L_0': 2.898031, 'L_inf': 0.100000, 'C': 2.025209, 'alpha': 0.056080},\n            2: {'L_0': 3.306184, 'L_inf': 2.808446, 'C': 0.299215, 'alpha': 0.101365},\n            3: {'L_0': 3.155623, 'L_inf': 0.100000, 'C': 2.414588, 'alpha': 0.030772},\n            4: {'L_0': 1.832974, 'L_inf': 0.100000, 'C': 1.022308, 'alpha': 0.092419},\n            5: {'L_0': 3.434413, 'L_inf': 0.100000, 'C': 2.819827, 'alpha': 0.042872},\n        },\n        '410M': {\n            1: {'L_0': 2.831881, 'L_inf': 0.100000, 'C': 1.971811, 'alpha': 0.055358},\n            2: {'L_0': 3.230276, 'L_inf': 2.748866, 'C': 0.304027, 'alpha': 0.075641},\n            3: {'L_0': 3.098252, 'L_inf': 0.100000, 'C': 2.348189, 'alpha': 0.031759},\n            4: {'L_0': 1.779367, 'L_inf': 0.524991, 'C': 0.569721, 'alpha': 0.146568},\n            5: {'L_0': 3.374611, 'L_inf': 0.100000, 'C': 2.743305, 'alpha': 0.044620},\n        },\n    }\n\n    def predict_loss(proportion: float, domain: int, group: str) -> float:\n        \"\"\"\n        Predict the validation loss for a domain given its proportion in training data.\n\n        Scaling law:\n            loss_i = L_0_i                          if proportion_i = 0\n            loss_i = L_inf_i + C_i * proportion_i^(-alpha_i)   if proportion_i > 0\n\n        where:\n            - L_0_i is the loss when domain i has zero proportion (intrinsic difficulty)\n            - L_inf_i is the asymptotic loss as proportion_i → ∞\n            - C_i is a scaling coefficient\n            - alpha_i is the scaling exponent (how fast loss decreases with proportion)\n        \"\"\"\n        p = params[group][domain]\n\n        if proportion == 0:\n            return p['L_0']\n        else:\n            return p['L_inf'] + p['C'] * (proportion ** (-p['alpha']))\n\n    # Generate predictions for each input data point\n    results = []\n\n    for data_point in input_data:\n        predictions = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            proportion_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            if proportion_key in data_point:\n                proportion = data_point[proportion_key]\n                predicted_loss = predict_loss(proportion, domain, group)\n                predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "opencode", "model_name": "gpt-5", "reward_r2": 0.970459, "solution": "from typing import List, Dict\nimport math\n\n# Offset power-law with asymptote per group and domain:\n# loss_domain_i = a + b * (proportion_domain_i + d) ** c\n# Parameters were fitted on the provided dataset.\n_PARAMS = {\n    \"70M\": {\n        1: {\"a\": 2.1328, \"b\": 0.4344, \"c\": -0.25, \"d\": 0.01320},\n        2: {\"a\": 3.5859, \"b\": 0.0212, \"c\": -0.80, \"d\": 0.05000},\n        3: {\"a\": 2.6248, \"b\": 0.3145, \"c\": -0.20, \"d\": 0.00349},\n        4: {\"a\": 1.3108, \"b\": 0.1693, \"c\": -0.40, \"d\": 0.01320},\n        5: {\"a\": 2.8438, \"b\": 0.6006, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"160M\": {\n        1: {\"a\": 1.7899, \"b\": 0.4896, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.2759, \"b\": 0.0149, \"c\": -0.75, \"d\": 0.03208},\n        3: {\"a\": 2.4432, \"b\": 0.2486, \"c\": -0.20, \"d\": 0.00224},\n        4: {\"a\": 0.9249, \"b\": 0.3149, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.4777, \"b\": 0.6157, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"305M\": {\n        1: {\"a\": 1.6614, \"b\": 0.4760, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.1208, \"b\": 0.0108, \"c\": -0.95, \"d\": 0.05000},\n        3: {\"a\": 2.3548, \"b\": 0.2163, \"c\": -0.20, \"d\": 0.00143},\n        4: {\"a\": 0.8390, \"b\": 0.3014, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.3085, \"b\": 0.6178, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"410M\": {\n        1: {\"a\": 1.6083, \"b\": 0.4704, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.0372, \"b\": 0.0275, \"c\": -0.45, \"d\": 0.01320},\n        3: {\"a\": 2.2824, \"b\": 0.2203, \"c\": -0.20, \"d\": 0.00143},\n        4: {\"a\": 0.8117, \"b\": 0.2936, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.2138, \"b\": 0.6352, \"c\": -0.20, \"d\": 0.05000},\n    },\n}\n\n# Fallback parameters: average across known groups per domain.\n# This is used if an unknown group is requested.\n_avg_params = {}\nfor dom in range(1, 6):\n    vals = [g[dom] for g in _PARAMS.values()]\n    _avg_params[dom] = {\n        \"a\": sum(v[\"a\"] for v in vals) / len(vals),\n        \"b\": sum(v[\"b\"] for v in vals) / len(vals),\n        \"c\": sum(v[\"c\"] for v in vals) / len(vals),\n        \"d\": sum(v[\"d\"] for v in vals) / len(vals),\n    }\n\n\ndef _predict_for_group(row: Dict[str, float], group: str) -> Dict[str, float]:\n    params = _PARAMS.get(group, _avg_params)\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n        if p < 0.0:\n            p = 0.0\n        pr = params[i]\n        a = pr[\"a\"]\n        b = pr[\"b\"]\n        c = pr[\"c\"]\n        d = pr[\"d\"]\n        y = a + b * ((p + d) ** c)\n        out[f\"loss_domain_{i}\"] = float(y)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    return [_predict_for_group(row, group) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "goose", "model_name": "gpt-5", "reward_r2": 0.970426, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Discovered functional form (per-domain, per-group parameters):\n#   loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + c_{g,i}) ** (-alpha_{g,i})\n# The same functional form is used for all groups g; parameters differ by group and domain.\n#\n# We fitted these parameters on the provided dataset. For the four observed groups\n# ('70M','160M','305M','410M'), we hard-code the best parameters found via a coarse grid\n# over (alpha, c) and least-squares fit for (a, b). For unseen groups, we linearly\n# regress each parameter versus log(model_size) and use that trend to extrapolate.\n\n# Parameters per observed group (a, b, alpha, c) per domain index 1..5\n_PARAMS_BY_GROUP: Dict[str, Dict[int, Dict[str, float]]] = {\n    '70M': {\n        1: {'a': 1.9887972995594814, 'b': 0.5747539023729472, 'alpha': 0.2, 'c': 0.010628090822653227},\n        2: {'a': 3.478375647232149,  'b': 0.10946818216563689,'alpha': 0.29743589743589743, 'c': 0.022132450145006925},\n        3: {'a': 2.662255623173122,  'b': 0.29249248967351243,'alpha': 0.2, 'c': 0.002944084977655257},\n        4: {'a': 1.3106586089315084, 'b': 0.17087362129489647,'alpha': 0.3948717948717949, 'c': 0.012767292740982497},\n        5: {'a': 3.269825262023636,  'b': 0.18572717232438435,'alpha': 0.5897435897435898, 'c': 0.11529793501972682},\n    },\n    '160M': {\n        1: {'a': 1.7790306985292368, 'b': 0.49777281762832476,'alpha': 0.2, 'c': 0.008847319226258555},\n        2: {'a': 3.147643888465916,  'b': 0.12605528549680592,'alpha': 0.2, 'c': 0.008847319226258555},\n        3: {'a': 2.426971402916676,  'b': 0.25800433609650775,'alpha': 0.2, 'c': 0.002450793850108051},\n        4: {'a': 0.7868481183561111, 'b': 0.4401977738333642, 'alpha': 0.2, 'c': 0.007364921771696289},\n        5: {'a': 2.453268819807029,  'b': 0.6393110153314099, 'alpha': 0.2, 'c': 0.05536652819843693},\n    },\n    '305M': {\n        1: {'a': 1.650844002886236,  'b': 0.48398936723908575,'alpha': 0.2, 'c': 0.008847319226258555},\n        2: {'a': 3.145345522432937,  'b': 0.000842624257026407,'alpha': 2.9282051282051285, 'c': 0.16638288403323945},\n        3: {'a': 2.329970918889095,  'b': 0.2305617062841119, 'alpha': 0.2, 'c': 0.0016983205437168204},\n        4: {'a': 0.7519113608459367, 'b': 0.39047606538528545,'alpha': 0.2, 'c': 0.006130904889496624},\n        5: {'a': 2.3271583641121616, 'b': 0.5999351396187924, 'alpha': 0.2, 'c': 0.046089684177061926},\n    },\n    '410M': {\n        1: {'a': 1.6401646612679839, 'b': 0.4464062192995915, 'alpha': 0.2, 'c': 0.007364921771696289},\n        2: {'a': 3.0569561734470225, 'b': 0.012635254264443395,'alpha': 0.6871794871794872, 'c': 0.022132450145006925},\n        3: {'a': 2.257122316003787,  'b': 0.23487158519383375,'alpha': 0.2, 'c': 0.0016983205437168204},\n        4: {'a': 0.6830677895878933, 'b': 0.4103719579323839, 'alpha': 0.2, 'c': 0.007364921771696289},\n        5: {'a': 2.2329632735094207, 'b': 0.6168094915316678, 'alpha': 0.2, 'c': 0.046089684177061926},\n    },\n}\n\n# Linear-in-log(size) trend for unseen groups: parameter ~= u + v * log(model_size)\n# model_size parsed from group string (e.g., '70M' -> 70e6). Values fitted from the same training data.\n# Structure: FITS[domain][param] = (u, v)\n_FITS_BY_LOGSIZE: Dict[int, Dict[str, tuple[float, float]]] = {\n    1: {\n        'a': (5.660934950865894, -0.20419887240098242),\n        'b': (1.7767893971925035, -0.0668774826289396),\n        'alpha': (0.2, 0.0),\n        'c': (0.03922594390798102, -0.0015882162200686668),\n    },\n    2: {\n        'a': (7.408699678416423, -0.22020436827726178),\n        'b': (1.3613732788033166, -0.06808625950778105),\n        'alpha': (-14.612320116647743, 0.8197106109208487),\n        'c': (-0.6519810182884438, 0.03704583868478927),\n    },\n    3: {\n        'a': (6.678219554420149, -0.2232189634307364),\n        'b': (0.9269627425670832, -0.035270492078040415),\n        'alpha': (0.2, 0.0),\n        'c': (0.016804595335682817, -0.0007655292505396046),\n    },\n    4: {\n        'a': (7.3750527374798045, -0.34023825047309186),\n        'b': (-2.0178969199781505, 0.12425623940963766),\n        'alpha': (2.3019722108868343, -0.10760983274344801),\n        'c': (0.07115201468643739, -0.0032884283026312243),\n    },\n    5: {\n        'a': (13.48467303818107, -0.5719893785046406),\n        'b': (-3.9077155402955737, 0.2315531989231151),\n        'alpha': (4.4039444217736685, -0.21521966548689597),\n        'c': (0.8131752563511551, -0.039174158612878644),\n    },\n}\n\n\ndef _parse_group_size(group: str) -> float | None:\n    \"\"\"Parse group string like '70M', '1.3B' into a numeric size in tokens.\n\n    Returns None if parsing fails.\n    \"\"\"\n    if not isinstance(group, str) or not group:\n        return None\n    s = group.strip().upper()\n    try:\n        if s.endswith('B'):\n            return float(s[:-1]) * 1e9\n        if s.endswith('M'):\n            return float(s[:-1]) * 1e6\n        if s.endswith('K'):\n            return float(s[:-1]) * 1e3\n        # Fallback: raw number\n        return float(s)\n    except Exception:\n        return None\n\n\ndef _get_params_for_group(group: str) -> Dict[int, Dict[str, float]]:\n    # If we have exact parameters for this group, return them.\n    if group in _PARAMS_BY_GROUP:\n        return _PARAMS_BY_GROUP[group]\n\n    # Otherwise, extrapolate/interpolate using linear fit versus log(size).\n    size = _parse_group_size(group)\n    if size is None or size <= 0:\n        # Fallback to median known group parameters (use 160M as a reasonable default)\n        return _PARAMS_BY_GROUP['160M']\n\n    logn = math.log(size)\n    params: Dict[int, Dict[str, float]] = {}\n    for d, fits in _FITS_BY_LOGSIZE.items():\n        a_u, a_v = fits['a']\n        b_u, b_v = fits['b']\n        al_u, al_v = fits['alpha']\n        c_u, c_v = fits['c']\n        a = a_u + a_v * logn\n        b = b_u + b_v * logn\n        alpha = al_u + al_v * logn\n        c = c_u + c_v * logn\n        # Guardrails: keep parameters in reasonable ranges\n        alpha = max(0.05, float(alpha))\n        c = max(1e-6, float(c))\n        b = max(1e-8, float(b))\n        params[d] = {'a': float(a), 'b': float(b), 'alpha': float(alpha), 'c': float(c)}\n    return params\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys: 'proportion_domain_1'..'_5'.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s):\n        'loss_domain_1'..'_5'.\n    \"\"\"\n    params_by_domain = _get_params_for_group(group)\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        out: Dict[str, float] = {}\n        for d in range(1, 6):\n            p = float(row.get(f'proportion_domain_{d}', 0.0))\n            par = params_by_domain[d]\n            a = par['a']\n            b = par['b']\n            alpha = par['alpha']\n            c = par['c']\n            # Ensure numerical stability\n            x = max(0.0, p) + c\n            y = a + b * (x ** (-alpha))\n            out[f'loss_domain_{d}'] = float(y)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-haiku-4-5", "reward_r2": 0.968671, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # The discovered scaling law: loss_domain_i = baseline_i + coeff_i * proportion_domain_i^alpha\n    # Where alpha is a universal exponent that applies to all domains and groups\n\n    ALPHA = 0.2724489795918367\n\n    # Fitted parameters for each domain and group\n    PARAMETERS = {\n        1: {\n            '70M': {'baseline': 3.406631, 'coeff': -0.944830},\n            '160M': {'baseline': 3.048356, 'coeff': -0.877175},\n            '305M': {'baseline': 2.884640, 'coeff': -0.852095},\n            '410M': {'baseline': 2.816754, 'coeff': -0.841401},\n        },\n        2: {\n            '70M': {'baseline': 3.818985, 'coeff': -0.259324},\n            '160M': {'baseline': 3.471953, 'coeff': -0.237293},\n            '305M': {'baseline': 3.306313, 'coeff': -0.225400},\n            '410M': {'baseline': 3.229731, 'coeff': -0.217373},\n        },\n        3: {\n            '70M': {'baseline': 3.600080, 'coeff': -0.919650},\n            '160M': {'baseline': 3.284799, 'coeff': -0.875579},\n            '305M': {'baseline': 3.154179, 'coeff': -0.904445},\n            '410M': {'baseline': 3.096721, 'coeff': -0.921298},\n        },\n        4: {\n            '70M': {'baseline': 2.261856, 'coeff': -0.996885},\n            '160M': {'baseline': 1.959060, 'coeff': -0.888851},\n            '305M': {'baseline': 1.828874, 'coeff': -0.850592},\n            '410M': {'baseline': 1.775782, 'coeff': -0.828452},\n        },\n        5: {\n            '70M': {'baseline': 3.963172, 'coeff': -0.528051},\n            '160M': {'baseline': 3.626490, 'coeff': -0.542729},\n            '305M': {'baseline': 3.461341, 'coeff': -0.544839},\n            '410M': {'baseline': 3.399876, 'coeff': -0.561194},\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            proportion_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            if proportion_key in data_point:\n                proportion = data_point[proportion_key]\n\n                # Get the fitted parameters for this domain and group\n                baseline = PARAMETERS[domain][group]['baseline']\n                coeff = PARAMETERS[domain][group]['coeff']\n\n                # Apply the scaling law: loss = baseline + coeff * proportion^alpha\n                if proportion == 0:\n                    # When proportion is 0, loss equals baseline\n                    predicted_loss = baseline\n                else:\n                    predicted_loss = baseline + coeff * (proportion ** ALPHA)\n\n                prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-sonnet-4-5", "reward_r2": 0.968336, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fitted parameters for each (group, domain) pair\n    # Model: loss_domain_i = a + b / (proportion_domain_i + eps)\n    params = {\n        '70M': {\n            1: {'a': 2.5662, 'b': 0.0488, 'eps': 0.0576},\n            2: {'a': 3.5963, 'b': 0.0141, 'eps': 0.0636},\n            3: {'a': 3.0578, 'b': 0.0084, 'eps': 0.0155},\n            4: {'a': 1.5025, 'b': 0.0288, 'eps': 0.0377},\n            5: {'a': 3.3529, 'b': 0.1127, 'eps': 0.1952},\n        },\n        '160M': {\n            1: {'a': 2.2834, 'b': 0.0402, 'eps': 0.0519},\n            2: {'a': 3.2866, 'b': 0.0083, 'eps': 0.0445},\n            3: {'a': 2.7769, 'b': 0.0073, 'eps': 0.0143},\n            4: {'a': 1.2831, 'b': 0.0255, 'eps': 0.0375},\n            5: {'a': 2.9952, 'b': 0.1205, 'eps': 0.2034},\n        },\n        '305M': {\n            1: {'a': 2.1469, 'b': 0.0374, 'eps': 0.0498},\n            2: {'a': 3.1226, 'b': 0.0097, 'eps': 0.0528},\n            3: {'a': 2.6482, 'b': 0.0059, 'eps': 0.0117},\n            4: {'a': 1.1838, 'b': 0.0240, 'eps': 0.0370},\n            5: {'a': 2.8383, 'b': 0.1097, 'eps': 0.1856},\n        },\n        '410M': {\n            1: {'a': 2.0943, 'b': 0.0350, 'eps': 0.0476},\n            2: {'a': 3.0684, 'b': 0.0057, 'eps': 0.0351},\n            3: {'a': 2.5829, 'b': 0.0059, 'eps': 0.0115},\n            4: {'a': 1.1439, 'b': 0.0241, 'eps': 0.0379},\n            5: {'a': 2.7604, 'b': 0.1109, 'eps': 0.1828},\n        },\n    }\n\n    # Get parameters for the specified group\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    # Generate predictions\n    results = []\n    for data_point in input_data:\n        predictions = {}\n\n        # Predict loss for each domain\n        for domain_idx in range(1, 6):\n            proportion_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n\n            # Get the proportion value\n            if proportion_key not in data_point:\n                raise ValueError(f\"Missing input key: {proportion_key}\")\n\n            proportion = data_point[proportion_key]\n\n            # Apply the scaling law: loss = a + b / (proportion + eps)\n            p = group_params[domain_idx]\n            predicted_loss = p['a'] + p['b'] / (proportion + p['eps'])\n\n            predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-2.5-flash", "reward_r2": 0.9683293523927846, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters (A, B, C) for each group and domain\n    # Structure: fitted_params[group][domain_key] = {'A': A_val, 'B': B_val, 'C': C_val}\n    fitted_params = {\n        '70M': {\n            'domain_1': {'A': 0.0488, 'B': 0.0576, 'C': 2.5662},\n            'domain_2': {'A': 0.0141, 'B': 0.0636, 'C': 3.5963},\n            'domain_3': {'A': 0.0084, 'B': 0.0155, 'C': 3.0578},\n            'domain_4': {'A': 0.0288, 'B': 0.0377, 'C': 1.5025},\n            'domain_5': {'A': 0.1127, 'B': 0.1952, 'C': 3.3529},\n        },\n        '160M': {\n            'domain_1': {'A': 0.0402, 'B': 0.0519, 'C': 2.2834},\n            'domain_2': {'A': 0.0083, 'B': 0.0445, 'C': 3.2866},\n            'domain_3': {'A': 0.0073, 'B': 0.0143, 'C': 2.7768},\n            'domain_4': {'A': 0.0255, 'B': 0.0375, 'C': 1.2831},\n            'domain_5': {'A': 0.1205, 'B': 0.2034, 'C': 2.9952},\n        },\n        '305M': {\n            'domain_1': {'A': 0.0374, 'B': 0.0498, 'C': 2.1469},\n            'domain_2': {'A': 0.0097, 'B': 0.0528, 'C': 3.1226},\n            'domain_3': {'A': 0.0059, 'B': 0.0117, 'C': 2.6482},\n            'domain_4': {'A': 0.0240, 'B': 0.0370, 'C': 1.1838},\n            'domain_5': {'A': 0.1097, 'B': 0.1856, 'C': 2.8383},\n        },\n        '410M': {\n            'domain_1': {'A': 0.0350, 'B': 0.0476, 'C': 2.0943},\n            'domain_2': {'A': 0.0057, 'B': 0.0351, 'C': 3.0684},\n            'domain_3': {'A': 0.0059, 'B': 0.0115, 'C': 2.5829},\n            'domain_4': {'A': 0.0241, 'B': 0.0379, 'C': 1.1439},\n            'domain_5': {'A': 0.1109, 'B': 0.1828, 'C': 2.7604},\n        },\n    }\n\n    predictions = []\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            prop_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            domain_key = f'domain_{i}'\n\n            if prop_key in data_point and group in fitted_params and domain_key in fitted_params[group]:\n                proportion = data_point[prop_key]\n                params = fitted_params[group][domain_key]\n                A, B, C = params['A'], params['B'], params['C']\n\n                # Apply the scaling law: Loss = C + A / (Proportion + B)\n                # Ensure Proportion + B is not zero; B is fitted to be positive, so this should be safe.\n                predicted_loss = C + A / (proportion + B)\n                predicted_losses[loss_key] = predicted_loss\n            else:\n                # If proportion data is missing or group/domain parameters are not found,\n                # we cannot make a prediction for this loss.\n                predicted_losses[loss_key] = float('nan') # or raise an error, or a default value\n\n        predictions.append(predicted_losses)\n\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9682354213902405, "solution": "import json\nimport math\n\n# Parameters for the inverse model: loss_i = a_i / (proportion_i + d_i) + b_i\n# Fitted for each group (70M, 160M, 305M, 410M) and each domain (1-5)\n# Format: {group: {domain: [a, d, b], ...}, ...}\n_PARAMETERS = {\n    \"70M\": {\n        1: [0.04878599086138536, 0.05761346405158714, 2.5662012376540178],\n        2: [0.014135387206822326, 0.06364850513781467, 3.596343634872201],\n        3: [0.008387904751418776, 0.015461968754208568, 3.05781474423163],\n        4: [0.028801775868037463, 0.03771563383219453, 1.5025024666563043],\n        5: [0.11267690590215215, 0.19519809588743528, 3.3529296445854784]\n    },\n    \"160M\": {\n        1: [0.04024460778235155, 0.051905757515631736, 2.283446663607906],\n        2: [0.008264021316061512, 0.044544788433985835, 3.286615779366045],\n        3: [0.007256837974328765, 0.014274056969858308, 2.77684652780886],\n        4: [0.025481918408022042, 0.037492446926947996, 1.2830889217612353],\n        5: [0.12054804290383295, 0.2034031192808916, 2.9951524979661333]\n    },\n    \"305M\": {\n        1: [0.03737314553894349, 0.04983513099255469, 2.1468583883982597],\n        2: [0.009696865197410729, 0.05283332111654462, 3.12264681601252],\n        3: [0.00592020572808425, 0.011673405159810744, 2.6482433111606776],\n        4: [0.024006354429815788, 0.03699452722522798, 1.1838139275097523],\n        5: [0.10974681269658985, 0.18557303638024017, 2.8382615981606016]\n    },\n    \"410M\": {\n        1: [0.03501902535540624, 0.04755839242291932, 2.0943034559232316],\n        2: [0.005683702629988877, 0.03511169823166188, 3.068401346850442],\n        3: [0.005902396531300411, 0.011459054884304518, 2.5829423357570143],\n        4: [0.024105740877550415, 0.03794729822983249, 1.1439330683530762],\n        5: [0.11091769605008381, 0.18275848984108722, 2.7603882181164185]\n    }\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Check if group is known\n    if group not in _PARAMETERS:\n        raise ValueError(f\"Unknown group: {group}. Available groups: {list(_PARAMETERS.keys())}\")\n\n    # Get parameters for this group\n    group_params = _PARAMETERS[group]\n\n    # For each data point, compute predictions for each domain\n    predictions = []\n    for data_point in input_data:\n        pred = {}\n        for domain in range(1, 6):\n            # Input key\n            prop_key = f\"proportion_domain_{domain}\"\n            # Output key\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get proportion value (should be present)\n            proportion = data_point.get(prop_key, 0.0)  # default 0 if missing\n\n            # Get parameters for this domain\n            a, d, b = group_params[domain]\n\n            # Apply the inverse model: loss = a / (proportion + d) + b\n            # To avoid division by zero, we use proportion + d (d is positive)\n            loss = a / (proportion + d) + b\n\n            # Store prediction\n            pred[loss_key] = loss\n\n        predictions.append(pred)\n\n    return predictions\n\n\n# Example usage (for testing only)\nif __name__ == \"__main__\":\n    # Example input\n    example_input = [\n        {\"proportion_domain_1\": 0.5, \"proportion_domain_2\": 0.1875,\n         \"proportion_domain_3\": 0.0625, \"proportion_domain_4\": 0.0625,\n         \"proportion_domain_5\": 0.1875},\n        {\"proportion_domain_1\": 0.0, \"proportion_domain_2\": 0.25,\n         \"proportion_domain_3\": 0.25, \"proportion_domain_4\": 0.25,\n         \"proportion_domain_5\": 0.25}\n    ]\n    example_group = \"70M\"\n    result = law(example_input, example_group)\n    print(\"Example predictions:\")\n    for i, pred in enumerate(result):\n        print(f\"  Data point {i}: {pred}\")"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-2.5-flash", "reward_r2": 0.968235, "solution": "import json\nimport os\n\n# Load the fitted parameters from the JSON file\ndef load_fitted_params():\n    try:\n        # Assuming fitted_params.json is in the same directory as law.py or /app\n        # During testing, the law.py might be in a different context, so we try to be robust.\n        current_dir = os.path.dirname(__file__)\n        param_path_local = os.path.join(current_dir, 'fitted_params.json')\n        param_path_app = os.path.join('/app', 'fitted_params.json')\n\n        if os.path.exists(param_path_local):\n            with open(param_path_local, 'r') as f:\n                return json.load(f)\n        elif os.path.exists(param_path_app):\n            with open(param_path_app, 'r') as f:\n                return json.load(f)\n        else:\n            raise FileNotFoundError(\"fitted_params.json not found in current directory or /app.\")\n    except Exception as e:\n        print(f\"Error loading fitted parameters: {e}\")\n        # Fallback to empty parameters if loading fails, or raise a more specific error\n        return {}\n\nFITTED_PARAMS = load_fitted_params()\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    predictions = []\n    group_params = FITTED_PARAMS.get(group)\n\n    if not group_params:\n        # Handle cases where the group is not found in the fitted parameters.\n        # This could return default values, raise an error, or return None/NaN.\n        # For now, let's return None for all predictions.\n        print(f\"Warning: Group '{group}' not found in fitted parameters. Returning None for losses.\")\n        for _ in input_data:\n            predicted_losses = {}\n            for i in range(1, 6):\n                predicted_losses[f'loss_domain_{i}'] = None\n            predictions.append(predicted_losses)\n        return predictions\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            \n            proportion = data_point.get(proportion_key)\n            domain_params = group_params.get(f'domain_{i}')\n\n            if proportion is None or domain_params is None or any(p is None for p in domain_params.values()):\n                predicted_losses[loss_key] = None\n                continue\n            \n            A = domain_params['A']\n            k = domain_params['k']\n            B = domain_params['B']\n            \n            # Apply the scaling law: loss = A / (proportion + k) + B\n            # Ensure k is not extremely small if proportion is 0 or very small to avoid overflow\n            # We've bounded k to be >= 0.0001 during fitting.\n            predicted_loss = A / (proportion + k) + B\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n        \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.967271, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nAdvanced scaling law for multi-domain LLM training with robust optimization\nCombines power laws, pairwise interactions, and entropy regularization (35 parameters)\nUses differential evolution + multi-phase L-BFGS-B for reliable convergence\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Multi-domain scaling law predicting 5 domain losses from mixture proportions.\n    \n    Parameters (35 total):\n    - [0:5]: Base losses per domain (1.5-4.5)\n    - [5:10]: Domain weight coefficients (0.01-10.0)\n    - [10:20]: Pairwise interaction coefficients (10 pairs, -5.0-5.0)\n    - [20:30]: Polynomial correction terms - quadratic + linear (2 per domain)\n    - [30:35]: Entropy regularization coefficients (-2.0-2.0)\n    \n    Returns: (N, 5) array of predicted multi-domain losses\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64)\n    \n    # Extract and clip parameter groups for stability\n    base_loss = np.asarray(params[0:5], dtype=np.float64)\n    weight_coef = np.clip(params[5:10], 0.01, 10.0)\n    interactions = np.asarray(params[10:20], dtype=np.float64)\n    poly_coefs = np.asarray(params[20:30], dtype=np.float64)\n    entropy_coefs = np.clip(params[30:35], -2.0, 2.0)\n    \n    # Safely clip input proportions\n    X_safe = np.clip(X, 1e-8, 1.0)\n    \n    # Initialize predictions (N, 5)\n    pred = np.zeros((N, 5), dtype=np.float64)\n    \n    for d in range(5):\n        # Base component: constant per domain\n        pred[:, d] = base_loss[d]\n        \n        # Domain-specific weighted term: coef * x_d\n        pred[:, d] += weight_coef[d] * X_safe[:, d]\n        \n        # Pairwise interaction terms from upper triangle of 5x5 matrix\n        # This creates symmetric interactions between domains\n        idx = 0\n        for i in range(5):\n            for j in range(i + 1, 5):\n                # Interaction between domains i and j affects domain d\n                interaction_strength = interactions[idx]\n                pred[:, d] += interaction_strength * X_safe[:, i] * X_safe[:, j]\n                idx += 1\n        \n        # Polynomial correction terms: quadratic + linear adjustments\n        # Quadratic saturation term\n        pred[:, d] += poly_coefs[d] * (X_safe[:, d] ** 2)\n        # Linear correction term (second derivative-like behavior)\n        pred[:, d] += poly_coefs[d + 5] * X_safe[:, d]\n        \n        # Entropy regularization: penalize extreme domain concentrations\n        # Shannon entropy approximation: -x*ln(x)\n        # This regularizes the model to not overfit to concentrated mixtures\n        safe_x = np.clip(X_safe[:, d], 1e-8, 1.0)\n        entropy_term = -safe_x * np.log(safe_x + 1e-8)\n        pred[:, d] += entropy_coefs[d] * entropy_term\n    \n    # Ensure predictions stay in valid loss range\n    return np.clip(pred, 0.5, 5.5)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit scaling law using hybrid optimization strategy:\n    1. Global search with differential_evolution to escape local minima\n    2. Local refinement with L-BFGS-B for fine-tuning\n    3. Multiple targeted restarts to ensure robustness\n    \n    Args:\n        data_points: (N, 5) array of domain mixture proportions\n        loss_values: (N, 5) array of multi-domain losses\n    \n    Returns:\n        params: (35,) array of optimized parameters\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    \n    # Handle shape variations in input\n    if y.shape[0] == 1 and y.shape[1] > 5:\n        y = y.T\n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N, F = X.shape\n    \n    # Data normalization for improved numerical conditioning\n    y_mean = np.mean(y, axis=0, keepdims=True)\n    y_std = np.std(y, axis=0, keepdims=True)\n    y_std = np.where(y_std < 1e-8, 1.0, y_std)\n    y_normalized = (y - y_mean) / y_std\n    \n    def objective(params_flat):\n        \"\"\"Objective function: normalized MSE with multi-component regularization.\"\"\"\n        try:\n            pred = scaling_law_func(X, params_flat)\n            pred_normalized = (pred - y_mean) / y_std\n            \n            # Main error term: MSE on normalized data\n            mse = np.mean((pred_normalized - y_normalized) ** 2)\n            \n            # Multi-component regularization for stability\n            # 1. Penalize extreme interaction coefficients\n            reg_interactions = 0.002 * np.sum(params_flat[10:20] ** 2)\n            \n            # 2. Penalize divergence of base_loss from reasonable range [1.5, 4.5]\n            reg_base = 0.001 * np.sum((np.clip(params_flat[0:5], 1.5, 4.5) - params_flat[0:5]) ** 2)\n            \n            # Total objective with regularization\n            total_reg = reg_interactions + reg_base\n            \n            return mse + total_reg\n        except:\n            return 1e10\n    \n    # Define tight parameter bounds for stability\n    bounds = [\n        (1.5, 4.5),      # base_loss[0:5] - reasonable CE loss range\n        (1.5, 4.5),\n        (1.5, 4.5),\n        (1.5, 4.5),\n        (1.5, 4.5),\n        (0.01, 10.0),    # weight_coef[5:10] - positive domain sensitivity\n        (0.01, 10.0),\n        (0.01, 10.0),\n        (0.01, 10.0),\n        (0.01, 10.0),\n        (-5.0, 5.0),     # interactions[10:20] - upper triangle pairs\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-5.0, 5.0),\n        (-2.0, 2.0),     # poly_coefs[20:30] - polynomial corrections\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),     # entropy_coefs[30:35] - entropy regularization\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n        (-2.0, 2.0),\n    ]\n    \n    # ===== PHASE 1: Global Search with Differential Evolution =====\n    result_de = differential_evolution(\n        objective,\n        bounds,\n        seed=42,\n        maxiter=300,\n        popsize=15,\n        atol=1e-10,\n        tol=1e-10,\n        workers=1,\n        polish=True,\n        updating='deferred',\n        mutation=(0.5, 1.5),\n        recombination=0.7\n    )\n    \n    params_best = result_de.x\n    best_loss = result_de.fun\n    \n    # ===== PHASE 2: Local Refinement from DE Solution =====\n    result_local1 = minimize(\n        objective,\n        params_best,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 250, 'ftol': 1e-11, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    \n    if result_local1.fun < best_loss:\n        params_best = result_local1.x\n        best_loss = result_local1.fun\n    \n    # ===== PHASE 3: Multiple Targeted Restarts =====\n    for restart_idx in range(4):\n        # Generate small perturbation with adaptive magnitude\n        perturbation_scale = 0.01 * (1.0 - restart_idx / 5.0)\n        x0_pert = params_best + np.random.randn(35) * perturbation_scale * (\n            np.abs(params_best) + 0.1\n        )\n        \n        # Clip perturbed parameters to bounds\n        x0_pert = np.array([\n            np.clip(x0_pert[i], bounds[i][0], bounds[i][1])\n            for i in range(35)\n        ])\n        \n        result_pert = minimize(\n            objective,\n            x0_pert,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 200, 'ftol': 1e-11, 'gtol': 1e-8, 'maxcor': 20}\n        )\n        \n        if result_pert.fun < best_loss:\n            params_best = result_pert.x\n            best_loss = result_pert.fun\n    \n    return params_best\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5", "reward_r2": 0.966399, "solution": "from __future__ import annotations\nimport math\nfrom typing import Dict, List\n\n# Discovered functional form (same for all groups):\n#   loss_domain_i = a[group][i] + b[group][i] * log(proportion_domain_i + EPS)\n# A small epsilon handles zero proportions.\nEPS = 0.003125  # min positive proportion observed / 10\n\n# Fitted coefficients (a, b) per group and domain i in {1..5}\nCOEFS: Dict[str, Dict[int, tuple[float, float]]] = {\n    \"160M\": {\n        1: (2.2424684059708717, -0.1412039367794934),\n        2: (3.2615055117992546, -0.036505329124803795),\n        3: (2.503204375028289, -0.13515973441168078),\n        4: (1.167057464525046, -0.1374903829439023),\n        5: (3.1050702723961345, -0.09354145265121051),\n    },\n    \"305M\": {\n        1: (2.101566717029658, -0.137250480526824),\n        2: (3.1064301933569487, -0.034670424237437165),\n        3: (2.347069418836081, -0.1395498701909293),\n        4: (1.0709224798061698, -0.13158540389099785),\n        5: (2.9376949827197802, -0.09412677108369148),\n    },\n    \"410M\": {\n        1: (2.0433633841009002, -0.1355817799554127),\n        2: (3.036930564759394, -0.033447198672712085),\n        3: (2.2745949072179825, -0.14214431175511827),\n        4: (1.0375379829435523, -0.1281673168777551),\n        5: (2.860506493994419, -0.09695843157676008),\n    },\n    \"70M\": {\n        1: (2.538957210154492, -0.15195781604593908),\n        2: (3.589039351229478, -0.03988167092985459),\n        3: (2.7789903294958576, -0.1420028367096475),\n        4: (1.3734832511282675, -0.15423705944877764),\n        5: (3.455746153156199, -0.09113184327935854),\n    },\n}\n\n# Fallback coefficients: average across known groups per domain (used if group not found)\nAVG_COEFS = {\n    i: (\n        sum(COEFS[g][i][0] for g in COEFS) / len(COEFS),\n        sum(COEFS[g][i][1] for g in COEFS) / len(COEFS),\n    )\n    for i in range(1, 6)\n}\n\n\ndef _resolve_group(group: str) -> Dict[int, tuple[float, float]]:\n    if group in COEFS:\n        return COEFS[group]\n    # Try numeric nearest match like '300M' -> closest of known keys\n    import re\n    m = re.search(r\"(\\d+(?:\\.\\d+)?)\", group)\n    if m:\n        target = float(m.group(1))\n        def num(k: str) -> float:\n            mk = re.search(r\"(\\d+(?:\\.\\d+)?)\", k)\n            return float(mk.group(1)) if mk else float(\"inf\")\n        nearest = min(COEFS.keys(), key=lambda k: abs(num(k) - target))\n        return COEFS[nearest]\n    return AVG_COEFS  # last-resort fallback\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups, while\n                coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coefs = _resolve_group(group)\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        out: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            a, b = coefs[i]\n            out[f\"loss_domain_{i}\"] = a + b * math.log(p + EPS)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-haiku-4-5", "reward_r2": 0.96542, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law follows a power law model: loss_domain_i = a_i * (proportion_domain_i)^(-b_i)\n    where a_i and b_i are domain-specific coefficients that vary by model size group.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys are 'proportion_domain_1'\n                    through 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n                Should be one of: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted loss_domain_1 through loss_domain_5.\n    \"\"\"\n\n    # Fitted parameters for power law model: loss = a * (proportion)^(-b)\n    # These were fitted using scipy.optimize.curve_fit on the experimental dataset\n    parameters = {\n        \"160M\": {\n            1: {\"a\": 2.2622394738070954, \"b\": 0.05305230055807545},\n            2: {\"a\": 3.2646400859561724, \"b\": 0.010446160735262215},\n            3: {\"a\": 2.6379005393307238, \"b\": 0.030809881360294254},\n            4: {\"a\": 1.2168896281087662, \"b\": 0.08337725170858164},\n            5: {\"a\": 3.0857224560131016, \"b\": 0.039875164268842474},\n        },\n        \"305M\": {\n            1: {\"a\": 2.124886418748417, \"b\": 0.053673219712720203},\n            2: {\"a\": 3.101532158667605, \"b\": 0.011722348365702549},\n            3: {\"a\": 2.5141204041736103, \"b\": 0.029664627417788678},\n            4: {\"a\": 1.1208478064340652, \"b\": 0.08553463709842556},\n            5: {\"a\": 2.9197997341340862, \"b\": 0.04145492785494071},\n        },\n        \"410M\": {\n            1: {\"a\": 2.071494574419323, \"b\": 0.052918397685703625},\n            2: {\"a\": 3.04952958026863, \"b\": 0.008635628449930979},\n            3: {\"a\": 2.447692223080657, \"b\": 0.03058843279314549},\n            4: {\"a\": 1.0827937621785138, \"b\": 0.0874293859289332},\n            5: {\"a\": 2.8432374398519316, \"b\": 0.04312786024778481},\n        },\n        \"70M\": {\n            1: {\"a\": 2.547611674057732, \"b\": 0.05370607557144707},\n            2: {\"a\": 3.5729182354113966, \"b\": 0.013180493524131429},\n            3: {\"a\": 2.908629500537471, \"b\": 0.030739009328856415},\n            4: {\"a\": 1.4271407005367267, \"b\": 0.08068857938614302},\n            5: {\"a\": 3.436049097361814, \"b\": 0.03538256136605673},\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        for domain in range(1, 6):\n            prop_key = f\"proportion_domain_{domain}\"\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get proportion for this domain\n            proportion = data_point[prop_key]\n\n            # Get parameters for this domain and group\n            params = parameters[group][domain]\n            a = params[\"a\"]\n            b = params[\"b\"]\n\n            # Apply power law: loss = a * (proportion)^(-b)\n            if proportion > 0:\n                predicted_loss = a * (proportion ** (-b))\n            else:\n                # When proportion is 0, the loss is theoretically undefined (approaches infinity)\n                # For practical purposes, we use the baseline value (approaching when prop -> 0)\n                # which is approximated by the maximum observed baseline\n                predicted_loss = a * (0.001 ** (-b))  # Use a small value as proxy\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "goose", "model_name": "gpt-5", "reward_r2": 0.965021, "solution": "from __future__ import annotations\n\nfrom typing import Dict, List\n\n# Discovered scaling law (same functional form for all groups):\n#   loss_domain_i(p) = a_i + b_i * p + c_i * sqrt(p_eps) + d_i / sqrt(p_eps)\n# where p_eps = max(p, eps) with eps = 1e-8 for numerical stability at p=0.\n# Coefficients (a_i, b_i, c_i, d_i) vary by experimental group and by domain i.\n# These were fit via least squares on the provided dataset.\n\nimport math\n\n# Group- and domain-specific parameters (a, b, c, d)\n_PARAMS: Dict[str, Dict[int, List[float]]] = {\n    # 160M\n    \"160M\": {\n        1: [2.852643507112996, 0.5079373888086807, -1.0796605253398814, 2.0787176420883635e-05],\n        2: [3.354383873226113, -0.37869509907848437, 0.0896600728386695, 1.177445603007801e-05],\n        3: [2.347377598500148, -13.805640202637328, 5.641670151072003, 9.376136946540265e-05],\n        4: [1.8042334785143834, 0.8353298815978365, -1.2981302396786871, 1.589539097983332e-05],\n        5: [3.5849119562787277, 0.2559696004923059, -0.758695672520524, 1.0076550902818928e-06],\n    },\n    # 305M\n    \"305M\": {\n        1: [2.706834215254821, 0.5880677740198962, -1.1276437000774089, 1.9130955209367118e-05],\n        2: [3.2017362691741527, -0.37187187552261475, 0.07149899259874355, 1.0444032008682824e-05],\n        3: [2.169244558185138, -14.140803471052271, 5.865028311622053, 9.857923560960747e-05],\n        4: [1.7004746672414206, 0.9689402903601732, -1.3627948240241132, 1.326360669137494e-05],\n        5: [3.4565779916529773, 0.38064813646972745, -0.9095505284674135, -2.207395814670825e-06],\n    },\n    # 410M\n    \"410M\": {\n        1: [2.6309647984255844, 0.5673263318126346, -1.085609650668732, 2.010250371618536e-05],\n        2: [3.112610624866361, -0.3369700553653706, 0.10271603621695127, 1.1765535009316992e-05],\n        3: [2.090987095821702, -14.41194746580547, 5.986201940650726, 0.00010066659956643521],\n        4: [1.6814138568367192, 1.1507555128736286, -1.4921179761773755, 9.810216859529726e-06],\n        5: [3.3436836877501346, 0.2595236072021162, -0.7633518960486015, 3.100351755000551e-06],\n    },\n    # 70M\n    \"70M\": {\n        1: [3.232089856516487, 0.6342384359826007, -1.284813531062777, 1.8294673741495497e-05],\n        2: [3.7081016310626413, -0.444398338713182, 0.06425723222437253, 1.1032071859421705e-05],\n        3: [2.727368677530126, -12.547994180570411, 4.978112989985035, 8.727731059550263e-05],\n        4: [2.1695959734275863, 1.5585707751928726, -1.9218402484504162, 9.693091047463545e-06],\n        5: [4.009115069855263, 0.48733821145363176, -1.042620301077381, -7.726820572537514e-06],\n    },\n}\n\n# Fallback parameters if group not found: fit across all groups jointly per domain.\n_DEFAULT_PARAMS: Dict[int, List[float]] = {\n    1: [2.8556330943274784, 0.5743924826559886, -1.1444318517872365, 1.9578827271952478e-05],\n    2: [3.3442080995823162, -0.3829838421699124, 0.08203308346968709, 1.1254023726860987e-05],\n    3: [2.3337444824927815, -13.726596330795772, 5.617753348542388, 9.507112880974419e-05],\n    4: [1.838929494005038, 1.1283991150062023, -1.5187208220827102, 1.2165576394408536e-05],\n    5: [3.5985721763843057, 0.34586988890451575, -0.8685545995285723, -1.4565523854500717e-06],\n}\n\n\ndef _predict_loss_for_domain(p: float, coeffs: List[float]) -> float:\n    eps = 1e-8\n    p_clamped = max(min(float(p), 1.0), 0.0)\n    r = math.sqrt(max(p_clamped, eps))\n    a, b, c, d = coeffs\n    return a + b * p_clamped + c * r + d / r\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s):\n        loss_domain_1 .. loss_domain_5.\n    \"\"\"\n    group_params = _PARAMS.get(group, None)\n    if group_params is None:\n        group_params = _DEFAULT_PARAMS\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = row.get(f\"proportion_domain_{i}\")\n            if p is None:\n                # If missing, assume zero contribution for that domain.\n                p = 0.0\n            coeffs = group_params.get(i, _DEFAULT_PARAMS[i])\n            pred[f\"loss_domain_{i}\"] = _predict_loss_for_domain(p, coeffs)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2", "reward_r2": 0.9603660325813097, "solution": "import numpy as np\nfrom typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    \n    The scaling law for domain mixture in language model pre-training follows an\n    exponential decay form:\n        loss_domain_i = α_i * exp(-β_i * proportion_domain_i) + γ_i\n    \n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    \n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Parameters fitted from the training data for each group and domain\n    # Format: {group: {domain: (alpha, beta, gamma)}}\n    parameters = {\n        '70M': {\n            1: (0.739473, 12.863625, 2.666348),\n            2: (0.177182, 14.641798, 3.641247),\n            3: (0.456612, 50.442102, 3.143153),\n            4: (0.654308, 19.872508, 1.610290),\n            5: (0.461188, 4.603233, 3.458564)\n        },\n        '160M': {\n            1: (0.682762, 13.822854, 2.369861),\n            2: (0.155643, 17.604643, 3.316494),\n            3: (0.430615, 53.729461, 2.854183),\n            4: (0.582318, 19.938110, 1.378787),\n            5: (0.469066, 4.358060, 3.104265)\n        },\n        '305M': {\n            1: (0.662690, 14.266620, 2.228801),\n            2: (0.150461, 16.158770, 3.155723),\n            3: (0.437894, 61.560421, 2.717248),\n            4: (0.556845, 20.098877, 1.274425),\n            5: (0.474134, 4.678997, 2.942271)\n        },\n        '410M': {\n            1: (0.653111, 14.678035, 2.172276),\n            2: (0.139821, 19.700209, 3.090455),\n            3: (0.445505, 62.332274, 2.652279),\n            4: (0.543846, 19.799034, 1.233952),\n            5: (0.486017, 4.649797, 2.865121)\n        }\n    }\n    \n    # Check if group is valid\n    if group not in parameters:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(parameters.keys())}\")\n    \n    # Get parameters for this group\n    group_params = parameters[group]\n    \n    # Initialize result list\n    results = []\n    \n    for data_point in input_data:\n        # Initialize prediction dictionary\n        prediction = {}\n        \n        # Predict loss for each domain\n        for domain in range(1, 6):\n            prop_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n            \n            # Get proportion for this domain\n            if prop_key not in data_point:\n                raise ValueError(f\"Missing input variable: {prop_key}\")\n            \n            proportion = data_point[prop_key]\n            \n            # Get parameters for this domain\n            alpha, beta, gamma = group_params[domain]\n            \n            # Apply exponential decay law\n            # Handle edge case where proportion might be exactly 0\n            if proportion <= 0:\n                # When proportion is 0, use the limit as p->0: alpha + gamma\n                predicted_loss = alpha + gamma\n            else:\n                predicted_loss = alpha * np.exp(-beta * proportion) + gamma\n            \n            prediction[loss_key] = float(predicted_loss)\n        \n        results.append(prediction)\n    \n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.958728, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nRefined multi-output scaling law with domain-adaptive modeling\nUses 35 parameters: baselines (5) + linear (5) + log-saturation (5) + \nselective interactions (10) + dual nonlinear terms (10: 5 sqrt + 5 cbrt)\nFocus: Better domain 2 performance and cross-domain stability\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Domain-adaptive mixture model:\n    loss_d = base_d + linear + log_sat + key_interactions + sqrt_terms + cbrt_terms\n    \n    Parameters (35 total):\n    - 5: domain-specific baselines\n    - 5: linear mixture coefficients\n    - 5: log-saturation (diminishing returns)\n    - 10: selective pairwise interactions (most important pairs)\n    - 5: sqrt nonlinearity (convex effects)\n    - 5: cubic root nonlinearity (smoother than sqrt for small values)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    N, F = X.shape\n    params = np.asarray(params)\n    \n    if params.ndim == 1:\n        params = params.reshape(1, -1)\n    T, P = params.shape\n    \n    # Parameter allocation\n    baselines = params[:, :5]           # (T, 5)\n    linear_coefs = params[:, 5:10]      # (T, 5)\n    saturation = params[:, 10:15]       # (T, 5)\n    interactions = params[:, 15:25]     # (T, 10) - selective pairs\n    sqrt_terms = params[:, 25:30]       # (T, 5)\n    cbrt_terms = params[:, 30:35]       # (T, 5)\n    \n    pred = np.zeros((N, T))\n    \n    # Define most important interaction pairs (based on domain theory)\n    # Focus on adjacent and complementary domains\n    interaction_pairs = [(0,1), (0,2), (1,2), (1,3), (2,3), (2,4), (3,4), (0,3), (1,4), (0,4)]\n    \n    for t in range(T):\n        # Domain-specific baseline\n        pred[:, t] = baselines[t, t]\n        \n        # Linear mixture effects\n        pred[:, t] += np.dot(X, linear_coefs[t, :])\n        \n        # Log-saturation with numerical stability\n        pred[:, t] += np.dot(np.log1p(X), saturation[t, :])\n        \n        # Selective pairwise interactions (10 most important)\n        for idx, (i, j) in enumerate(interaction_pairs):\n            pred[:, t] += X[:, i] * X[:, j] * interactions[t, idx]\n        \n        # Sqrt terms (good for convex diminishing returns)\n        pred[:, t] += np.dot(np.sqrt(X + 1e-10), sqrt_terms[t, :])\n        \n        # Cubic root terms (smoother, better for small proportions)\n        pred[:, t] += np.dot(np.cbrt(X + 1e-10), cbrt_terms[t, :])\n    \n    return pred[:, 0] if T == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Enhanced fitting with domain-aware initialization and adaptive regularization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    \n    if y.ndim == 1:\n        y2d = y[:, None]\n    else:\n        y2d = y\n    T = y2d.shape[1]\n    \n    P = 35\n    init = np.zeros((T, P))\n    \n    # Compute domain-specific statistics for adaptive initialization\n    domain_stds = np.std(y2d, axis=0)\n    domain_ranges = np.ptp(y2d, axis=0)\n    \n    for t in range(T):\n        # Adaptive baseline: use weighted percentile based on domain variance\n        percentile = 50 if domain_stds[t] < np.median(domain_stds) else 55\n        init[t, t] = np.percentile(y2d[:, t], percentile)\n        \n        # Linear coefficients via adaptive ridge regression\n        residual = y2d[:, t] - init[t, t]\n        \n        try:\n            # Adaptive ridge lambda based on domain characteristics\n            lambda_ridge = 0.012 * np.std(residual) * (1 + 0.5 * (domain_stds[t] / np.mean(domain_stds)))\n            XTX = X.T @ X + lambda_ridge * np.eye(F)\n            XTy = X.T @ residual\n            coefs = np.linalg.solve(XTX, XTy)\n            # Scale based on domain difficulty (higher variance = more conservative)\n            scale_factor = 0.42 if domain_stds[t] > np.median(domain_stds) else 0.50\n            init[t, 5:10] = coefs * scale_factor\n        except:\n            # Robust fallback\n            for f in range(F):\n                corr = np.corrcoef(X[:, f], residual)[0, 1]\n                if not np.isnan(corr):\n                    init[t, 5+f] = corr * 0.13\n        \n        # Log-saturation: slightly negative (domain-adaptive)\n        sat_scale = -0.082 if t == 2 else -0.090  # Special treatment for domain 2\n        init[t, 10:15] = sat_scale\n        \n        # Interactions: controlled random with domain-specific variance\n        interact_std = 0.014 if domain_stds[t] < np.median(domain_stds) else 0.020\n        init[t, 15:25] = np.random.randn(10) * interact_std\n        \n        # Sqrt terms: positive with domain adaptation\n        sqrt_scale = 0.032 if t == 2 else 0.040\n        init[t, 25:30] = sqrt_scale\n        \n        # Cbrt terms: smaller magnitude, smoother effect\n        init[t, 30:35] = 0.022\n    \n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        pred = scaling_law_func(X, params)\n        \n        # MSE loss with per-domain weighting (emphasize difficult domains)\n        per_domain_mse = np.mean((pred - y2d) ** 2, axis=0)\n        # Weight harder domains more (but not too much)\n        domain_weights = 1.0 + 0.2 * (domain_stds / np.mean(domain_stds))\n        mse = np.mean(per_domain_mse * domain_weights)\n        \n        # Adaptive regularization based on parameter groups\n        reg_linear = 0.0010 * np.sum(params[:, 5:10]**2)      # Light on linear\n        reg_sat = 0.0020 * np.sum(params[:, 10:15]**2)        # Light on saturation\n        reg_interact = 0.0070 * np.sum(params[:, 15:25]**2)   # Medium on interactions\n        reg_sqrt = 0.0028 * np.sum(params[:, 25:30]**2)       # Light on sqrt\n        reg_cbrt = 0.0025 * np.sum(params[:, 30:35]**2)       # Light on cbrt\n        \n        return mse + reg_linear + reg_sat + reg_interact + reg_sqrt + reg_cbrt\n    \n    # Strategic multi-start optimization\n    best_params = init.copy()\n    best_loss = objective(init.ravel())\n    \n    restart_strategies = [\n        ('original', lambda: init),\n        ('small_noise', lambda: init + np.random.randn(T, P) * 0.025),\n        ('medium_noise', lambda: init + np.random.randn(T, P) * 0.048),\n        ('scale_perturb', lambda: init * (1 + np.random.randn(T, P) * 0.13)),\n        ('baseline_shift', lambda: shift_baseline(init.copy(), y2d, percentile=60)),\n        ('large_explore', lambda: init + np.random.randn(T, P) * 0.070),\n    ]\n    \n    def shift_baseline(params, y_data, percentile):\n        for t in range(T):\n            params[t, t] = np.percentile(y_data[:, t], percentile)\n        params += np.random.randn(T, P) * 0.022\n        return params\n    \n    for name, strategy in restart_strategies:\n        init_restart = strategy()\n        \n        result = minimize(\n            objective,\n            init_restart.ravel(),\n            method='L-BFGS-B',\n            options={'maxiter': 2000, 'ftol': 1e-11, 'gtol': 1e-8}\n        )\n        \n        if result.fun < best_loss:\n            best_loss = result.fun\n            best_params = result.x.reshape(T, P)\n    \n    return best_params[0] if T == 1 else best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "o4-mini", "reward_r2": 0.94959, "solution": "import numpy as np\n\n# EVOLVE-BLOCK-START\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Quadratic-per-domain scaling law:\n      For each domain j:\n        pred[:,j] = X @ a_j + (X[:,j]**2)*d_j + b_j\n    where a_j is a 5-vector, d_j is a scalar weight on the j-th squared input,\n    and b_j is a bias.  Total parameters = 5*(5 + 1 + 1) = 35.\n    \"\"\"\n    X = np.atleast_2d(data_points).astype(float)\n    N, D = X.shape\n    assert D == 5, \"Expected 5 domain proportions\"\n    p = params.ravel()\n    assert p.size == 35, \"Expected 35 parameters\"\n    pred = np.zeros((N, 5), dtype=float)\n    # unpack and compute per-domain prediction\n    for j in range(5):\n        base = j * 7\n        a_j = p[base:base+5]       # linear weights for domain j\n        d_j = p[base+5]            # squared-term weight for domain j\n        b_j = p[base+6]            # bias for domain j\n        pred[:, j] = X.dot(a_j) + (X[:, j]**2) * d_j + b_j\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the quadratic-per-domain model via ridge‐regularized least squares.\n    For each domain j, design matrix is [X, X[:,j]**2, 1] of shape (N,7).\n    Solve (D^T D + λI) θ = D^T y for θ of length 7, then pack into params.\n    \"\"\"\n    X = np.atleast_2d(data_points).astype(float)\n    Y = np.atleast_2d(loss_values).astype(float)\n    N, D = X.shape\n    assert D == 5 and Y.shape == (N, 5)\n    X2 = X * X\n    lam = 1e-6\n    params = np.zeros(35, dtype=float)\n    for j in range(5):\n        # build per-domain design matrix (N,7)\n        Xj2 = X2[:, j:j+1]\n        Dmat = np.hstack((X, Xj2, np.ones((N, 1), dtype=float)))\n        yj = Y[:, j]\n        # normal equations with ridge on all but bias\n        G = Dmat.T.dot(Dmat)\n        for k in range(6):\n            G[k, k] += lam\n        rhs = Dmat.T.dot(yj)\n        theta = np.linalg.solve(G, rhs)\n        # pack parameters\n        base = j * 7\n        params[base:base+5] = theta[:5]\n        params[base+5]      = theta[5]\n        params[base+6]      = theta[6]\n    return params\n\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.93872, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nCompositional scaling law with additive log-ratio (ALR) features.\nParams (<=35): W(5x4)=20, c(5), d(5), b(5)  -> total 35.\ny = alr(p) @ W.T + conc * c + ent * d + b\nFeatures:\n- alr(p)_i = log(p_i / p_5) for i=1..4 (simplex-aware, removes redundancy)\n- conc = sum(p^2) captures concentration\n- ent = -sum(p*log p) captures diversity/curvature\nFit via closed-form ridge regression over [alr(4), conc(1), ent(1), bias(1)].\n\"\"\"\nimport numpy as np\n_EPS = 1e-8\n\ndef _unpack_params(params):\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size < 35: p = np.pad(p, (0, 35 - p.size))\n    if p.size > 35: p = p[:35]\n    W = p[:20].reshape(5, 4); c = p[20:25]; d = p[25:30]; b = p[30:35]\n    return W, c, d, b\n\ndef _features(X):\n    P = np.clip(np.atleast_2d(np.asarray(X, float)), _EPS, 1.0)\n    P /= np.maximum(P.sum(1, keepdims=True), _EPS)\n    alr = np.log(P[:, :4] / P[:, [4]])                 # (N,4)\n    conc = np.sum(P*P, 1, keepdims=True)               # (N,1)\n    ent = -np.sum(P*np.log(P), 1, keepdims=True)       # (N,1)\n    return alr, conc, ent\n\ndef scaling_law_func(data_points, params):\n    W, c, d, b = _unpack_params(params)\n    alr, conc, ent = _features(data_points)\n    return alr @ W.T + conc @ c[None, :] + ent @ d[None, :] + b[None, :]\n\ndef fit_scaling_law(data_points, loss_values):\n    Y = np.asarray(loss_values, float)\n    if Y.ndim == 1: Y = Y[:, None]\n    alr, conc, ent = _features(data_points)\n    Z = np.concatenate([alr, conc, ent, np.ones((alr.shape[0], 1))], 1)  # (N,7)\n    lam = 1e-3\n    ZTZ = Z.T @ Z + lam * np.eye(7)\n    ZTY = Z.T @ Y\n    try:\n        Theta = np.linalg.solve(ZTZ, ZTY)  # (7,5)\n    except np.linalg.LinAlgError:\n        Theta, *_ = np.linalg.lstsq(ZTZ, ZTY, rcond=None)\n    W = Theta[:4, :].T\n    c = Theta[4, :]\n    d = Theta[5, :]\n    b = Theta[6, :]\n    return np.concatenate([W.ravel(), c, d, b])\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o3", "reward_r2": 0.9379072323150286, "solution": "import math\n\neps = 1e-4  # small constant to avoid log(0)\n\n# Pre-computed coefficients for each model size (group)\n# loss_domain_i = intercept[group][i] + slope[group][i] * ln(proportion_domain_i + eps)\n_slope = {\n    '70M':   {1: -0.08576965184491259, 2: -0.02112006507699999, 3: -0.0670701925472496, 4: -0.08177119680651854, 5: -0.05565421575888993},\n    '160M':  {1: -0.07999971655626487, 2: -0.019391734511526593, 3: -0.06387334934699097, 4: -0.07289970938551527, 5: -0.05717848070226565},\n    '305M':  {1: -0.07786837968301963, 2: -0.018390798367162706, 3: -0.06604426750330716, 4: -0.06978964405667937, 5: -0.057705526180705076},\n    '410M':  {1: -0.07705237903612913, 2: -0.017799580108894552, 3: -0.06728070485198959, 4: -0.06794024380520877, 5: -0.05954798320878324},\n}\n\n_intercept = {\n    '70M':   {1: 2.642865350100738, 2: 3.624938704129457, 3: 2.984082886764596, 4: 1.5197999684082721, 5: 3.4777450913291093},\n    '160M':  {1: 2.3379482012968085, 2: 3.2941184797908094, 3: 2.698234126707024, 4: 1.29746010289275, 5: 3.127593940602042},\n    '305M':  {1: 2.1939843086827238, 2: 3.137512552401725, 3: 2.5479442702720028, 4: 1.1956432975415627, 5: 2.9601767597640576},\n    '410M':  {1: 2.1341887819076444, 2: 3.0666775401979516, 3: 2.4791606926460794, 4: 1.159161359690975, 5: 2.8835495923809824},\n}\n\n\ndef _predict_single(x: dict[str, float], g: str) -> dict[str, float]:\n    \"\"\"Predict losses for the five domains for one observation.\"\"\"\n    if g not in _slope:\n        raise ValueError(f\"Unknown group '{g}'. Known groups: {list(_slope)}\")\n    out = {}\n    for i in range(1, 5 + 1):\n        prop_key = f\"proportion_domain_{i}\"\n        loss_key = f\"loss_domain_{i}\"\n        prop = x.get(prop_key)\n        if prop is None:\n            raise ValueError(f\"Missing key '{prop_key}' in input data point\")\n        out[loss_key] = _intercept[g][i] + _slope[g][i] * math.log(prop + eps)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts domain-specific validation losses from domain mixture proportions using the discovered scaling law.\n\n    The law is identical for all groups: a linear relationship between the loss for a domain and the natural\n    logarithm of that domain's proportion in the training mix. Each group (model size) possesses its own\n    set of slope and intercept parameters.\n    \"\"\"\n    return [_predict_single(row, group) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5.2", "reward_r2": 0.9331370854792963, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Scaling law discovered from /app/data.\n# Functional form (same for all groups and domains):\n#   loss_j = base_{g,j} - sum_{k=1..5} b_{g,j,k} * sqrt(p_k)\n# where p_k is the mixture proportion of training domain k.\n#\n# Parameters below were fit per group (g) and per target domain (j)\n# using ordinary least squares on sqrt(proportion) features.\n_PARAMS: Dict[str, Dict[str, Dict[str, object]]] = {\n    \"160M\": {\n        \"1\": {\n            \"base\": 3.4983868111735097,\n            \"b\": [-1.322244801503619, -0.05648236883892693, -0.1945208834476242, -0.07961426797704284, -0.48684961017065925],\n        },\n        \"2\": {\n            \"base\": 3.5901800975050886,\n            \"b\": [0.020259593136957545, -0.4261579533746848, -0.030500964720705733, 0.03195749881367703, -0.15925155887706166],\n        },\n        \"3\": {\n            \"base\": 3.404587658123985,\n            \"b\": [0.03082205478652992, -0.070071859031754, -1.636082759861348, -0.007193427564357435, -0.16048151875641523],\n        },\n        \"4\": {\n            \"base\": 2.229735317270052,\n            \"b\": [-0.13253328937666387, -0.0683463072293981, -0.06043866341791307, -1.4101893804572472, -0.2745437017330747],\n        },\n        \"5\": {\n            \"base\": 3.758978235206044,\n            \"b\": [-0.12040510823857463, -0.10138882666075966, -0.009625053520307786, -0.09995476483700354, -0.6624104061161924],\n        },\n    },\n    \"305M\": {\n        \"1\": {\n            \"base\": 3.368983334496183,\n            \"b\": [-1.3045025214887618, -0.07366983178813807, -0.19351469275990807, -0.1032498767800255, -0.5115484434058833],\n        },\n        \"2\": {\n            \"base\": 3.4650847175005275,\n            \"b\": [-0.0011777777393013559, -0.42534878260827746, -0.031637315796954495, 0.007651237757482249, -0.189691856112857],\n        },\n        \"3\": {\n            \"base\": 3.292456822440978,\n            \"b\": [0.016558822514137258, -0.06880890106063497, -1.692086053867289, -0.02043102024195373, -0.17469071753225673],\n        },\n        \"4\": {\n            \"base\": 2.1117242275989283,\n            \"b\": [-0.14209291141875355, -0.08185652172224422, -0.05933113785945075, -1.3567444624497598, -0.2795164803421837],\n        },\n        \"5\": {\n            \"base\": 3.6389244724608583,\n            \"b\": [-0.14463627940451954, -0.11811233876990179, -0.012096699259550073, -0.12940575716222572, -0.6983566433599756],\n        },\n    },\n    \"410M\": {\n        \"1\": {\n            \"base\": 3.2842119964419947,\n            \"b\": [-1.2888843000766006, -0.048995574255384367, -0.21360402703448367, -0.07761669368320008, -0.5026384873856817],\n        },\n        \"2\": {\n            \"base\": 3.378529873283224,\n            \"b\": [0.00731609765505886, -0.4046999067248046, -0.049914824967428884, 0.026006973552861305, -0.18578313165497676],\n        },\n        \"3\": {\n            \"base\": 3.240001825430652,\n            \"b\": [0.01628141300127803, -0.06568083623469262, -1.7274618646873994, -0.018326862157837145, -0.18278824660933443],\n        },\n        \"4\": {\n            \"base\": 2.057463260748564,\n            \"b\": [-0.15491308151495475, -0.05840055193974876, -0.06049410956879981, -1.3202941320593231, -0.28063147396807486],\n        },\n        \"5\": {\n            \"base\": 3.568172168062507,\n            \"b\": [-0.14806051645884583, -0.10343029066915999, -0.03544970070841352, -0.11508274131132587, -0.7078376382372219],\n        },\n    },\n    \"70M\": {\n        \"1\": {\n            \"base\": 3.902118280566893,\n            \"b\": [-1.425775576227664, -0.0833917420961342, -0.19694968669320845, -0.10889703541483507, -0.524387171419523],\n        },\n        \"2\": {\n            \"base\": 3.9510586938545873,\n            \"b\": [0.00835098938085932, -0.46316767641389445, -0.03510385309121029, 0.0115013961440507, -0.16308414687557324],\n        },\n        \"3\": {\n            \"base\": 3.745961102254981,\n            \"b\": [0.026773593153325063, -0.08530223577932154, -1.715458399253737, -0.04218357993957652, -0.17722638893079976],\n        },\n        \"4\": {\n            \"base\": 2.605542887734501,\n            \"b\": [-0.17483150982599954, -0.0974811357922756, -0.06917847354633115, -1.5953611426115695, -0.33815426297860407],\n        },\n        \"5\": {\n            \"base\": 4.106409751948681,\n            \"b\": [-0.11776391265174756, -0.10968999915729594, -0.013571098407638444, -0.11195553430775985, -0.6537211226224747],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts domain validation losses from domain mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group {group!r}. Known groups: {sorted(_PARAMS)}\")\n\n    out: List[Dict[str, float]] = []\n    for row in input_data:\n        p = [float(row.get(f\"proportion_domain_{k}\", 0.0)) for k in range(1, 6)]\n        # Numerical safety: proportions are in [0,1] and sum to 1 in training data;\n        # clamp to avoid negative due to noise.\n        sp = [math.sqrt(max(0.0, x)) for x in p]\n\n        pred: Dict[str, float] = {}\n        for j in range(1, 6):\n            par = _PARAMS[group][str(j)]\n            base = float(par[\"base\"])\n            b = par[\"b\"]  # type: ignore[assignment]\n            y = base\n            for k in range(5):\n                y += float(b[k]) * sp[k]\n            pred[f\"loss_domain_{j}\"] = float(y)\n        out.append(pred)\n\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5.2", "reward_r2": 0.933137085479296, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Scaling law discovered from the provided dataset.\n#\n# For each group g and validation domain j:\n#   loss_domain_j = a[g][j] + sum_k b[g][j][k] * sqrt(proportion_domain_k)\n#\n# Coefficients were fit with ordinary least squares on the training dataset.\n_PARAMS: dict[str, dict[int, dict[str, list[float] | float]]] = {\n    \"70M\": {\n        1: {\n            \"intercept\": 3.902118280566895,\n            \"b\": [\n                -1.425775576227666,\n                -0.08339174209613366,\n                -0.19694968669320942,\n                -0.10889703541483398,\n                -0.5243871714195225,\n            ],\n        },\n        2: {\n            \"intercept\": 3.951058693854588,\n            \"b\": [\n                0.008350989380858986,\n                -0.4631676764138939,\n                -0.035103853091210474,\n                0.011501396144051866,\n                -0.1630841468755727,\n            ],\n        },\n        3: {\n            \"intercept\": 3.745961102254981,\n            \"b\": [\n                0.02677359315332495,\n                -0.08530223577932106,\n                -1.715458399253737,\n                -0.04218357993957588,\n                -0.17722638893079964,\n            ],\n        },\n        4: {\n            \"intercept\": 2.605542887734503,\n            \"b\": [\n                -0.1748315098260007,\n                -0.09748113579227549,\n                -0.06917847354633108,\n                -1.5953611426115701,\n                -0.33815426297860457,\n            ],\n        },\n        5: {\n            \"intercept\": 4.106409751948681,\n            \"b\": [\n                -0.11776391265174813,\n                -0.10968999915729538,\n                -0.013571098407638814,\n                -0.11195553430775876,\n                -0.6537211226224754,\n            ],\n        },\n    },\n    \"160M\": {\n        1: {\n            \"intercept\": 3.4983868111735092,\n            \"b\": [\n                -1.3222448015036194,\n                -0.05648236883892562,\n                -0.19452088344762403,\n                -0.07961426797704164,\n                -0.4868496101706582,\n            ],\n        },\n        2: {\n            \"intercept\": 3.5901800975050886,\n            \"b\": [\n                0.02025959313695644,\n                -0.4261579533746849,\n                -0.030500964720706167,\n                0.03195749881367785,\n                -0.1592515588770622,\n            ],\n        },\n        3: {\n            \"intercept\": 3.404587658123986,\n            \"b\": [\n                0.030822054786528774,\n                -0.0700718590317547,\n                -1.6360827598613485,\n                -0.007193427564357451,\n                -0.16048151875641617,\n            ],\n        },\n        4: {\n            \"intercept\": 2.2297353172700536,\n            \"b\": [\n                -0.13253328937666442,\n                -0.06834630722939852,\n                -0.06043866341791265,\n                -1.410189380457248,\n                -0.2745437017330755,\n            ],\n        },\n        5: {\n            \"intercept\": 3.758978235206044,\n            \"b\": [\n                -0.12040510823857553,\n                -0.10138882666075939,\n                -0.009625053520308261,\n                -0.09995476483700287,\n                -0.6624104061161932,\n            ],\n        },\n    },\n    \"305M\": {\n        1: {\n            \"intercept\": 3.3689833344961833,\n            \"b\": [\n                -1.3045025214887638,\n                -0.07366983178813784,\n                -0.19351469275990896,\n                -0.10324987678002452,\n                -0.5115484434058839,\n            ],\n        },\n        2: {\n            \"intercept\": 3.4650847175005275,\n            \"b\": [\n                -0.0011777777393015554,\n                -0.4253487826082773,\n                -0.03163731579695457,\n                0.007651237757482987,\n                -0.18969185611285697,\n            ],\n        },\n        3: {\n            \"intercept\": 3.2924568224409776,\n            \"b\": [\n                0.01655882251413715,\n                -0.06880890106063427,\n                -1.6920860538672895,\n                -0.020431020241952865,\n                -0.17469071753225646,\n            ],\n        },\n        4: {\n            \"intercept\": 2.1117242275989287,\n            \"b\": [\n                -0.14209291141875402,\n                -0.08185652172224457,\n                -0.05933113785945045,\n                -1.3567444624497598,\n                -0.27951648034218374,\n            ],\n        },\n        5: {\n            \"intercept\": 3.638924472460857,\n            \"b\": [\n                -0.14463627940451984,\n                -0.11811233876990086,\n                -0.012096699259550365,\n                -0.12940575716222455,\n                -0.6983566433599759,\n            ],\n        },\n    },\n    \"410M\": {\n        1: {\n            \"intercept\": 3.2842119964419956,\n            \"b\": [\n                -1.2888843000766026,\n                -0.04899557425538448,\n                -0.21360402703448442,\n                -0.0776166936831994,\n                -0.5026384873856827,\n            ],\n        },\n        2: {\n            \"intercept\": 3.3785298732832234,\n            \"b\": [\n                0.007316097655057974,\n                -0.40469990672480444,\n                -0.049914824967429404,\n                0.02600697355286197,\n                -0.18578313165497698,\n            ],\n        },\n        3: {\n            \"intercept\": 3.240001825430652,\n            \"b\": [\n                0.016281413001278167,\n                -0.06568083623469226,\n                -1.7274618646873992,\n                -0.01832686215783651,\n                -0.18278824660933396,\n            ],\n        },\n        4: {\n            \"intercept\": 2.0574632607485643,\n            \"b\": [\n                -0.1549130815149553,\n                -0.058400551939748885,\n                -0.060494109568799366,\n                -1.3202941320593231,\n                -0.2806314739680751,\n            ],\n        },\n        5: {\n            \"intercept\": 3.568172168062505,\n            \"b\": [\n                -0.14806051645884644,\n                -0.10343029066915954,\n                -0.03544970070841392,\n                -0.11508274131132501,\n                -0.7078376382372229,\n            ],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts domain validation losses from domain mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group {group!r}. Known groups: {sorted(_PARAMS)}\")\n\n    out: list[dict[str, float]] = []\n\n    for row in input_data:\n        s = [\n            math.sqrt(max(0.0, float(row.get(f\"proportion_domain_{k}\", 0.0))))\n            for k in range(1, 6)\n        ]\n\n        pred: dict[str, float] = {}\n        for j in range(1, 6):\n            intercept = float(_PARAMS[group][j][\"intercept\"])  # type: ignore[index]\n            b = _PARAMS[group][j][\"b\"]  # type: ignore[index]\n            val = intercept\n            for k in range(5):\n                val += float(b[k]) * s[k]\n            pred[f\"loss_domain_{j}\"] = float(val)\n\n        out.append(pred)\n\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.931141, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nAdvanced scaling law for multi-domain LLM finetuning\nRefined with domain-specific modulation and improved interaction terms\nUses 34 parameters with enhanced domain 3 modeling and numerical stability\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64)\n    \n    # Parameter organization (34 params total):\n    # [0:5] base losses for each domain\n    # [5:10] linear mixture coefficients (self-domain sensitivity)\n    # [10:15] cross-domain coefficients (sum of other domains)\n    # [15:20] quadratic mixture terms (non-linear self-sensitivity)\n    # [20:25] entropy modulation per domain\n    # [25:30] concentration modulation per domain\n    # [30:33] pairwise interaction coefficients (3 shared params)\n    # [33] domain 3 boost factor (special handling for high-variance domain)\n    \n    base_losses = params[0:5]\n    linear_coeff = params[5:10]\n    cross_coeff = params[10:15]\n    quad_coeff = params[15:20]\n    entropy_coeff = params[20:25]\n    conc_coeff = params[25:30]\n    pair_coeff = params[30:33]\n    domain3_boost = params[33]\n    \n    # Compute global features (numerically stable)\n    eps = 1e-8\n    X_safe = np.clip(X, eps, 1.0 - eps)\n    \n    # Entropy: measure of mixture diversity\n    entropy = -np.sum(X_safe * np.log(X_safe + eps), axis=1, keepdims=True)  # (N, 1)\n    max_entropy = np.log(5.0)\n    entropy_norm = entropy / max_entropy  # (N, 1) in [0, 1]\n    \n    # Concentration: L2 norm squared (measure of dominance)\n    concentration = np.sum(X ** 2, axis=1, keepdims=True)  # (N, 1)\n    \n    # Max proportion: identifies dominant domain\n    max_prop = np.max(X, axis=1, keepdims=True)  # (N, 1)\n    \n    # Compute pairwise interaction strength for each sample\n    pair_strength = np.zeros((N, 1))\n    for i in range(5):\n        for j in range(i+1, 5):\n            pair_strength += X[:, i:i+1] * X[:, j:j+1]\n    pair_strength *= 2  # account for symmetry\n    \n    # Compute domain 3 specific feature (sensitivity to its own proportion changes)\n    domain3_prop = X[:, 3:4]\n    \n    # Build predictions for each domain\n    preds = np.zeros((N, 5))\n    \n    for d in range(5):\n        # Start with base loss\n        pred_d = base_losses[d]\n        \n        # Linear term: own domain proportion effect\n        pred_d = pred_d + linear_coeff[d] * X[:, d:d+1]\n        \n        # Cross-domain term: sum of all other domain proportions\n        other_domains = np.sum(X[:, [i for i in range(5) if i != d]], axis=1, keepdims=True)\n        pred_d = pred_d + cross_coeff[d] * other_domains\n        \n        # Quadratic term: non-linear self-sensitivity\n        pred_d = pred_d + quad_coeff[d] * (X[:, d:d+1] ** 2)\n        \n        # Entropy modulation: how diversity affects this domain\n        pred_d = pred_d + entropy_coeff[d] * entropy_norm\n        \n        # Concentration modulation: how dominance affects this domain\n        pred_d = pred_d + conc_coeff[d] * concentration\n        \n        # Pairwise interaction effects: domain-specific response to mixed compositions\n        pred_d = pred_d + pair_coeff[0] * pair_strength * X[:, d:d+1]\n        pred_d = pred_d + pair_coeff[1] * (max_prop - X[:, d:d+1]) * X[:, d:d+1]\n        \n        # Domain 3 special modulation: interaction with its own proportion\n        if d == 3:\n            pred_d = pred_d + domain3_boost * domain3_prop * (1.0 - domain3_prop)\n        \n        preds[:, d] = pred_d.squeeze()\n    \n    return preds\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    \n    N, F = X.shape\n    if y.ndim == 1:\n        y = y[:, None]\n    \n    n_params = 34\n    \n    # Parameter bounds based on expected ranges\n    bounds = [\n        (1.8, 4.2),   # base losses (5): observed loss range\n        (1.8, 4.2),\n        (1.8, 4.2),\n        (1.8, 4.2),\n        (1.8, 4.2),\n        (-1.8, 1.8),  # linear coefficients (5): sensitivity to own proportion\n        (-1.8, 1.8),\n        (-1.8, 1.8),\n        (-1.8, 1.8),\n        (-1.8, 1.8),\n        (-1.2, 1.2),  # cross-domain coefficients (5): interaction strength\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-1.2, 1.2),  # quadratic coefficients (5): non-linearity\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-1.2, 1.2),\n        (-0.6, 0.6),  # entropy modulation (5): diversity effect\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.6, 0.6),  # concentration modulation (5): dominance effect\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.6, 0.6),\n        (-0.4, 0.4),  # pairwise coefficients (3): interaction terms\n        (-0.4, 0.4),\n        (-0.4, 0.4),\n        (-0.8, 0.8),  # domain 3 boost factor: special modulation for domain 3\n    ]\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        mse = np.mean((pred - y) ** 2)\n        # Light regularization to prevent overfitting\n        reg = 0.0002 * np.sum(params ** 2)\n        return mse + reg\n    \n    # Two-phase optimization: global + local refinement\n    result_de = differential_evolution(\n        objective,\n        bounds,\n        seed=42,\n        maxiter=350,\n        popsize=30,\n        atol=1e-8,\n        tol=1e-8,\n        workers=1,\n        updating='deferred',\n        polish=True\n    )\n    \n    # Local refinement with L-BFGS-B for fine-tuning\n    result_final = minimize(\n        objective,\n        result_de.x,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 250, 'ftol': 1e-9, 'gtol': 1e-8}\n    )\n    \n    # Return best result\n    best_fun = min(result_final.fun, result_de.fun)\n    return result_final.x if result_final.fun <= result_de.fun else result_de.x\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.930786, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nAdvanced per-domain scaling law with refined domain-specific modeling.\nUses 35 parameters (7 per domain) for enhanced multi-domain loss prediction.\nImproved handling of difficult domains through dedicated modulation and interaction terms.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Per-domain scaling law with polynomial, cross-domain interactions, and domain-specific refinement.\n    \n    Parameters (35 total, 7 per domain):\n    - [0:5]: Base losses L0_i\n    - [5:10]: Linear coefficients a_i\n    - [10:15]: Quadratic coefficients b_i\n    - [15:20]: Cross-domain interaction weights c_i\n    - [20:25]: Secondary linear modulation d_i\n    - [25:30]: Domain-specific coupling e_i\n    - [30:35]: Per-domain modulation factors m_i\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64).ravel()\n    \n    # Pad if necessary\n    if len(params) < 35:\n        params = np.pad(params, (0, 35 - len(params)), mode='constant')\n    \n    # Parse and clip parameters for numerical stability\n    L0 = np.clip(params[0:5], 0.5, 4.5)\n    a = np.clip(params[5:10], -2.0, 2.0)\n    b = np.clip(params[10:15], -1.5, 1.5)\n    c = np.clip(params[15:20], -0.8, 0.8)\n    d = np.clip(params[20:25], -1.2, 1.2)\n    e = np.clip(params[25:30], -0.4, 0.4)\n    mod = np.clip(params[30:35], 0.8, 1.2)\n    \n    pred = np.zeros((N, F))\n    \n    for i in range(F):\n        x_i = X[:, i]\n        \n        # Primary: polynomial in own domain\n        pred[:, i] = L0[i] + a[i] * x_i + b[i] * (x_i ** 2)\n        \n        # Secondary: interactions with other domains\n        other_sum = np.sum(X[:, [j for j in range(F) if j != i]], axis=1)\n        pred[:, i] += c[i] * other_sum + d[i] * other_sum * x_i\n        \n        # Tertiary: weighted coupling from specific domains\n        for j in range(F):\n            if j != i:\n                # Adaptive coupling strength based on domain interaction\n                coupling_strength = e[i] * np.sign(a[j]) * np.abs(a[j]) / (1.0 + np.abs(a[j]))\n                pred[:, i] += coupling_strength * X[:, j]\n        \n        # Apply per-domain modulation factor\n        pred[:, i] *= mod[i]\n    \n    return np.clip(pred, 1.2, 4.3)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit scaling law with enhanced multi-start optimization and domain-specific refinement.\n    Uses 35 parameters with adaptive regularization targeting weak dimensions.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    \n    if y.shape[0] == 1:\n        y = y.T\n    if y.ndim == 1 or y.shape[1] == 1:\n        y = y.reshape(-1, 1)\n    \n    N, F = X.shape\n    n_domains = y.shape[1]\n    P = 35\n    \n    # Compute per-domain error statistics for adaptive regularization\n    per_domain_loss = np.std(y, axis=0)\n    domain_importance = per_domain_loss / (np.mean(per_domain_loss) + 1e-6)\n    \n    def objective(params):\n        try:\n            pred = scaling_law_func(X, params)\n            if pred.shape != y.shape:\n                return 1e10\n            \n            # MSE with domain-weighted importance\n            domain_errors = np.mean((pred - y) ** 2, axis=0)\n            mse = np.mean(domain_errors * domain_importance)\n            \n            # Adaptive regularization: stronger on modulation factors and coupling\n            reg_l0 = 0.00008 * np.sum(params[0:5] ** 2)\n            reg_linear = 0.0001 * np.sum(params[5:10] ** 2)\n            reg_quad = 0.00015 * np.sum(params[10:15] ** 2)\n            reg_interact = 0.0002 * np.sum(params[15:25] ** 2)\n            reg_coupling = 0.00025 * np.sum(params[25:30] ** 2)\n            reg_mod = 0.00015 * np.sum((params[30:35] - 1.0) ** 2)\n            \n            reg = reg_l0 + reg_linear + reg_quad + reg_interact + reg_coupling + reg_mod\n            return mse + reg\n        except:\n            return 1e10\n    \n    def smart_init():\n        \"\"\"Enhanced data-driven initialization with domain-specific tuning\"\"\"\n        init = np.zeros(P)\n        \n        for i in range(n_domains):\n            y_i = y[:, i]\n            x_i = X[:, i]\n            \n            # L0: mean loss\n            init[i] = np.clip(np.mean(y_i), 0.5, 4.5)\n            \n            # a: linear slope with domain-specific scaling\n            x_range = np.max(x_i) - np.min(x_i)\n            if x_range > 1e-6:\n                y_range = np.max(y_i) - np.min(y_i)\n                slope = y_range / x_range\n                # Strengthen slope estimation for high-variance domains\n                slope_scale = 0.7 + 0.15 * (domain_importance[i] - 1.0)\n                init[5 + i] = np.clip(slope * slope_scale, -2.0, 2.0)\n            \n            # b: quadratic term capturing curvature\n            y_sorted_idx = np.argsort(x_i)\n            y_sorted = y_i[y_sorted_idx]\n            if len(y_sorted) >= 3:\n                curvature = (y_sorted[-1] - 2 * y_sorted[len(y_sorted)//2] + y_sorted[0]) / (x_range ** 2 + 1e-6)\n                init[10 + i] = np.clip(curvature * 0.1, -1.5, 1.5)\n            else:\n                init[10 + i] = -0.02\n            \n            # c: cross-domain interaction (scaled by domain importance)\n            init[15 + i] = 0.06 * (1.0 + 0.3 * (domain_importance[i] - 1.0))\n            \n            # d: interaction modifier\n            init[20 + i] = -0.05 * (1.0 + 0.2 * (domain_importance[i] - 1.0))\n            \n            # e: coupling (very small, domain-aware)\n            init[25 + i] = 0.01 * (1.0 + 0.25 * (domain_importance[i] - 1.0))\n            \n            # m: per-domain modulation (near 1.0)\n            init[30 + i] = 1.0\n        \n        return init\n    \n    bounds = (\n        [(0.5, 4.5)] * 5 +\n        [(-2.0, 2.0)] * 5 +\n        [(-1.5, 1.5)] * 5 +\n        [(-0.8, 0.8)] * 5 +\n        [(-1.2, 1.2)] * 5 +\n        [(-0.4, 0.4)] * 5 +\n        [(0.8, 1.2)] * 5\n    )\n    \n    best_params = None\n    best_loss = np.inf\n    \n    # Start 1: Smart data-driven initialization\n    init1 = smart_init()\n    r1 = minimize(objective, init1, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r1.fun < best_loss:\n        best_loss = r1.fun\n        best_params = r1.x\n    \n    # Start 2: Amplified linear terms for high-variance domains\n    init2 = smart_init()\n    init2[5:10] = np.clip(init2[5:10] * (0.9 + 0.4 * (domain_importance - 1.0)), -2.0, 2.0)\n    r2 = minimize(objective, init2, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r2.fun < best_loss:\n        best_loss = r2.fun\n        best_params = r2.x\n    \n    # Start 3: Reduced quadratic terms\n    init3 = smart_init()\n    init3[10:15] = np.clip(init3[10:15] * 0.3, -1.5, 1.5)\n    r3 = minimize(objective, init3, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r3.fun < best_loss:\n        best_loss = r3.fun\n        best_params = r3.x\n    \n    # Start 4: Enhanced cross-domain interactions\n    init4 = smart_init()\n    init4[15:20] = np.clip(init4[15:20] * 2.2, -0.8, 0.8)\n    init4[20:25] = np.clip(init4[20:25] * 1.3, -1.2, 1.2)\n    r4 = minimize(objective, init4, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r4.fun < best_loss:\n        best_loss = r4.fun\n        best_params = r4.x\n    \n    # Start 5: Domain-specific modulation emphasis\n    init5 = smart_init()\n    init5[30:35] = np.clip(1.0 + 0.1 * np.random.randn(5), 0.8, 1.2)\n    r5 = minimize(objective, init5, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r5.fun < best_loss:\n        best_loss = r5.fun\n        best_params = r5.x\n    \n    # Start 6: Focused on coupling strength for problematic domains\n    init6 = smart_init()\n    init6[25:30] = np.clip(init6[25:30] * 1.8, -0.4, 0.4)\n    r6 = minimize(objective, init6, method='L-BFGS-B', bounds=bounds,\n                  options={'ftol': 1e-8, 'maxiter': 1400, 'gtol': 1e-6})\n    if r6.fun < best_loss:\n        best_loss = r6.fun\n        best_params = r6.x\n    \n    # Final polish with tighter tolerance\n    if best_params is not None:\n        try:\n            final = minimize(objective, best_params, method='L-BFGS-B', bounds=bounds,\n                           options={'ftol': 1e-9, 'maxiter': 1000, 'gtol': 1e-7})\n            if final.fun < best_loss:\n                best_params = final.x\n        except:\n            pass\n    \n    return best_params if best_params is not None else smart_init()\n\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.922399, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with improved cross-domain modeling and adaptive fitting\nUses 35 parameters: 7 per domain with refined interaction terms\nFocus: better extreme mixture handling, adaptive initialization, robust optimization\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Domain-specific scaling law with enhanced interaction modeling\n    Parameters (35 total = 7 per domain × 5 domains):\n    For each domain d:\n    - alpha_d: base coefficient for power law\n    - beta_d: power law exponent\n    - gamma_d: bias term\n    - delta_d: self-interaction (quadratic penalty)\n    - w1_d: cross-domain sum interaction\n    - w2_d: diversity bonus (balanced mixture reward)\n    - lambda_d: output scaling factor\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    N, F = X.shape\n    params = np.asarray(params).ravel()\n    \n    params_matrix = params.reshape(5, 7)\n    pred = np.zeros((N, F))\n    \n    for d in range(F):\n        alpha = params_matrix[d, 0]\n        beta = params_matrix[d, 1]\n        gamma = params_matrix[d, 2]\n        delta = params_matrix[d, 3]\n        w1 = params_matrix[d, 4]\n        w2 = params_matrix[d, 5]\n        lam = params_matrix[d, 6]\n        \n        # Stabilized domain proportion\n        x_d = np.clip(X[:, d], 1e-8, 1.0)\n        \n        # Power law with bounded exponent\n        beta_safe = np.clip(beta, -1.3, 1.3)\n        power_term = alpha * (x_d ** beta_safe)\n        \n        # Quadratic self-interaction (concentration penalty)\n        self_term = delta * (X[:, d] ** 2)\n        \n        # Cross-domain sum interaction\n        other_sum = np.sum(X, axis=1) - X[:, d]\n        cross_term = w1 * X[:, d] * other_sum\n        \n        # Enhanced diversity term with cubic component\n        # Captures non-linear mixture effects better\n        diversity_base = X[:, d] * (1.0 - X[:, d])\n        # Add cubic term for extreme mixture behavior\n        diversity_cubic = X[:, d] * (1.0 - X[:, d]) * (0.5 - X[:, d])\n        diversity_term = w2 * (diversity_base + 0.4 * diversity_cubic)\n        \n        # Combine with output scaling\n        pred[:, d] = lam * (power_term + self_term + cross_term + diversity_term) + gamma\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Enhanced fitting with adaptive initialization and extended optimization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    if y.shape[1] != F:\n        if y.shape[1] == 1:\n            y = np.tile(y, (1, F))\n    \n    init = np.zeros(35)\n    \n    for d in range(F):\n        y_d = y[:, d]\n        y_mean = np.mean(y_d)\n        y_std = np.std(y_d)\n        y_range = np.max(y_d) - np.min(y_d)\n        y_median = np.median(y_d)\n        \n        # Enhanced correlation analysis\n        x_d = X[:, d]\n        if np.std(x_d) > 1e-6:\n            corr = np.corrcoef(x_d, y_d)[0, 1]\n            if np.isnan(corr):\n                corr = 0.0\n            \n            # Compute rank correlation for non-linear relationships\n            try:\n                from scipy.stats import spearmanr\n                rank_corr, _ = spearmanr(x_d, y_d)\n                if np.isnan(rank_corr):\n                    rank_corr = corr\n            except:\n                rank_corr = corr\n        else:\n            corr = 0.0\n            rank_corr = 0.0\n        \n        # Analyze variance structure\n        x_high_mask = x_d > np.median(x_d)\n        y_high_var = np.std(y_d[x_high_mask]) if np.sum(x_high_mask) > 2 else y_std\n        y_low_var = np.std(y_d[~x_high_mask]) if np.sum(~x_high_mask) > 2 else y_std\n        var_ratio = y_high_var / (y_low_var + 1e-8)\n        \n        idx = d * 7\n        \n        # Alpha: adaptive to range and variance structure\n        init[idx] = y_range * (0.45 + 0.15 * abs(rank_corr)) * np.sqrt(var_ratio)\n        \n        # Beta: refined based on correlation patterns\n        if corr < -0.25:\n            init[idx + 1] = 0.35  # Strong negative -> sublinear\n        elif corr < -0.05:\n            init[idx + 1] = 0.5   # Weak negative\n        elif corr < 0.15:\n            init[idx + 1] = 0.65  # Weak positive\n        else:\n            init[idx + 1] = 0.75  # Strong positive -> more linear\n        \n        # Gamma: use median for robustness to outliers\n        init[idx + 2] = y_median * 0.92 + y_mean * 0.08\n        \n        # Delta: adaptive penalty based on variance structure\n        init[idx + 3] = -0.14 * (1.0 + 0.2 * abs(corr))\n        \n        # W1: cross-domain weight based on correlation direction\n        init[idx + 4] = 0.09 if rank_corr > 0 else 0.05\n        \n        # W2: diversity bonus with slight bias\n        init[idx + 5] = 0.065\n        \n        # Lambda: near unity\n        init[idx + 6] = 1.0\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        \n        # MSE loss\n        mse = np.mean((pred - y) ** 2)\n        \n        params_mat = params.reshape(5, 7)\n        \n        # Adaptive regularization\n        # Exponents toward moderate values\n        reg_beta = 0.0075 * np.sum((params_mat[:, 1] - 0.55) ** 2)\n        \n        # Alpha regularization toward reasonable scale\n        reg_alpha = 0.0004 * np.sum((params_mat[:, 0] - np.std(y, axis=0)) ** 2)\n        \n        # Light regularization on interaction terms\n        reg_interact = 0.00025 * np.sum(params_mat[:, 3:6] ** 2)\n        \n        # Lambda toward 1\n        reg_lambda = 0.00015 * np.sum((params_mat[:, 6] - 1.0) ** 2)\n        \n        return mse + reg_beta + reg_alpha + reg_interact + reg_lambda\n    \n    # Refined bounds\n    bounds = []\n    for d in range(5):\n        bounds.append((0.01, 9.0))      # alpha\n        bounds.append((-1.3, 1.3))      # beta\n        bounds.append((-3.5, 8.5))      # gamma\n        bounds.append((-0.85, 0.35))    # delta\n        bounds.append((-0.55, 0.55))    # w1\n        bounds.append((-0.35, 0.35))    # w2\n        bounds.append((0.2, 2.6))       # lambda\n    \n    # Extended multi-start optimization\n    best_result = None\n    best_loss = float('inf')\n    \n    for trial in range(4):\n        if trial == 0:\n            init_trial = init.copy()\n        else:\n            # Progressive exploration with controlled noise\n            noise_scale = 0.025 if trial == 1 else (0.04 if trial == 2 else 0.055)\n            init_trial = init + np.random.randn(35) * noise_scale\n            \n            # Clip to bounds\n            for i, (lb, ub) in enumerate(bounds):\n                init_trial[i] = np.clip(init_trial[i], lb, ub)\n        \n        result = minimize(\n            objective,\n            init_trial,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 1400, 'ftol': 1e-10, 'gtol': 1e-8}\n        )\n        \n        if result.fun < best_loss:\n            best_loss = result.fun\n            best_result = result\n    \n    params_opt = best_result.x if best_result and best_result.success else init\n    \n    return params_opt\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o3", "reward_r2": 0.9022243523590913, "solution": "import math\n\n# Pre-computed coefficients (α intercept and β slope) for each domain within each experimental group.\n# These values were obtained by ordinary least-squares fitting on the provided training split.\n_COEFFS = {\n    '70M':  {\n        1: {'alpha': 2.699187282650469, 'beta': -0.05301834148938922},\n        2: {'alpha': 3.641245514435248, 'beta': -0.012884478287480516},\n        3: {'alpha': 3.064817132296827, 'beta': -0.03887742050158055},\n        4: {'alpha': 1.5883178939323077, 'beta': -0.0495375219255563},\n        5: {'alpha': 3.4914427487846966, 'beta': -0.03541574159783459},\n    },\n    '160M': {\n        1: {'alpha': 2.3900234534690616, 'beta': -0.049548319842154825},\n        2: {'alpha': 3.308997240661732,  'beta': -0.01184703921853561},\n        3: {'alpha': 2.7750603405001533, 'beta': -0.03703330098796484},\n        4: {'alpha': 1.3585320071946527, 'beta': -0.044165574068738556},\n        5: {'alpha': 3.141635642438281,  'beta': -0.03640944805519766},\n    },\n    '305M': {\n        1: {'alpha': 2.244509680794173,  'beta': -0.04826257363962712},\n        2: {'alpha': 3.1516643395018273, 'beta': -0.011228110004842098},\n        3: {'alpha': 2.627225671804749,  'beta': -0.038315259040671304},\n        4: {'alpha': 1.2540806183376128, 'beta': -0.0422871397587127},\n        5: {'alpha': 2.974256168413441,  'beta': -0.03681503714017356},\n    },\n    '410M': {\n        1: {'alpha': 2.1839855870092384, 'beta': -0.047798856964779556},\n        2: {'alpha': 3.0802841205472316, 'beta': -0.010883493163969749},\n        3: {'alpha': 2.5599124453127016, 'beta': -0.03903464689969438},\n        4: {'alpha': 1.2161032438866797, 'beta': -0.04115602093820591},\n        5: {'alpha': 2.898019428647134,  'beta': -0.03803558851478692},\n    },\n}\n\n_EPS = 1e-6  # Numerical guard against log(0)\n\n\ndef _predict_single(sample: dict[str, float], coeffs: dict[int, dict[str, float]]) -> dict[str, float]:\n    \"\"\"Predict losses for one sample given coefficient set.\"\"\"\n    output: dict[str, float] = {}\n    for i in range(1, 6):\n        p = sample.get(f'proportion_domain_{i}', 0.0)\n        # Guard against log(0).  Values extremely close to zero do not impact loss materially.\n        p_clamped = max(p, _EPS)\n        alpha = coeffs[i]['alpha']\n        beta = coeffs[i]['beta']\n        output[f'loss_domain_{i}'] = float(alpha + beta * math.log(p_clamped))\n    return output\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts validation loss for each domain in the domain-mixture pre-training setting.\n\n    The discovered scaling relationship is, for every domain *i* (1-5):\n        loss_domain_i = α_{group,i} + β_{group,i} · ln(proportion_domain_i)\n\n    where (α, β) are constants determined by least-squares fitting on the\n    provided training data and differ per experimental *group* and *domain*.\n\n    Args:\n        input_data: list of samples, each containing keys\n            'proportion_domain_1' … 'proportion_domain_5'.\n        group: experimental group identifier (e.g. '70M', '160M', …).\n\n    Returns:\n        List of dictionaries mirroring *input_data* order, each populated with\n        predictions under keys 'loss_domain_1' … 'loss_domain_5'.\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Known groups: {list(_COEFFS)}\")\n\n    coeffs = _COEFFS[group]\n    return [_predict_single(sample, coeffs) for sample in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "mini-swe-agent", "model_name": "gpt-5", "reward_r2": 0.9022243523590899, "solution": "# Auto-generated scaling law function\n# Formula: loss_domain_i = a_{group,i} + b_{group,i} * log(1 / (proportion_domain_i + EPS))\n# EPS helps handle zero or tiny proportions.\nimport math\nfrom typing import List, Dict\n\nEPS = 1e-06\n\nCOEFFS = {\n  \"70M\": {\n    \"domain_1\": {\n      \"a\": 2.69918728265047,\n      \"b\": 0.05301834148938907\n    },\n    \"domain_2\": {\n      \"a\": 3.6412455144352482,\n      \"b\": 0.012884478287480374\n    },\n    \"domain_3\": {\n      \"a\": 3.064817132296828,\n      \"b\": 0.03887742050158043\n    },\n    \"domain_4\": {\n      \"a\": 1.5883178939323082,\n      \"b\": 0.049537521925556285\n    },\n    \"domain_5\": {\n      \"a\": 3.4914427487846966,\n      \"b\": 0.03541574159783458\n    }\n  },\n  \"160M\": {\n    \"domain_1\": {\n      \"a\": 2.390023453469063,\n      \"b\": 0.04954831984215471\n    },\n    \"domain_2\": {\n      \"a\": 3.3089972406617325,\n      \"b\": 0.011847039218535506\n    },\n    \"domain_3\": {\n      \"a\": 2.775060340500154,\n      \"b\": 0.03703330098796477\n    },\n    \"domain_4\": {\n      \"a\": 1.3585320071946532,\n      \"b\": 0.044165574068738486\n    },\n    \"domain_5\": {\n      \"a\": 3.141635642438281,\n      \"b\": 0.036409448055197705\n    }\n  },\n  \"305M\": {\n    \"domain_1\": {\n      \"a\": 2.244509680794174,\n      \"b\": 0.048262573639627\n    },\n    \"domain_2\": {\n      \"a\": 3.151664339501828,\n      \"b\": 0.011228110004841948\n    },\n    \"domain_3\": {\n      \"a\": 2.6272256718047498,\n      \"b\": 0.03831525904067123\n    },\n    \"domain_4\": {\n      \"a\": 1.2540806183376132,\n      \"b\": 0.04228713975871266\n    },\n    \"domain_5\": {\n      \"a\": 2.9742561684134405,\n      \"b\": 0.03681503714017352\n    }\n  },\n  \"410M\": {\n    \"domain_1\": {\n      \"a\": 2.1839855870092397,\n      \"b\": 0.04779885696477952\n    },\n    \"domain_2\": {\n      \"a\": 3.0802841205472307,\n      \"b\": 0.010883493163969587\n    },\n    \"domain_3\": {\n      \"a\": 2.559912445312702,\n      \"b\": 0.0390346468996943\n    },\n    \"domain_4\": {\n      \"a\": 1.2161032438866803,\n      \"b\": 0.04115602093820587\n    },\n    \"domain_5\": {\n      \"a\": 2.8980194286471335,\n      \"b\": 0.038035588514786826\n    }\n  },\n  \"GLOBAL\": {\n    \"domain_1\": {\n      \"a\": 2.379426500980737,\n      \"b\": 0.04965702298398756\n    },\n    \"domain_2\": {\n      \"a\": 3.2955478037865107,\n      \"b\": 0.0117107801687068\n    },\n    \"domain_3\": {\n      \"a\": 2.756753897478608,\n      \"b\": 0.03831515685747756\n    },\n    \"domain_4\": {\n      \"a\": 1.354258440837814,\n      \"b\": 0.04428656417280338\n    },\n    \"domain_5\": {\n      \"a\": 3.1263384970708885,\n      \"b\": 0.03666895382699818\n    }\n  }\n}\n\ndef _select_group_key(group: str) -> str:\n    if isinstance(group, str) and group in COEFFS:\n        return group\n    if isinstance(group, str):\n        gl = group.lower()\n        for k in COEFFS.keys():\n            if k.lower() == gl:\n                return k\n    return \"GLOBAL\" if \"GLOBAL\" in COEFFS else list(COEFFS.keys())[0]\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    gkey = _select_group_key(group)\n    params = COEFFS[gkey]\n    outputs: list[dict[str, float]] = []\n    domain_keys = sorted(params.keys(), key=lambda k: int(''.join(ch for ch in k if ch.isdigit())) if any(ch.isdigit() for ch in k) else 9999)\n    for row in input_data:\n        out: dict[str, float] = {}\n        for dom in domain_keys:\n            idx = ''.join(ch for ch in dom if ch.isdigit())\n            p_key = \"proportion_domain_\" + idx\n            y_key = \"loss_domain_\" + idx\n            p = row.get(p_key, None)\n            if p is None:\n                # try \"proportion_domain{idx}\" without underscore (legacy variant)\n                p = row.get(\"proportion_domain\" + idx, None)\n            if p is None:\n                p = 0.0\n            try:\n                p = float(p)\n            except Exception:\n                p = 0.0\n            a = float(params[dom].get(\"a\", 0.0))\n            b = float(params[dom].get(\"b\", 0.0))\n            val = a + b * math.log(1.0 / max(p, EPS))\n            out[y_key] = float(val)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-2.5-flash", "reward_r2": 0.902224, "solution": "import json\nimport numpy as np\n\n# Global variable to store loaded parameters\n_FITTED_PARAMETERS = None\n\ndef _load_parameters():\n    global _FITTED_PARAMETERS\n    if _FITTED_PARAMETERS is None:\n        try:\n            with open('/app/fitted_parameters.json', 'r') as f:\n                _FITTED_PARAMETERS = json.load(f)\n        except FileNotFoundError:\n            print(\"Error: fitted_parameters.json not found. Please run analyze_data.py first.\")\n            _FITTED_PARAMETERS = {}\n    return _FITTED_PARAMETERS\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = _load_parameters()\n\n    if group not in params:\n        # Fallback if group not found, e.g., use a default group or raise an error.\n        # For now, let's return an empty prediction or average parameters.\n        # The problem statement implies group will be one of the known groups.\n        # If unseen group, this would be an extrapolation case.\n        print(f\"Warning: Group '{group}' not found in fitted parameters. Cannot make predictions.\")\n        return [{'loss_domain_1': 0.0, 'loss_domain_2': 0.0, 'loss_domain_3': 0.0, 'loss_domain_4': 0.0, 'loss_domain_5': 0.0} for _ in input_data]\n\n    group_params = params[group]\n    predictions = []\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            domain_key = f'domain_{i}'\n\n            if proportion_key not in data_point:\n                predicted_losses[loss_key] = 0.0 # Or some default/error handling\n                continue\n\n            proportion = data_point[proportion_key]\n\n            if domain_key in group_params:\n                A = group_params[domain_key]['A']\n                B = group_params[domain_key]['B']\n                \n                # Apply the logarithmic scaling law\n                # Use 1e-6 as epsilon for proportions close to zero or exactly zero\n                predicted_loss = A - B * np.log(proportion + 1e-6)\n                predicted_losses[loss_key] = predicted_loss\n            else:\n                predicted_losses[loss_key] = 0.0 # Fallback if domain parameters not found\n        predictions.append(predicted_losses)\n\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "terminus-2", "model_name": "gpt-5", "reward_r2": 0.9022238569876261, "solution": "from math import log\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\"\"\"\n    EPS = 1e-06\n    COEFFS = {'160M': {'loss_domain_1': {'a': 2.3900239026095846, 'b': -0.0495482846415819},\n          'loss_domain_2': {'a': 3.3089973473750915, 'b': -0.011847031339016876},\n          'loss_domain_3': {'a': 2.7750612243262176, 'b': -0.03703323688495387},\n          'loss_domain_4': {'a': 1.3585325213280746, 'b': -0.04416553521395201},\n          'loss_domain_5': {'a': 3.1416357474005943, 'b': -0.03640943279009211}},\n '305M': {'loss_domain_1': {'a': 2.2445101169537405, 'b': -0.048262539632469084},\n          'loss_domain_2': {'a': 3.1516644411817394, 'b': -0.011228102438934438},\n          'loss_domain_3': {'a': 2.627226581493613, 'b': -0.03831519342481427},\n          'loss_domain_4': {'a': 1.254081110356706, 'b': -0.042287102605490595},\n          'loss_domain_5': {'a': 2.974256274101213, 'b': -0.03681502204362127}},\n '410M': {'loss_domain_1': {'a': 2.183986016421472, 'b': -0.04779882382417481},\n          'loss_domain_2': {'a': 3.0802842179137, 'b': -0.010883486046165937},\n          'loss_domain_3': {'a': 2.5599133716532463, 'b': -0.039034580115764335},\n          'loss_domain_4': {'a': 1.216103723749701, 'b': -0.04115598457974984},\n          'loss_domain_5': {'a': 2.898019537149145, 'b': -0.038035573443952336}},\n '70M': {'loss_domain_1': {'a': 2.6991877691852117, 'b': -0.05301830256954046},\n         'loss_domain_2': {'a': 3.6412456318395994, 'b': -0.012884469474280064},\n         'loss_domain_3': {'a': 3.064818062764666, 'b': -0.038877352813920324},\n         'loss_domain_4': {'a': 1.5883184714718916, 'b': -0.04953747817222867},\n         'loss_domain_5': {'a': 3.491442851657023, 'b': -0.035415726158268684}}}\n    # Determine coeff set for group; fallback to first available group if not found\n    group_coeffs = COEFFS.get(group)\n    if group_coeffs is None:\n        if COEFFS:\n            group_coeffs = COEFFS[sorted(COEFFS.keys())[0]]\n        else:\n            group_coeffs = {}\n    outputs = []\n    for row in input_data:\n        out = {}\n        # For each loss key we know, compute using corresponding proportion\n        for loss_key, ab in group_coeffs.items():\n            # Infer proportion key by replacing loss_ with proportion_ in the key name\n            prop_key = loss_key.replace('loss_', 'proportion_')\n            p = float(row.get(prop_key, 0.0))\n            a = float(ab.get('a', 0.0))\n            b = float(ab.get('b', 0.0))\n            pred = a + b * log(p + EPS)\n            out[loss_key] = float(pred)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-4.1", "reward_r2": 0.9022238561044323, "solution": "import numpy as np\n\ngroup_params = {\n    '70M': [\n        (-0.0530183, 2.69918777),\n        (-0.01288447, 3.64124563),\n        (-0.03887735, 3.06481806),\n        (-0.04953748, 1.58831847),\n        (-0.03541573, 3.49144285),\n    ],\n    '160M': [\n        (-0.04954828, 2.3900239),\n        (-0.01184703, 3.30899735),\n        (-0.03703324, 2.77506122),\n        (-0.04416554, 1.35853252),\n        (-0.03640943, 3.14163575),\n    ],\n    '305M': [\n        (-0.04826254, 2.24451012),\n        (-0.0112281, 3.15166444),\n        (-0.03831519, 2.62722658),\n        (-0.0422871, 1.25408111),\n        (-0.03681502, 2.97425627),\n    ],\n    '410M': [\n        (-0.04779882, 2.18398602),\n        (-0.01088349, 3.08028422),\n        (-0.03903458, 2.55991337),\n        (-0.04115598, 1.21610372),\n        (-0.03803557, 2.89801954),\n    ],\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = group_params[group]\n    eps = 1e-6\n    results = []\n    for row in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = row[f'proportion_domain_{i}']\n            b, c = params[i-1]\n            pred[f'loss_domain_{i}'] = b * np.log(p + eps) + c\n        results.append(pred)\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9021493866194279, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters for the scaling law: loss_i = a_i + b_i * log(proportion_i + epsilon)\n    # Fitted parameters for each group and domain\n    epsilon = 1e-6\n    \n    # Group: 70M, 160M, 305M, 410M\n    # Domain: 1 to 5\n    parameters = {\n        '70M': {\n            'a': [2.6992, 3.6412, 3.0648, 1.5883, 3.4914],\n            'b': [-0.0530, -0.0129, -0.0389, -0.0495, -0.0354]\n        },\n        '160M': {\n            'a': [2.3900, 3.3090, 2.7751, 1.3585, 3.1416],\n            'b': [-0.0495, -0.0118, -0.0370, -0.0442, -0.0364]\n        },\n        '305M': {\n            'a': [2.2445, 3.1517, 2.6272, 1.2541, 2.9743],\n            'b': [-0.0483, -0.0112, -0.0383, -0.0423, -0.0368]\n        },\n        '410M': {\n            'a': [2.1840, 3.0803, 2.5599, 1.2161, 2.8980],\n            'b': [-0.0478, -0.0109, -0.0390, -0.0412, -0.0380]\n        }\n    }\n    \n    # Check if group is valid\n    if group not in parameters:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(parameters.keys())}\")\n    \n    # Get parameters for the specified group\n    group_params = parameters[group]\n    a_coeffs = group_params['a']\n    b_coeffs = group_params['b']\n    \n    # Prepare the output list\n    output_data = []\n    \n    for data_point in input_data:\n        # We expect the input data point to have keys: proportion_domain_1, ..., proportion_domain_5\n        # However, we can be flexible and look for any key that starts with 'proportion_domain_'\n        # and has a domain number.\n        predicted_losses = {}\n        \n        for domain_idx in range(5):  # Domains 1 to 5\n            domain_num = domain_idx + 1\n            prop_key = f'proportion_domain_{domain_num}'\n            \n            if prop_key not in data_point:\n                # If the exact key is not found, try to find any proportion key for this domain?\n                # Alternatively, we could skip, but for now we assume the keys are present.\n                raise KeyError(f\"Input data point missing expected key: {prop_key}\")\n            \n            proportion = data_point[prop_key]\n            a = a_coeffs[domain_idx]\n            b = b_coeffs[domain_idx]\n            \n            # Apply the scaling law: loss = a + b * log(proportion + epsilon)\n            # Note: if proportion is exactly 0, we use epsilon to avoid log(0)\n            loss = a + b * math.log(proportion + epsilon)\n            predicted_losses[f'loss_domain_{domain_num}'] = loss\n        \n        output_data.append(predicted_losses)\n    \n    return output_data"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5", "reward_r2": 0.899569, "solution": "from typing import List, Dict\n\n# Quadratic scaling law in the five mixture proportions.\n# For group g and domain k (k in 1..5):\n#   loss_k = b_k(g) + sum_i A_{k,i}(g) * p_i + sum_{i<=j} C_{k,ij}(g) * p_i * p_j\n# The feature order for the quadratic terms matches sklearn PolynomialFeatures(degree=2, include_bias=False):\n# [p1, p2, p3, p4, p5,\n#  p1^2, p1*p2, p1*p3, p1*p4, p1*p5,\n#  p2^2, p2*p3, p2*p4, p2*p5,\n#  p3^2, p3*p4, p3*p5,\n#  p4^2, p4*p5,\n#  p5^2]\n\n# Group-specific coefficients (5 outputs x 20 features) and intercepts (5,)\nCOEFFS: Dict[str, Dict[str, list]] = {\n    '70M': {\n        'intercept': [2.67722332, 3.61228621, 2.31848257, 1.43615424, 3.64312409],\n        'coef': [\n            [-1.93535793e+00,  9.21334406e-01,  6.07166334e-01,  1.86436004e-01,  2.20421182e-01,\n              4.17217173e+00, -2.50227074e+00, -8.71756745e-01, -1.45116852e+00, -1.28233366e+00,\n              7.50528746e-01,  1.88319583e+00,  7.09251012e-01,  8.06295581e-02,  1.85529330e-01,\n             -4.30245275e-01, -1.59556803e-01,  1.88349071e-01,  1.17024971e+00,  4.11432370e-01],\n            [ 1.87287409e-01, -5.24704674e-01,  8.19666923e-02,  2.31939372e-01,  2.35112001e-02,\n              1.93535049e-01, -6.08606198e-01,  1.18874357e-01,  3.03342030e-01,  1.80142171e-01,\n              8.76312565e-01,  2.24295717e-02, -4.31900052e-01, -3.82940560e-01,  4.86317090e-03,\n             -4.36420286e-03, -5.98362049e-02,  2.27452590e-01,  1.37409008e-01,  1.48736786e-01],\n            [ 6.06770406e-01,  1.02165804e+00, -3.42499870e+00,  8.48286144e-01,  9.48284111e-01,\n              1.17883254e+00,  9.85326844e-02, -2.30473008e+00,  7.43287853e-01,  8.90847414e-01,\n              2.23385663e+00, -5.42165653e-01, -6.21380452e-01, -1.47185169e-01,  8.79046349e-01,\n             -1.00148149e+00, -4.55667827e-01,  1.32253646e+00,  4.05323777e-01,  2.54965916e-01],\n            [ 2.25923880e-01,  5.66280635e-01,  3.05380370e-01, -1.18219021e+00,  8.46053253e-02,\n              2.43370013e-02,  8.12183458e-01, -5.58959006e-01, -1.43199391e+00,  1.38035634e+00,\n              5.92080565e-01,  1.49390820e+00, -3.14016004e+00,  8.08268452e-01,  1.55579420e-01,\n             -1.08989614e+00,  3.04747891e-01,  7.59681480e+00, -3.11695492e+00,  7.08187563e-01],\n            [ 9.59227049e-02,  2.87475915e-01,  7.95688772e-02,  2.43121301e-01, -7.06088798e-01,\n              8.03086961e-02, -2.38179581e-01,  4.09505305e-02,  3.70796209e-01, -1.57953149e-01,\n              4.77055325e-01,  3.25008377e-01,  2.93142645e-01, -5.69550850e-01, -2.33055409e-02,\n             -7.62471460e-02, -1.86837343e-01,  1.05339029e-01, -4.49909436e-01,  6.58161980e-01],\n        ],\n    },\n    '160M': {\n        'intercept': [2.38262016, 3.30071035, 2.07434882, 1.22527058, 3.31341378],\n        'coef': [\n            [-1.89040168e+00,  9.35907232e-01,  5.60322596e-01,  1.88444025e-01,  2.05727823e-01,\n              4.04736833e+00, -2.51535118e+00, -8.90374549e-01, -1.39350885e+00, -1.13853543e+00,\n              7.41695287e-01,  1.84860556e+00,  8.63861779e-01, -2.90421544e-03,  1.62855446e-01,\n             -4.13720211e-01, -1.47043649e-01, -1.33076166e-02,  1.14511892e+00,  3.49092199e-01],\n            [ 1.06414210e-01, -4.42355222e-01,  1.03243547e-01,  2.22326118e-01,  1.03713464e-02,\n              2.92828097e-01, -7.03779930e-01,  8.34271237e-03,  2.86433970e-01,  2.22589362e-01,\n              8.46832622e-01,  1.48524491e-01, -2.84465358e-01, -4.49467047e-01,  2.13811575e-02,\n             -2.55730882e-02, -4.94317254e-02,  6.67143569e-02,  1.79216238e-01,  1.07464519e-01],\n            [ 5.10457626e-01,  1.02965945e+00, -3.28863673e+00,  8.27651985e-01,  9.20867673e-01,\n              1.21104590e+00, -1.00467440e-01, -2.27536431e+00,  7.92543737e-01,  8.82699738e-01,\n              2.29667733e+00, -4.60952149e-01, -4.56400842e-01, -2.49197452e-01,  8.27965658e-01,\n             -1.01341930e+00, -3.66866637e-01,  1.04265170e+00,  4.62276688e-01,  1.91955335e-01],\n            [ 1.85394175e-01,  5.25641652e-01,  2.36807484e-01, -1.05531359e+00,  1.07470277e-01,\n              4.73960405e-02,  6.76640181e-01, -4.99158421e-01, -1.30202159e+00,  1.26253796e+00,\n              4.71471468e-01,  1.33050402e+00, -2.67005203e+00,  7.17078014e-01,  9.60652672e-02,\n             -1.01089455e+00,  3.20291164e-01,  6.69954159e+00, -2.77188702e+00,  5.79450152e-01],\n            [ 4.30213821e-03,  3.81322064e-01,  1.04070391e-01,  2.40299130e-01, -7.29993724e-01,\n              2.03468508e-01, -3.88921326e-01, -6.90314075e-02,  3.50874845e-01, -9.20884817e-02,\n              5.96457642e-01,  4.54420685e-01,  4.50055879e-01, -7.30690815e-01, -9.18767274e-03,\n             -1.08473047e-01, -1.63658167e-01, -5.80983095e-02, -3.94060239e-01,  6.50503979e-01],\n        ],\n    },\n    '305M': {\n        'intercept': [2.20308636, 3.13378774, 2.0099542, 1.21090615, 3.12985208],\n        'coef': [\n            [-1.36612527e+00,  5.08760751e-01,  2.58565850e-01,  1.92426844e-01,  4.06371822e-01,\n              3.02406051e+00, -1.36341416e+00, -5.63422763e-01, -9.77163560e-01, -1.48618530e+00,\n              3.33590741e-01,  7.44731825e-01,  2.29382605e-01,  5.64469737e-01,  1.08830884e-01,\n             -2.29650547e-01,  1.98076451e-01,  1.70915773e-01,  9.98942574e-01,  1.31068355e-01],\n            [ 1.42155086e-01, -4.77001278e-01,  5.52264024e-02,  2.48408466e-01,  3.12113236e-02,\n              2.58768506e-01, -3.66936157e-01, -1.32405322e-02,  1.23555478e-01,  1.40007791e-01,\n              2.60734738e-01,  5.87994884e-02, -1.60567442e-01, -2.69031906e-01,  2.22863705e-02,\n             -7.35552021e-03, -5.26340404e-03,  2.24335488e-01,  6.84404625e-02,  9.70583802e-02],\n            [ 6.18506100e-01,  8.68693819e-01, -3.08914591e+00,  7.89619410e-01,  8.12326585e-01,\n              1.02643703e+00,  1.14784592e-01, -1.46267084e+00,  1.80033336e-01,  7.59921974e-01,\n              1.17239480e+00, -3.63996838e-01, -2.03911468e-01,  1.49422736e-01,  1.90418054e-01,\n             -5.68054905e-01, -8.84841389e-01,  8.13849714e-01,  5.67702734e-01,  2.20120530e-01],\n            [ 2.12633425e-01,  4.83463627e-01,  2.22311810e-01, -8.90787050e-01, -2.76218129e-02,\n             -9.29330545e-02,  1.99613262e-01, -1.14370686e-01, -7.24874326e-01,  9.45198229e-01,\n              1.74912693e-01,  6.25137716e-01, -1.05562696e+00,  5.39426918e-01,  8.52526881e-02,\n             -3.88847751e-01,  1.51398435e-02,  3.43073712e+00, -2.15217513e+00,  6.24788323e-01],\n            [ 8.99898182e-02,  2.46945404e-01,  7.24765027e-02,  2.55936575e-01, -6.65348300e-01,\n              1.05642777e-01, -1.69323103e-02, -2.45232236e-02,  2.31633418e-01, -2.05830842e-01,\n              2.57824563e-01,  1.81182032e-01,  3.00383115e-01, -4.75511996e-01,  1.31255309e-02,\n              1.56611021e-03, -9.88739467e-02,  2.04976089e-01, -4.82622158e-01,  5.97490643e-01],\n        ],\n    },\n    '410M': {\n        'intercept': [2.1439425, 3.06228202, 1.93134234, 1.17352651, 3.0537303],\n        'coef': [\n            [-1.36744907e+00,  5.27110822e-01,  2.03069104e-01,  2.31281112e-01,  4.05988034e-01,\n              3.00741578e+00, -1.41296472e+00, -5.90698287e-01, -9.03407996e-01, -1.46779385e+00,\n              3.36508765e-01,  7.13154207e-01,  3.03122142e-01,  5.87290423e-01,  1.12114917e-01,\n             -2.17566149e-01,  1.86064416e-01,  6.33336092e-02,  9.85799506e-01,  1.14627539e-01],\n            [ 1.31827459e-01, -4.52442821e-01,  5.13599152e-03,  2.81922129e-01,  3.35572415e-02,\n              2.56204289e-01, -4.04028535e-01, -4.49805105e-02,  1.62119123e-01,  1.62513091e-01,\n              2.84260949e-01,  2.83929538e-02, -1.02570552e-01, -2.58497637e-01,  2.72658790e-02,\n             -3.64530850e-03, -1.89702227e-03,  1.75991829e-01,  5.00270374e-02,  8.14117726e-02],\n            [ 6.14742174e-01,  8.65449930e-01, -3.13247704e+00,  8.30315141e-01,  8.21969797e-01,\n              1.11981200e+00, -7.51476148e-02, -1.43672744e+00,  2.46278198e-01,  7.60527037e-01,\n              1.21696703e+00, -4.08532829e-01, -8.73310035e-02,  2.19494346e-01,  1.95911154e-01,\n             -5.50356121e-01, -9.32771804e-01,  6.76874073e-01,  5.44849994e-01,  2.29870224e-01],\n            [ 1.90124684e-01,  4.83340554e-01,  1.97559385e-01, -8.65266788e-01, -5.75783540e-03,\n             -7.48247755e-02,  1.37556138e-01, -1.54467045e-01, -5.85386297e-01,  8.67246663e-01,\n              1.95357715e-01,  6.05246418e-01, -1.03503337e+00,  5.80213650e-01,  9.95312192e-02,\n             -4.27655041e-01,  7.49038343e-02,  3.29049485e+00, -2.10768693e+00,  5.79564950e-01],\n            [ 7.34423141e-02,  2.79758269e-01,  1.55572901e-02,  2.98213415e-01, -6.66971288e-01,\n              1.21456923e-01, -7.12563637e-02, -7.33905276e-02,  2.86905077e-01, -1.90272795e-01,\n              2.93628007e-01,  1.67735765e-01,  3.62290116e-01, -4.72639254e-01,  1.73275864e-02,\n              8.76400932e-04, -9.69919342e-02,  1.48793013e-01, -5.00651192e-01,  5.93583887e-01],\n        ],\n    },\n}\n\nNAME_ORDER = [\n    'proportion_domain_1', 'proportion_domain_2', 'proportion_domain_3',\n    'proportion_domain_4', 'proportion_domain_5'\n]\n\n\ndef _poly2_features(vals: List[float]) -> List[float]:\n    \"\"\"Build polynomial features up to degree 2 for five inputs.\n\n    Order matches sklearn PolynomialFeatures(deg=2, include_bias=False).\n    \"\"\"\n    x1, x2, x3, x4, x5 = vals\n    feats = [\n        x1, x2, x3, x4, x5,\n        x1*x1, x1*x2, x1*x3, x1*x4, x1*x5,\n        x2*x2, x2*x3, x2*x4, x2*x5,\n        x3*x3, x3*x4, x3*x5,\n        x4*x4, x4*x5,\n        x5*x5,\n    ]\n    return feats\n\n\ndef _choose_group(group: str) -> str:\n    if group in COEFFS:\n        return group\n    # Fallback: map to nearest known parameterized size by numeric value\n    import re\n    m = re.search(r\"(\\d+)\", str(group))\n    if not m:\n        return '160M'\n    val = int(m.group(1))\n    def gnum(g: str) -> int:\n        mg = re.search(r\"(\\d+)\", g)\n        return int(mg.group(1)) if mg else 0\n    return min(COEFFS.keys(), key=lambda g: abs(gnum(g) - val))\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    grp = _choose_group(group)\n    params = COEFFS[grp]\n    W = params['coef']  # 5 x 20\n    b = params['intercept']  # 5\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        vals = [float(row.get(k, 0.0)) for k in NAME_ORDER]\n        feats = _poly2_features(vals)\n        y = [b[k] + sum(W[k][j] * feats[j] for j in range(20)) for k in range(5)]\n        outputs.append({f'loss_domain_{i+1}': y[i] for i in range(5)})\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Per-group, per-target coefficients fitted on /app/data using OLS.\n# Model: loss_k = b0 + sum_j b_j * log(proportion_j + eps)\n_EPS = 1e-6\n_COEFS: Dict[str, Dict[int, List[float]]] = {\n    \"160M\": {\n        1: [\n            2.4042025803406553,\n            -0.04878456902733,\n            -0.001374141990715709,\n            0.00425143168670042,\n            -0.004036425589256114,\n            0.007660516801797778,\n        ],\n        2: [\n            3.329935868696654,\n            0.002613273350161564,\n            -0.011891996557419415,\n            2.5572674116154508e-05,\n            0.003256629390122456,\n            -0.005952205705195364,\n        ],\n        3: [\n            2.8006343825165314,\n            0.0038463371713819104,\n            0.0010271048724917586,\n            -0.03834514983761013,\n            0.003370871701657902,\n            -0.004992444549614571,\n        ],\n        4: [\n            1.3842254371292473,\n            0.0001770125184695076,\n            0.0019008052563317707,\n            0.00040985897229494966,\n            -0.04321725706040489,\n            0.005203775943861618,\n        ],\n        5: [\n            3.22070848376419,\n            0.005681276339156933,\n            0.004826630941776033,\n            -0.00156340623248917,\n            0.005473022220321205,\n            -0.030039287316337895,\n        ],\n    },\n    \"305M\": {\n        1: [\n            2.254097761686427,\n            -0.047618308465165196,\n            -0.0013550046237553216,\n            0.004003192579882288,\n            -0.0042909223510698554,\n            0.006758159630738282,\n        ],\n        2: [\n            3.166848403701224,\n            0.0023474329227610603,\n            -0.011422287483401284,\n            4.890168638858063e-06,\n            0.0026987505976661923,\n            -0.006498741023938888,\n        ],\n        3: [\n            2.6500359123770685,\n            0.0035548451771600445,\n            0.0011888369505355075,\n            -0.03964115398330717,\n            0.0028959099742507494,\n            -0.004828358464409514,\n        ],\n        4: [\n            1.2751944568968825,\n            -0.00024923215104993846,\n            0.0015693015233153626,\n            0.0004352251902864513,\n            -0.041431801189773,\n            0.00487564464080058,\n        ],\n        5: [\n            3.0477226649403506,\n            0.005424541586794049,\n            0.004752807485933067,\n            -0.0015473014680650466,\n            0.004775541162988269,\n            -0.030879417992197157,\n        ],\n    },\n    \"410M\": {\n        1: [\n            2.1969109104720936,\n            -0.04711053566759858,\n            -0.0005929598868873091,\n            0.0032935545744610275,\n            -0.0034299883497122844,\n            0.006245418652814477,\n        ],\n        2: [\n            3.096774846113359,\n            0.002683578481767577,\n            -0.01079532012122322,\n            -0.0006061709329291659,\n            0.003289915492216653,\n            -0.007059325448456723,\n        ],\n        3: [\n            2.5854698484676923,\n            0.0036493802056689617,\n            0.0014981486567392278,\n            -0.04057579585320029,\n            0.003422309711002145,\n            -0.005304641068008599,\n        ],\n        4: [\n            1.2374283294627488,\n            -0.0007019754734980677,\n            0.0023220834389344196,\n            0.00022483249867104974,\n            -0.040255057354183485,\n            0.0043998574943246395,\n        ],\n        5: [\n            2.9734625677861892,\n            0.005474979456664107,\n            0.005292433569369652,\n            -0.0021983339298417347,\n            0.005328130514437292,\n            -0.031848404298810445,\n        ],\n    },\n    \"70M\": {\n        1: [\n            2.709753097020149,\n            -0.05223448519596348,\n            -0.002096832291136182,\n            0.0049309629806633245,\n            -0.005121637241168704,\n            0.008582633603168494,\n        ],\n        2: [\n            3.6589940901202946,\n            0.0024594854221674217,\n            -0.013037306540415955,\n            0.00018013301869309603,\n            0.0025694674548725075,\n            -0.0055192991657138785,\n        ],\n        3: [\n            3.0882519411499176,\n            0.004138978755961604,\n            0.000999612740187218,\n            -0.040138664176247195,\n            0.002610113875372775,\n            -0.00489486237526904,\n        ],\n        4: [\n            1.6123830068952152,\n            -0.0002626923872205353,\n            0.001840626589019361,\n            0.0004871656033250259,\n            -0.048580221686945084,\n            0.0053741041526707105,\n        ],\n        5: [\n            3.567736623439916,\n            0.005906206376564464,\n            0.00463119283202685,\n            -0.0015851069219014631,\n            0.004999662780701288,\n            -0.029220774259506147,\n        ],\n    },\n}\n\n\ndef _predict_row(props: List[float], coefs: List[float]) -> float:\n    # coefs: [b0, b1..b5], props: [p1..p5]\n    x = [1.0] + [math.log(max(p, 0.0) + _EPS) for p in props]\n    return sum(c * xi for c, xi in zip(coefs, x))\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Choose coefficients for the requested group; if unseen, fall back to the\n    # average of known groups to remain robust.\n    if group in _COEFS:\n        group_coefs = _COEFS[group]\n    else:\n        # Average coefficients across groups\n        group_coefs = {}\n        for k in range(1, 6):\n            # Collect coefs per group for this k\n            mats = [v[k] for v in _COEFS.values()]\n            avg = [sum(col) / len(mats) for col in zip(*mats)]\n            group_coefs[k] = avg\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        props = [\n            float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)\n        ]\n        pred = {}\n        for k in range(1, 6):\n            yk = _predict_row(props, group_coefs[k])\n            pred[f\"loss_domain_{k}\"] = float(yk)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "opencode", "model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n# Small epsilon to avoid log(0) when a domain proportion is zero\n_EPS = 1e-6\n\n# Per-group, per-output coefficients for the multi-log linear law.\n# For each group and each output loss_domain_k, the list is:\n#   [intercept, b1, b2, b3, b4, b5]\n_COEFS: Dict[str, Dict[str, List[float]]] = {\n    \"160M\": {\n        \"loss_domain_1\": [\n            2.4042025803406553,\n            -0.04878456902733,\n            -0.001374141990715709,\n            0.00425143168670042,\n            -0.004036425589256114,\n            0.007660516801797778,\n        ],\n        \"loss_domain_2\": [\n            3.329935868696654,\n            0.002613273350161564,\n            -0.011891996557419415,\n            2.5572674116154508e-05,\n            0.003256629390122456,\n            -0.005952205705195364,\n        ],\n        \"loss_domain_3\": [\n            2.8006343825165314,\n            0.0038463371713819104,\n            0.0010271048724917586,\n            -0.03834514983761013,\n            0.003370871701657902,\n            -0.004992444549614571,\n        ],\n        \"loss_domain_4\": [\n            1.3842254371292473,\n            0.0001770125184695076,\n            0.0019008052563317707,\n            0.00040985897229494966,\n            -0.04321725706040489,\n            0.005203775943861618,\n        ],\n        \"loss_domain_5\": [\n            3.22070848376419,\n            0.005681276339156933,\n            0.004826630941776033,\n            -0.00156340623248917,\n            0.005473022220321205,\n            -0.030039287316337895,\n        ],\n    },\n    \"305M\": {\n        \"loss_domain_1\": [\n            2.254097761686427,\n            -0.047618308465165196,\n            -0.0013550046237553216,\n            0.004003192579882288,\n            -0.0042909223510698554,\n            0.006758159630738282,\n        ],\n        \"loss_domain_2\": [\n            3.166848403701224,\n            0.0023474329227610603,\n            -0.011422287483401284,\n            4.890168638858063e-06,\n            0.0026987505976661923,\n            -0.006498741023938888,\n        ],\n        \"loss_domain_3\": [\n            2.6500359123770685,\n            0.0035548451771600445,\n            0.0011888369505355075,\n            -0.03964115398330717,\n            0.0028959099742507494,\n            -0.004828358464409514,\n        ],\n        \"loss_domain_4\": [\n            1.2751944568968825,\n            -0.00024923215104993846,\n            0.0015693015233153626,\n            0.0004352251902864513,\n            -0.041431801189773,\n            0.00487564464080058,\n        ],\n        \"loss_domain_5\": [\n            3.0477226649403506,\n            0.005424541586794049,\n            0.004752807485933067,\n            -0.0015473014680650466,\n            0.004775541162988269,\n            -0.030879417992197157,\n        ],\n    },\n    \"410M\": {\n        \"loss_domain_1\": [\n            2.1969109104720936,\n            -0.04711053566759858,\n            -0.0005929598868873091,\n            0.0032935545744610275,\n            -0.0034299883497122844,\n            0.006245418652814477,\n        ],\n        \"loss_domain_2\": [\n            3.096774846113359,\n            0.002683578481767577,\n            -0.01079532012122322,\n            -0.0006061709329291659,\n            0.003289915492216653,\n            -0.007059325448456723,\n        ],\n        \"loss_domain_3\": [\n            2.5854698484676923,\n            0.0036493802056689617,\n            0.0014981486567392278,\n            -0.04057579585320029,\n            0.003422309711002145,\n            -0.005304641068008599,\n        ],\n        \"loss_domain_4\": [\n            1.2374283294627488,\n            -0.0007019754734980677,\n            0.0023220834389344196,\n            0.00022483249867104974,\n            -0.040255057354183485,\n            0.0043998574943246395,\n        ],\n        \"loss_domain_5\": [\n            2.9734625677861892,\n            0.005474979456664107,\n            0.005292433569369652,\n            -0.0021983339298417347,\n            0.005328130514437292,\n            -0.031848404298810445,\n        ],\n    },\n    \"70M\": {\n        \"loss_domain_1\": [\n            2.709753097020149,\n            -0.05223448519596348,\n            -0.002096832291136182,\n            0.0049309629806633245,\n            -0.005121637241168704,\n            0.008582633603168494,\n        ],\n        \"loss_domain_2\": [\n            3.6589940901202946,\n            0.0024594854221674217,\n            -0.013037306540415955,\n            0.00018013301869309603,\n            0.0025694674548725075,\n            -0.0055192991657138785,\n        ],\n        \"loss_domain_3\": [\n            3.0882519411499176,\n            0.004138978755961604,\n            0.000999612740187218,\n            -0.040138664176247195,\n            0.002610113875372775,\n            -0.00489486237526904,\n        ],\n        \"loss_domain_4\": [\n            1.6123830068952152,\n            -0.0002626923872205353,\n            0.001840626589019361,\n            0.0004871656033250259,\n            -0.048580221686945084,\n            0.0053741041526707105,\n        ],\n        \"loss_domain_5\": [\n            3.567736623439916,\n            0.005906206376564464,\n            0.00463119283202685,\n            -0.0015851069219014631,\n            0.004999662780701288,\n            -0.029220774259506147,\n        ],\n    },\n}\n\n\ndef _predict_one(input_point: Dict[str, float], beta: List[float]) -> float:\n    # Build feature vector: [1, log(p1+eps), ..., log(p5+eps)]\n    logs = [math.log(max(input_point.get(f\"proportion_domain_{i}\", 0.0), 0.0) + _EPS) for i in range(1, 6)]\n    return beta[0] + sum(b * x for b, x in zip(beta[1:], logs))\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _COEFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {sorted(_COEFS.keys())}\")\n\n    group_coefs = _COEFS[group]\n    outputs: List[Dict[str, float]] = []\n    for point in input_data:\n        pred: Dict[str, float] = {}\n        for k in range(1, 6):\n            key = f\"loss_domain_{k}\"\n            beta = group_coefs[key]\n            pred[key] = _predict_one(point, beta)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "mini-swe-agent", "model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from typing import List, Dict\nimport math\n\n# Fitted coefficients for the linear-in-log proportions model:\n# loss_domain_i = intercept_i + sum_j beta_ij * log(proportion_domain_j + eps)\nCOEFS = {\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.4042025803406553,\n      \"betas\": {\n        \"proportion_domain_1\": -0.04878456902733,\n        \"proportion_domain_2\": -0.001374141990715709,\n        \"proportion_domain_3\": 0.00425143168670042,\n        \"proportion_domain_4\": -0.004036425589256114,\n        \"proportion_domain_5\": 0.007660516801797778\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.329935868696654,\n      \"betas\": {\n        \"proportion_domain_1\": 0.002613273350161564,\n        \"proportion_domain_2\": -0.011891996557419415,\n        \"proportion_domain_3\": 2.5572674116154508e-05,\n        \"proportion_domain_4\": 0.003256629390122456,\n        \"proportion_domain_5\": -0.005952205705195364\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8006343825165314,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0038463371713819104,\n        \"proportion_domain_2\": 0.0010271048724917586,\n        \"proportion_domain_3\": -0.03834514983761013,\n        \"proportion_domain_4\": 0.003370871701657902,\n        \"proportion_domain_5\": -0.004992444549614571\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.3842254371292473,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0001770125184695076,\n        \"proportion_domain_2\": 0.0019008052563317707,\n        \"proportion_domain_3\": 0.00040985897229494966,\n        \"proportion_domain_4\": -0.04321725706040489,\n        \"proportion_domain_5\": 0.005203775943861618\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.22070848376419,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005681276339156933,\n        \"proportion_domain_2\": 0.004826630941776033,\n        \"proportion_domain_3\": -0.00156340623248917,\n        \"proportion_domain_4\": 0.005473022220321205,\n        \"proportion_domain_5\": -0.030039287316337895\n      }\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.254097761686427,\n      \"betas\": {\n        \"proportion_domain_1\": -0.047618308465165196,\n        \"proportion_domain_2\": -0.0013550046237553216,\n        \"proportion_domain_3\": 0.004003192579882288,\n        \"proportion_domain_4\": -0.0042909223510698554,\n        \"proportion_domain_5\": 0.006758159630738282\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.166848403701224,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0023474329227610603,\n        \"proportion_domain_2\": -0.011422287483401284,\n        \"proportion_domain_3\": 4.890168638858063e-06,\n        \"proportion_domain_4\": 0.0026987505976661923,\n        \"proportion_domain_5\": -0.006498741023938888\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.6500359123770685,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0035548451771600445,\n        \"proportion_domain_2\": 0.0011888369505355075,\n        \"proportion_domain_3\": -0.03964115398330717,\n        \"proportion_domain_4\": 0.0028959099742507494,\n        \"proportion_domain_5\": -0.004828358464409514\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.2751944568968825,\n      \"betas\": {\n        \"proportion_domain_1\": -0.00024923215104993846,\n        \"proportion_domain_2\": 0.0015693015233153626,\n        \"proportion_domain_3\": 0.0004352251902864513,\n        \"proportion_domain_4\": -0.041431801189773,\n        \"proportion_domain_5\": 0.00487564464080058\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.0477226649403506,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005424541586794049,\n        \"proportion_domain_2\": 0.004752807485933067,\n        \"proportion_domain_3\": -0.0015473014680650466,\n        \"proportion_domain_4\": 0.004775541162988269,\n        \"proportion_domain_5\": -0.030879417992197157\n      }\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.1969109104720936,\n      \"betas\": {\n        \"proportion_domain_1\": -0.04711053566759858,\n        \"proportion_domain_2\": -0.0005929598868873091,\n        \"proportion_domain_3\": 0.0032935545744610275,\n        \"proportion_domain_4\": -0.0034299883497122844,\n        \"proportion_domain_5\": 0.006245418652814477\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.096774846113359,\n      \"betas\": {\n        \"proportion_domain_1\": 0.002683578481767577,\n        \"proportion_domain_2\": -0.01079532012122322,\n        \"proportion_domain_3\": -0.0006061709329291659,\n        \"proportion_domain_4\": 0.003289915492216653,\n        \"proportion_domain_5\": -0.007059325448456723\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.5854698484676923,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0036493802056689617,\n        \"proportion_domain_2\": 0.0014981486567392278,\n        \"proportion_domain_3\": -0.04057579585320029,\n        \"proportion_domain_4\": 0.003422309711002145,\n        \"proportion_domain_5\": -0.005304641068008599\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.2374283294627488,\n      \"betas\": {\n        \"proportion_domain_1\": -0.0007019754734980677,\n        \"proportion_domain_2\": 0.0023220834389344196,\n        \"proportion_domain_3\": 0.00022483249867104974,\n        \"proportion_domain_4\": -0.040255057354183485,\n        \"proportion_domain_5\": 0.0043998574943246395\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 2.9734625677861892,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005474979456664107,\n        \"proportion_domain_2\": 0.005292433569369652,\n        \"proportion_domain_3\": -0.0021983339298417347,\n        \"proportion_domain_4\": 0.005328130514437292,\n        \"proportion_domain_5\": -0.031848404298810445\n      }\n    }\n  },\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.709753097020149,\n      \"betas\": {\n        \"proportion_domain_1\": -0.05223448519596348,\n        \"proportion_domain_2\": -0.002096832291136182,\n        \"proportion_domain_3\": 0.0049309629806633245,\n        \"proportion_domain_4\": -0.005121637241168704,\n        \"proportion_domain_5\": 0.008582633603168494\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.6589940901202946,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0024594854221674217,\n        \"proportion_domain_2\": -0.013037306540415955,\n        \"proportion_domain_3\": 0.00018013301869309603,\n        \"proportion_domain_4\": 0.0025694674548725075,\n        \"proportion_domain_5\": -0.0055192991657138785\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 3.0882519411499176,\n      \"betas\": {\n        \"proportion_domain_1\": 0.004138978755961604,\n        \"proportion_domain_2\": 0.000999612740187218,\n        \"proportion_domain_3\": -0.040138664176247195,\n        \"proportion_domain_4\": 0.002610113875372775,\n        \"proportion_domain_5\": -0.00489486237526904\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.6123830068952152,\n      \"betas\": {\n        \"proportion_domain_1\": -0.0002626923872205353,\n        \"proportion_domain_2\": 0.001840626589019361,\n        \"proportion_domain_3\": 0.0004871656033250259,\n        \"proportion_domain_4\": -0.048580221686945084,\n        \"proportion_domain_5\": 0.0053741041526707105\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.567736623439916,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005906206376564464,\n        \"proportion_domain_2\": 0.00463119283202685,\n        \"proportion_domain_3\": -0.0015851069219014631,\n        \"proportion_domain_4\": 0.004999662780701288,\n        \"proportion_domain_5\": -0.029220774259506147\n      }\n    }\n  }\n}\n\nPROP_COLS = ['proportion_domain_1', 'proportion_domain_2', 'proportion_domain_3', 'proportion_domain_4', 'proportion_domain_5']\nLOSS_COLS = ['loss_domain_1', 'loss_domain_2', 'loss_domain_3', 'loss_domain_4', 'loss_domain_5']\nEPS = 1e-6\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Functional form (same across groups):\n        For each loss_domain_k:\n            loss_k = intercept_k(group) + sum_j beta_kj(group) * log(proportion_domain_j + EPS)\n\n    Args:\n        input_data: List of dicts with keys including all 'proportion_domain_*' inputs.\n        group: The experimental group for which to make predictions.\n               Coefficients differ per group but the functional form is fixed.\n\n    Returns:\n        List of dicts with keys equal to LOSS_COLS and predicted float values.\n    \"\"\"\n    if group not in COEFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(COEFS.keys())}\")\n    group_coefs = COEFS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Build predictions for each loss column\n        out = {}\n        for loss_key in LOSS_COLS:\n            params = group_coefs[loss_key]\n            val = float(params[\"intercept\"])\n            for p in PROP_COLS:\n                x = float(row.get(p, 0.0))\n                val += float(params[\"betas\"][p]) * math.log(max(x, 0.0) + EPS)\n            out[loss_key] = val\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5.2", "reward_r2": 0.8992005438971138, "solution": "import math\nfrom typing import Dict, List\n\n\n# Discovered scaling law:\n#   loss_domain_i = b_i(group) + sum_j w_{i,j}(group) * ln(proportion_domain_j + eps)\n# where eps is a small constant to safely handle zero proportions.\n\n_EPS = 1e-6\n\n_PARAMS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"70M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.7097530970201484,\n            \"proportion_domain_1\": -0.0522344851959636,\n            \"proportion_domain_2\": -0.002096832291136152,\n            \"proportion_domain_3\": 0.004930962980663215,\n            \"proportion_domain_4\": -0.005121637241168656,\n            \"proportion_domain_5\": 0.008582633603168563,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.658994090120295,\n            \"proportion_domain_1\": 0.0024594854221674004,\n            \"proportion_domain_2\": -0.013037306540415919,\n            \"proportion_domain_3\": 0.00018013301869302894,\n            \"proportion_domain_4\": 0.0025694674548724715,\n            \"proportion_domain_5\": -0.005519299165713886,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 3.088251941149917,\n            \"proportion_domain_1\": 0.004138978755961478,\n            \"proportion_domain_2\": 0.0009996127401871256,\n            \"proportion_domain_3\": -0.04013866417624731,\n            \"proportion_domain_4\": 0.0026101138753726574,\n            \"proportion_domain_5\": -0.004894862375269067,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.6123830068952154,\n            \"proportion_domain_1\": -0.0002626923872205609,\n            \"proportion_domain_2\": 0.0018406265890194163,\n            \"proportion_domain_3\": 0.00048716560332494747,\n            \"proportion_domain_4\": -0.04858022168694516,\n            \"proportion_domain_5\": 0.00537410415267081,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.567736623439917,\n            \"proportion_domain_1\": 0.0059062063765644435,\n            \"proportion_domain_2\": 0.00463119283202682,\n            \"proportion_domain_3\": -0.001585106921901497,\n            \"proportion_domain_4\": 0.004999662780701258,\n            \"proportion_domain_5\": -0.029220774259506154,\n        },\n    },\n    \"160M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.404202580340655,\n            \"proportion_domain_1\": -0.04878456902733004,\n            \"proportion_domain_2\": -0.0013741419907157414,\n            \"proportion_domain_3\": 0.004251431686700347,\n            \"proportion_domain_4\": -0.004036425589256161,\n            \"proportion_domain_5\": 0.007660516801797868,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.329935868696656,\n            \"proportion_domain_1\": 0.0026132733501615223,\n            \"proportion_domain_2\": -0.011891996557419387,\n            \"proportion_domain_3\": 2.55726741161342e-05,\n            \"proportion_domain_4\": 0.003256629390122431,\n            \"proportion_domain_5\": -0.005952205705195353,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.8006343825165327,\n            \"proportion_domain_1\": 0.0038463371713819065,\n            \"proportion_domain_2\": 0.0010271048724917568,\n            \"proportion_domain_3\": -0.03834514983761017,\n            \"proportion_domain_4\": 0.003370871701657851,\n            \"proportion_domain_5\": -0.004992444549614443,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.3842254371292473,\n            \"proportion_domain_1\": 0.00017701251846946727,\n            \"proportion_domain_2\": 0.0019008052563317963,\n            \"proportion_domain_3\": 0.00040985897229483783,\n            \"proportion_domain_4\": -0.04321725706040493,\n            \"proportion_domain_5\": 0.005203775943861722,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.22070848376419,\n            \"proportion_domain_1\": 0.005681276339156912,\n            \"proportion_domain_2\": 0.004826630941775963,\n            \"proportion_domain_3\": -0.0015634062324892195,\n            \"proportion_domain_4\": 0.005473022220321154,\n            \"proportion_domain_5\": -0.030039287316337846,\n        },\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.254097761686427,\n            \"proportion_domain_1\": -0.04761830846516523,\n            \"proportion_domain_2\": -0.0013550046237552692,\n            \"proportion_domain_3\": 0.004003192579882179,\n            \"proportion_domain_4\": -0.004290922351069889,\n            \"proportion_domain_5\": 0.006758159630738393,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.166848403701226,\n            \"proportion_domain_1\": 0.0023474329227610503,\n            \"proportion_domain_2\": -0.011422287483401315,\n            \"proportion_domain_3\": 4.890168638751796e-06,\n            \"proportion_domain_4\": 0.0026987505976661533,\n            \"proportion_domain_5\": -0.006498741023938811,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.65003591237707,\n            \"proportion_domain_1\": 0.003554845177160056,\n            \"proportion_domain_2\": 0.001188836950535425,\n            \"proportion_domain_3\": -0.03964115398330725,\n            \"proportion_domain_4\": 0.0028959099742506974,\n            \"proportion_domain_5\": -0.004828358464409561,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.2751944568968825,\n            \"proportion_domain_1\": -0.0002492321510499463,\n            \"proportion_domain_2\": 0.0015693015233153589,\n            \"proportion_domain_3\": 0.0004352251902864149,\n            \"proportion_domain_4\": -0.04143180118977307,\n            \"proportion_domain_5\": 0.004875644640800658,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.0477226649403515,\n            \"proportion_domain_1\": 0.0054245415867939665,\n            \"proportion_domain_2\": 0.00475280748593301,\n            \"proportion_domain_3\": -0.001547301468065229,\n            \"proportion_domain_4\": 0.004775541162988245,\n            \"proportion_domain_5\": -0.030879417992197206,\n        },\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.196910910472094,\n            \"proportion_domain_1\": -0.04711053566759865,\n            \"proportion_domain_2\": -0.0005929598868872977,\n            \"proportion_domain_3\": 0.0032935545744609243,\n            \"proportion_domain_4\": -0.003429988349712316,\n            \"proportion_domain_5\": 0.006245418652814499,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.09677484611336,\n            \"proportion_domain_1\": 0.0026835784817675957,\n            \"proportion_domain_2\": -0.010795320121223216,\n            \"proportion_domain_3\": -0.0006061709329293224,\n            \"proportion_domain_4\": 0.00328991549221664,\n            \"proportion_domain_5\": -0.007059325448456602,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.5854698484676923,\n            \"proportion_domain_1\": 0.0036493802056689595,\n            \"proportion_domain_2\": 0.0014981486567391834,\n            \"proportion_domain_3\": -0.040575795853200314,\n            \"proportion_domain_4\": 0.003422309711002071,\n            \"proportion_domain_5\": -0.005304641068008607,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.2374283294627486,\n            \"proportion_domain_1\": -0.0007019754734981021,\n            \"proportion_domain_2\": 0.0023220834389343996,\n            \"proportion_domain_3\": 0.00022483249867098906,\n            \"proportion_domain_4\": -0.04025505735418355,\n            \"proportion_domain_5\": 0.004399857494324675,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 2.973462567786192,\n            \"proportion_domain_1\": 0.005474979456664085,\n            \"proportion_domain_2\": 0.0052924335693696496,\n            \"proportion_domain_3\": -0.002198333929841836,\n            \"proportion_domain_4\": 0.005328130514437295,\n            \"proportion_domain_5\": -0.03184840429881037,\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n            point containing input variable names as keys and their corresponding values.\n        group: The name of the experimental group for which to make predictions.\n            The functional form of the law is shared across groups, but coefficients\n            differ.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group: {group}. Known groups: {sorted(_PARAMS.keys())}\")\n\n    group_params = _PARAMS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Precompute log proportions (with epsilon for safety at p=0)\n        logp = {}\n        for j in range(1, 6):\n            k = f\"proportion_domain_{j}\"\n            v = float(row.get(k, 0.0))\n            if v < 0:\n                v = 0.0\n            logp[k] = math.log(v + _EPS)\n\n        pred_row: Dict[str, float] = {}\n        for i in range(1, 6):\n            outk = f\"loss_domain_{i}\"\n            p = group_params[outk]\n            val = p[\"intercept\"]\n            for j in range(1, 6):\n                ink = f\"proportion_domain_{j}\"\n                val += p[ink] * logp[ink]\n            pred_row[outk] = float(val)\n\n        outputs.append(pred_row)\n\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5.2", "reward_r2": 0.8992005438971136, "solution": "import math\nfrom typing import Dict, List\n\n# Scaling law discovered from the dataset:\n# For each group g and each domain d (1..5):\n#   loss_domain_d = b[g,d] + sum_{k=1..5} a[g,d,k] * log(proportion_domain_k + eps)\n# with a small eps to handle zero proportions.\n\n_EPS = 1e-6\n\n_PARAMS: Dict[str, Dict[str, Dict[str, List[float] | float]]] = {\n    \"70M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.7097530970201484,\n            \"a\": [-0.0522344851959636, -0.002096832291136152, 0.004930962980663215, -0.005121637241168656, 0.008582633603168563],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.658994090120295,\n            \"a\": [0.0024594854221674004, -0.013037306540415919, 0.00018013301869302894, 0.0025694674548724715, -0.005519299165713886],\n        },\n        \"loss_domain_3\": {\n            \"b\": 3.088251941149917,\n            \"a\": [0.004138978755961478, 0.0009996127401871256, -0.04013866417624731, 0.0026101138753726574, -0.004894862375269067],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.6123830068952154,\n            \"a\": [-0.0002626923872205609, 0.0018406265890194163, 0.00048716560332494747, -0.04858022168694516, 0.00537410415267081],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.567736623439917,\n            \"a\": [0.0059062063765644435, 0.00463119283202682, -0.001585106921901497, 0.004999662780701258, -0.029220774259506154],\n        },\n    },\n    \"160M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.404202580340655,\n            \"a\": [-0.04878456902733004, -0.0013741419907157414, 0.004251431686700347, -0.004036425589256161, 0.007660516801797868],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.329935868696656,\n            \"a\": [0.0026132733501615223, -0.011891996557419387, 2.55726741161342e-05, 0.003256629390122431, -0.005952205705195353],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.8006343825165327,\n            \"a\": [0.0038463371713819065, 0.0010271048724917568, -0.03834514983761017, 0.003370871701657851, -0.004992444549614443],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.3842254371292473,\n            \"a\": [0.00017701251846946727, 0.0019008052563317963, 0.00040985897229483783, -0.04321725706040493, 0.005203775943861722],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.22070848376419,\n            \"a\": [0.005681276339156912, 0.004826630941775963, -0.0015634062324892195, 0.005473022220321154, -0.030039287316337846],\n        },\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.254097761686427,\n            \"a\": [-0.04761830846516523, -0.0013550046237552692, 0.004003192579882179, -0.004290922351069889, 0.006758159630738393],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.166848403701226,\n            \"a\": [0.0023474329227610503, -0.011422287483401315, 4.890168638751796e-06, 0.0026987505976661533, -0.006498741023938811],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.65003591237707,\n            \"a\": [0.003554845177160056, 0.001188836950535425, -0.03964115398330725, 0.0028959099742506974, -0.004828358464409561],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.2751944568968825,\n            \"a\": [-0.0002492321510499463, 0.0015693015233153589, 0.0004352251902864149, -0.04143180118977307, 0.004875644640800658],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.0477226649403515,\n            \"a\": [0.0054245415867939665, 0.00475280748593301, -0.001547301468065229, 0.004775541162988245, -0.030879417992197206],\n        },\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.196910910472094,\n            \"a\": [-0.04711053566759865, -0.0005929598868872977, 0.0032935545744609243, -0.003429988349712316, 0.006245418652814499],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.09677484611336,\n            \"a\": [0.0026835784817675957, -0.010795320121223216, -0.0006061709329293224, 0.00328991549221664, -0.007059325448456602],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.5854698484676923,\n            \"a\": [0.0036493802056689595, 0.0014981486567391834, -0.040575795853200314, 0.003422309711002071, -0.005304641068008607],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.2374283294627486,\n            \"a\": [-0.0007019754734981021, 0.0023220834389343996, 0.00022483249867098906, -0.04025505735418355, 0.004399857494324675],\n        },\n        \"loss_domain_5\": {\n            \"b\": 2.973462567786192,\n            \"a\": [0.005474979456664085, 0.0052924335693696496, -0.002198333929841836, 0.005328130514437295, -0.03184840429881037],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts validation loss per domain from training mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise KeyError(f\"Unknown group '{group}'. Known groups: {sorted(_PARAMS.keys())}\")\n\n    group_params = _PARAMS[group]\n    outputs: List[Dict[str, float]] = []\n\n    for row in input_data:\n        props = [\n            float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)\n        ]\n        logp = [math.log(p + _EPS) for p in props]\n\n        pred: Dict[str, float] = {}\n        for d in range(1, 6):\n            key = f\"loss_domain_{d}\"\n            b = float(group_params[key][\"b\"])  # type: ignore[index]\n            a = group_params[key][\"a\"]  # type: ignore[index]\n            pred[key] = b + sum(float(ai) * li for ai, li in zip(a, logp))\n\n        outputs.append(pred)\n\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "aider", "model_name": "gpt-5", "reward_r2": 0.8992, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Implementation notes:\n    # - Functional form (shared across groups):\n    #     For each domain k:\n    #         loss_domain_k = a_k + sum_j w_{k,j} * log(proportion_domain_j + eps)\n    #   where eps is a small constant to stabilize log near zero.\n    # - Coefficients (a_k, w_{k,j}) are fitted per `group` by ridge regression\n    #   on the dataset loaded from /app/data (if available).\n    # - Fitted parameters are cached on the function object (law._params)\n    #   and also written to /app/fit.json for inspection.\n    import re\n    import os\n    import json\n    import math\n\n    # Lazy imports to avoid hard dependency unless actually called.\n    try:\n        import numpy as _np\n    except Exception as _e:\n        raise RuntimeError(\"numpy is required to run the scaling law\") from _e\n\n    # Create persistent cache on the function object\n    if not hasattr(law, \"_params\"):\n        law._params = {}\n\n    def _load_dataset():\n        try:\n            from datasets import load_from_disk as _load_from_disk\n            return _load_from_disk(\"/app/data\")\n        except Exception:\n            return None\n\n    def _extract_columns(ds):\n        # Determine proportion and loss columns from dataset schema\n        def _cols_from_split(split_ds):\n            # datasets.Dataset has attribute column_names\n            return list(getattr(split_ds, \"column_names\", []))\n        if hasattr(ds, \"keys\"):\n            # DatasetDict\n            first_key = next(iter(ds.keys()))\n            cols = _cols_from_split(ds[first_key])\n        else:\n            cols = _cols_from_split(ds)\n        prop_cols = [c for c in cols if c.startswith(\"proportion_domain_\")]\n        loss_cols = [c for c in cols if c.startswith(\"loss_domain_\")]\n\n        def _idx(c):\n            m = re.search(r\"(\\d+)$\", c)\n            return int(m.group(1)) if m else 10**9\n\n        prop_cols = sorted(prop_cols, key=_idx)\n        loss_cols = sorted(loss_cols, key=_idx)\n        group_col = \"group\" if \"group\" in cols else None\n        return prop_cols, loss_cols, group_col\n\n    def _iter_train_rows(ds):\n        # Normalize to a list of dict rows, preferring 'train' if present\n        if hasattr(ds, \"keys\"):\n            # DatasetDict\n            if \"train\" in ds:\n                return list(ds[\"train\"])\n            # else merge all splits\n            rows = []\n            for sp in ds.keys():\n                rows.extend(ds[sp])\n            return rows\n        # Single Dataset\n        rows = list(ds)\n        if rows and isinstance(rows[0], dict) and \"split\" in rows[0]:\n            rows = [r for r in rows if r.get(\"split\") == \"train\"]\n        return rows\n\n    def _fit_for_group(g: str):\n        ds = _load_dataset()\n        eps = 1e-6\n        reg = 1e-3  # L2 regularization to mitigate collinearity (proportions sum to 1)\n\n        # If dataset is unavailable, create a degenerate model based on input_data structure.\n        if ds is None:\n            # Infer columns from input_data if possible\n            prop_cols = []\n            loss_cols = []\n            if input_data:\n                keys = list(input_data[0].keys())\n                prop_cols = sorted([k for k in keys if k.startswith(\"proportion_domain_\")],\n                                   key=lambda c: int(re.search(r\"(\\d+)$\", c).group(1)) if re.search(r\"(\\d+)$\", c) else 10**9)\n                # Default order for losses mirrors proportions\n                loss_cols = [f\"loss_domain_{i+1}\" for i in range(len(prop_cols))]\n            beta = _np.zeros((len(prop_cols) + 1, len(loss_cols) if loss_cols else 0), dtype=float)\n            return {\"beta\": beta, \"prop_cols\": prop_cols, \"loss_cols\": loss_cols, \"eps\": eps}\n\n        prop_cols, loss_cols, group_col = _extract_columns(ds)\n        rows = _iter_train_rows(ds)\n\n        # Filter by group if a group column is present; otherwise use all rows\n        if group_col is not None:\n            grp_rows = [r for r in rows if r.get(group_col) == g]\n            if len(grp_rows) == 0:\n                # Fallback to all rows if the requested group does not exist\n                grp_rows = rows\n        else:\n            grp_rows = rows\n\n        # Assemble design matrix X (log proportions) and targets Y (losses)\n        X_list = []\n        Y_list = []\n        for r in grp_rows:\n            try:\n                xrow = [math.log(float(r.get(c, 0.0)) + eps) for c in prop_cols]\n                yrow = [float(r[c]) for c in loss_cols if c in r]\n            except Exception:\n                continue\n            if len(yrow) != len(loss_cols):\n                continue\n            if any(math.isinf(v) or math.isnan(v) for v in xrow):\n                continue\n            X_list.append(xrow)\n            Y_list.append(yrow)\n\n        if len(X_list) == 0:\n            beta = _np.zeros((len(prop_cols) + 1, len(loss_cols)), dtype=float)\n        else:\n            X = _np.asarray(X_list, dtype=float)\n            Y = _np.asarray(Y_list, dtype=float)\n            # Add bias column\n            Xb = _np.concatenate([_np.ones((X.shape[0], 1), dtype=float), X], axis=1)\n            XtX = Xb.T @ Xb\n            I = _np.eye(XtX.shape[0], dtype=float)\n            I[0, 0] = 0.0  # do not regularize intercept\n            A = XtX + reg * I\n            XtY = Xb.T @ Y\n            try:\n                beta = _np.linalg.solve(A, XtY)\n            except _np.linalg.LinAlgError:\n                beta = _np.linalg.pinv(A) @ XtY\n\n        params = {\"beta\": beta, \"prop_cols\": prop_cols, \"loss_cols\": loss_cols, \"eps\": eps}\n\n        # Persist for inspection\n        try:\n            serial = {\n                \"prop_cols\": prop_cols,\n                \"loss_cols\": loss_cols,\n                \"eps\": eps,\n                \"beta\": beta.tolist(),\n            }\n            fit_path = \"/app/fit.json\"\n            blob = {}\n            if os.path.exists(fit_path):\n                try:\n                    with open(fit_path, \"r\") as f:\n                        blob = json.load(f) or {}\n                except Exception:\n                    blob = {}\n            blob[g] = serial\n            with open(fit_path, \"w\") as f:\n                json.dump(blob, f, indent=2)\n        except Exception:\n            # Best-effort; do not fail predictions because of I/O issues.\n            pass\n\n        return params\n\n    # Ensure we have fitted parameters for this group\n    if group not in law._params:\n        law._params[group] = _fit_for_group(group)\n\n    params = law._params[group]\n    beta = params[\"beta\"]\n    prop_cols = params[\"prop_cols\"]\n    loss_cols = params[\"loss_cols\"]\n    eps = params[\"eps\"]\n\n    # Produce predictions\n    results: list[dict[str, float]] = []\n    for row in input_data:\n        # If dataset unavailable and we couldn't infer loss columns, return empty predictions\n        if not loss_cols:\n            results.append({})\n            continue\n        x = [_np.log(float(row.get(c, 0.0)) + eps) for c in prop_cols]\n        xb = _np.concatenate([_np.array([1.0], dtype=float), _np.asarray(x, dtype=float)])\n        y = xb @ beta  # shape (num_losses,)\n        pred = {loss_cols[j]: float(y[j]) for j in range(len(loss_cols))}\n        results.append(pred)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gemini-3-flash-preview", "reward_r2": 0.8946454997391851, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters fitted for each group and domain\n    # Format: {group: {domain_index: [a, b, c]}}\n    # Formula: loss = a * (proportion + 1e-6)**b + c\n    params = {\n      \"70M\": {\n        1: [2.710378086839277, -0.017029560929862225, 6.122502542461153e-14],\n        2: [3.641822387148273, -0.0034420417203390283, 3.664705437636036e-05],\n        3: [3.0734642770039438, -0.011482268379146534, 5.0279160217933075e-14],\n        4: [1.6059732325280927, -0.025094792442143306, 5.6645199147913316e-14],\n        5: [3.4932889198309134, -0.00937863408600572, 2.2025811669049076e-16]\n      },\n      \"160M\": {\n        1: [2.40081495799688, -0.017856712579836093, 1.465625882756652e-12],\n        2: [3.309549242165178, -0.003481852718743377, 1.2595135178484399e-05],\n        3: [2.7836616124660836, -0.012021613474412426, 5.855820258273959e-08],\n        4: [1.3747713978038434, -0.025950009122439683, 2.5488849307414626e-15],\n        5: [3.143771112943483, -0.010605607268231104, 1.8784228785426135e-13]\n      },\n      \"305M\": {\n        1: [2.255287703195163, -0.01843295718759498, 1.2051934351546453e-12],\n        2: [3.1521871451134604, -0.0034650075599279586, 1.233711008468711e-05],\n        3: [2.636809538515421, -0.013020916869252688, 9.041291266491046e-09],\n        4: [1.2700474990878887, -0.026723137945954697, 3.622469806622208e-16],\n        5: [2.976521908043489, -0.011269643911003876, 1.18352489559479e-13]\n      },\n      \"410M\": {\n        1: [2.1947499199085096, -0.018720868396927052, 7.767189923855914e-14],\n        2: [3.080781816845867, -0.003437582676076585, 1.1255468841608477e-05],\n        3: [2.5700550678168232, -0.013549146350127213, 2.6549168297135425e-17],\n        4: [1.2317060817465844, -0.02679817668535512, 5.066308580862643e-13],\n        5: [2.9004711222062993, -0.011890602184807265, 4.2759352333656383e-14]\n      }\n    }\n\n    group_params = params.get(group)\n    if not group_params:\n        # Default to 410M if group not found\n        group_params = params[\"410M\"]\n\n    predictions = []\n    for data_point in input_data:\n        prediction = {}\n        for i in range(1, 6):\n            proportion = data_point.get(f\"proportion_domain_{i}\", 0.0)\n            a, b, c = group_params[i]\n            prediction[f\"loss_domain_{i}\"] = a * (proportion + 1e-6)**b + c\n        predictions.append(prediction)\n    \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.886782, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nRobust multi-domain scaling law with sqrt-log hybrid features\nKey improvements:\n1. Novel sqrt-log hybrid transformation for better numerical properties\n2. 7 parameters per domain (35 total): baseline + 5 linear + 1 quadratic\n3. Adaptive mixture regularization based on domain balance\n4. Robust initialization with Tikhonov regularization\n5. Multi-stage optimization with smart warm-starting\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Hybrid sqrt-log compositional model with centered quadratic.\n    Parameters per output domain (7 × 5 = 35 total):\n    - 1 baseline offset\n    - 5 linear mixture coefficients (on transformed features)\n    - 1 centered quadratic self-interaction coefficient\n    \n    Uses sqrt(log(1+4x)) transformation for optimal conditioning.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params)\n    \n    if params.ndim == 1:\n        params = params[None, :]\n    T = params.shape[0]\n    \n    params = params.reshape(T, 7)\n    predictions = np.zeros((N, T))\n    \n    # Hybrid transformation: sqrt(log(1+4x)) combines benefits of both\n    # - log handles sparse regions well\n    # - sqrt reduces extreme values\n    # - 4x scaling optimizes for [0,1] range\n    X_transformed = np.sqrt(np.log1p(X * 4.0))\n    \n    for t in range(T):\n        # Linear component on transformed features\n        pred = params[t, 0] + np.dot(X_transformed, params[t, 1:6])\n        \n        # Centered quadratic on original space for interpretability\n        x_centered = X[:, t] - 0.2\n        pred += params[t, 6] * (x_centered ** 2)\n        \n        predictions[:, t] = pred\n    \n    return predictions[:, 0] if T == 1 else predictions\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Tikhonov-regularized initialization with adaptive optimization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    \n    y2d = y[:, None] if y.ndim == 1 else y\n    T = y2d.shape[1]\n    \n    # Transform features\n    X_transformed = np.sqrt(np.log1p(X * 4.0))\n    \n    # Compute domain balance for adaptive regularization\n    domain_balance = np.std(y2d, axis=0) / (np.mean(y2d, axis=0) + 1e-8)\n    \n    # Tikhonov-regularized initialization\n    init = np.zeros((T, 7))\n    \n    for t in range(T):\n        try:\n            # Adaptive Tikhonov parameter based on domain balance\n            lambda_tikh = 0.05 * (1.0 + domain_balance[t])\n            \n            # Tikhonov regularization: (X^T X + λI)^{-1} X^T y\n            XtX = X_transformed.T @ X_transformed\n            XtX_reg = XtX + lambda_tikh * np.eye(F)\n            Xty = X_transformed.T @ y2d[:, t]\n            \n            coeffs = np.linalg.solve(XtX_reg, Xty)\n            \n            # Baseline from median residual\n            linear_pred = X_transformed @ coeffs\n            residuals = y2d[:, t] - linear_pred\n            init[t, 0] = np.median(residuals)\n            \n            # Linear coefficients\n            init[t, 1:6] = coeffs\n            \n            # Quadratic: weighted correlation\n            x_centered = X[:, t] - 0.2\n            x_sq = x_centered ** 2\n            \n            if np.std(x_sq) > 1e-9:\n                # Weight by residual variance\n                res_var = np.var(residuals)\n                corr = np.corrcoef(x_sq, residuals)[0, 1]\n                if not np.isnan(corr):\n                    # Scale by domain balance\n                    scale_factor = 0.45 / (domain_balance[t] + 0.5)\n                    init[t, 6] = scale_factor * corr\n                else:\n                    init[t, 6] = -0.18\n            else:\n                init[t, 6] = -0.18\n                \n        except:\n            # Fallback\n            init[t, 0] = np.median(y2d[:, t])\n            init[t, 1:6] = np.zeros(5)\n            init[t, 1:6][t] = 0.82\n            init[t, 6] = -0.18\n    \n    def objective(flat_params):\n        params = flat_params.reshape(T, 7)\n        pred = scaling_law_func(X, params)\n        mse = np.mean((pred - y2d) ** 2)\n        \n        # Adaptive mixture regularization\n        reg_total = 0.0\n        for t in range(T):\n            # Balance-aware regularization\n            weight = 1.0 / (domain_balance[t] + 0.4)\n            \n            # Stratified regularization\n            baseline_reg = 0.0006 * weight * params[t, 0] ** 2\n            linear_reg = 0.0024 * weight * np.sum(params[t, 1:6] ** 2)\n            quad_reg = 0.012 * weight * params[t, 6] ** 2\n            \n            reg_total += baseline_reg + linear_reg + quad_reg\n        \n        return mse + reg_total\n    \n    # Conservative bounds\n    bounds = []\n    for t in range(T):\n        bounds.append((0.22, 6.4))  # Baseline\n        bounds.extend([(-5.2, 7.2)] * 5)  # Linear\n        bounds.append((-3.2, 1.7))  # Quadratic\n    \n    # Stage 1: Primary optimization\n    result = minimize(\n        objective, init.ravel(), method='L-BFGS-B',\n        bounds=bounds, options={'maxiter': 2700, 'ftol': 1e-11, 'gtol': 1e-9}\n    )\n    \n    best_result = result if result.success else None\n    best_loss = result.fun if result.success else float('inf')\n    \n    # Stage 2: Refinement if promising\n    if best_result is not None and best_loss < 0.33:\n        result2 = minimize(\n            objective, best_result.x, method='L-BFGS-B',\n            bounds=bounds, options={'maxiter': 1100, 'ftol': 1e-12, 'gtol': 1e-10}\n        )\n        if result2.success and result2.fun < best_loss:\n            best_result = result2\n            best_loss = result2.fun\n    \n    # Stage 3: Alternative start if needed\n    if best_loss > 0.27:\n        # Small perturbation\n        init_pert = init + np.random.randn(*init.shape) * 0.07\n        result3 = minimize(\n            objective, init_pert.ravel(), method='L-BFGS-B',\n            bounds=bounds, options={'maxiter': 2400, 'ftol': 1e-11, 'gtol': 1e-9}\n        )\n        if result3.success and result3.fun < best_loss:\n            best_result = result3\n    \n    params_opt = best_result.x.reshape(T, 7) if best_result is not None else init\n    return params_opt[0] if T == 1 else params_opt\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "terminus-2", "model_name": "gpt-5", "reward_r2": 0.880748, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nEPS = 1e-06\nPARAMS = {\n  \"70M\": {\n    \"domain_1\": {\n      \"a\": 2.0220104987927257,\n      \"b\": 0.7051338858670713,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.478057239579727,\n      \"b\": 0.17092206236157978,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.59254285900522,\n      \"b\": 0.5058051486225027,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.9657272567112907,\n      \"b\": 0.6545542978911262,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 3.024670907672095,\n      \"b\": 0.47441444506610303,\n      \"alpha\": 0.05\n    }\n  },\n  \"160M\": {\n    \"domain_1\": {\n      \"a\": 1.7563840140531553,\n      \"b\": 0.6595784179449191,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.1588187107783474,\n      \"b\": 0.15725443050159807,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.3251342372559862,\n      \"b\": 0.48184980806156213,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.8034472891804151,\n      \"b\": 0.583579912689387,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.661628446598834,\n      \"b\": 0.487853772515834,\n      \"alpha\": 0.05\n    }\n  },\n  \"305M\": {\n    \"domain_1\": {\n      \"a\": 1.6270228147234236,\n      \"b\": 0.6426830441112606,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.0093884970464932,\n      \"b\": 0.14899746540873754,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.1615746228783017,\n      \"b\": 0.4986333285709002,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.7225593285828501,\n      \"b\": 0.5587932312372937,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.4882874593817577,\n      \"b\": 0.49385627236422996,\n      \"alpha\": 0.05\n    }\n  },\n  \"410M\": {\n    \"domain_1\": {\n      \"a\": 1.5720931564397607,\n      \"b\": 0.6367651459833956,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 2.9422498586244026,\n      \"b\": 0.14451569098864436,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.0855052306255972,\n      \"b\": 0.5080046213375579,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.6988734728738473,\n      \"b\": 0.5437907496465303,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.395591013031327,\n      \"b\": 0.5105511796136086,\n      \"alpha\": 0.05\n    }\n  }\n}\n\ndef _predict_one(sample: Dict[str, float], group: str) -> Dict[str, float]:\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n    out: Dict[str, float] = {}\n    gparams = PARAMS[group]\n    for key, coeffs in gparams.items():\n        # key looks like 'domain_i' from fitting stage\n        d_idx = ''.join(ch for ch in key if ch.isdigit())\n        p_key = f'proportion_domain_{d_idx}'\n        y_key = f'loss_domain_{d_idx}'\n        p = float(sample.get(p_key, 0.0))\n        a = coeffs[\"a\"]; b = coeffs[\"b\"]; alpha = coeffs[\"alpha\"]\n        x = (p + EPS) ** (-alpha) if alpha != 0.0 else p\n        y = a + b * x\n        out[y_key] = float(y)\n    return out\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: List of dicts, each containing at least proportion_domain_i keys.\n        group: Name of the experimental group for which to make predictions.\n    Returns:\n        List of dicts with predicted loss_domain_i for i=1..5 (as available).\n    \"\"\"\n    return [_predict_one(sample, group) for sample in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.874813, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef _softplus(x):\n    z = np.clip(np.asarray(x, dtype=float), -40.0, 40.0)\n    return np.log1p(np.exp(-np.abs(z))) + np.maximum(z, 0.0)\n\ndef _sigmoid(x):\n    z = np.clip(np.asarray(x, dtype=float), -40.0, 40.0)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef _pad5(X):\n    X = np.atleast_2d(np.asarray(X, dtype=float))\n    n, f = X.shape\n    if f < 5: X = np.hstack([X, np.zeros((n, 5 - f))])\n    elif f > 5: X = X[:, :5]\n    return X\n\ndef _unpack_params(params):\n    p = np.asarray(params, dtype=float).ravel()\n    if p.size < 35: p = np.pad(p, (0, 35 - p.size))\n    elif p.size > 35: p = p[:35]\n    i = 0\n    raw_a = p[i:i+5]; i += 5\n    raw_p = p[i];     i += 1\n    raw_q = p[i];     i += 1\n    W     = p[i:i+8].reshape(4, 2); i += 8\n    U     = p[i:i+10].reshape(2, 5); i += 10\n    b     = p[i:i+5]; i += 5\n    d     = p[i:i+5]\n    a = _softplus(raw_a)\n    p_exp = 0.3 + 2.2 * _sigmoid(raw_p)\n    q_exp = 0.3 + 2.2 * _sigmoid(raw_q)\n    return a, p_exp, q_exp, W, U, b, d, (raw_a, raw_p, raw_q)\n\ndef _build_Z(X, a, p_exp, q_exp):\n    eps = 1e-8\n    Xc = np.clip(X, eps, 1.0)\n    z0 = np.power(Xc @ a + eps, p_exp)\n    z1 = np.sum(Xc * Xc, axis=1)\n    z2 = -(Xc * np.log(Xc)).sum(axis=1)\n    z3 = np.sum(np.power(Xc, q_exp), axis=1)\n    return np.stack([z0, z1, z2, z3], axis=1)\n\ndef scaling_law_func(data_points, params):\n    X = _pad5(data_points)\n    a, p_exp, q_exp, W, U, b, d, _ = _unpack_params(params)\n    S = np.tanh(_build_Z(X, a, p_exp, q_exp) @ W)\n    return b[None, :] + S @ U + X * d[None, :]\n\ndef fit_scaling_law(data_points, loss_values):\n    X = _pad5(data_points)\n    Y = np.asarray(loss_values, dtype=float)\n    if Y.ndim == 1: Y = Y[:, None]\n    if Y.shape[1] != 5:\n        Y = Y[:, :5] if Y.shape[1] > 5 else np.pad(Y, ((0,0),(0,5 - Y.shape[1])), mode='edge')\n    N = X.shape[0]\n    y_mean = np.mean(Y, axis=0)\n\n    def inv_sigmoid_target(t):\n        t = np.clip(t, 1e-6, 1 - 1e-6)\n        return np.log(t / (1.0 - t))\n    target = (1.0 - 0.3) / 2.2\n    raw_a = np.zeros(5)\n    raw_p = np.array([inv_sigmoid_target(target)])\n    raw_q = np.array([inv_sigmoid_target(target)])\n\n    a0, p0, q0 = _softplus(raw_a), 0.3 + 2.2 * _sigmoid(raw_p), 0.3 + 2.2 * _sigmoid(raw_q)\n    Z0 = _build_Z(X, a0, p0, q0)\n    M = np.hstack([Z0, X, np.ones((N, 1))])\n    lam = 5e-4\n    D = np.diag([1]*9 + [0])\n    MTM = M.T @ M + lam * D + 1e-8 * np.eye(M.shape[1])\n    MTY = M.T @ Y\n    try:\n        B = np.linalg.solve(MTM, MTY)\n    except np.linalg.LinAlgError:\n        B = np.linalg.lstsq(MTM, MTY, rcond=None)[0]\n    A_Z = B[:4, :]\n    A_X = B[4:9, :]\n    b_init = B[9, :]\n    U_svd, S_svd, Vt = np.linalg.svd(A_Z, full_matrices=False)\n    r = 2\n    W_init = U_svd[:, :r] * np.sqrt(S_svd[:r][None, :])\n    U_init = (np.sqrt(S_svd[:r])[:, None]) * Vt[:r, :]\n    d_init = np.mean(A_X, axis=1)\n    theta = np.concatenate([raw_a, raw_p, raw_q, W_init.ravel(), U_init.ravel(), b_init, d_init])\n\n    def huber(res, delta=0.25):\n        a = np.abs(res)\n        return np.where(a <= delta, 0.5 * res * res, delta * (a - 0.5 * delta))\n\n    def objective(th):\n        pred = scaling_law_func(X, th)\n        loss = np.mean(huber(pred - Y))\n        a, p_exp, q_exp, W, U, b, d, raws = _unpack_params(th)\n        reg = 8e-4 * (np.sum(W*W) + np.sum(U*U) + np.sum(d*d))\n        reg += 4e-4 * ((p_exp - 1.0)**2 + (q_exp - 1.0)**2) + 8e-4 * np.sum(raws[0]**2)\n        reg += 8e-4 * np.sum((b - y_mean)**2)\n        return loss + reg\n\n    def _update_linear(th, lam=5e-4):\n        a, p_exp, q_exp, W, _, _, _, _ = _unpack_params(th)\n        S = np.tanh(_build_Z(X, a, p_exp, q_exp) @ W)\n        M = np.hstack([S, X, np.ones((N, 1))])\n        Dm = np.diag([1,1] + [1]*5 + [0])\n        MTM = M.T @ M + lam * Dm + 1e-8 * np.eye(M.shape[1])\n        MTY = M.T @ Y\n        try:\n            B = np.linalg.solve(MTM, MTY)\n        except np.linalg.LinAlgError:\n            B = np.linalg.lstsq(MTM, MTY, rcond=None)[0]\n        U_new = B[0:2, :]\n        d_new = np.mean(B[2:7, :], axis=1)\n        b_new = B[7, :]\n        out = th.copy()\n        out[15:25] = U_new.ravel()\n        out[25:30] = b_new\n        out[30:35] = d_new\n        return out\n\n    def pack_nl(th): return np.concatenate([th[:5], th[5:7], th[7:15]])\n    def unpack_nl(nl, th):\n        out = th.copy()\n        out[:5] = nl[:5]; out[5:7] = nl[5:7]; out[7:15] = nl[7:15]\n        return out\n    def objective_nl(nl, fixed):\n        th = unpack_nl(nl, fixed)\n        th[15:25] = fixed[15:25]; th[25:30] = fixed[25:30]; th[30:35] = fixed[30:35]\n        return objective(th)\n\n    theta = _update_linear(theta)\n    rng = np.random.default_rng(42)\n    best = theta.copy()\n    best_val = objective(best)\n\n    for _ in range(3):\n        nl = pack_nl(best) + rng.normal(0.0, 0.05, size=15)\n        res = minimize(lambda v: objective_nl(v, best), nl, method='L-BFGS-B', options={'maxiter': 300})\n        nl_opt = res.x if res.success else nl\n        cand = unpack_nl(nl_opt, best)\n        cand = _update_linear(cand)\n        res2 = minimize(objective, cand, method='L-BFGS-B', options={'maxiter': 200})\n        cand2 = res2.x if res2.success else cand\n        val = objective(cand2)\n        if val < best_val:\n            best_val, best = val, cand2\n\n    for _ in range(2):\n        start = best + rng.normal(0.0, 0.02, size=best.size)\n        res = minimize(objective, start, method='L-BFGS-B', options={'maxiter': 200})\n        cand = res.x if res.success else start\n        val = objective(cand)\n        if val < best_val:\n            best_val, best = val, cand\n\n    return best\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.874726, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\n_RANK=2; _DOMS=5; _DEL=1e-6; _L2=1e-3; _HUB=0.5\n\ndef _softplus(x):\n    return np.log1p(np.exp(-np.abs(x)))+np.maximum(x,0.0)\n\ndef scaling_law_func(data_points, params):\n    X=np.atleast_2d(np.asarray(data_points)); N,F=X.shape; assert F==_DOMS\n    p=np.asarray(params).ravel(); r=_RANK; i=0\n    U=p[i:i+F*r].reshape(F,r); i+=F*r\n    V=p[i:i+_DOMS*r].reshape(_DOMS,r); i+=_DOMS*r\n    c=p[i:i+_DOMS]; i+=_DOMS\n    rho=p[i] if i<p.size else -6.9; i+=1\n    beta=p[i:i+_DOMS] if i+_DOMS<=p.size else np.zeros(_DOMS)\n    eps=_softplus(rho)+_DEL\n    X=np.clip(X,0.0,1.0); Xlog=np.log(X+eps); z=Xlog@U\n    h=-(X*(np.log(X+eps))).sum(axis=1)\n    return z@V.T+c+h[:,None]*beta\n\ndef fit_scaling_law(data_points, loss_values):\n    X=np.atleast_2d(np.asarray(data_points)); Y=np.atleast_2d(np.asarray(loss_values))\n    N,F=X.shape; C=Y.shape[1]; assert F==_DOMS and C==_DOMS; r=_RANK\n    eps0=1e-3; Xc=np.clip(X,0.0,1.0); Xlog0=np.log(Xc+eps0)\n    W,_,_,_=np.linalg.lstsq(Xlog0,Y,rcond=None)\n    A,S,Bt=np.linalg.svd(W,full_matrices=False)\n    U0=A[:,:r]*np.sqrt(S[:r]); V0=Bt[:r,:].T*np.sqrt(S[:r])\n    z0=Xlog0@U0; c0=Y.mean(axis=0)-z0.mean(axis=0)@V0.T\n    h0=-(Xc*np.log(Xc+eps0)).sum(axis=1); denom=(h0@h0)+1e-8\n    R0=Y-(z0@V0.T+c0); beta0=np.array([(h0@R0[:,d])/denom for d in range(C)])\n    rho0=np.log(np.exp(eps0)-1.0)\n    init=np.concatenate([U0.ravel(),V0.ravel(),c0.ravel(),[rho0],beta0])\n\n    def obj_grad(p):\n        i=0\n        U=p[i:i+F*r].reshape(F,r); i+=F*r\n        V=p[i:i+C*r].reshape(C,r); i+=C*r\n        c=p[i:i+C]; i+=C\n        rho=p[i]; i+=1\n        beta=p[i:i+C]\n        eps=_softplus(rho)+_DEL\n        Xp=Xc+eps; Xlog=np.log(Xp); z=Xlog@U\n        h=-(Xc*np.log(Xp)).sum(axis=1)\n        Yhat=z@V.T+c+h[:,None]*beta\n        E=Yhat-Y\n        s=np.sqrt(1.0+(E/_HUB)**2)\n        coef=1.0/(N*C)\n        loss=( (_HUB**2)*(s-1.0) ).sum()*coef +0.5*_L2*((U**2).sum()+(V**2).sum()+(beta**2).sum())\n        gY=coef*E/s\n        dV=gY.T@z+_L2*V\n        dc=gY.sum(axis=0)\n        dz=gY@V\n        dU=Xlog.T@dz+_L2*U\n        dbeta=(gY*h[:,None]).sum(axis=0)+_L2*beta\n        invXp=1.0/Xp\n        S_eps=invXp@U\n        dh_deps=-(Xc*invXp).sum(axis=1)\n        drho=((dz*S_eps).sum() + ( (gY@beta)*dh_deps ).sum())*(1.0/(1.0+np.exp(-rho)))\n        grad=np.concatenate([dU.ravel(),dV.ravel(),dc.ravel(),[drho],dbeta])\n        return loss,grad\n\n    res=minimize(lambda p: obj_grad(p)[0], init, jac=lambda p: obj_grad(p)[1], method='L-BFGS-B', options={'maxiter':500})\n    return res.x if res.success else init\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o3", "reward_r2": 0.873244167574921, "solution": "import math\n\n# Pre-fitted coefficients (A, B) for each experimental group and domain.\n# The relationship is:  loss_domain_i = A + B * ln(proportion_domain_i + eps)\n# where eps is a very small constant to avoid log(0).\n_COEFFS = {\n    \"70M\": {\n        1: (2.73390692, -0.03347999),\n        2: (3.65073552, -0.00812046),\n        3: (3.10822759, -0.02380902),\n        4: (1.62816371, -0.03102932),\n        5: (3.50046709, -0.02257996),\n    },\n    \"160M\": {\n        1: (2.42222454, -0.03132683),\n        2: (3.31767555, -0.00747288),\n        3: (2.81638464, -0.02268263),\n        4: (1.39405071, -0.02766535),\n        5: (3.15089490, -0.02322456),\n    },\n    \"305M\": {\n        1: (2.27578800, -0.03052741),\n        2: (3.15991008, -0.00707973),\n        3: (2.66991027, -0.02347551),\n        4: (1.28807431, -0.02649081),\n        5: (2.98356485, -0.02351572),\n    },\n    \"410M\": {\n        1: (2.21485658, -0.03025062),\n        2: (3.08823094, -0.00686847),\n        3: (2.60339221, -0.02391696),\n        4: (1.24921386, -0.02577835),\n        5: (2.90760213, -0.02431623),\n    },\n}\n\n_EPS = 1e-9\n\ndef _predict_single(point: dict[str, float], coeffs: dict[int, tuple[float, float]]) -> dict[str, float]:\n    \"\"\"Apply the scaling law to a single data point.\"\"\"\n    out: dict[str, float] = {}\n    for i in range(1, 6):\n        p = point.get(f\"proportion_domain_{i}\")\n        if p is None:\n            raise KeyError(f\"Input missing 'proportion_domain_{i}'\")\n        A, B = coeffs[i]\n        out[f\"loss_domain_{i}\"] = A + B * math.log(p + _EPS)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is shared across groups, while the\n                coefficients differ.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {sorted(_COEFFS.keys())}\")\n\n    group_coeffs = _COEFFS[group]\n    return [_predict_single(pt, group_coeffs) for pt in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "mini-swe-agent", "model_name": "gpt-5", "reward_r2": 0.873244, "solution": "# Auto-generated scaling law implementation\nfrom __future__ import annotations\nimport math\nfrom typing import Dict, List\n\nEPS = 1e-9\nMODEL_FAMILY = \"log\"\n\n# Per-group, per-domain coefficients (domain indices 1..5)\nPARAMS: dict[str, dict[int, dict[str, float]]] = {\n  \"70M\": {\n    \"1\": {\n      \"a\": 2.73390692,\n      \"b\": -0.03347999\n    },\n    \"2\": {\n      \"a\": 3.65073552,\n      \"b\": -0.00812046\n    },\n    \"3\": {\n      \"a\": 3.10822759,\n      \"b\": -0.02380902\n    },\n    \"4\": {\n      \"a\": 1.62816371,\n      \"b\": -0.03102932\n    },\n    \"5\": {\n      \"a\": 3.50046709,\n      \"b\": -0.02257996\n    }\n  },\n  \"160M\": {\n    \"1\": {\n      \"a\": 2.42222453,\n      \"b\": -0.03132683\n    },\n    \"2\": {\n      \"a\": 3.31767555,\n      \"b\": -0.00747288\n    },\n    \"3\": {\n      \"a\": 2.81638464,\n      \"b\": -0.02268263\n    },\n    \"4\": {\n      \"a\": 1.39405071,\n      \"b\": -0.02766535\n    },\n    \"5\": {\n      \"a\": 3.1508949,\n      \"b\": -0.02322456\n    }\n  },\n  \"305M\": {\n    \"1\": {\n      \"a\": 2.275788,\n      \"b\": -0.03052741\n    },\n    \"2\": {\n      \"a\": 3.15991008,\n      \"b\": -0.00707973\n    },\n    \"3\": {\n      \"a\": 2.66991027,\n      \"b\": -0.02347551\n    },\n    \"4\": {\n      \"a\": 1.28807431,\n      \"b\": -0.02649081\n    },\n    \"5\": {\n      \"a\": 2.98356485,\n      \"b\": -0.02351572\n    }\n  },\n  \"410M\": {\n    \"1\": {\n      \"a\": 2.21485658,\n      \"b\": -0.03025062\n    },\n    \"2\": {\n      \"a\": 3.08823094,\n      \"b\": -0.00686847\n    },\n    \"3\": {\n      \"a\": 2.60339221,\n      \"b\": -0.02391696\n    },\n    \"4\": {\n      \"a\": 1.24921386,\n      \"b\": -0.02577835\n    },\n    \"5\": {\n      \"a\": 2.90760213,\n      \"b\": -0.02431623\n    }\n  }\n}\n\n# Fallback median parameters across groups (for unseen groups)\nDEFAULT_PARAMS: dict[int, dict[str, float]] = {\n  \"1\": {\n    \"a\": 2.34900627,\n    \"b\": -0.03092712\n  },\n  \"2\": {\n    \"a\": 3.23879282,\n    \"b\": -0.0072763\n  },\n  \"3\": {\n    \"a\": 2.74314745,\n    \"b\": -0.02364227\n  },\n  \"4\": {\n    \"a\": 1.34106251,\n    \"b\": -0.02707808\n  },\n  \"5\": {\n    \"a\": 3.06722987,\n    \"b\": -0.02337014\n  }\n}\n\ndef _normalize_coefs(coefs_any) -> dict[int, dict[str, float]]:\n    try:\n        return {int(k): v for k, v in coefs_any.items()}\n    except Exception:\n        return coefs_any\n\ndef _predict_one(proportions: dict[str, float], coefs: dict[int, dict[str, float]]) -> dict[str, float]:\n    out: dict[str, float] = {}\n    default_norm = _normalize_coefs(DEFAULT_PARAMS)\n    for d in range(1, 6):\n        p = float(proportions.get(f\"proportion_domain_{d}\", 0.0))\n        p_eff = p if p > EPS else EPS\n        dc = coefs.get(d)\n        if dc is None:\n            dc = default_norm.get(d, {})\n        if MODEL_FAMILY == \"power\":\n            c0 = float(dc.get(\"c0\", 0.0)); c1 = float(dc.get(\"c1\", 0.0)); a = float(dc.get(\"alpha\", 0.0))\n            y = c0 + c1 * (p_eff ** (-a))\n        else:\n            A = float(dc.get(\"a\", 0.0)); B = float(dc.get(\"b\", 0.0))\n            y = A + B * math.log(p_eff)\n        out[f\"loss_domain_{d}\"] = float(y)\n    return out\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    raw = PARAMS.get(group, DEFAULT_PARAMS)\n    coefs = _normalize_coefs(raw)\n    return [_predict_one(x, coefs) for x in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "terminus-2", "model_name": "gpt-5", "reward_r2": 0.873244, "solution": "from __future__ import annotations\nimport math\n\n# Single functional form across all groups:\n# L_i = A_{g,i} + B_{g,i} * log(p_i) with clipping p_i -> max(p_i, 1e-9)\nCOEFS = {\n  \"160M\": {\n    \"0\": [\n      2.4222245349880027,\n      -0.03132683285487464\n    ],\n    \"1\": [\n      3.317675549031874,\n      -0.007472875400866049\n    ],\n    \"2\": [\n      2.816384638275675,\n      -0.02268262544964051\n    ],\n    \"3\": [\n      1.3940507054184053,\n      -0.027665347428548207\n    ],\n    \"4\": [\n      3.150894901274346,\n      -0.023224558453472387\n    ]\n  },\n  \"305M\": {\n    \"0\": [\n      2.2757879996895203,\n      -0.03052740882148673\n    ],\n    \"1\": [\n      3.1599100833879104,\n      -0.007079726627925801\n    ],\n    \"2\": [\n      2.6699102690836165,\n      -0.02347551348122601\n    ],\n    \"3\": [\n      1.2880743105690569,\n      -0.02649081104120577\n    ],\n    \"4\": [\n      2.9835648450347945,\n      -0.023515715591365914\n    ]\n  },\n  \"410M\": {\n    \"0\": [\n      2.214856577818689,\n      -0.030250619653697765\n    ],\n    \"1\": [\n      3.0882309384163564,\n      -0.006868468175494051\n    ],\n    \"2\": [\n      2.6033922128748963,\n      -0.023916963654028817\n    ],\n    \"3\": [\n      1.2492138560279822,\n      -0.025778353486428242\n    ],\n    \"4\": [\n      2.907602130692238,\n      -0.024316234178981166\n    ]\n  },\n  \"70M\": {\n    \"0\": [\n      2.733906922836672,\n      -0.033479990654170164\n    ],\n    \"1\": [\n      3.6507355235720613,\n      -0.008120459606531918\n    ],\n    \"2\": [\n      3.108227590265197,\n      -0.02380902449916114\n    ],\n    \"3\": [\n      1.6281637129068052,\n      -0.03102931954295701\n    ],\n    \"4\": [\n      3.5004670876544726,\n      -0.022579959393990854\n    ]\n  }\n}\n\ndef _predict_log(p: float, A: float, B: float) -> float:\n    if p <= 0.0:\n        p = 1e-9\n    return A + B * math.log(p)\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\"\"\"\n    g = group if group in COEFS else (next(iter(COEFS.keys())) if COEFS else group)\n    out_list = []\n    for row in input_data:\n        out = {}\n        for i in range(5):\n            pc = f\"proportion_domain_{i+1}\"\n            lc = f\"loss_domain_{i+1}\"\n            p = float(row.get(pc, 0.0))\n            A,B = COEFS[g].get(str(i), [0.0, 0.0])\n            out[lc] = float(_predict_log(p, A, B))\n        out_list.append(out)\n    return out_list"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "gpt-5", "reward_r2": 0.870736, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\n\n_P_LEN = 35  # p(5), q(5), a(5), g(5), d(5), eta(5), b(5)\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    if X.shape[1] != 5:\n        raise ValueError(\"Expected 5 domain proportion features.\")\n    p = np.asarray(params, dtype=np.float64)\n    if p.ndim == 2:\n        p = p[0]\n    if p.size != _P_LEN:\n        raise ValueError(f\"Expected {_P_LEN} parameters, got {p.size}.\")\n    off = 0\n    pv = p[off:off+5]; off += 5\n    qv = p[off:off+5]; off += 5\n    av = p[off:off+5]; off += 5\n    gv = p[off:off+5]; off += 5\n    dv = p[off:off+5]; off += 5\n    et = p[off:off+5]; off += 5\n    b  = p[off:off+5]\n    s = X @ pv\n    z = (X @ av) ** 2\n    s2 = np.sum(X * X, axis=1)\n    return b[None, :] + X * dv[None, :] + np.outer(s, qv) + np.outer(z, gv) + s2[:, None] * et[None, :]\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    Y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    N, F = X.shape\n    if F != 5 or Y.shape != (N, 5):\n        raise ValueError(\"Shapes must be (N,5) for both data_points and loss_values.\")\n    s2 = np.sum(X * X, axis=1)\n    ones = np.ones(N, dtype=np.float64)\n\n    # Output weighting for balanced fit\n    var = np.var(Y, axis=0) + 1e-8\n    w_out = (1.0 / var)\n    w_out /= np.mean(w_out)\n\n    # PCA-like init on centered simplex\n    Xc = X - 0.2\n    U, _, Vt = np.linalg.svd(Xc, full_matrices=False)\n    p = Vt[0].astype(np.float64) if Vt.shape[0] > 0 else np.array([1,-1,0,0,0.], dtype=np.float64)\n    a = Vt[1].astype(np.float64) if Vt.shape[0] > 1 else np.array([0,1,-1,0,0.], dtype=np.float64)\n    for v in (p, a):\n        v -= v.mean()\n        nrm = np.linalg.norm(v); v /= (nrm if nrm > 1e-12 else 1.0)\n    a = a - p * float(np.dot(a, p))\n    a -= a.mean()\n    a /= max(np.linalg.norm(a), 1e-12)\n\n    alpha = 1e-3\n    I5 = np.eye(5)\n\n    def fit_linear_blocks(pv, av):\n        s = X @ pv\n        z = (X @ av) ** 2\n        B = np.empty(5); D = np.empty(5); Q = np.empty(5); G = np.empty(5); E = np.empty(5)\n        for o in range(5):\n            Phi = np.column_stack((ones, X[:, o], s, z, s2))\n            XtX = Phi.T @ Phi\n            # stabilized ridge using diagonal of XtX\n            theta = np.linalg.solve(XtX + alpha * (np.diag(np.diag(XtX)) + I5), Phi.T @ Y[:, o])\n            B[o], D[o], Q[o], G[o], E[o] = theta\n        return B, D, Q, G, E\n\n    def wmse(pv, av, B, D, Q, G, E):\n        s = X @ pv\n        z = (X @ av) ** 2\n        pred = B[None, :] + X * D[None, :] + np.outer(s, Q) + np.outer(z, G) + s2[:, None] * E[None, :]\n        return np.mean(((pred - Y) ** 2) * w_out[None, :])\n\n    B, D, Q, G, E = fit_linear_blocks(p, a)\n    best = wmse(p, a, B, D, Q, G, E)\n\n    for _ in range(80):\n        # Closed-form update for p with output weights\n        z = (X @ a) ** 2\n        R = Y - (B[None, :] + X * D[None, :] + np.outer(z, G) + s2[:, None] * E[None, :])\n        Qw = Q * w_out\n        qtq = float(np.dot(Qw, Qw)) + 1e-12\n        C = qtq * (X.T @ X) + 1e-4 * I5\n        r = (X * (R @ Qw)[:, None]).sum(axis=0)\n        p_new = np.linalg.solve(C, r)\n        p_new -= p_new.mean()\n        p_new /= max(np.linalg.norm(p_new), 1e-12)\n\n        # Gradient update for a with projection and backtracking\n        s = X @ p_new\n        pred_lin = B[None, :] + X * D[None, :] + np.outer(s, Q) + s2[:, None] * E[None, :]\n        ax = X @ a\n        z = ax ** 2\n        Rfull = pred_lin + np.outer(z, G) - Y\n        dz = (2.0 / (N * 5)) * ((Rfull * w_out[None, :]) @ G)\n        grad_a = ((dz * 2.0 * ax)[:, None] * X).sum(axis=0)\n        # project gradient\n        grad_a = grad_a - p_new * float(np.dot(grad_a, p_new))\n        grad_a -= grad_a.mean()\n\n        t = 0.8\n        a_new = a.copy()\n        base = best\n        for _ls in range(12):\n            cand = a - t * grad_a\n            cand -= cand.mean()\n            cand = cand - p_new * float(np.dot(cand, p_new))\n            cand /= max(np.linalg.norm(cand), 1e-12)\n            Bc, Dc, Qc, Gc, Ec = fit_linear_blocks(p_new, cand)\n            cur = wmse(p_new, cand, Bc, Dc, Qc, Gc, Ec)\n            if cur <= base - 1e-10:\n                a_new = cand\n                B, D, Q, G, E = Bc, Dc, Qc, Gc, Ec\n                best = cur\n                break\n            t *= 0.5\n        else:\n            # If no improvement, still refit with updated p\n            B, D, Q, G, E = fit_linear_blocks(p_new, a)\n            best = wmse(p_new, a, B, D, Q, G, E)\n            a_new = a\n\n        a = a_new\n        p = p_new\n        if t < 1e-4 or best < 1e-12:\n            break\n\n    return np.concatenate([p, Q, a, G, D, E, B])\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5", "reward_r2": 0.865747, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The discovered law relates each domain's validation loss to its training mixture\n    proportion via an offset power-decay form shared across groups:\n        loss_domain_i = L_{g,i} + C_{g,i} * (proportion_domain_i + eps) ** (-a_{g,i})\n    where g is the experimental group and i in {1..5} is the domain index.\n\n    Coefficients (L, C, a) are group- and domain-specific, fitted from the provided dataset.\n    For unknown groups, coefficients are obtained by size-aware interpolation (based on\n    the numeric model size parsed from the group string), falling back to a cross-group\n    average if size cannot be parsed.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: Group name (e.g., '70M', '160M', '305M', '410M'). The functional form is\n               shared across groups; parameters differ per group.\n\n    Returns:\n        List of dicts, each containing keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n    # Fitted parameters per group and domain (pow-decay): y = L + C * (p + eps)^(-a)\n    # Derived from analysis over /app/data.\n    PARAMS = {\n        '70M': {\n            1: {'L': 2.3407861901608458, 'C': 0.3990228222403043, 'a': 0.07229469195307019},\n            2: {'L': 3.5878039240958115, 'C': 0.06619054334605488, 'a': 0.0904125332037067},\n            3: {'L': 3.0397589847616344, 'C': 0.09422712368752768, 'a': 0.12920486135303147},\n            4: {'L': 1.367615184081782,  'C': 0.2717420228432777,  'a': 0.08687316390039175},\n            5: {'L': 3.3039469037556852, 'C': 0.20054244651774714, 'a': 0.08493556412221587},\n        },\n        '160M': {\n            1: {'L': 2.110715267629515,  'C': 0.3198800255123924,  'a': 0.07933154901483508},\n            2: {'L': 3.1803738201935277, 'C': 0.13638680248291965, 'a': 0.05512158489827569},\n            3: {'L': 2.6798939752704323, 'C': 0.15139248008383838, 'a': 0.10036697294631616},\n            4: {'L': 1.1773152518677579, 'C': 0.2282095425768926,  'a': 0.08971340404690013},\n            5: {'L': 2.946602702302544,  'C': 0.20862088187761854, 'a': 0.08441408092133072},\n        },\n        '305M': {\n            1: {'L': 1.9852814769362541, 'C': 0.2993480669803183,  'a': 0.08124887473179328},\n            2: {'L': 3.023213559842957,  'C': 0.13550118870333341, 'a': 0.053401380352092805},\n            3: {'L': 2.5983613840895425, 'C': 0.09639191662897745, 'a': 0.12705347977640008},\n            4: {'L': 1.0610600002430857, 'C': 0.2366422762032485,  'a': 0.08578219015376275},\n            5: {'L': 2.7799528735574452, 'C': 0.2083464473165256,  'a': 0.08504953874822715},\n        },\n        '410M': {\n            1: {'L': 1.9365715595185018, 'C': 0.2874401995772295,  'a': 0.08276413468982288},\n            2: {'L': 2.9481641120717006, 'C': 0.13858188362142976, 'a': 0.05155133454157204},\n            3: {'L': 2.5316764731368075, 'C': 0.0972583597842447,  'a': 0.1275873334591975},\n            4: {'L': 1.0267251504383115, 'C': 0.23166028198558963, 'a': 0.08550863921076982},\n            5: {'L': 2.700651481039514,  'C': 0.21075075625956133, 'a': 0.0863440646491576},\n        },\n    }\n\n    # Cross-group per-domain averages (fallback when group is unknown and size cannot be parsed)\n    AVG = {\n        1: {'L': 2.093338623561279, 'C': 0.32642277857756113, 'a': 0.07890981259738035},\n        2: {'L': 3.1848888540509996, 'C': 0.11916510453843443, 'a': 0.0626217082489118},\n        3: {'L': 2.712422704314604, 'C': 0.10981747004614704, 'a': 0.1210531618837363},\n        4: {'L': 1.1581788966577342, 'C': 0.24206353090225213, 'a': 0.08696934932795611},\n        5: {'L': 2.932788490163797, 'C': 0.20706513299286314, 'a': 0.08518581211023284},\n    }\n\n    # Parse numeric model size (in millions) from group string like '70M', '1.3B', '410M'\n    def _parse_size_millions(g: str):\n        if not isinstance(g, str):\n            return None\n        s = g.strip().upper()\n        num = ''\n        unit = ''\n        for ch in s:\n            if (ch.isdigit() or ch == '.' or ch == '+') and unit == '':\n                num += ch\n            elif ch.isalpha():\n                unit += ch\n            # stop at first non-alnum/decimal\n        try:\n            val = float(num) if num else None\n        except Exception:\n            val = None\n        if val is None:\n            return None\n        if 'B' in unit:\n            return val * 1000.0\n        if 'M' in unit or unit == '':\n            return val\n        return None\n\n    # Retrieve parameters for a group, with size-aware interpolation when needed\n    def _get_params_for_group(g: str):\n        if g in PARAMS:\n            return PARAMS[g]\n        # Try interpolation based on parsed size\n        known = sorted(((k, v) for k, v in PARAMS.items()), key=lambda kv: _parse_size_millions(kv[0]) or float('inf'))\n        sizes = [\n            (_parse_size_millions(k) if _parse_size_millions(k) is not None else float('inf'))\n            for k, _ in known\n        ]\n        size = _parse_size_millions(g)\n        if size is None or any(x == float('inf') for x in sizes):\n            return AVG\n        # Clamp to range if outside\n        if size <= sizes[0]:\n            return known[0][1]\n        if size >= sizes[-1]:\n            return known[-1][1]\n        # Find enclosing bracket for interpolation\n        lo_idx = 0\n        for i in range(len(sizes) - 1):\n            if sizes[i] <= size <= sizes[i + 1]:\n                lo_idx = i\n                break\n        hi_idx = lo_idx + 1\n        s0, s1 = sizes[lo_idx], sizes[hi_idx]\n        t = (size - s0) / (s1 - s0) if s1 > s0 else 0.0\n        # Linear interpolation of each parameter per domain\n        interp = {}\n        for di in range(1, 6):\n            L0 = known[lo_idx][1][di]['L']; L1 = known[hi_idx][1][di]['L']\n            C0 = known[lo_idx][1][di]['C']; C1 = known[hi_idx][1][di]['C']\n            a0 = known[lo_idx][1][di]['a']; a1 = known[hi_idx][1][di]['a']\n            interp[di] = {\n                'L': L0 + (L1 - L0) * t,\n                'C': C0 + (C1 - C0) * t,\n                'a': a0 + (a1 - a0) * t,\n            }\n        return interp\n\n    coeffs = _get_params_for_group(group)\n\n    # Small epsilon to regularize p=0 behavior; chosen to be tiny relative to [0,1]\n    eps = 1e-6\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out_row: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f'proportion_domain_{i}', 0.0))\n            L = float(coeffs[i]['L'])\n            C = float(coeffs[i]['C'])\n            a = float(coeffs[i]['a'])\n            # Ensure non-negative proportion and stable power\n            if p < 0.0:\n                p = 0.0\n            y = L + C * (p + eps) ** (-a)\n            out_row[f'loss_domain_{i}'] = float(y)\n        outputs.append(out_row)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "mini-swe-agent", "model_name": "gpt-5", "reward_r2": 0.862092, "solution": "# Auto-generated scaling law implementation\n# Functional form (shared across groups):\n#   For each k in 1..5:\n#     loss_domain_k = intercept_k(group) + sum_j weights_kj(group) * log(proportion_domain_j + EPS)\n# Where EPS = 1e-09 for numerical stability when proportions are very small.\n\nfrom math import log\nfrom typing import List, Dict\n\nEPS = 1e-09\n\nCOEFFS = {\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.4300949275826182,\n      \"weights\": [\n        -0.031068969226910582,\n        -0.001251970877444727,\n        0.0030660145955268667,\n        -0.0031923386958230627,\n        0.005666961729262434\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.337559043041566,\n      \"weights\": [\n        0.0016673078859499708,\n        -0.0074170755968738225,\n        3.951373748949835e-05,\n        0.0021209041901585806,\n        -0.003650890207385123\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8401818458556174,\n      \"weights\": [\n        0.002460792554312405,\n        0.0007625714631167231,\n        -0.023536811266176717,\n        0.002221882872273453,\n        -0.0031186174972866886\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.416827910017676,\n      \"weights\": [\n        -8.308604862155428e-06,\n        0.0012190503004574596,\n        0.00032399676863148686,\n        -0.027078844363725037,\n        0.004010845050737586\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.224543906049842,\n      \"weights\": [\n        0.003905427350717464,\n        0.0034498008479645842,\n        -0.0011438176738371758,\n        0.0038638961855052053,\n        -0.019328119858345046\n      ]\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.280243138043643,\n      \"weights\": [\n        -0.030323002210805715,\n        -0.001211700929338305,\n        0.0028947830670476163,\n        -0.003324913376723421,\n        0.005113800264772562\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.1750876801913237,\n      \"weights\": [\n        0.00150916925021668,\n        -0.007107484565596503,\n        2.2128083411678632e-05,\n        0.0017728620875118904,\n        -0.003972454524642374\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.6913398939739,\n      \"weights\": [\n        0.0022708344859972973,\n        0.0008607850265603164,\n        -0.024331792555795703,\n        0.001910605212900081,\n        -0.0029724526398083326\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.3069649592755754,\n      \"weights\": [\n        -0.0002767577345750335,\n        0.0010150273630255805,\n        0.0003398702171026176,\n        -0.02596334582661601,\n        0.003801676322267153\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.0528011051041433,\n      \"weights\": [\n        0.0037594659546520076,\n        0.0034195441188783746,\n        -0.0011368975113422175,\n        0.0034318689601916725,\n        -0.01984609557908483\n      ]\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.222420776701192,\n      \"weights\": [\n        -0.0299923842986247,\n        -0.0007219779189471512,\n        0.002445903709102556,\n        -0.0027711721407406966,\n        0.004740901934744721\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.104708939329123,\n      \"weights\": [\n        0.0017336451412636902,\n        -0.006704834426433428,\n        -0.00036440290744305196,\n        0.0021542694049179936,\n        -0.004373253420089363\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.627413222019661,\n      \"weights\": [\n        0.0023403919816331487,\n        0.0010679528945524257,\n        -0.02491463877855555,\n        0.002264951898769409,\n        -0.003300544791425321\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.268317554857684,\n      \"weights\": [\n        -0.0005477525270926719,\n        0.001495543684942191,\n        0.00019725923029861728,\n        -0.025223453889896486,\n        0.0034567257553997934\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 2.9787362389662326,\n      \"weights\": [\n        0.0038069030391143168,\n        0.003769753651834665,\n        -0.0015455123055104936,\n        0.003789696705093632,\n        -0.020506176044717026\n      ]\n    }\n  },\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.738245966324205,\n      \"weights\": [\n        -0.033247355909198334,\n        -0.0017297973305077973,\n        0.0035157990300376508,\n        -0.003918220474936535,\n        0.006317934127290131\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.6680087213236505,\n      \"weights\": [\n        0.0015828220455031585,\n        -0.008139958599724776,\n        0.00014447856565839406,\n        0.0016881696288695383,\n        -0.0033595783088106986\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 3.1304945178100705,\n      \"weights\": [\n        0.0026532896643926083,\n        0.0007600084197338211,\n        -0.024637174874146756,\n        0.001757289151853728,\n        -0.0030248438725573442\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.6498551367668615,\n      \"weights\": [\n        -0.0002889005529800089,\n        0.001194603021950033,\n        0.00037667659236133267,\n        -0.030432861224971082,\n        0.004231626123725879\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.5718538704762426,\n      \"weights\": [\n        0.004053652875855752,\n        0.0033280287851346077,\n        -0.0011542215280026637,\n        0.0035637709733213644,\n        -0.01878431722025504\n      ]\n    }\n  },\n  \"DEFAULT\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.417751202162915,\n      \"weights\": [\n        -0.03115792791138483,\n        -0.001228861764059495,\n        0.0029806251004286725,\n        -0.0033016611720559285,\n        0.005459899514017462\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.3213410959714156,\n      \"weights\": [\n        0.0016232360807333748,\n        -0.007342338297157132,\n        -3.957063022087023e-05,\n        0.0019340513278645007,\n        -0.00383904411523189\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8223573699148123,\n      \"weights\": [\n        0.002431327171583865,\n        0.0008628294509908216,\n        -0.024355104368668678,\n        0.0020386822839491676,\n        -0.003104114700269422\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.4104913902294491,\n      \"weights\": [\n        -0.00028042985487746743,\n        0.001231056092593816,\n        0.0003094507020985136,\n        -0.027174626326302152,\n        0.003875218313032603\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.2069837801491152,\n      \"weights\": [\n        0.003881362305084885,\n        0.0034917818509530582,\n        -0.0012451122546731377,\n        0.0036623082060279685,\n        -0.019616177175600488\n      ]\n    }\n  }\n}\n\nPROP_ORDER = [\"proportion_domain_1\", \"proportion_domain_2\", \"proportion_domain_3\", \"proportion_domain_4\", \"proportion_domain_5\"]\nLOSS_ORDER = [\"loss_domain_1\", \"loss_domain_2\", \"loss_domain_3\", \"loss_domain_4\", \"loss_domain_5\"]\n\ndef _select_group(group: str) -> Dict[str, Dict[str, float]]:\n    # Use exact match if available; otherwise fallback to DEFAULT\n    if group in COEFFS:\n        return COEFFS[group]\n    # Try case-insensitive match\n    for g in COEFFS:\n        if g.lower() == (group or \"\").lower():\n            return COEFFS[g]\n    return COEFFS[\"DEFAULT\"]\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups, while\n               coefficients differ by group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) for keys in LOSS_ORDER.\n    \"\"\"\n    coeffs = _select_group(group)\n    outputs = []\n    for row in input_data:\n        # Build feature vector in canonical order\n        logs = [log(max(float(row.get(name, 0.0)), 0.0) + EPS) for name in PROP_ORDER]\n        pred = {}\n        for lkey in LOSS_ORDER:\n            c = coeffs[lkey]\n            y = c[\"intercept\"] + sum(w * x for w, x in zip(c[\"weights\"], logs))\n            pred[lkey] = float(y)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "aider", "model_name": "gpt-5", "reward_r2": 0.85774, "solution": "from __future__ import annotations\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups, but the\n               coefficients are learned per-group from /app/data (cached).\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    import math\n\n    # Initialize caches as function attributes (keeps module surface minimal: single function)\n    if not hasattr(law, \"_coeff_cache\"):\n        law._coeff_cache = {}  # type: ignore[attr-defined]\n    if not hasattr(law, \"_domains_by_group\"):\n        law._domains_by_group = {}  # type: ignore[attr-defined]\n    if not hasattr(law, \"_fit_attempted\"):\n        law._fit_attempted = set()  # type: ignore[attr-defined]\n\n    def _safe_log(p: float, eps: float = 1e-12) -> float:\n        # Avoid log(0) while keeping behavior stable for tiny proportions.\n        return math.log(p if p > eps else eps)\n\n    def _discover_domain_indices_from_columns(columns: list[str]) -> list[int]:\n        idxs = []\n        prefix = \"proportion_domain_\"\n        for c in columns:\n            if c.startswith(prefix):\n                try:\n                    i = int(c[len(prefix):])\n                    idxs.append(i)\n                except ValueError:\n                    continue\n        return sorted(set(idxs))\n\n    def _fit_group_coeffs(_group: str) -> None:\n        # Skip refit if already attempted (prevents repeated I/O on missing env)\n        if _group in law._fit_attempted:  # type: ignore[attr-defined]\n            return\n        law._fit_attempted.add(_group)  # type: ignore[attr-defined]\n\n        # Defaults if dataset is unavailable: reasonable monotone log response.\n        default_coeffs = {}\n        # If we can infer domain ids from incoming data later, we'll update per call.\n        try:\n            # Lazy import to avoid hard dependency if environment lacks datasets\n            from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore\n            import numpy as np  # type: ignore\n\n            ds_any = load_from_disk(\"/app/data\")\n            if isinstance(ds_any, DatasetDict):\n                # Prefer 'train' split when available; otherwise choose the first split.\n                if \"train\" in ds_any:\n                    ds = ds_any[\"train\"]\n                else:\n                    first_split = list(ds_any.keys())[0]\n                    ds = ds_any[first_split]\n            else:\n                ds = ds_any  # Already a Dataset\n\n            colnames = list(getattr(ds, \"column_names\", []))\n            domain_idxs = _discover_domain_indices_from_columns(colnames)\n\n            # Filter by group if the column exists, else fit globally.\n            use_ds = ds\n            if \"group\" in colnames and _group not in (\"\", None):\n                try:\n                    use_ds = ds.filter(lambda x: x.get(\"group\") == _group)\n                    # If filter produced empty set, fall back to global\n                    if len(use_ds) == 0:\n                        use_ds = ds\n                except Exception:\n                    use_ds = ds\n\n            coeffs = {}\n            # Ordinary Least Squares per domain: y = a + b * ln(p)\n            for i in domain_idxs:\n                p_col = f\"proportion_domain_{i}\"\n                y_col = f\"loss_domain_{i}\"\n                if p_col not in colnames or y_col not in colnames:\n                    continue\n                try:\n                    p_vals = use_ds[p_col]\n                    y_vals = use_ds[y_col]\n                except Exception:\n                    continue\n\n                # Convert to numpy arrays and robustly filter invalid/edge entries\n                try:\n                    x = np.array([_safe_log(float(p)) for p in p_vals], dtype=float)\n                    y = np.array([float(v) for v in y_vals], dtype=float)\n                    mask = np.isfinite(x) & np.isfinite(y)\n                    x = x[mask]\n                    y = y[mask]\n                except Exception:\n                    x = []\n                    y = []\n\n                # If insufficient variance or data, fall back to defaults\n                if len(x) < 2 or float(np.var(x)) <= 1e-18:\n                    # Reasonable fallback: a ~ mean(y), b ~ 0 if no variance; else mild slope\n                    if len(y) >= 1 and all(np.isfinite(y)):\n                        a = float(np.mean(y))\n                    else:\n                        a = 2.5\n                    b = -0.2  # gentle improvement with more data\n                else:\n                    # Closed-form OLS for simple linear regression\n                    x_mean = float(np.mean(x))\n                    y_mean = float(np.mean(y))\n                    cov = float(np.mean((x - x_mean) * (y - y_mean)))\n                    var = float(np.mean((x - x_mean) ** 2))\n                    if var <= 1e-18:\n                        b = 0.0\n                        a = y_mean\n                    else:\n                        b = cov / var\n                        a = y_mean - b * x_mean\n\n                coeffs[i] = (a, b)\n\n            law._coeff_cache[_group] = coeffs  # type: ignore[attr-defined]\n            law._domains_by_group[_group] = sorted(coeffs.keys())  # type: ignore[attr-defined]\n\n        except Exception:\n            # Dataset not available or fitting failed; set a permissive default.\n            # Assume 5 domains if unseen; coefficients encourage modest loss decrease with higher p.\n            for i in range(1, 6):\n                default_coeffs[i] = (2.5, -0.2)\n            law._coeff_cache[_group] = default_coeffs  # type: ignore[attr-defined]\n            law._domains_by_group[_group] = sorted(default_coeffs.keys())  # type: ignore[attr-defined]\n\n    # Ensure coefficients for this group are fitted/cached\n    _fit_group_coeffs(group)\n\n    # Inference: for each input point, predict loss_domain_i for available proportion_domain_i\n    coeffs_for_group = law._coeff_cache.get(group, {})  # type: ignore[attr-defined]\n    results: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        # Use domains inferred from the row's keys; if empty, fall back to known domains for the group\n        keys = [k for k in row.keys() if k.startswith(\"proportion_domain_\")]\n        if not keys:\n            # Predict for all known domains for this group with p assumed ~0 (minimal exposure)\n            for i in law._domains_by_group.get(group, []):  # type: ignore[attr-defined]\n                a, b = coeffs_for_group.get(i, (2.5, -0.2))\n                p = 0.0\n                out[f\"loss_domain_{i}\"] = a + b * _safe_log(p)\n        else:\n            for k in keys:\n                try:\n                    i = int(k.split(\"proportion_domain_\")[1])\n                except Exception:\n                    continue\n                a, b = coeffs_for_group.get(i, (2.5, -0.2))\n                p_val = float(row.get(k, 0.0))\n                out[f\"loss_domain_{i}\"] = a + b * _safe_log(p_val)\n        results.append(out)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "terminus-2", "model_name": "gpt-5", "reward_r2": 0.852129, "solution": "from typing import List, Dict\n\n_PARAMS = {\n  \"160M\": {\n    \"1\": {\n      \"a\": 0.4777718023660796,\n      \"b\": 1.9651028132012958,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.853979648026824,\n      \"b\": 0.46913461080072405,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.4416433577788708,\n      \"b\": 1.3992301424068143,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.31200145102844623,\n      \"b\": 1.7281252544418098,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.6988970818463138,\n      \"b\": 1.458160686737703,\n      \"alpha\": 0.01\n    }\n  },\n  \"305M\": {\n    \"1\": {\n      \"a\": 0.380355700173621,\n      \"b\": 1.9155046248750955,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.720727222497648,\n      \"b\": 0.4443470722501475,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.2468028929418933,\n      \"b\": 1.4484164085003954,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.34563740615899546,\n      \"b\": 1.6548398159870399,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.5118433832803935,\n      \"b\": 1.4779286413154575,\n      \"alpha\": 0.01\n    }\n  },\n  \"410M\": {\n    \"1\": {\n      \"a\": 0.3358801860263219,\n      \"b\": 1.8988049175912776,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.6618934312385854,\n      \"b\": 0.43132199370582547,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.153496029332803,\n      \"b\": 1.475677952459072,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.3403973481604199,\n      \"b\": 1.6101852758013921,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.3848194753051277,\n      \"b\": 1.5291803128611205,\n      \"alpha\": 0.01\n    }\n  },\n  \"70M\": {\n    \"1\": {\n      \"a\": 0.6576096958255063,\n      \"b\": 2.098519217677623,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 3.14714983348316,\n      \"b\": 0.5095246197489386,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.6653425516285103,\n      \"b\": 1.4686043699822906,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.2852957436802397,\n      \"b\": 1.9382200618934766,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 2.089249143779692,\n      \"b\": 1.4172201529137438,\n      \"alpha\": 0.01\n    }\n  }\n}\n\ndef _get_params(group: str):\n    return _PARAMS.get(group, _PARAMS.get(str(group), None)) or next(iter(_PARAMS.values()))\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups,\n               but parameters can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = _get_params(group)\n    outputs = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            p = float(row.get('proportion_domain_%d' % i, 0.0))\n            if p <= 0:\n                p = 1e-12\n            dom = params[str(i)]\n            a = dom['a']; b = dom['b']; alpha = dom['alpha']\n            out['loss_domain_%d' % i] = float(a + b * (p ** (-alpha)))\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "o4-mini", "reward_r2": 0.845752, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts domain losses based on mixture proportions using a linear scaling law.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_i'.\n        group: Experimental group identifier ('70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with predicted 'loss_domain_i' values.\n    \"\"\"\n    # Coefficients from linear regression: loss = a + b * proportion\n    coefs = {\n        '70M': {\n            1: (3.1806393062811864, -1.2285149798567838),\n            2: (3.801253923348644, -0.8613783672356026),\n            3: (3.5566666364458976, -5.9220601237842345),\n            4: (2.092666552495471, -2.465721152611366),\n            5: (3.7712578339232965, -0.4114998066410033),\n        },\n        '160M': {\n            1: (2.835641724877016, -1.125053454723479),\n            2: (3.4544639388150142, -0.7759431938007308),\n            3: (3.243292438159674, -5.633806849579989),\n            4: (1.8086663359915747, -2.2029709658709042),\n            5: (3.430618715766743, -0.4253675130276537),\n        },\n        '305M': {\n            1: (2.6765745376811547, -1.0852461829640883),\n            2: (3.290256899590436, -0.7424407706785378),\n            3: (3.1100613611088073, -5.787743320347023),\n            4: (1.684474243911805, -2.103498009394809),\n            5: (3.2619262695047944, -0.4221154025882421),\n        },\n        '410M': {\n            1: (2.609996558045746, -1.0646721511212967),\n            2: (3.2130253217275206, -0.7041613598604771),\n            3: (3.0516738117194397, -5.89283215213306),\n            4: (1.6354678769114128, -2.0519200859166604),\n            5: (3.194356949340593, -0.4345763744862935),\n        },\n    }\n    if group not in coefs:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = coefs[group]\n    predictions: List[Dict[str, float]] = []\n    for point in input_data:\n        out: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = point.get(f'proportion_domain_{i}', 0.0)\n            a, b = params[i]\n            out[f'loss_domain_{i}'] = a + b * p\n        predictions.append(out)\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.8457519810005265, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Linear coefficients per group per domain: intercept and slope\n    # These were fitted on the provided dataset.\n    coefficients = {\n        '70M': {\n            'intercept_domain_1': 3.180639,\n            'slope_domain_1': -1.228515,\n            'intercept_domain_2': 3.801254,\n            'slope_domain_2': -0.861378,\n            'intercept_domain_3': 3.556667,\n            'slope_domain_3': -5.922060,\n            'intercept_domain_4': 2.092667,\n            'slope_domain_4': -2.465721,\n            'intercept_domain_5': 3.771258,\n            'slope_domain_5': -0.411500,\n        },\n        '160M': {\n            'intercept_domain_1': 2.835642,\n            'slope_domain_1': -1.125053,\n            'intercept_domain_2': 3.454464,\n            'slope_domain_2': -0.775943,\n            'intercept_domain_3': 3.243292,\n            'slope_domain_3': -5.633807,\n            'intercept_domain_4': 1.808666,\n            'slope_domain_4': -2.202971,\n            'intercept_domain_5': 3.430619,\n            'slope_domain_5': -0.425368,\n        },\n        '305M': {\n            'intercept_domain_1': 2.676575,\n            'slope_domain_1': -1.085246,\n            'intercept_domain_2': 3.290257,\n            'slope_domain_2': -0.742441,\n            'intercept_domain_3': 3.110061,\n            'slope_domain_3': -5.787743,\n            'intercept_domain_4': 1.684474,\n            'slope_domain_4': -2.103498,\n            'intercept_domain_5': 3.261926,\n            'slope_domain_5': -0.422115,\n        },\n        '410M': {\n            'intercept_domain_1': 2.609997,\n            'slope_domain_1': -1.064672,\n            'intercept_domain_2': 3.213025,\n            'slope_domain_2': -0.704161,\n            'intercept_domain_3': 3.051674,\n            'slope_domain_3': -5.892832,\n            'intercept_domain_4': 1.635468,\n            'slope_domain_4': -2.051920,\n            'intercept_domain_5': 3.194357,\n            'slope_domain_5': -0.434576,\n        }\n    }\n\n    # Check if the group is known\n    if group not in coefficients:\n        raise ValueError(f\"Unknown group: {group}\")\n\n    group_coeffs = coefficients[group]\n\n    result = []\n    for point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            prop_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            # If the proportion key is missing, we cannot predict. Assume it's present.\n            prop = point[prop_key]\n            intercept = group_coeffs[f'intercept_domain_{i}']\n            slope = group_coeffs[f'slope_domain_{i}']\n            pred[loss_key] = intercept + slope * prop\n        result.append(pred)\n\n    return result"}
{"task": "domain_mixture_scaling_law", "agent_name": "goose", "model_name": "gpt-5", "reward_r2": 0.842023, "solution": "from __future__ import annotations\n\nfrom math import log\nfrom typing import Dict, List\n\n# Discovered scaling law:\n#   For each output loss_domain_i, predict as a linear function of the\n#   logarithms of all five domain proportions (with a tiny epsilon for stability):\n#       \\hat{L}_{g,i} = bias_{g,i} + sum_j w_{g,i,j} * log(p_j + eps)\n#   The functional form is fixed; parameters (biases and weights) depend on group g.\n#   Coefficients were fit by ordinary least squares on the provided dataset.\n\nEPS = 1e-12\n\n# Per-group, per-output coefficients learned on the provided dataset.\n# Keys: group -> domain_{i} -> {bias, log_p1..log_p5}\nCOEFFS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"160M\": {\n        \"domain_1\": {\n            \"bias\": 2.4422650614387416,\n            \"log_p1\": -0.02277638164592572,\n            \"log_p2\": -0.0010526071930561662,\n            \"log_p3\": 0.0023643145622796475,\n            \"log_p4\": -0.0025583225480046595,\n            \"log_p5\": 0.004378832847069199,\n        },\n        \"domain_2\": {\n            \"bias\": 3.3408696428701075,\n            \"log_p1\": 0.0012198254051165147,\n            \"log_p2\": -0.005388376734279845,\n            \"log_p3\": 3.717373734748179e-05,\n            \"log_p4\": 0.001569209092862663,\n            \"log_p5\": -0.002615879540461004,\n        },\n        \"domain_3\": {\n            \"bias\": 2.8574312884633413,\n            \"log_p1\": 0.0018047356370065415,\n            \"log_p2\": 0.0005929693186934749,\n            \"log_p3\": -0.016974557643524405,\n            \"log_p4\": 0.0016523205850350147,\n            \"log_p5\": -0.0022616680413822805,\n        },\n        \"domain_4\": {\n            \"bias\": 1.4313732131696744,\n            \"log_p1\": -5.1765954939435764e-05,\n            \"log_p2\": 0.00089014156596828,\n            \"log_p3\": 0.00025768087959735246,\n            \"log_p4\": -0.019712789545137503,\n            \"log_p5\": 0.0031631020272182907,\n        },\n        \"domain_5\": {\n            \"bias\": 3.2257864919233756,\n            \"log_p1\": 0.0029557696510542136,\n            \"log_p2\": 0.0026611908853740054,\n            \"log_p3\": -0.0008885188514808986,\n            \"log_p4\": 0.002963471212841412,\n            \"log_p5\": -0.014216518710413713,\n        },\n    },\n    \"305M\": {\n        \"domain_1\": {\n            \"bias\": 2.2924869661791267,\n            \"log_p1\": -0.022229067565166443,\n            \"log_p2\": -0.001013806799899392,\n            \"log_p3\": 0.00223467186554643,\n            \"log_p4\": -0.002645873902860267,\n            \"log_p5\": 0.003982864991197694,\n        },\n        \"domain_2\": {\n            \"bias\": 3.178656522099369,\n            \"log_p1\": 0.001107302828752052,\n            \"log_p2\": -0.005157973767038715,\n            \"log_p3\": 2.3050110124257784e-05,\n            \"log_p4\": 0.001316251689675477,\n            \"log_p5\": -0.0028425410208344046,\n        },\n        \"domain_3\": {\n            \"bias\": 2.709373598108862,\n            \"log_p1\": 0.0016635063654583334,\n            \"log_p2\": 0.0006628818961311013,\n            \"log_p3\": -0.017547370822364454,\n            \"log_p4\": 0.0014206959628518868,\n            \"log_p5\": -0.002139388852229081,\n        },\n        \"domain_4\": {\n            \"bias\": 1.3211315997161879,\n            \"log_p1\": -0.00024768872185789104,\n            \"log_p2\": 0.0007428895039710641,\n            \"log_p3\": 0.0002693184497085781,\n            \"log_p4\": -0.01890218970791502,\n            \"log_p5\": 0.0030105938208213993,\n        },\n        \"domain_5\": {\n            \"bias\": 3.0545845875446473,\n            \"log_p1\": 0.002854476470176697,\n            \"log_p2\": 0.0026446139943498434,\n            \"log_p3\": -0.0008847006391603586,\n            \"log_p4\": 0.002650748867952872,\n            \"log_p5\": -0.014590961474216001,\n        },\n    },\n    \"410M\": {\n        \"domain_1\": {\n            \"bias\": 2.2343717924971283,\n            \"log_p1\": -0.021983874135919367,\n            \"log_p2\": -0.0006526879839640017,\n            \"log_p3\": 0.0019063557364627178,\n            \"log_p4\": -0.0022375323165275815,\n            \"log_p5\": 0.003693691571213775,\n        },\n        \"domain_2\": {\n            \"bias\": 3.1081386488108924,\n            \"log_p1\": 0.0012757792324108383,\n            \"log_p2\": -0.004861060292668727,\n            \"log_p3\": -0.00025971561825861964,\n            \"log_p4\": 0.0015980855298738693,\n            \"log_p5\": -0.003151617877548417,\n        },\n        \"domain_3\": {\n            \"bias\": 2.6457079548318867,\n            \"log_p1\": 0.0017179901900188285,\n            \"log_p2\": 0.0008184656194115004,\n            \"log_p3\": -0.017971019588506255,\n            \"log_p4\": 0.0016870680564514456,\n            \"log_p5\": -0.0023896692822175354,\n        },\n        \"domain_4\": {\n            \"bias\": 1.2820975851894605,\n            \"log_p1\": -0.00043994437500341244,\n            \"log_p2\": 0.0010961729632694766,\n            \"log_p3\": 0.00016179126609633246,\n            \"log_p4\": -0.01836235643857456,\n            \"log_p5\": 0.0027422260133483827,\n        },\n        \"domain_5\": {\n            \"bias\": 2.9806037129271403,\n            \"log_p1\": 0.0028948032957862636,\n            \"log_p2\": 0.002904176943938192,\n            \"log_p3\": -0.0011826414336436638,\n            \"log_p4\": 0.00291564957025589,\n            \"log_p5\": -0.015090031561988756,\n        },\n    },\n    \"70M\": {\n        \"domain_1\": {\n            \"bias\": 2.7516102214862785,\n            \"log_p1\": -0.024366773302687862,\n            \"log_p2\": -0.00140949302483008,\n            \"log_p3\": 0.0026992835128934697,\n            \"log_p4\": -0.0031027937470278375,\n            \"log_p5\": 0.0048722418014892025,\n        },\n        \"domain_2\": {\n            \"bias\": 3.6719317859686043,\n            \"log_p1\": 0.0011625721914211736,\n            \"log_p2\": -0.005916469283232795,\n            \"log_p3\": 0.0001159058459700631,\n            \"log_p4\": 0.001253484647821796,\n            \"log_p5\": -0.0023991782312973043,\n        },\n        \"domain_3\": {\n            \"bias\": 3.148901142003107,\n            \"log_p1\": 0.0019476114702619417,\n            \"log_p2\": 0.0005957023862280053,\n            \"log_p3\": -0.01776813563794145,\n            \"log_p4\": 0.0013179243483677264,\n            \"log_p5\": -0.002183184626543552,\n        },\n        \"domain_4\": {\n            \"bias\": 1.6665569458710474,\n            \"log_p1\": -0.00025847451148676925,\n            \"log_p2\": 0.0008758521651152874,\n            \"log_p3\": 0.00029735558927713497,\n            \"log_p4\": -0.022152540972955964,\n            \"log_p5\": 0.0033608606565960417,\n        },\n        \"domain_5\": {\n            \"bias\": 3.5732020332438177,\n            \"log_p1\": 0.003066036404538771,\n            \"log_p2\": 0.002572530514690887,\n            \"log_p3\": -0.0008951939451058563,\n            \"log_p4\": 0.0027436364335349703,\n            \"log_p5\": -0.013811286335446366,\n        },\n    },\n}\n\n# Fallback pooled coefficients (across all groups) in case an unknown group is requested.\nFALLBACK: Dict[str, Dict[str, float]] = {\n    \"domain_1\": {\n        \"bias\": 2.4301835104003184,\n        \"log_p1\": -0.02283902416242487,\n        \"log_p2\": -0.001032148750437415,\n        \"log_p3\": 0.002301156419295567,\n        \"log_p4\": -0.0026361306286050916,\n        \"log_p5\": 0.004231907802742471,\n    },\n    \"domain_2\": {\n        \"bias\": 3.3248991499372433,\n        \"log_p1\": 0.0011913699144251472,\n        \"log_p2\": -0.005330970019305012,\n        \"log_p3\": -2.0896481204204573e-05,\n        \"log_p4\": 0.001434257740058452,\n        \"log_p5\": -0.0027523041675352787,\n    },\n    \"domain_3\": {\n        \"bias\": 2.8403534958517995,\n        \"log_p1\": 0.0017834609156864088,\n        \"log_p2\": 0.00066750480511602,\n        \"log_p3\": -0.01756527092308412,\n        \"log_p4\": 0.0015195022381765178,\n        \"log_p5\": -0.0022434777005931115,\n    },\n    \"domain_4\": {\n        \"bias\": 1.4252898359865924,\n        \"log_p1\": -0.0002494683908218775,\n        \"log_p2\": 0.0009012640495810252,\n        \"log_p3\": 0.0002465365461698426,\n        \"log_p4\": -0.019782469166145775,\n        \"log_p5\": 0.003069195629496027,\n    },\n    \"domain_5\": {\n        \"bias\": 3.208544206409745,\n        \"log_p1\": 0.0029427714553889924,\n        \"log_p2\": 0.0026956280845882408,\n        \"log_p3\": -0.0009627637173476962,\n        \"log_p4\": 0.0028183765211462894,\n        \"log_p5\": -0.014427199520516213,\n    },\n}\n\n\ndef _predict_one(row: Dict[str, float], coeffs: Dict[str, Dict[str, float]]) -> Dict[str, float]:\n    # Build log-proportion features with numerical stability for zeros\n    logs = [log(float(row.get(f\"proportion_domain_{j}\", 0.0)) + EPS) for j in range(1, 6)]\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        c = coeffs.get(f\"domain_{i}\")\n        if c is None:\n            # If missing, fall back to pooled per-domain coefficients\n            c = FALLBACK[f\"domain_{i}\"]\n        val = c[\"bias\"]\n        for j in range(1, 6):\n            val += c[f\"log_p{j}\"] * logs[j - 1]\n        out[f\"loss_domain_{i}\"] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s): loss_domain_1..5.\n    \"\"\"\n    # Select coefficients for the provided group (fallback if unknown)\n    coeffs = COEFFS.get(group)\n    if coeffs is None:\n        # Unknown group: use pooled (group-agnostic) coefficients for all domains\n        coeffs = FALLBACK\n    return [_predict_one(row, coeffs) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "aider", "model_name": "gpt-5", "reward_r2": 0.840301, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Lazy-fit parameters on first call and cache them on the function object.\n    # We keep all helper logic enclosed to satisfy the \"single function\" constraint.\n    import math\n\n    # Small numerical constants\n    EPS_P = 1e-12   # floor for proportions (avoids log(0))\n    EPS_Y = 1e-12   # floor for losses in fitting\n    L2 = 1e-6       # small ridge regularization for stability\n\n    # Initialize cache containers if missing\n    if not hasattr(law, \"_fitted\"):\n        law._fitted = False\n    if not hasattr(law, \"_model\"):\n        law._model = {}\n    if not hasattr(law, \"_names\"):\n        law._names = {\"prop\": [], \"loss\": []}\n\n    def _int_suffix(s: str) -> int:\n        try:\n            return int(s.rsplit(\"_\", 1)[-1])\n        except Exception:\n            return 1 << 30  # push non-numeric to the end deterministically\n\n    def _ensure_fit():\n        if law._fitted:\n            return\n        try:\n            from datasets import load_from_disk\n            import numpy as np\n        except Exception:\n            # If required libs are unavailable, fall back to a neutral, constant model.\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Load dataset\n        rows = []\n        prop_cols = []\n        loss_cols = []\n        group_col = None\n        try:\n            ds_any = load_from_disk(\"/app/data\")\n        except Exception:\n            # Dataset missing; use fallback\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Iterate rows from either a DatasetDict or a single Dataset\n        def _iter_rows(ds):\n            try:\n                ds = ds.with_format(\"python\")\n            except Exception:\n                pass\n            for r in ds:\n                yield r\n\n        if hasattr(ds_any, \"items\"):\n            # DatasetDict\n            for _, ds in ds_any.items():\n                for r in _iter_rows(ds):\n                    rows.append(r)\n        else:\n            # Single split\n            for r in _iter_rows(ds_any):\n                rows.append(r)\n\n        if not rows:\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Detect columns\n        all_cols = set()\n        for r in rows:\n            all_cols.update(r.keys())\n\n        prop_cols = sorted(\n            [c for c in all_cols if c.startswith(\"proportion_domain_\")],\n            key=_int_suffix,\n        )\n        loss_cols = sorted(\n            [c for c in all_cols if c.startswith(\"loss_domain_\")],\n            key=_int_suffix,\n        )\n        for cand in (\"group\", \"Group\", \"group_name\", \"group_id\", \"subset\", \"task\", \"exp_group\"):\n            if cand in all_cols:\n                group_col = cand\n                break\n\n        if not prop_cols or not loss_cols:\n            # No usable columns; fallback to constant model\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Group rows\n        by_group = {}\n        for r in rows:\n            g = r.get(group_col, \"ALL\") if group_col else \"ALL\"\n            by_group.setdefault(str(g), []).append(r)\n\n        # Utility: fit per group and per loss target using log-linear regression with ridge\n        def _fit_group(g_rows):\n            # Returns: dict loss_col -> (intercept, [beta_j for each prop_col])\n            params = {}\n            # Prepare X rows per record (log-proportions)\n            X_logp = []\n            mask_valid = []\n            for r in g_rows:\n                # Build log-proportion vector with safe normalization and floors\n                vals = [r.get(c, None) for c in prop_cols]\n                present = [(i, v) for i, v in enumerate(vals) if v is not None]\n                m = len(prop_cols)\n                if not present:\n                    p = [1.0 / m] * m\n                else:\n                    # Normalize present proportions to sum 1; add tiny floor to all dims and renormalize\n                    p = [0.0] * m\n                    s = 0.0\n                    for i, v in present:\n                        try:\n                            fv = float(v)\n                        except Exception:\n                            fv = 0.0\n                        if not math.isfinite(fv) or fv < 0:\n                            fv = 0.0\n                        p[i] = fv\n                        s += fv\n                    if s <= 0.0:\n                        p = [1.0 / m] * m\n                    else:\n                        p = [pi / s for pi in p]\n                        # add small floor and renormalize\n                        p = [pi + EPS_P for pi in p]\n                        tot = sum(p)\n                        p = [pi / tot for pi in p]\n                X_logp.append([math.log(max(pi, EPS_P)) for pi in p])\n                mask_valid.append(True)\n            # Fit a separate model for each loss dimension\n            import numpy as np\n            X = np.asarray(X_logp, dtype=float)\n            n = X.shape[0]\n            d = X.shape[1]\n            ones = np.ones((n, 1), dtype=float)\n            X_aug = np.concatenate([ones, X], axis=1)  # [1, log p_1, ..., log p_d]\n            XtX = X_aug.T @ X_aug\n            XtX += L2 * np.eye(d + 1)\n\n            for loss_col in loss_cols:\n                y_list = []\n                idx_list = []\n                for idx, r in enumerate(g_rows):\n                    v = r.get(loss_col, None)\n                    if v is None:\n                        continue\n                    try:\n                        fv = float(v)\n                    except Exception:\n                        continue\n                    if not math.isfinite(fv) or fv <= 0:\n                        continue\n                    y_list.append(math.log(max(fv, EPS_Y)))\n                    idx_list.append(idx)\n                if len(y_list) < d + 1:\n                    # Not enough data; fall back to neutral params\n                    params[loss_col] = (0.0, [0.0] * d)\n                    continue\n                y = np.asarray(y_list, dtype=float)\n                Xa = X_aug[idx_list, :]\n                A = Xa.T @ Xa + L2 * np.eye(d + 1)\n                b = Xa.T @ y\n                try:\n                    w = np.linalg.solve(A, b)\n                except np.linalg.LinAlgError:\n                    w = np.linalg.lstsq(Xa, y, rcond=None)[0]\n                intercept = float(w[0])\n                betas = [float(bi) for bi in w[1:].tolist()]\n                params[loss_col] = (intercept, betas)\n            return params\n\n        groups_params = {}\n        for g, g_rows in by_group.items():\n            groups_params[g] = _fit_group(g_rows)\n\n        # Also fit a global \"ALL\" group over every row for fallback/extrapolation\n        if \"ALL\" not in groups_params or len(groups_params) > 1:\n            all_rows = rows\n            groups_params[\"ALL\"] = _fit_group(all_rows)\n\n        law._model = {\"groups\": groups_params, \"is_fallback\": False}\n        law._names = {\"prop\": prop_cols, \"loss\": loss_cols}\n        law._fitted = True\n\n    # Ensure model is fitted\n    _ensure_fit()\n\n    # If input_data is empty, allow callers to trigger fitting/caching without predicting.\n    if not input_data:\n        return []\n\n    # Choose group parameters; if unknown, fall back to \"ALL\" then any available group\n    groups = law._model.get(\"groups\", {})\n    group_key = str(group)\n    if group_key not in groups:\n        if \"ALL\" in groups:\n            group_key = \"ALL\"\n        elif groups:\n            group_key = next(iter(groups.keys()))\n        else:\n            group_key = \"ALL\"\n    params_for_group = groups.get(group_key, {})\n    prop_cols = law._names.get(\"prop\", [])\n    loss_cols = law._names.get(\"loss\", [])\n\n    # Prediction helper: build log-proportions vector aligned to fitted prop_cols\n    def _row_logp(row):\n        m = len(prop_cols)\n        if m == 0:\n            return []\n        present_vals = []\n        for c in prop_cols:\n            v = row.get(c, None)\n            if v is None:\n                continue\n            try:\n                fv = float(v)\n            except Exception:\n                continue\n            if not math.isfinite(fv) or fv < 0:\n                continue\n            present_vals.append((c, fv))\n        if not present_vals:\n            # Default to uniform if nothing provided\n            p = [1.0 / m] * m\n        else:\n            # Start with zeros, set present, normalize to sum 1\n            p = [0.0] * m\n            s = sum(v for _, v in present_vals)\n            if s <= 0:\n                p = [1.0 / m] * m\n            else:\n                for idx, c in enumerate(prop_cols):\n                    val = row.get(c, None)\n                    if val is None:\n                        continue\n                    try:\n                        fv = float(val)\n                    except Exception:\n                        fv = 0.0\n                    if not math.isfinite(fv) or fv < 0:\n                        fv = 0.0\n                    p[idx] = fv\n                # normalize and apply tiny floor to avoid zero\n                s = sum(p)\n                if s > 0:\n                    p = [pi / s for pi in p]\n                else:\n                    p = [1.0 / m] * m\n                p = [pi + EPS_P for pi in p]\n                total = sum(p)\n                p = [pi / total for pi in p]\n        return [math.log(max(pi, EPS_P)) for pi in p]\n\n    # Produce predictions\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        logp = _row_logp(row)\n        # If no fitted names, return empty dict\n        if not loss_cols or not prop_cols:\n            outputs.append({})\n            continue\n        pred = {}\n        for lc in loss_cols:\n            intercept, betas = params_for_group.get(lc, (0.0, [0.0] * len(prop_cols)))\n            # If group didn't have params for this target, try ALL\n            if (intercept, betas) == (0.0, [0.0] * len(prop_cols)) and \"ALL\" in groups:\n                intercept, betas = groups[\"ALL\"].get(lc, (0.0, [0.0] * len(prop_cols)))\n            s = intercept\n            for bj, lj in zip(betas, logp):\n                s += bj * lj\n            yhat = math.exp(s)\n            pred[lc] = float(yhat)\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "codex", "model_name": "gpt-5", "reward_r2": 0.834132, "solution": "from __future__ import annotations\nimport math\nfrom typing import Dict, List\n\n\n# Coefficients fitted on the provided dataset using\n# least squares for the model:\n#   loss_domain_i = c_{g,i} + b_{g,i} * log(1 / (proportion_domain_i + eps))\n# where g is the experimental group.\n_COEFFS: Dict[str, Dict[int, tuple[float, float]]] = {\n    \"160M\": {\n        1: (2.437447, 0.022863),\n        2: (3.321687, 0.005456),\n        3: (2.834681, 0.016343),\n        4: (1.410384, 0.020125),\n        5: (3.155392, 0.016970),\n    },\n    \"305M\": {\n        1: (2.290583, 0.022284),\n        2: (3.163720, 0.005168),\n        3: (2.688818, 0.016917),\n        4: (1.303708, 0.019271),\n        5: (2.988093, 0.017195),\n    },\n    \"410M\": {\n        1: (2.229470, 0.022088),\n        2: (3.091907, 0.005016),\n        3: (2.622653, 0.017235),\n        4: (1.264438, 0.018752),\n        5: (2.912269, 0.017788),\n    },\n    \"70M\": {\n        1: (2.750293, 0.024420),\n        2: (3.655117, 0.005927),\n        3: (3.127444, 0.017154),\n        4: (1.646486, 0.022572),\n        5: (3.504848, 0.016495),\n    },\n}\n\n\ndef _get_group_coeffs(group: str) -> Dict[int, tuple[float, float]]:\n    \"\"\"Return per-domain (c, b) coefficients for the given group.\n\n    If the group is unknown, fall back to the average of known groups.\n    \"\"\"\n    if group in _COEFFS:\n        return _COEFFS[group]\n    # Fallback: average coefficients across all known groups\n    avg: Dict[int, List[float]] = {i: [0.0, 0.0] for i in range(1, 6)}\n    n = float(len(_COEFFS))\n    for g in _COEFFS.values():\n        for i in range(1, 6):\n            c, b = g[i]\n            avg[i][0] += c / n\n            avg[i][1] += b / n\n    return {i: (avg[i][0], avg[i][1]) for i in range(1, 6)}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coeffs = _get_group_coeffs(group)\n    eps = 1e-9\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            q = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            c, b = coeffs[i]\n            pred[f\"loss_domain_{i}\"] = c + b * math.log(1.0 / (q + eps))\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2", "reward_r2": 0.8339644854356709, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Model parameters discovered from data analysis\n    # Model: loss_domain_i = slope_i * proportion_domain_i + alpha_i * (model_size)^(beta_i)\n    \n    # Map group names to model sizes (in millions of parameters)\n    group_sizes = {\n        '70M': 70,\n        '160M': 160,\n        '305M': 305,\n        '410M': 410\n    }\n    \n    # Get model size for this group\n    if group not in group_sizes:\n        raise ValueError(f\"Unknown group: {group}. Expected one of {list(group_sizes.keys())}\")\n    \n    size = group_sizes[group]\n    \n    # Domain-specific parameters (optimized from data)\n    # Format: [slope, alpha, beta] for each domain 1-5\n    domain_params = [\n        # Domain 1\n        [-1.1256, 4.9466, -0.1069],\n        # Domain 2\n        [-0.7693, 5.5977, -0.0929],\n        # Domain 3\n        [-5.8083, 5.1105, -0.0869],\n        # Domain 4\n        [-2.2043, 3.5415, -0.1287],\n        # Domain 5\n        [-0.4231, 5.6630, -0.0965]\n    ]\n    \n    # Initialize result list\n    results = []\n    \n    for data_point in input_data:\n        # Create prediction dictionary for this data point\n        prediction = {}\n        \n        # Predict loss for each domain\n        for i in range(1, 6):\n            # Get proportion for this domain\n            prop_key = f'proportion_domain_{i}'\n            if prop_key not in data_point:\n                # If proportion not provided, assume 0\n                proportion = 0.0\n            else:\n                proportion = data_point[prop_key]\n            \n            # Get parameters for this domain\n            slope, alpha, beta = domain_params[i-1]\n            \n            # Apply scaling law: loss = slope * proportion + alpha * size^beta\n            loss = slope * proportion + alpha * (size ** beta)\n            \n            # Store prediction\n            prediction[f'loss_domain_{i}'] = loss\n        \n        results.append(prediction)\n    \n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-haiku-4-5", "reward_r2": 0.830099, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law models the validation loss for each domain as a linear function of:\n    1. The proportion of that domain in the training mixture\n    2. The logarithm of the model size (in millions of parameters)\n\n    Functional form: loss_domain_i = a_i * proportion_domain_i + b_i * log(M) + c_i\n    where M is the model size in millions.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys are 'proportion_domain_1'\n                    through 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n               Must be one of: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variables 'loss_domain_1'\n        through 'loss_domain_5'.\n    \"\"\"\n\n    # Model size in millions for each group\n    group_sizes = {\n        '70M': 70,\n        '160M': 160,\n        '305M': 305,\n        '410M': 410\n    }\n\n    # Fitted parameters: {domain: {'a': coef_prop, 'b': coef_log_model_size, 'c': intercept}}\n    params = {\n        1: {'a': -1.125872, 'b': -0.304742, 'c': 4.430192},\n        2: {'a': -0.770981, 'b': -0.321885, 'c': 5.134486},\n        3: {'a': -5.809111, 'b': -0.283582, 'c': 4.733491},\n        4: {'a': -2.206028, 'b': -0.234356, 'c': 3.039213},\n        5: {'a': -0.423390, 'b': -0.332244, 'c': 5.163817},\n    }\n\n    # Get model size for the group\n    model_size = group_sizes[group]\n    log_model_size = math.log(model_size)\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            prop_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            proportion = data_point[prop_key]\n\n            # Apply the scaling law formula\n            p = params[domain]\n            predicted_loss = p['a'] * proportion + p['b'] * log_model_size + p['c']\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "mini-swe-agent", "model_name": "gpt-5", "reward_r2": 0.825888, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: list of dicts containing keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: group name string selecting the parameter set.\n\n    Returns:\n        list of dicts with keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n    # Fitted parameters per group and domain for the power-law model:\n    # loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + EPS)^(-alpha_{g,i})\n    EPS = 1e-06\n    _PARAMS = {'160M': {'domain_1': {'a': 2.447836676230049, 'b': 0.01947604243193669, 'alpha': 0.25}, 'domain_2': {'a': 3.324865267668977, 'b': 0.0046611055397499865, 'alpha': 0.25}, 'domain_3': {'a': 2.8536617392943797, 'b': 0.013668198149884528, 'alpha': 0.25}, 'domain_4': {'a': 1.4245257489216498, 'b': 0.017073641554700338, 'alpha': 0.25}, 'domain_5': {'a': 3.1503851621943806, 'b': 0.014361051003233668, 'alpha': 0.25}}, '305M': {'domain_1': {'a': 2.300638607022169, 'b': 0.01899092355571077, 'alpha': 0.25}, 'domain_2': {'a': 3.1667447973834975, 'b': 0.004413698745158715, 'alpha': 0.25}, 'domain_3': {'a': 2.7084277465750337, 'b': 0.014151032173366376, 'alpha': 0.25}, 'domain_4': {'a': 1.3172396091096554, 'b': 0.016350509608112452, 'alpha': 0.25}, 'domain_5': {'a': 2.9829538687037322, 'b': 0.014576042006844646, 'alpha': 0.25}}, '410M': {'domain_1': {'a': 2.239353613802408, 'b': 0.018832858223664272, 'alpha': 0.25}, 'domain_2': {'a': 3.0948104946214414, 'b': 0.004286808779512701, 'alpha': 0.25}, 'domain_3': {'a': 2.6426284303607965, 'b': 0.014417587188864664, 'alpha': 0.25}, 'domain_4': {'a': 1.2776219014512213, 'b': 0.015907819042873157, 'alpha': 0.25}, 'domain_5': {'a': 2.9069132394663955, 'b': 0.01509328547538018, 'alpha': 0.25}}, '70M': {'domain_1': {'a': 2.761596918863867, 'b': 0.020779709248785288, 'alpha': 0.25}, 'domain_2': {'a': 3.6586060682487, 'b': 0.005059595461040137, 'alpha': 0.25}, 'domain_3': {'a': 3.1473798666330657, 'b': 0.014345011881448757, 'alpha': 0.25}, 'domain_4': {'a': 1.6623493783999272, 'b': 0.019149169215257496, 'alpha': 0.25}, 'domain_5': {'a': 3.4999972282096126, 'b': 0.013952976677258505, 'alpha': 0.25}}}\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(_PARAMS.keys())}\")\n    group_params = _PARAMS[group]\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            pkey = f'proportion_domain_{i}'\n            lkey = f'loss_domain_{i}'\n            if pkey not in row:\n                raise KeyError(f\"Missing required input key: {pkey}\")\n            p = float(row[pkey])\n            par = group_params[f'domain_{i}']\n            a = float(par['a']); b = float(par['b']); alpha = float(par['alpha'])\n            pred = a + b * (max(p, 0.0) + EPS) ** (-alpha)\n            out[lkey] = float(pred)\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "SLDAgent", "model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.813832, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nHybrid scaling law combining quadratic specialization and cross-domain interactions\nUses 7 parameters per output: 5 domain weights + 1 bias + 1 combined interaction term\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    N, F = X.shape\n    params = np.asarray(params)\n    \n    if params.ndim == 1:\n        params = params[None, :]                         # (1, P)\n    T, P = params.shape\n    \n    # Parameters structure per output (7 params):\n    # - 5 domain-specific coefficients\n    # - 1 bias term\n    # - 1 combined interaction coefficient\n    \n    coeffs = params[:, :F]                               # (T, F)\n    bias = params[:, F]                                  # (T,)\n    interaction_coeff = params[:, F+1]                   # (T,)\n    \n    # Linear term: weighted sum of domain proportions\n    linear_pred = np.dot(X, coeffs.T)                    # (N, T)\n    \n    # Combined interaction term:\n    # 1. Quadratic specialization: measures deviation from uniform\n    # 2. Geometric interaction: captures synergy between domains\n    uniform = 1.0 / F\n    quad_term = np.sum((X - uniform) ** 2, axis=1)       # (N,)\n    \n    # Geometric mean interaction (with safety for log)\n    X_safe = np.clip(X, 1e-10, 1.0)\n    geom_term = np.exp(np.mean(np.log(X_safe), axis=1))  # (N,)\n    \n    # Combine both interaction types (weighted average)\n    combined_interaction = 0.5 * quad_term + 0.5 * (1.0 - geom_term)  # (N,)\n    interaction_pred = combined_interaction[:, None] * interaction_coeff[None, :]  # (N, T)\n    \n    pred = linear_pred + interaction_pred + bias[None, :]  # (N, T)\n    \n    return pred[:, 0] if pred.shape[1] == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))           # (N, F)\n    y = np.asarray(loss_values)\n    N, F = X.shape\n    P = F + 2  # 5 coeffs + 1 bias + 1 interaction = 7 params per output\n    \n    if y.ndim == 1:\n        y2d = y[:, None]\n    else:\n        y2d = y\n    T = y2d.shape[1]\n    \n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        pred = scaling_law_func(X, params)               # (N, T)\n        \n        # Per-dimension weighted MSE to handle variance across domains\n        residuals = pred - y2d\n        \n        # Adaptive weighting based on loss scale per dimension\n        dim_scales = np.std(y2d, axis=0, keepdims=True) + 1e-6\n        weighted_residuals = residuals / dim_scales\n        mse = np.mean(weighted_residuals ** 2)\n        \n        # Adaptive L2 regularization (stronger for larger coefficients)\n        reg = 0.0005 * np.sum(params[:, :F] ** 2)\n        reg += 0.0001 * np.sum(params[:, F:] ** 2)\n        \n        return mse + reg\n    \n    # Multi-start optimization with improved strategies\n    best_params = None\n    best_loss = float('inf')\n    \n    # Try 4 different initialization strategies\n    for trial in range(4):\n        init = np.zeros((T, P))\n        \n        for t in range(T):\n            if trial == 0:\n                # Least squares initialization\n                X_augmented = np.column_stack([X, np.ones(N)])\n                try:\n                    lstsq_result = np.linalg.lstsq(X_augmented, y2d[:, t], rcond=None)\n                    init[t, :F] = lstsq_result[0][:F]\n                    init[t, F] = lstsq_result[0][F]\n                except:\n                    init[t, :F] = np.mean(y2d[:, t]) / F\n                    init[t, F] = np.mean(y2d[:, t])\n                init[t, F+1] = 0.0\n                \n            elif trial == 1:\n                # Domain-specific initialization\n                for f in range(F):\n                    domain_mask = X[:, f] > 0.5\n                    if np.any(domain_mask):\n                        init[t, f] = np.mean(y2d[domain_mask, t])\n                    else:\n                        init[t, f] = np.mean(y2d[:, t])\n                init[t, F] = np.mean(y2d[:, t]) * 0.3\n                init[t, F+1] = 0.15\n                \n            elif trial == 2:\n                # Initialize based on variance structure\n                y_mean = np.mean(y2d[:, t])\n                y_std = np.std(y2d[:, t])\n                init[t, :F] = y_mean / F + np.random.randn(F) * 0.05 * y_std\n                init[t, F] = y_mean * 0.5\n                init[t, F+1] = y_std * 0.2\n                \n            else:\n                # Random initialization with domain correlation\n                domain_corrs = np.corrcoef(X.T)\n                init[t, :F] = np.mean(y2d[:, t]) / F * (1 + 0.1 * np.random.randn(F))\n                init[t, F] = np.mean(y2d[:, t])\n                init[t, F+1] = np.random.randn() * 0.1\n        \n        # Optimize with adaptive bounds\n        y_range = np.ptp(y2d)\n        bound_scale = max(15.0, y_range * 2)\n        bounds = [(-bound_scale, bound_scale)] * (T * P)\n        \n        result = minimize(\n            objective, \n            init.ravel(), \n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 2500, 'ftol': 1e-11, 'gtol': 1e-8}\n        )\n        \n        if result.fun < best_loss:\n            best_loss = result.fun\n            best_params = result.x.reshape(T, P)\n    \n    # Fallback to differential evolution if fit is poor\n    if best_loss > 0.3:\n        bound_scale = max(15.0, np.ptp(y2d) * 2)\n        bounds_list = [(-bound_scale, bound_scale)] * (T * P)\n        result_de = differential_evolution(\n            objective,\n            bounds_list,\n            maxiter=150,\n            seed=42,\n            workers=1,\n            atol=1e-10,\n            tol=1e-10\n        )\n        if result_de.fun < best_loss:\n            best_params = result_de.x.reshape(T, P)\n    \n    return best_params[0] if T == 1 else best_params\n# EVOLVE-BLOCK-END"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-2.5-flash", "reward_r2": 0.809195, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Fitted parameters (A and B) for each group and domain\n    fitted_parameters = {\n        \"70M\": {\n            \"domain_1\": {\"A\": 2.7954155842226505, \"B\": 6.194945290313312e-07},\n            \"domain_2\": {\"A\": 3.666992583441158, \"B\": 1.5143629378910318e-07},\n            \"domain_3\": {\"A\": 3.1770876336775062, \"B\": 4.2355238205837483e-07},\n            \"domain_4\": {\"A\": 1.6957346873000834, \"B\": 5.706010522162532e-07},\n            \"domain_5\": {\"A\": 3.5171284546533235, \"B\": 4.146173044780857e-07},\n        },\n        \"160M\": {\n            \"domain_1\": {\"A\": 2.4794515045367107, \"B\": 5.809575446413638e-07},\n            \"domain_2\": {\"A\": 3.3325754438215687, \"B\": 1.3956202936091392e-07},\n            \"domain_3\": {\"A\": 2.8819574855777637, \"B\": 4.035983480572172e-07},\n            \"domain_4\": {\"A\": 1.4542888945325934, \"B\": 5.087695900137449e-07},\n            \"domain_5\": {\"A\": 3.168009255701514, \"B\": 4.2690655326847836e-07},\n        },\n        \"305M\": {\n            \"domain_1\": {\"A\": 2.331439324565376, \"B\": 5.665932970655655e-07},\n            \"domain_2\": {\"A\": 3.174052635323413, \"B\": 1.3213118365722513e-07},\n            \"domain_3\": {\"A\": 2.737698767011762, \"B\": 4.179249834595783e-07},\n            \"domain_4\": {\"A\": 1.3457382302771388, \"B\": 4.872370934063391e-07},\n            \"domain_5\": {\"A\": 3.000827223421394, \"B\": 4.3358908846628955e-07},\n        },\n        \"410M\": {\n            \"domain_1\": {\"A\": 2.269862140426057, \"B\": 5.620207432145631e-07},\n            \"domain_2\": {\"A\": 3.101893008310117, \"B\": 1.2838329492243058e-07},\n            \"domain_3\": {\"A\": 2.6724486393426763, \"B\": 4.2580340959834756e-07},\n            \"domain_4\": {\"A\": 1.3053577148261022, \"B\": 4.74009961564549e-07},\n            \"domain_5\": {\"A\": 2.925409380072483, \"B\": 4.492046744951497e-07},\n        },\n    }\n\n    epsilon = 1e-6\n    predictions = []\n\n    if group not in fitted_parameters:\n        raise ValueError(f\"Unknown group: {group}\")\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6): # Domains 1 to 5\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            \n            if proportion_key not in data_point:\n                # If a proportion is missing, we cannot predict its loss.\n                # Or, based on problem, assume it's 0 if not present, or raise error.\n                # For now, let's assume all proportions are present.\n                # If a proportion for a domain is 0 and it's explicitly in input_data,\n                # the epsilon handles it.\n                continue \n\n            proportion = data_point[proportion_key]\n            \n            domain_params = fitted_parameters[group][f'domain_{i}']\n            A = domain_params['A']\n            B = domain_params['B']\n            \n            # Apply the scaling law\n            # Using np.maximum to prevent division by an extremely small or zero value, consistent with fitting.\n            predicted_loss = A + B / np.maximum(proportion + epsilon, 1e-10)\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n    \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-4.1", "reward_r2": 0.7922160496910731, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Coefficients and intercepts for each group and domain\n    params = {\n        '70M': [\n            (2.9696, [-1.2024, 0.7226, -0.2714, 0.6117, 0.1395]),\n            (3.6191, [0.3115, -0.7697, -0.0298, 0.3476, 0.1404]),\n            (2.3841, [1.3538, 1.0329, -4.6770, 1.1539, 1.1364]),\n            (1.6263, [0.3999, 0.7476, 0.4032, -1.9906, 0.4400]),\n            (3.7121, [0.0403, 0.0147, 0.1869, 0.1151, -0.3571]),\n        ],\n        '160M': [\n            (2.6457, [-1.1195, 0.7054, -0.2893, 0.5850, 0.1183]),\n            (3.2983, [0.2957, -0.7264, -0.0158, 0.3383, 0.1082]),\n            (2.1324, [1.2790, 0.9910, -4.4730, 1.1336, 1.0695]),\n            (1.3894, [0.3669, 0.6810, 0.3411, -1.7833, 0.3944]),\n            (3.3748, [0.0278, 0.0268, 0.2030, 0.1190, -0.3766]),\n        ],\n        '305M': [\n            (2.4976, [-1.0844, 0.6722, -0.2556, 0.5587, 0.1091]),\n            (3.1454, [0.2803, -0.7053, 0.0042, 0.3240, 0.0969]),\n            (1.9688, [1.2989, 1.0459, -4.6125, 1.1685, 1.0992]),\n            (1.2834, [0.3504, 0.6285, 0.3385, -1.6974, 0.3799]),\n            (3.2108, [0.0211, 0.0243, 0.2216, 0.1120, -0.3790]),\n        ],\n        '410M': [\n            (2.4321, [-1.0761, 0.7076, -0.3202, 0.5842, 0.1045]),\n            (3.0703, [0.2823, -0.6691, -0.0522, 0.3466, 0.0925]),\n            (1.8897, [1.3286, 1.0731, -4.6999, 1.1806, 1.1176]),\n            (1.2548, [0.3111, 0.6544, 0.3396, -1.6598, 0.3546]),\n            (3.1335, [0.0202, 0.0634, 0.1552, 0.1434, -0.3823]),\n        ],\n    }\n    group_params = params[group]\n    output = []\n    for row in input_data:\n        props = [row[f'proportion_domain_{i+1}'] for i in range(5)]\n        pred = {}\n        for i, (intercept, coefs) in enumerate(group_params):\n            pred[f'loss_domain_{i+1}'] = intercept + sum(c*p for c, p in zip(coefs, props))\n        output.append(pred)\n    return output"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-5", "reward_r2": 0.792215, "solution": "from math import log10\nfrom typing import Dict, List\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The law is a linear model for each domain's validation loss as a function of\n    the five domain mixture proportions. The functional form is the same across\n    experimental groups, but coefficients differ by group.\n\n    For domain k in {1..5} and proportions p_j that sum to 1:\n        loss_domain_k = intercept[g][k] + sum_j coef[g][k,j] * proportion_domain_j\n\n    To generalize to unseen groups, the group-specific parameters are also\n    modeled as linear functions of log10(model_size) fitted from the observed\n    groups. If a group string encodes a size (e.g., \"550M\", \"1.3B\"), we use this\n    mapping to synthesize parameters for that group; otherwise we fall back to\n    the average of known-group parameters.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys: 'proportion_domain_1' .. 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) with keys\n        'loss_domain_1' .. 'loss_domain_5'.\n    \"\"\"\n    # Learned coefficients from the provided dataset (one linear model per loss per known group)\n    COEFS: Dict[str, Dict] = {\n        \"160M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.645699436335381,\n                \"loss_domain_2\": 3.2982774153184558,\n                \"loss_domain_3\": 2.132434044675029,\n                \"loss_domain_4\": 1.3893657641467703,\n                \"loss_domain_5\": 3.3747611697919497,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.119495690625232,\n                    \"proportion_domain_2\": 0.7054294745261317,\n                    \"proportion_domain_3\": -0.28927157238419526,\n                    \"proportion_domain_4\": 0.584990034524239,\n                    \"proportion_domain_5\": 0.11834775395905708,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.2957017578513571,\n                    \"proportion_domain_2\": -0.7264047971224965,\n                    \"proportion_domain_3\": -0.015757968287745085,\n                    \"proportion_domain_4\": 0.338268574484209,\n                    \"proportion_domain_5\": 0.10819243307467594,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.2789627462923447,\n                    \"proportion_domain_2\": 0.9909852370273848,\n                    \"proportion_domain_3\": -4.473015910337291,\n                    \"proportion_domain_4\": 1.1335508900914408,\n                    \"proportion_domain_5\": 1.0695170369261213,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.366883293836219,\n                    \"proportion_domain_2\": 0.6809673154560535,\n                    \"proportion_domain_3\": 0.3411068917199623,\n                    \"proportion_domain_4\": -1.7833344940309523,\n                    \"proportion_domain_5\": 0.3943769930187157,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.02781842295027701,\n                    \"proportion_domain_2\": 0.02677570609375618,\n                    \"proportion_domain_3\": 0.20298408706514612,\n                    \"proportion_domain_4\": 0.1189855046814393,\n                    \"proportion_domain_5\": -0.37656372079061845,\n                },\n            },\n        },\n        \"305M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.497559209494426,\n                \"loss_domain_2\": 3.14536997892604,\n                \"loss_domain_3\": 1.968805837824704,\n                \"loss_domain_4\": 1.2833765608574346,\n                \"loss_domain_5\": 3.210803311891907,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.0843715656873325,\n                    \"proportion_domain_2\": 0.6721539878426451,\n                    \"proportion_domain_3\": -0.25559189326785264,\n                    \"proportion_domain_4\": 0.5586611040857702,\n                    \"proportion_domain_5\": 0.1091483670267703,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.2802593171012234,\n                    \"proportion_domain_2\": -0.705345666382081,\n                    \"proportion_domain_3\": 0.004212615597317157,\n                    \"proportion_domain_4\": 0.32402103174261404,\n                    \"proportion_domain_5\": 0.0968527019409263,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.2989242757705795,\n                    \"proportion_domain_2\": 1.0459184205941086,\n                    \"proportion_domain_3\": -4.612543266518341,\n                    \"proportion_domain_4\": 1.1685296372435943,\n                    \"proportion_domain_5\": 1.0991709329100599,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.35041793228110724,\n                    \"proportion_domain_2\": 0.628523569343145,\n                    \"proportion_domain_3\": 0.33852456736125136,\n                    \"proportion_domain_4\": -1.6973603028352418,\n                    \"proportion_domain_5\": 0.37989423384973636,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.021137176480833044,\n                    \"proportion_domain_2\": 0.024298666101822186,\n                    \"proportion_domain_3\": 0.22155472081459832,\n                    \"proportion_domain_4\": 0.11202244079079128,\n                    \"proportion_domain_5\": -0.37901300418804473,\n                },\n            },\n        },\n        \"410M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.432148695798992,\n                \"loss_domain_2\": 3.070321096782969,\n                \"loss_domain_3\": 1.889657943760822,\n                \"loss_domain_4\": 1.254798446077109,\n                \"loss_domain_5\": 3.1335037706175104,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.0761151307100763,\n                    \"proportion_domain_2\": 0.7075633674705835,\n                    \"proportion_domain_3\": -0.32021933049462503,\n                    \"proportion_domain_4\": 0.584248540833674,\n                    \"proportion_domain_5\": 0.10452255290044439,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.28229210571466595,\n                    \"proportion_domain_2\": -0.6691020114749978,\n                    \"proportion_domain_3\": -0.05223402368178615,\n                    \"proportion_domain_4\": 0.3465500387546539,\n                    \"proportion_domain_5\": 0.09249389068746447,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.3285868291288925,\n                    \"proportion_domain_2\": 1.0730826822922905,\n                    \"proportion_domain_3\": -4.69986390494191,\n                    \"proportion_domain_4\": 1.1805807813503215,\n                    \"proportion_domain_5\": 1.1176136121704066,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.3110943108385622,\n                    \"proportion_domain_2\": 0.6544173298915442,\n                    \"proportion_domain_3\": 0.339609109190628,\n                    \"proportion_domain_4\": -1.6597557356277692,\n                    \"proportion_domain_5\": 0.3546349857070334,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.02023592248594768,\n                    \"proportion_domain_2\": 0.06340878588569265,\n                    \"proportion_domain_3\": 0.15519736526922107,\n                    \"proportion_domain_4\": 0.14343010613675986,\n                    \"proportion_domain_5\": -0.38227217977762123,\n                },\n            },\n        },\n        \"70M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.9696038495486428,\n                \"loss_domain_2\": 3.619059870697084,\n                \"loss_domain_3\": 2.3841489786591965,\n                \"loss_domain_4\": 1.6263083817663921,\n                \"loss_domain_5\": 3.712126062067348,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.2024018889698118,\n                    \"proportion_domain_2\": 0.7225748684796697,\n                    \"proportion_domain_3\": -0.2714083488994125,\n                    \"proportion_domain_4\": 0.6116971390006383,\n                    \"proportion_domain_5\": 0.1395382303889172,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.31154794402259256,\n                    \"proportion_domain_2\": -0.7696911526036795,\n                    \"proportion_domain_3\": -0.029834711619449426,\n                    \"proportion_domain_4\": 0.3475522996421775,\n                    \"proportion_domain_5\": 0.14042562055835917,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.3538168492045202,\n                    \"proportion_domain_2\": 1.0328737163751711,\n                    \"proportion_domain_3\": -4.676968208089681,\n                    \"proportion_domain_4\": 1.1539207687720698,\n                    \"proportion_domain_5\": 1.1363568737379204,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.3998862788060258,\n                    \"proportion_domain_2\": 0.747594278597924,\n                    \"proportion_domain_3\": 0.40318411868033077,\n                    \"proportion_domain_4\": -1.9906253596015544,\n                    \"proportion_domain_5\": 0.439960683517272,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.040341011044849134,\n                    \"proportion_domain_2\": 0.014732615064055155,\n                    \"proportion_domain_3\": 0.18686886153522414,\n                    \"proportion_domain_4\": 0.11511773826952129,\n                    \"proportion_domain_5\": -0.3570602259136497,\n                },\n            },\n        },\n    }\n\n    # Parameter scaling w.r.t. model size: each parameter theta is fit as\n    # theta = beta0 + beta1 * log10(model_size)\n    PARAM_MAP = {\n        'intercepts': {\n            'loss_domain_1': {'beta0': 8.410693789482913, 'beta1': -0.6968428684065728},\n            'loss_domain_2': {'beta0': 9.159453070486071, 'beta1': -0.709122366641981},\n            'loss_domain_3': {'beta0': 7.408200748969857, 'beta1': -0.64133116180932},\n            'loss_domain_4': {'beta0': 5.432810327260841, 'beta1': -0.4880602455582854},\n            'loss_domain_5': {'beta0': 9.564414879713457, 'beta1': -0.7489965370842978},\n        },\n        'coefs': {\n            'loss_domain_1': {\n                'proportion_domain_1': {'beta0': -2.497615657116547, 'beta1': 0.16617474851983763},\n                'proportion_domain_2': {'beta0': 1.022994012405379, 'beta1': -0.03874502688716343},\n                'proportion_domain_3': {'beta0': -0.013900448056208633, 'beta1': -0.03260965165867218},\n                'proportion_domain_4': {'beta0': 0.9963146912448967, 'beta1': -0.04964843319473607},\n                'proportion_domain_5': {'beta0': 0.4922074015224826, 'beta1': -0.0451716367792662},\n            },\n            'loss_domain_2': {\n                'proportion_domain_1': {'beta0': 0.6366841433930845, 'beta1': -0.041541148709937525},\n                'proportion_domain_2': {'beta0': -1.7192214869138862, 'beta1': 0.12086845627811361},\n                'proportion_domain_3': {'beta0': 0.01801791154655993, 'beta1': -0.004998619018912918},\n                'proportion_domain_4': {'beta0': 0.43967439538852, 'beta1': -0.012137270708032335},\n                'proportion_domain_5': {'beta0': 0.6248450365857227, 'beta1': -0.06219141784123091},\n            },\n            'loss_domain_3': {\n                'proportion_domain_1': {'beta0': 1.623227750301966, 'beta1': -0.03718726484996544},\n                'proportion_domain_2': {'beta0': 0.5786824938578662, 'beta1': 0.05515336511355352},\n                'proportion_domain_3': {'beta0': -4.340004860354267, 'beta1': -0.0332577630477254},\n                'proportion_domain_4': {'beta0': 0.842562300321158, 'beta1': 0.03820434891706093},\n                'proportion_domain_5': {'beta0': 1.2955323158732615, 'beta1': -0.022912686132921745},\n            },\n            'loss_domain_4': {\n                'proportion_domain_1': {'beta0': 1.2168426320766839, 'beta1': -0.10375482434593292},\n                'proportion_domain_2': {'beta0': 1.8340679378772013, 'beta1': -0.1395259506615147},\n                'proportion_domain_3': {'beta0': 1.0335256104698727, 'beta1': -0.08180936075274063},\n                'proportion_domain_4': {'beta0': -5.334693091404859, 'beta1': 0.428635948410881},\n                'proportion_domain_5': {'beta0': 1.2502569109811015, 'beta1': -0.10354581265069283},\n            },\n            'loss_domain_5': {\n                'proportion_domain_1': {'beta0': 0.24988780248626472, 'beta1': -0.026851221126731266},\n                'proportion_domain_2': {'beta0': -0.3681011545143189, 'beta1': 0.048319731256692484},\n                'proportion_domain_3': {'beta0': 0.30625861824469436, 'beta1': -0.013830485288663432},\n                'proportion_domain_4': {'beta0': -0.07581595507286405, 'beta1': 0.023918795432970087},\n                'proportion_domain_5': {'beta0': -0.11222931114377548, 'beta1': -0.03155682027426793},\n            },\n        },\n    }\n\n    def _parse_group_size(g: str) -> float | None:\n        if not isinstance(g, str):\n            return None\n        s = g.strip().upper()\n        try:\n            if s.endswith('B'):\n                return float(s[:-1]) * 1e9\n            if s.endswith('M'):\n                return float(s[:-1]) * 1e6\n            if s.isdigit():\n                return float(s)\n        except ValueError:\n            return None\n        return None\n\n    def _params_from_size(model_size: float) -> Dict[str, Dict[str, float]]:\n        logS = log10(model_size)\n        losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n        props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n        out = {\"intercepts\": {}, \"coefs\": {}}\n        for l in losses:\n            b = PARAM_MAP['intercepts'][l]\n            out[\"intercepts\"][l] = b['beta0'] + b['beta1'] * logS\n            out[\"coefs\"][l] = {}\n            for p in props:\n                bp = PARAM_MAP['coefs'][l][p]\n                out[\"coefs\"][l][p] = bp['beta0'] + bp['beta1'] * logS\n        return out\n\n    # Choose parameters for this group\n    if group in COEFS:\n        group_params = COEFS[group]\n    else:\n        size = _parse_group_size(group)\n        if size is not None and size > 0:\n            group_params = _params_from_size(size)\n        else:\n            # Fallback: average coefficients across known groups\n            groups = list(COEFS.keys())\n            losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n            props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n            avg = {\"intercepts\": {}, \"coefs\": {}}\n            for loss in losses:\n                avg[\"intercepts\"][loss] = sum(COEFS[g][\"intercepts\"][loss] for g in groups) / len(groups)\n                avg[\"coefs\"][loss] = {p: sum(COEFS[g][\"coefs\"][loss][p] for g in groups) / len(groups) for p in props}\n            group_params = avg\n\n    losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n    props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        pred: dict[str, float] = {}\n        for loss in losses:\n            val = group_params[\"intercepts\"][loss]\n            for p in props:\n                val += group_params[\"coefs\"][loss][p] * float(row.get(p, 0.0))\n            pred[loss] = float(val)\n        outputs.append(pred)\n\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "claude-code", "model_name": "claude-haiku-4-5", "reward_r2": 0.792215, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Coefficients for each group, domain, and proportion\n    # Discovered through linear regression: loss_i = intercept_i + sum_j(coeff_ij * proportion_j)\n    coefficients = {\n        \"160M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.2047495302794857,\n                \"proportions\": [-0.6785457845693336, 1.1463793805820295, 0.15167833367170308, 1.025939940580135, 0.559297660014955]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.7485645127653795,\n                \"proportions\": [0.8454146604044328, -0.17669189456942216, 0.5339549342653283, 0.8879814770372856, 0.657905335627755]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.777028370562523,\n                \"proportions\": [1.6343684204048485, 1.3463909111398886, -4.1176102362247855, 1.4889565642039464, 1.4249227110386287]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.157804803455643,\n                \"proportions\": [0.5984442545273441, 0.9125282761471838, 0.5726678524110912, -1.5517735333398215, 0.6259379537098435]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.8123009748266243,\n                \"proportions\": [0.5902786179156017, 0.5892359010590805, 0.7654442820304688, 0.6814456996467663, 0.18589647417470898]\n            }\n        },\n        \"305M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.0812993412453547,\n                \"proportions\": [-0.6681116974382604, 1.0884138560917158, 0.16066797498121796, 0.9749209723348417, 0.5254082352758425]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.6211416491050334,\n                \"proportions\": [0.8044876469222303, -0.18111733656107531, 0.528440945418323, 0.8482493615636207, 0.6210810317619353]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.6406715315205858,\n                \"proportions\": [1.6270585820746972, 1.3740527268982263, -4.2844089602142255, 1.4966639435477096, 1.4273052392141794]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.0694804673811962,\n                \"proportions\": [0.5643140257573426, 0.8424196628193863, 0.55242066083749, -1.4834642093589991, 0.5937903273259748]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.675669426576589,\n                \"proportions\": [0.5562710617961506, 0.5594325514171398, 0.7566886061299141, 0.647156326106111, 0.15612088112727496]\n            }\n        },\n        \"410M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.026790579832494,\n                \"proportions\": [-0.6707570147435764, 1.1129214834370829, 0.0851387854718725, 0.9896066568001738, 0.5098806688669443]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.5586009139858072,\n                \"proportions\": [0.7940122885118273, -0.157381828677838, 0.45948615911537244, 0.8582702215518173, 0.6042140734846292]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.574714953134018,\n                \"proportions\": [1.6435298197556953, 1.3880256729190952, -4.3849209143151056, 1.4955237719771226, 1.4325566027972132]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.0456653717309252,\n                \"proportions\": [0.5202273851847438, 0.863550404237731, 0.5487421835368134, -1.450622661281582, 0.5637680600532177]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.611253142181258,\n                \"proportions\": [0.5424865509221992, 0.5856594143219438, 0.6774479937054709, 0.665680734573013, 0.13997844865863204]\n            }\n        },\n        \"70M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.47466987462387,\n                \"proportions\": [-0.7074679140450373, 1.2175088434044439, 0.22352562602536008, 1.106631113925414, 0.6344722053136926]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 3.0158832255809034,\n                \"proportions\": [0.9147245891387732, -0.16651450748750043, 0.5733419334967286, 0.9507289447583598, 0.7436022656745435]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.9867908155493292,\n                \"proportions\": [1.7511750123143863, 1.4302318794850386, -4.279610044979821, 1.5512789318819353, 1.53371503684779]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.355256984805328,\n                \"proportions\": [0.6709376757670867, 1.0186456755589912, 0.674235515641396, -1.7195739626404851, 0.7110120804783369]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 3.093438385056124,\n                \"proportions\": [0.6590286880560735, 0.63342029207528, 0.805556538546447, 0.7338054152807485, 0.2616274510975776]\n            }\n        }\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Get coefficients for this group\n        group_coeffs = coefficients[group]\n\n        # Predict each domain's loss\n        for domain_idx in range(1, 6):\n            domain_key = f\"loss_domain_{domain_idx}\"\n            domain_params = group_coeffs[domain_key]\n\n            # Calculate: intercept + sum(coeff_j * proportion_j)\n            intercept = domain_params[\"intercept\"]\n            proportions = domain_params[\"proportions\"]\n\n            loss = intercept\n            for j in range(5):\n                proportion_key = f\"proportion_domain_{j+1}\"\n                if proportion_key in data_point:\n                    loss += proportions[j] * data_point[proportion_key]\n\n            prediction[domain_key] = loss\n\n        results.append(prediction)\n\n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-4.1", "reward_r2": 0.7922146059149126, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Coefficients for each group/domain: [const, c1, c2, c3, c4, c5]\n    coeffs = {\n        '70M': {\n            1: [2.47466987, -0.70746791, 1.21750884, 0.22352563, 1.10663111, 0.63447221],\n            2: [3.01588323, 0.91472459, -0.16651451, 0.57334193, 0.95072894, 0.74360227],\n            3: [1.98679082, 1.75117501, 1.43023188, -4.27961004, 1.55127893, 1.53371504],\n            4: [1.35525698, 0.67093768, 1.01864568, 0.67423552, -1.71957396, 0.71101208],\n            5: [3.09343839, 0.65902869, 0.63342029, 0.80555654, 0.73380542, 0.26162745],\n        },\n        '160M': {\n            1: [2.20474953, -0.67854578, 1.14637938, 0.15167833, 1.02593994, 0.55929766],\n            2: [2.74856451, 0.84541466, -0.17669189, 0.53395493, 0.88798148, 0.65790534],\n            3: [1.77702837, 1.63436842, 1.34639091, -4.11761024, 1.48895656, 1.42492271],\n            4: [1.1578048, 0.59844425, 0.91252828, 0.57266785, -1.55177353, 0.62593795],\n            5: [2.81230097, 0.59027862, 0.5892359, 0.76544428, 0.6814457, 0.18589647],\n        },\n        '305M': {\n            1: [2.08129934, -0.6681117, 1.08841386, 0.16066797, 0.97492097, 0.52540824],\n            2: [2.62114165, 0.80448765, -0.18111734, 0.52844095, 0.84824936, 0.62108103],\n            3: [1.64067153, 1.62705858, 1.37405273, -4.28440896, 1.49666394, 1.42730524],\n            4: [1.06948047, 0.56431403, 0.84241966, 0.55242066, -1.48346421, 0.59379033],\n            5: [2.67566943, 0.55627106, 0.55943255, 0.75668861, 0.64715633, 0.15612088],\n        },\n        '410M': {\n            1: [2.02679058, -0.67075701, 1.11292148, 0.08513879, 0.98960666, 0.50988067],\n            2: [2.55860091, 0.79401229, -0.15738183, 0.45948616, 0.85827022, 0.60421407],\n            3: [1.57471495, 1.64352982, 1.38802567, -4.38492091, 1.49552377, 1.4325566],\n            4: [1.04566537, 0.52022739, 0.8635504, 0.54874218, -1.45062266, 0.56376806],\n            5: [2.61125314, 0.54248655, 0.58565941, 0.67744799, 0.66568073, 0.13997845],\n        },\n    }\n    out = []\n    for row in input_data:\n        pred = {}\n        for i in range(1,6):\n            c = coeffs[group][i]\n            x = [1.0] + [row[f'proportion_domain_{j}'] for j in range(1,6)]\n            pred[f'loss_domain_{i}'] = sum(ci*xi for ci,xi in zip(c,x))\n        out.append(pred)\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o4-mini", "reward_r2": 0.7922146054081852, "solution": "# Auto-generated scaling law implementation\n# Coefficients were fitted via linear regression per group and domain\n\n# Mapping from group to parameters for each loss_domain\n# Each parameter list has [intercept, coef_p1, coef_p2, coef_p3, coef_p4, coef_p5]\nGROUP_PARAMS = {\n    '70M': {\n        'loss_domain_1': [2.47466987462387, -0.7074679140450373, 1.2175088434044439, 0.22352562602536008, 1.106631113925414, 0.6344722053136926],\n        'loss_domain_2': [3.0158832255809034, 0.9147245891387732, -0.16651450748750043, 0.5733419334967286, 0.9507289447583598, 0.7436022656745435],\n        'loss_domain_3': [1.9867908155493292, 1.7511750123143863, 1.4302318794850386, -4.279610044979821, 1.5512789318819353, 1.53371503684779],\n        'loss_domain_4': [1.355256984805328, 0.6709376757670867, 1.0186456755589912, 0.674235515641396, -1.7195739626404851, 0.7110120804783369],\n        'loss_domain_5': [3.093438385056124, 0.6590286880560735, 0.63342029207528, 0.805556538546447, 0.7338054152807485, 0.2616274510975776],\n    },\n    '160M': {\n        'loss_domain_1': [2.2047495302794857, -0.6785457845693336, 1.1463793805820295, 0.15167833367170308, 1.025939940580135, 0.559297660014955],\n        'loss_domain_2': [2.7485645127653795, 0.8454146604044328, -0.17669189456942216, 0.5339549342653283, 0.8879814770372856, 0.657905335627755],\n        'loss_domain_3': [1.777028370562523, 1.6343684204048485, 1.3463909111398886, -4.1176102362247855, 1.4889565642039464, 1.4249227110386287],\n        'loss_domain_4': [1.157804803455643, 0.5984442545273441, 0.9125282761471838, 0.5726678524110912, -1.5517735333398215, 0.6259379537098435],\n        'loss_domain_5': [2.8123009748266243, 0.5902786179156017, 0.5892359010590805, 0.7654442820304688, 0.6814456996467663, 0.18589647417470898],\n    },\n    '305M': {\n        'loss_domain_1': [2.0812993412453547, -0.6681116974382604, 1.0884138560917158, 0.16066797498121796, 0.9749209723348417, 0.5254082352758425],\n        'loss_domain_2': [2.6211416491050334, 0.8044876469222303, -0.18111733656107531, 0.528440945418323, 0.8482493615636207, 0.6210810317619353],\n        'loss_domain_3': [1.6406715315205858, 1.6270585820746972, 1.3740527268982263, -4.2844089602142255, 1.4966639435477096, 1.4273052392141794],\n        'loss_domain_4': [1.0694804673811962, 0.5643140257573426, 0.8424196628193863, 0.55242066083749, -1.4834642093589991, 0.5937903273259748],\n        'loss_domain_5': [2.675669426576589, 0.5562710617961506, 0.5594325514171398, 0.7566886061299141, 0.647156326106111, 0.15612088112727496],\n    },\n    '410M': {\n        'loss_domain_1': [2.026790579832494, -0.6707570147435764, 1.1129214834370829, 0.0851387854718725, 0.9896066568001738, 0.5098806688669443],\n        'loss_domain_2': [2.5586009139858072, 0.7940122885118273, -0.157381828677838, 0.45948615911537244, 0.8582702215518173, 0.6042140734846292],\n        'loss_domain_3': [1.574714953134018, 1.6435298197556953, 1.3880256729190952, -4.3849209143151056, 1.4955237719771226, 1.4325566027972132],\n        'loss_domain_4': [1.0456653717309252, 0.5202273851847438, 0.863550404237731, 0.5487421835368134, -1.450622661281582, 0.5637680600532177],\n        'loss_domain_5': [2.611253142181258, 0.5424865509221992, 0.5856594143219438, 0.6774479937054709, 0.665680734573013, 0.13997844865863204],\n    },\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to the discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, each containing keys 'proportion_domain_1' through 'proportion_domain_5'.\n        group: The experimental group key corresponding to pre-fitted parameters.\n\n    Returns:\n        A list of dictionaries, each with predicted 'loss_domain_1' through 'loss_domain_5'.\n    \"\"\"\n    if group not in GROUP_PARAMS:\n        raise KeyError(f\"Unknown group: {group}\")\n    preds = []\n    params = GROUP_PARAMS[group]\n    # Feature keys in order\n    feat_keys = [f\"proportion_domain_{i}\" for i in range(1,6)]\n    out_keys = [f\"loss_domain_{i}\" for i in range(1,6)]\n    for entry in input_data:\n        # Build feature vector\n        x = [1.0] + [entry[k] for k in feat_keys]\n        # Compute predicted losses\n        out = {}\n        for key in out_keys:\n            theta = params[key]\n            # dot product\n            pred = sum(t * xi for t, xi in zip(theta, x))\n            out[key] = pred\n        preds.append(out)\n    return preds"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "gpt-4.1", "reward_r2": 0.7922146054081847, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    COEFS = {\n        '70M': {\n            'loss_domain_1': {'coef': [-0.7074679140450348, 1.2175088434044405, 0.223525626025359, 1.1066311139254104, 0.6344722053136927], 'intercept': 2.47466987462387},\n            'loss_domain_2': {'coef': [0.9147245891387733, -0.16651450748750252, 0.5733419334967271, 0.950728944758362, 0.7436022656745442], 'intercept': 3.0158832255809047},\n            'loss_domain_3': {'coef': [1.7511750123143854, 1.4302318794850444, -4.279610044979821, 1.5512789318819356, 1.5337150368477888], 'intercept': 1.986790815549331},\n            'loss_domain_4': {'coef': [0.6709376757670921, 1.0186456755589888, 0.6742355156413925, -1.7195739626404885, 0.7110120804783404], 'intercept': 1.3552569848053264},\n            'loss_domain_5': {'coef': [0.6590286880560785, 0.6334202920752747, 0.805556538546449, 0.7338054152807434, 0.2616274510975761], 'intercept': 3.0934383850561247},\n        },\n        '160M': {\n            'loss_domain_1': {'coef': [-0.6785457845693319, 1.146379380582026, 0.15167833367169947, 1.0259399405801335, 0.5592976600149551], 'intercept': 2.204749530279485},\n            'loss_domain_2': {'coef': [0.8454146604044344, -0.1766918945694198, 0.5339549342653316, 0.8879814770372834, 0.6579053356277531], 'intercept': 2.7485645127653826},\n            'loss_domain_3': {'coef': [1.6343684204048525, 1.3463909111398857, -4.11761023622479, 1.4889565642039502, 1.424922711038629], 'intercept': 1.7770283705625238},\n            'loss_domain_4': {'coef': [0.5984442545273474, 0.9125282761471805, 0.5726678524110904, -1.551773533339824, 0.6259379537098458], 'intercept': 1.1578048034556419},\n            'loss_domain_5': {'coef': [0.5902786179156042, 0.5892359010590775, 0.7654442820304671, 0.6814456996467655, 0.18589647417470864], 'intercept': 2.8123009748266248},\n        },\n        '305M': {\n            'loss_domain_1': {'coef': [-0.6681116974382557, 1.0884138560917078, 0.1606679749812158, 0.9749209723348401, 0.5254082352758443], 'intercept': 2.0812993412453538},\n            'loss_domain_2': {'coef': [0.804487646922234, -0.18111733656107631, 0.5284409454183221, 0.8482493615636183, 0.621081031761935], 'intercept': 2.6211416491050334},\n            'loss_domain_3': {'coef': [1.627058582074697, 1.374052726898228, -4.284408960214225, 1.4966639435477105, 1.427305239214179], 'intercept': 1.6406715315205878},\n            'loss_domain_4': {'coef': [0.5643140257573457, 0.8424196628193856, 0.5524206608374863, -1.4834642093590011, 0.5937903273259771], 'intercept': 1.0694804673811953},\n            'loss_domain_5': {'coef': [0.5562710617961523, 0.5594325514171392, 0.7566886061299118, 0.6471563261061084, 0.15612088112727607], 'intercept': 2.6756694265765897},\n        },\n        '410M': {\n            'loss_domain_1': {'coef': [-0.6707570147435775, 1.1129214834370798, 0.08513878547187051, 0.9896066568001731, 0.5098806688669455], 'intercept': 2.0267905798324937},\n            'loss_domain_2': {'coef': [0.7940122885118303, -0.1573818286778379, 0.4594861591153763, 0.8582702215518133, 0.6042140734846265], 'intercept': 2.558600913985809},\n            'loss_domain_3': {'coef': [1.643529819755696, 1.3880256729190958, -4.384920914315108, 1.4955237719771257, 1.4325566027972132], 'intercept': 1.574714953134019},\n            'loss_domain_4': {'coef': [0.5202273851847482, 0.8635504042377276, 0.5487421835368104, -1.450622661281584, 0.5637680600532203], 'intercept': 1.045665371730924},\n            'loss_domain_5': {'coef': [0.5424865509221987, 0.5856594143219445, 0.6774479937054712, 0.6656807345730104, 0.13997844865863152], 'intercept': 2.6112531421812584},\n        },\n    }\n    \n    dom_props = [f'proportion_domain_{i}' for i in range(1,6)]\n    out = []\n    for row in input_data:\n        pred = {}\n        for dom in range(1,6):\n            key = f'loss_domain_{dom}'\n            c = COEFS[group][key]['coef']\n            b = COEFS[group][key]['intercept']\n            x = [row.get(p, 0.0) for p in dom_props]\n            pred[key] = float(sum(ci*xi for ci,xi in zip(c,x)) + b)\n        out.append(pred)\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2", "reward_r2": 0.7766522374268384, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Convert group name to model size in millions\n    model_sizes = {\n        '70M': 70.0,\n        '160M': 160.0,\n        '305M': 305.0,\n        '410M': 410.0\n    }\n    \n    if group not in model_sizes:\n        raise ValueError(f\"Unknown group: {group}. Expected one of {list(model_sizes.keys())}\")\n    \n    N = model_sizes[group]\n    logN = math.log(N)\n    \n    # Model parameters obtained from fitting the dataset\n    # Formula: loss_i = a_i + b_i * log(N) + Σ_{j=1}^5 c_ij * p_j\n    # Where p_j are the mixture proportions\n    \n    # Parameters for each domain (i = 1..5)\n    params = {\n        1: {\n            'a': 3.533943,\n            'b': -0.304742,\n            'c': [-0.413807, 1.408719, 0.422666, 1.291688, 0.824678]\n        },\n        2: {\n            'a': 4.148328,\n            'b': -0.321885,\n            'c': [1.122116, 0.112030, 0.806262, 1.168764, 0.939157]\n        },\n        3: {\n            'a': 2.989025,\n            'b': -0.283582,\n            'c': [1.912878, 1.633520, -4.017793, 1.756950, 1.703470]\n        },\n        4: {\n            'a': 2.185297,\n            'b': -0.234356,\n            'c': [0.794130, 1.114935, 0.792666, -1.345710, 0.829276]\n        },\n        5: {\n            'a': 4.255896,\n            'b': -0.332244,\n            'c': [0.878562, 0.883483, 1.042831, 0.973568, 0.477452]\n        }\n    }\n    \n    results = []\n    \n    for data_point in input_data:\n        # Extract proportions\n        p1 = data_point.get('proportion_domain_1', 0.0)\n        p2 = data_point.get('proportion_domain_2', 0.0)\n        p3 = data_point.get('proportion_domain_3', 0.0)\n        p4 = data_point.get('proportion_domain_4', 0.0)\n        p5 = data_point.get('proportion_domain_5', 0.0)\n        \n        # Create vector of proportions\n        p_vec = [p1, p2, p3, p4, p5]\n        \n        # Compute predictions for each domain\n        pred_dict = {}\n        \n        for i in range(1, 6):\n            param = params[i]\n            \n            # Compute dot product: Σ c_ij * p_j\n            dot_product = sum(c * p for c, p in zip(param['c'], p_vec))\n            \n            # Compute prediction: a + b*log(N) + dot_product\n            prediction = param['a'] + param['b'] * logN + dot_product\n            \n            pred_dict[f'loss_domain_{i}'] = prediction\n        \n        results.append(pred_dict)\n    \n    return results"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o3", "reward_r2": 0.7543921987682078, "solution": "import math\n\n# Pre-computed coefficients (a, b) for each group and domain.\n_COEFFS = {\n    \"70M\": {\n        1: (2.6991877691852126, -0.05301830256954043),\n        2: (3.641245631839601, -0.01288446947428002),\n        3: (3.0648180627646675, -0.03887735281392035),\n        4: (1.5883184714718914, -0.04953747817222871),\n        5: (3.491442851657022, -0.03541572615826875),\n    },\n    \"160M\": {\n        1: (2.390023902609585, -0.04954828464158185),\n        2: (3.308997347375093, -0.01184703133901682),\n        3: (2.775061224326219, -0.03703323688495387),\n        4: (1.3585325213280748, -0.04416553521395205),\n        5: (3.1416357474005943, -0.036409432790092094),\n    },\n    \"305M\": {\n        1: (2.2445101169537414, -0.04826253963246907),\n        2: (3.151664441181741, -0.011228102438934438),\n        3: (2.6272265814936144, -0.03831519342481426),\n        4: (1.254081110356706, -0.04228710260549062),\n        5: (2.974256274101213, -0.036815022043621345),\n    },\n    \"410M\": {\n        1: (2.1839860164214726, -0.04779882382417476),\n        2: (3.080284217913702, -0.010883486046165911),\n        3: (2.5599133716532467, -0.039034580115764377),\n        4: (1.216103723749701, -0.04115598457974986),\n        5: (2.8980195371491457, -0.038035573443952364),\n    },\n}\n\n_EPS = 1e-12  # Avoid log(0)\n\ndef _predict_single(row: dict[str, float], coeffs: dict[int, tuple[float, float]]) -> dict[str, float]:\n    \"\"\"Compute predictions for one data point given the coeff dict for its group.\"\"\"\n    out: dict[str, float] = {}\n    for i in range(1, 6):\n        p = row.get(f\"proportion_domain_{i}\", 0.0)\n        # Clamp to avoid -inf.\n        p = max(p, _EPS)\n        a, b = coeffs[i]\n        out[f\"loss_domain_{i}\"] = a + b * math.log(p)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law:\n        loss_domain_i = a_{group,i} + b_{group,i} * ln(proportion_domain_i)\n    where (a_{group,i}, b_{group,i}) are constants determined for each (group, domain).\n\n    Args:\n        input_data: List of data points containing 'proportion_domain_*' keys.\n        group: Experimental group label (e.g., '70M', '160M', ...).\n\n    Returns:\n        List with predicted losses for each input example.\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Expected one of {list(_COEFFS)}\")\n\n    coeffs = _COEFFS[group]\n    return [_predict_single(row, coeffs) for row in input_data]"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o4-mini", "reward_r2": -1.0, "solution": "import math\n\n# Parameters for the power-law scaling: loss = A * p^b\n# Parameters A and b per group and domain index\ngroup_params = {\n    \"70M\": {\n        1: {\"A\": 2.5469231031223694, \"b\": -0.053856817382467885},\n        2: {\"A\": 3.572940616953032,  \"b\": -0.013140624030475783},\n        3: {\"A\": 2.9029843430954707, \"b\": -0.03138671540754998},\n        4: {\"A\": 1.4276962543227714, \"b\": -0.0805007638328445},\n        5: {\"A\": 3.4360881599441604, \"b\": -0.035362869076753844},\n    },\n    \"160M\": {\n        1: {\"A\": 2.2615462410312324, \"b\": -0.05322286098991343},\n        2: {\"A\": 3.264432162767659,  \"b\": -0.010428494948258562},\n        3: {\"A\": 2.631691497596162,  \"b\": -0.0315962095456069},\n        4: {\"A\": 1.2163575770768627, \"b\": -0.08356911748409235},\n        5: {\"A\": 3.08550213386204,   \"b\": -0.039978594566019446},\n    },\n    \"305M\": {\n        1: {\"A\": 2.1245282223544177, \"b\": -0.0537648313675756},\n        2: {\"A\": 3.10132258895579,   \"b\": -0.011700109195731317},\n        3: {\"A\": 2.507951292630382,  \"b\": -0.0304824591544152},\n        4: {\"A\": 1.120707470602762,  \"b\": -0.08558188369742958},\n        5: {\"A\": 2.919700368120316,  \"b\": -0.041502398419353326},\n    },\n    \"410M\": {\n        1: {\"A\": 2.071078836571189,  \"b\": -0.053026238258029254},\n        2: {\"A\": 3.0492072217672646, \"b\": -0.008625461782159526},\n        3: {\"A\": 2.4411353600422157, \"b\": -0.03147846919171784},\n        4: {\"A\": 1.0829659757348267, \"b\": -0.08734153465469903},\n        5: {\"A\": 2.842998459668121,  \"b\": -0.04324831870633425},\n    },\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts loss values for each domain based on the power-law scaling law:\n        loss_domain_i = A_{i} * (proportion_domain_i)^{b_{i}}\n\n    Args:\n        input_data: List of data points, each containing 'proportion_domain_i' keys.\n        group: One of the defined model sizes: '70M', '160M', '305M', or '410M'.\n\n    Returns:\n        List of dictionaries with keys 'loss_domain_i' for i in 1..5.\n    \"\"\"\n    if group not in group_params:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = group_params[group]\n    outputs = []\n    for entry in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p_key = f\"proportion_domain_{i}\"\n            loss_key = f\"loss_domain_{i}\"\n            p = entry.get(p_key, 0.0)\n            A = params[i][\"A\"]\n            b = params[i][\"b\"]\n            # Compute prediction (if p <= 0, prediction set to A*0^b = inf or nan; clamp to zero)\n            try:\n                val = A * (p ** b)\n            except Exception:\n                val = float('nan')\n            pred[loss_key] = val\n        outputs.append(pred)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o4-mini", "reward_r2": -1.0, "solution": "import math\n\n# Parameters for scaling law: loss = a * (proportion)^b\n# a and b coefficients per experimental group and domain\nPARAMS = {\n    '70M': {\n        1: (2.5469231031223694, -0.053856817382467885),\n        2: (3.572940616953032, -0.013140624030475783),\n        3: (2.9029843430954707, -0.03138671540754998),\n        4: (1.4276962543227714, -0.0805007638328445),\n        5: (3.4360881599441604, -0.035362869076753844),\n    },\n    '160M': {\n        1: (2.2615462410312324, -0.05322286098991343),\n        2: (3.264432162767659, -0.010428494948258562),\n        3: (2.631691497596162, -0.0315962095456069),\n        4: (1.2163575770768627, -0.08356911748409235),\n        5: (3.08550213386204, -0.039978594566019446),\n    },\n    '305M': {\n        1: (2.1245282223544177, -0.0537648313675756),\n        2: (3.10132258895579, -0.011700109195731317),\n        3: (2.507951292630382, -0.0304824591544152),\n        4: (1.120707470602762, -0.08558188369742958),\n        5: (2.919700368120316, -0.041502398419353326),\n    },\n    '410M': {\n        1: (2.071078836571189, -0.053026238258029254),\n        2: (3.0492072217672646, -0.008625461782159526),\n        3: (2.4411353600422157, -0.03147846919171784),\n        4: (1.0829659757348267, -0.08734153465469903),\n        5: (2.842998459668121, -0.04324831870633425),\n    },\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts validation losses for each domain based on mixture proportions using a power-law scaling.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_i' for i in 1..5.\n        group: Experimental group name (e.g., '70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_i' containing predicted losses.\n    \"\"\"\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(PARAMS.keys())}\")\n    coeffs = PARAMS[group]\n    predictions = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            a, b = coeffs[i]\n            p = row.get(f'proportion_domain_{i}')\n            if p is None:\n                raise KeyError(f\"Input data missing 'proportion_domain_{i}'\")\n            # power-law prediction\n            out[f'loss_domain_{i}'] = a * (p ** b)\n        predictions.append(out)\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "o4-mini", "reward_r2": -1.0, "solution": "\"\"\"\nScaling law prediction function.\nDiscovered form: loss = a * proportion^b + c\nCoefficients (per group and domain) embedded.\n\"\"\"\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts loss for each domain based on mixture proportion using the scaling law.\n\n    Args:\n        input_data: List of dicts, each containing keys 'proportion_domain_1'... 'proportion_domain_5'.\n        group: Experimental group name ('70M', '160M', '305M', or '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_1'... 'loss_domain_5'.\n    \"\"\"\n    # Precomputed coefficients: a, b, c for each domain\n    coeffs = {\n        \"70M\": {\n            \"domain_1\": {\"a\": 0.01345935217392868, \"b\": -1.2494096276943556, \"c\": 2.6205794786947907},\n            \"domain_2\": {\"a\": 0.004993332921529321, \"b\": -1.0911516501426637, \"c\": 3.6100070391814723},\n            \"domain_3\": {\"a\": 0.00044534115466757863, \"b\": -1.8023950929710377, \"c\": 3.0742037259615387},\n            \"domain_4\": {\"a\": 0.0013839903291475184, \"b\": -1.8680188768809969, \"c\": 1.588180693069307},\n            \"domain_5\": {\"a\": 0.004960131366262632, \"b\": -2.106904001314134, \"c\": 3.462343661422903}\n        },\n        \"160M\": {\n            \"domain_1\": {\"a\": 0.005223819076583144, \"b\": -1.5760633456244681, \"c\": 2.333516554006229},\n            \"domain_2\": {\"a\": 0.018538103040269167, \"b\": -0.5723997798081769, \"c\": 3.2651763761104062},\n            \"domain_3\": {\"a\": 0.0004398425225265252, \"b\": -1.8019610984309542, \"c\": 2.7797504131610573},\n            \"domain_4\": {\"a\": 0.0029902400448121756, \"b\": -1.5481918524361098, \"c\": 1.3486876031353137},\n            \"domain_5\": {\"a\": 0.005481054936761751, \"b\": -2.075664514028258, \"c\": 3.1112351190476195}\n        },\n        \"305M\": {\n            \"domain_1\": {\"a\": 0.011864288528888557, \"b\": -1.22567788447283, \"c\": 2.185522820374434},\n            \"domain_2\": {\"a\": 0.012171471112974626, \"b\": -0.7364570533455278, \"c\": 3.1095881583121825},\n            \"domain_3\": {\"a\": 0.00038056373793330696, \"b\": -1.834588448547919, \"c\": 2.638818359375},\n            \"domain_4\": {\"a\": 0.0004109153221451477, \"b\": -2.281718804345559, \"c\": 1.256210679661716},\n            \"domain_5\": {\"a\": 0.007337127428867711, \"b\": -1.893167760790138, \"c\": 2.9427326920351473}\n        },\n        \"410M\": {\n            \"domain_1\": {\"a\": 0.007763706139059348, \"b\": -1.3765396889516939, \"c\": 2.1338753273640996},\n            \"domain_2\": {\"a\": 0.009925468816668701, \"b\": -0.7686654599189278, \"c\": 3.040671101681472},\n            \"domain_3\": {\"a\": 0.000621910962969547, \"b\": -1.681240873197639, \"c\": 2.571829458383413},\n            \"domain_4\": {\"a\": 0.0016201279671757462, \"b\": -1.748267976324254, \"c\": 1.2112767184921616},\n            \"domain_5\": {\"a\": 0.0032863494213504533, \"b\": -2.3772423234520086, \"c\": 2.86961949493339}\n        }\n    }\n    if group not in coeffs:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = coeffs[group]\n    predictions = []\n    for entry in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = entry.get(f'proportion_domain_{i}', 0.0)\n            a = params[f'domain_{i}']['a']\n            b = params[f'domain_{i}']['b']\n            c = params[f'domain_{i}']['c']\n            # Compute scaling law prediction\n            try:\n                pred[f'loss_domain_{i}'] = a * (p ** b) + c\n            except Exception:\n                pred[f'loss_domain_{i}'] = float('nan')\n        predictions.append(pred)\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "openhands", "model_name": "DeepSeek-V3.2-reasoning", "reward_r2": -1.0, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    import math\n    \n    # Fitted parameters for each group and domain.\n    # The structure: params[group][domain] = (intercept, coeff_log_prop, coeff_other_props_list)\n    # For domain i, the model is:\n    #   log(loss_i) = intercept + coeff_log_prop * log(proportion_i) + sum_{j!=i} coeff_other_props[k] * proportion_j\n    # where k indexes the other domains in order 1..5 (skipping i).\n    \n    params = {\n        '160M': {\n            1: (0.7833111038, -0.0402786347, [0.0912184552, 0.1068224498, 0.0812000158, 0.0653049753]),\n            2: (2.2492950825, -0.2741726925, [-1.8338729843, -1.8419387947, -1.8129411557, -1.8804824561]),\n            3: (0.0564101912, 0.0238035455, [1.1670604404, 1.1402007955, 1.1646792046, 1.1238881078]),\n            4: (0.0906714462, -0.0636907107, [0.1736209965, 0.2185072027, 0.1762865172, 0.1599946777]),\n            5: (1.1228678945, -0.0293363470, [0.0305269549, 0.0225717669, 0.0270728741, 0.0242907798]),\n        },\n        '305M': {\n            1: (0.7304779650, -0.0447141164, [0.0723279282, 0.1063034222, 0.0462948884, 0.0440165074]),\n            2: (0.4428432618, 0.1286117688, [1.1628232105, 1.1585308411, 1.1738698450, 1.1107456140]),\n            3: (-0.1061667057, 0.0322450117, [1.3122632380, 1.2939195211, 1.3023673626, 1.2668419380]),\n            4: (0.0340802509, -0.0700722733, [0.1267262336, 0.1819456223, 0.1536312260, 0.1207205545]),\n            5: (1.0702681738, -0.0373910763, [0.0131496196, 0.0074083668, 0.0124928399, 0.0046385109]),\n        },\n        '410M': {\n            1: (0.6972040078, -0.0443869518, [0.0954638183, 0.0645940941, 0.0773371879, 0.0503260941]),\n            2: (-0.0657555465, 0.2435245209, [1.9965293996, 1.9734092160, 2.0250391329, 1.9451754386]),\n            3: (-0.0475965193, 0.0248594403, [1.2067481889, 1.1725598696, 1.2007621976, 1.1551819732]),\n            4: (0.0365064485, -0.0800197148, [0.0603258709, 0.1420420825, 0.0576457840, 0.0576871365]),\n            5: (1.0409566623, -0.0331349649, [0.0267617634, 0.0320288953, 0.0042076465, 0.0272967701]),\n        },\n        '70M': {\n            1: (0.9101144519, -0.0429647243, [0.0705110055, 0.0893060363, 0.0584434242, 0.0527811612]),\n            2: (0.8397054645, 0.0709814360, [0.7289539413, 0.6915313019, 0.7397147774, 0.6875000000]),\n            3: (0.4282782001, 0.0059402138, [0.8299496047, 0.7770592599, 0.8056670941, 0.7803879986]),\n            4: (0.3683151287, -0.0865753489, [-0.0264035861, -0.0042538750, -0.0003404132, -0.0375423314]),\n            5: (1.2364780752, -0.0373359187, [-0.0022038097, -0.0122877413, -0.0185141866, -0.0119700068]),\n        },\n    }\n    \n    if group not in params:\n        raise ValueError(f'Unknown group: {group}')\n    \n    predictions = []\n    for data_point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            param = params[group][i]\n            if param is None:\n                pred[f'loss_domain_{i}'] = 0.0\n                continue\n            \n            intercept, coeff_log, coeff_other = param\n            prop_i = data_point.get(f'proportion_domain_{i}', 0.0)\n            if prop_i <= 0:\n                log_prop_i = math.log(1e-10)\n            else:\n                log_prop_i = math.log(prop_i)\n            \n            # Build the list of other proportions in order 1..5 (skipping i)\n            other_props = []\n            for j in range(1, 6):\n                if j != i:\n                    other_props.append(data_point.get(f'proportion_domain_{j}', 0.0))\n            \n            log_loss = intercept + coeff_log * log_prop_i\n            for k, prop_j in enumerate(other_props):\n                log_loss += coeff_other[k] * prop_j\n            \n            loss = math.exp(log_loss)\n            pred[f'loss_domain_{i}'] = loss\n        \n        predictions.append(pred)\n    \n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "aider", "model_name": "gpt-5", "reward_r2": -1.0, "solution": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, List, Tuple\n\nimport numpy as np\n\n# Try to import datasets lazily and robustly.\ntry:\n    from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore\nexcept Exception:  # pragma: no cover - optional dependency\n    load_from_disk = None  # type: ignore\n    Dataset = None  # type: ignore\n    DatasetDict = None  # type: ignore\n\n\n@dataclass\nclass FittedModel:\n    # theta: shape (n_targets, n_features), where n_features = 1 (intercept) + n_proportions\n    theta: np.ndarray\n    proportion_keys: List[str]  # ordered list of feature keys used during fitting\n    target_keys: List[str]      # ordered list of target keys used during fitting\n\n\n# Global store of fitted coefficients per group.\n_FITTED_BY_GROUP: Dict[str, FittedModel] = {}\n# Numerical stabilizers\n_EPS_P: float = 1e-12\n_EPS_L: float = 1e-12\n_RIDGE: float = 1e-3\n\n\ndef _safe_log(x: np.ndarray, eps: float) -> np.ndarray:\n    return np.log(np.clip(x, eps, None))\n\n\ndef _detect_keys(columns: Iterable[str]) -> Tuple[List[str], List[str], str | None]:\n    cols = list(columns)\n    # Proportion keys like \"proportion_domain_1\" ... \"proportion_domain_5\"\n    prop_keys = sorted(\n        [c for c in cols if re.fullmatch(r\"proportion_domain_\\d+\", c)],\n        key=lambda k: int(k.rsplit(\"_\", 1)[1]),\n    )\n    # Target keys like \"loss_domain_1\" ... \"loss_domain_5\"\n    tgt_keys = sorted(\n        [c for c in cols if re.fullmatch(r\"loss_domain_\\d+\", c)],\n        key=lambda k: int(k.rsplit(\"_\", 1)[1]),\n    )\n    # Group column\n    group_col: str | None = None\n    if \"group\" in cols:\n        group_col = \"group\"\n    else:\n        # Fallback to any column ending with '_group' or named 'Group'\n        for cand in cols:\n            if cand.lower().ends_with(\"_group\") or cand == \"Group\":\n                group_col = cand\n                break\n    return prop_keys, tgt_keys, group_col\n\n\ndef _dataset_to_rows(ds_obj) -> List[Dict[str, float]]:\n    # Convert Dataset or DatasetDict to a list of dict rows\n    rows: List[Dict[str, float]] = []\n    if DatasetDict is not None and isinstance(ds_obj, DatasetDict):\n        for split in ds_obj.values():\n            rows.extend(_dataset_to_rows(split))\n        return rows\n    # ds_obj is a Dataset or something iterable over dicts\n    try:\n        # Iterating a datasets.Dataset yields dicts row-wise efficiently\n        for row in ds_obj:  # type: ignore\n            rows.append(row)\n    except Exception:\n        # Fallback: try to_dict\n        try:\n            data_dict = ds_obj.to_dict()  # type: ignore\n            n = len(next(iter(data_dict.values())))\n            for i in range(n):\n                rows.append({k: v[i] for k, v in data_dict.items()})\n        except Exception:\n            pass\n    return rows\n\n\ndef _fit_group(rows: List[Dict[str, float]], proportion_keys: List[str], target_keys: List[str]) -> FittedModel:\n    # Build X (design) and Y (targets)\n    X_list: List[List[float]] = []\n    Y_lists: List[List[float]] = [[] for _ in target_keys]\n\n    for r in rows:\n        try:\n            p_vec = np.array([float(r[k]) for k in proportion_keys], dtype=float)\n            if np.any(~np.isfinite(p_vec)):\n                continue\n            # Require all targets present and finite\n            y_vals = []\n            valid = True\n            for tk in target_keys:\n                val = float(r[tk])\n                if not (np.isfinite(val) and val > 0):\n                    valid = False\n                    break\n                y_vals.append(val)\n            if not valid:\n                continue\n        except Exception:\n            continue\n\n        x = [1.0]\n        x.extend(_safe_log(p_vec, _EPS_P).tolist())\n        X_list.append(x)\n        for i, y in enumerate(y_vals):\n            Y_lists[i].append(float(y))\n\n    if not X_list or any(len(yc) == 0 for yc in Y_lists):\n        # Fallback: identity-like tiny coefficients to keep the model running\n        n_features = 1 + len(proportion_keys)\n        theta = np.zeros((len(target_keys), n_features), dtype=float)\n        # Intercepts default to log(1.0) = 0\n        return FittedModel(theta=theta, proportion_keys=proportion_keys, target_keys=target_keys)\n\n    X = np.asarray(X_list, dtype=float)  # shape (n_samples, n_features)\n    n_features = X.shape[1]\n    XtX = X.T @ X\n    reg = np.eye(n_features, dtype=float)\n    reg[0, 0] = 0.0  # do not regularize intercept\n    XtX_reg = XtX + _RIDGE * reg\n\n    theta_rows: List[np.ndarray] = []\n    Xt = X.T\n    for y_vals in Y_lists:\n        y = _safe_log(np.asarray(y_vals, dtype=float), _EPS_L)\n        Xty = Xt @ y\n        try:\n            beta = np.linalg.solve(XtX_reg, Xty)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.lstsq(XtX_reg, Xty, rcond=None)[0]\n        theta_rows.append(beta)\n\n    theta = np.vstack(theta_rows)  # (n_targets, n_features)\n    return FittedModel(theta=theta, proportion_keys=proportion_keys, target_keys=target_keys)\n\n\ndef _fit_all_groups() -> None:\n    global _FITTED_BY_GROUP\n    if _FITTED_BY_GROUP:\n        return  # already fit\n\n    # Attempt to load the dataset from disk\n    rows: List[Dict[str, float]] = []\n    if load_from_disk is not None:\n        try:\n            ds = load_from_disk(\"/app/data\")\n            rows = _dataset_to_rows(ds)\n        except Exception:\n            rows = []\n    # If no rows, leave empty and we will populate a default model\n    all_columns = set()\n    for r in rows:\n        all_columns.update(r.keys())\n    prop_keys, tgt_keys, group_col = _detect_keys(all_columns)\n\n    # Ensure we have expected 5 domains; if not, try to infer from any present keys\n    if not prop_keys:\n        # Default to proportion_domain_1..5 if not present\n        prop_keys = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n    if not tgt_keys:\n        tgt_keys = [f\"loss_domain_{i}\" for i in range(1, 6)]\n\n    if not rows:\n        # Build a default \"ALL\" model\n        _FITTED_BY_GROUP[\"ALL\"] = _fit_group([], prop_keys, tgt_keys)\n        _write_explain_md(_FITTED_BY_GROUP)\n        return\n\n    # Partition rows by group (or single ALL group)\n    grouped: Dict[str, List[Dict[str, float]]] = {}\n    if group_col is None:\n        grouped[\"ALL\"] = rows\n    else:\n        for r in rows:\n            g = r.get(group_col, \"ALL\")\n            grouped.setdefault(str(g), []).append(r)\n\n    # Fit per group\n    for g, gr_rows in grouped.items():\n        _FITTED_BY_GROUP[g] = _fit_group(gr_rows, prop_keys, tgt_keys)\n\n    # Also fit a global ALL group across everything for fallback\n    if \"ALL\" not in _FITTED_BY_GROUP:\n        _FITTED_BY_GROUP[\"ALL\"] = _fit_group(rows, prop_keys, tgt_keys)\n\n    # Write explanation file including fitted parameters\n    _write_explain_md(_FITTED_BY_GROUP)\n\n\ndef _write_explain_md(fitted: Dict[str, FittedModel]) -> None:\n    # Prepare a deterministic JSON-like dump of parameters\n    payload = {}\n    for g, fm in fitted.items():\n        payload[g] = {\n            \"proportion_keys\": fm.proportion_keys,\n            \"target_keys\": fm.target_keys,\n            \"theta\": np.asarray(fm.theta, dtype=float).round(8).tolist(),\n            \"model\": \"log-linear power law: log(loss_i) = theta[i,0] + sum_j theta[i,j] * log(p_j + eps); eps = %.0e\"\n            % _EPS_P,\n            \"ridge\": _RIDGE,\n        }\n\n    section = [\n        \"<!-- PARAMS START -->\",\n        \"Fitted parameter tensors by group (JSON):\",\n        \"\",\n        \"```json\",\n        json.dumps(payload, indent=2),\n        \"```\",\n        \"<!-- PARAMS END -->\",\n    ]\n    section_text = \"\\n\".join(section) + \"\\n\"\n\n    path = \"/app/explain.md\"\n    # If explain.md exists, replace the PARAM section; else, create a full file.\n    try:\n        if os.path.exists(path):\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            new_content: str\n            if \"<!-- PARAMS START -->\" in content and \"<!-- PARAMS END -->\" in content:\n                new_content = re.sub(\n                    r\"<!-- PARAMS START -->.*?<!-- PARAMS END -->\",\n                    section_text,\n                    content,\n                    flags=re.DOTALL,\n                )\n            else:\n                new_content = content.rstrip() + \"\\n\\n\" + section_text\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(new_content)\n        else:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(_default_explain_md_header().rstrip() + \"\\n\\n\" + section_text)\n    except Exception:\n        # Best-effort; ignore file writing errors.\n        pass\n\n\ndef _default_explain_md_header() -> str:\n    return \"\"\"# Scaling law for domain mixture in language model pre-training\n\nThis document describes the discovered scaling law that predicts per-domain validation loss from the domain mixture proportions used during pre-training.\n\n## Functional form (shared across groups)\n\nWe model each domain's validation loss as a multiplicative power-law function of the mixture proportions:\n\n- For domain i in {1..5}, with mixture proportions p_j for j in {1..5}:\n  loss_i = A_i * Π_j (p_j + ε)^{β_{i,j}}\n\nEquivalently in log-space (which is what we fit):\n\n- log(loss_i) = θ_{i,0} + Σ_j θ_{i,j} * log(p_j + ε)\n  where A_i = exp(θ_{i,0}) and β_{i,j} = θ_{i,j}.\n\nWe fix ε = 1e-12 for numerical stability when a proportion is zero.\nWe estimate parameters with ridge-regularized least squares on the log-transformed variables, with L2 regularization λ = 1e-3 applied to non-intercept weights.\n\nThis form captures:\n- Diminishing returns (via negative exponents).\n- Cross-domain transfer (exponents β_{i,j} coupling domains).\n- Scale invariance with respect to multiplicative changes in the mixture.\n\n## Methodology\n\n- Load the dataset at /app/data using datasets.load_from_disk().\n- Identify input features: proportion_domain_1..proportion_domain_5.\n- Identify targets: loss_domain_1..loss_domain_5.\n- If a 'group' column is present, fit a separate parameter set per group; otherwise, fit a single ALL group.\n- Optimize θ for each target independently using ridge regression in log space.\n- Use the fitted θ to predict losses for new inputs by exponentiating the linear predictor.\n\n## Fitted parameters per group\n\nThe exact fitted coefficients depend on the dataset available at runtime.\nThe block below is automatically populated by /app/law.py when the module is imported and the model is fit.\n\"\"\"\n\n\n# Fit on import so that law() can immediately use the parameters and /app/explain.md is populated.\n_fit_all_groups()\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Ensure models are fit (idempotent)\n    _fit_all_groups()\n\n    # Choose fitted coefficients for the requested group; fallback to \"ALL\"\n    fm = _FITTED_BY_GROUP.get(group) or _FITTED_BY_GROUP.get(\"ALL\")\n    if fm is None:\n        # Last-resort fallback with default keys and zero coefficients\n        proportion_keys = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n        target_keys = [f\"loss_domain_{i}\" for i in range(1, 6)]\n        fm = FittedModel(theta=np.zeros((len(target_keys), 1 + len(proportion_keys))),  # type: ignore\n                         proportion_keys=proportion_keys,\n                         target_keys=target_keys)\n\n    # Build predictions\n    out: List[Dict[str, float]] = []\n    prop_keys = fm.proportion_keys\n    theta = np.asarray(fm.theta, dtype=float)\n\n    # If incoming dicts have a different set of proportion keys, try to realign\n    # to the canonical order based on numeric suffix.\n    def canonicalize_keys(keys: List[str]) -> List[str]:\n        return sorted(keys, key=lambda k: int(k.rsplit(\"_\", 1)[1]) if re.fullmatch(r\".*_\\d+\", k) else math.inf)\n\n    for row in input_data:\n        # If row has all canonical keys, use them; else, try to align using suffix.\n        if not all(k in row for k in prop_keys):\n            candidate = [k for k in row.keys() if re.fullmatch(r\"proportion_domain_\\d+\", k)]\n            if candidate:\n                prop_keys_runtime = canonicalize_keys(candidate)\n            else:\n                prop_keys_runtime = prop_keys  # fall back\n        else:\n            prop_keys_runtime = prop_keys\n\n        p_vec = np.array([float(row.get(k, 0.0)) for k in prop_keys_runtime], dtype=float)\n        x = np.empty(1 + p_vec.size, dtype=float)\n        x[0] = 1.0\n        x[1:] = _safe_log(p_vec, _EPS_P)\n\n        # Predict each target independently in log-space then exponentiate.\n        yhat_log = theta @ x  # shape (n_targets,)\n        yhat = np.exp(yhat_log)\n        # Produce outputs with canonical target keys (loss_domain_1..5)\n        pred: Dict[str, float] = {}\n        # Map predictions to fm.target_keys order; also ensure we output exactly loss_domain_1..5\n        for idx, tk in enumerate(fm.target_keys):\n            pred[tk] = float(yhat[idx])\n        # If any expected loss_domain_i missing (e.g., dataset had different naming), fill canonical keys\n        for i in range(1, 6):\n            key = f\"loss_domain_{i}\"\n            if key not in pred and idx < yhat.size:\n                pred[key] = float(yhat[min(i - 1, yhat.size - 1)])\n        out.append(pred)\n\n    return out"}
{"task": "domain_mixture_scaling_law", "agent_name": "gemini-cli", "model_name": "gemini-2.5-flash", "reward_r2": -1.0, "solution": "import json\nimport numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n\n    \"\"\"    \n    epsilon = 1e-6 # Must match the epsilon used during fitting\n    \n    # Fitted parameters for each group and domain\n    _fitted_params = json.loads( + fitted_params_json_str + '')\n\n    if group not in _fitted_params:\n        raise ValueError(f\"Unknown group: {{group}}\")\n\n    predictions = []\n    for data_point in input_data:\n        predicted_losses = {{}}\n        for domain_idx in range(1, 6): # Domains 1 to 5\n            proportion_key = f'proportion_domain_{{domain_idx}}'\n            loss_key = f'loss_domain_{{domain_idx}}'\n            \n            if proportion_key not in data_point:\n                predicted_losses[loss_key] = float('nan') \n                continue\n\n            proportion = data_point[proportion_key]\n            \n            domain_str_idx = str(domain_idx)\n            if domain_str_idx not in _fitted_params[group]:\n                predicted_losses[loss_key] = float('nan') \n                continue\n\n            params = _fitted_params[group][domain_str_idx]\n            A = params['A']\n            B = params['B']\n            C = params['C']\n\n            if np.isnan(A) or np.isnan(B) or np.isnan(C):\n                predicted_losses[loss_key] = float('nan') # No valid parameters\n                continue\n\n            # Apply the scaling law, ensuring (proportion + epsilon) is positive\n            predicted_loss = A * (np.maximum(proportion + epsilon, epsilon))**B + C\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n    return predictions"}
{"task": "domain_mixture_scaling_law", "agent_name": "terminus-2", "model_name": "gpt-5", "reward_r2": -1.0, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Embedded fitted parameters per group and domain.\n# Each domain uses either a power form: y = a + k * p**c\n# or a log form: y = a + b * log(p)\nPARAMS = {'160M': {'1': {'a': 3.358131379887285,\n                'c': 0.15000000000000036,\n                'form': 'power',\n                'k': -1.1267042149203725,\n                'r2': 0.9941862658661204},\n          '2': {'a': 3.318780734454316,\n                'c': -3.0,\n                'form': 'power',\n                'k': 2.2734784837905097e-05,\n                'r2': 0.06062614424441071},\n          '3': {'a': 2.931566771454378, 'c': 3.0, 'form': 'power', 'k': -174.62340395796082, 'r2': 0.7061503010752743},\n          '4': {'a': 3.8716421496206315,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.6874577135715296,\n                'r2': 0.9762826201636258},\n          '5': {'a': 3.876397587476255,\n                'c': 0.20000000000000018,\n                'form': 'power',\n                'k': -0.8015051387810923,\n                'r2': 0.995953951272856}},\n '305M': {'1': {'a': 4.832122416331462,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.721490588848711,\n                'r2': 0.9923678438528458},\n          '2': {'a': 3.1584886677226094,\n                'c': -2.75,\n                'form': 'power',\n                'k': 4.513541219029818e-05,\n                'r2': 0.066949255913345},\n          '3': {'a': 2.7848933137166916, 'c': 3.0, 'form': 'power', 'k': -166.12136907423988, 'r2': 0.6853980994244002},\n          '4': {'a': -0.9540330670713865,\n                'c': -0.04999999999999982,\n                'form': 'power',\n                'k': 2.067152378143916,\n                'r2': 0.9773900180368574},\n          '5': {'a': 5.599492209154898,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.6836119274365684,\n                'r2': 0.9937674933260627}},\n '410M': {'1': {'a': 4.670978316018143,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.613131878961905,\n                'r2': 0.9885683848277665},\n          '2': {'a': 3.091279676924703,\n                'c': -3.0,\n                'form': 'power',\n                'k': 1.7491883130243175e-05,\n                'r2': 0.03333320137761886},\n          '3': {'a': 2.7201830236984295, 'c': 3.0, 'form': 'power', 'k': -168.02135230322037, 'r2': 0.6489941549529987},\n          '4': {'a': 0.5427391497008389,\n                'c': -0.1499999999999999,\n                'form': 'power',\n                'k': 0.5526416189568344,\n                'r2': 0.9732551074881751},\n          '5': {'a': 3.634036329340057,\n                'c': 0.20000000000000018,\n                'form': 'power',\n                'k': -0.8017750604189218,\n                'r2': 0.9939499265653914}},\n '70M': {'1': {'a': 4.297380991024045,\n               'c': 0.10000000000000009,\n               'form': 'power',\n               'k': -1.775827036691083,\n               'r2': 0.9952894285760332},\n         '2': {'a': 3.646005195575224,\n               'c': -2.6,\n               'form': 'power',\n               'k': 8.526182739288986e-05,\n               'r2': 0.12259002392235419},\n         '3': {'a': 3.2284514219048646, 'c': 3.0, 'form': 'power', 'k': -180.79876453488362, 'r2': 0.6915505904622448},\n         '4': {'a': 1.1444240089156006, 'c': -0.25, 'form': 'power', 'k': 0.3211850398410141, 'r2': 0.9856867978166857},\n         '5': {'a': 2.3099069373584133,\n               'c': -0.09999999999999964,\n               'form': 'power',\n               'k': 1.1287948701911399,\n               'r2': 0.9930440593027846}}}\n\n# Default fallback params (averaged) if group not present\nFALLBACK = {'1': {'a': 4.289653275815233, 'c': 0.08750000000000024, 'form': 'power', 'k': -2.059288429855518},\n '2': {'a': 3.3036385686692133, 'c': -2.8375, 'form': 'power', 'k': 4.265597688783408e-05},\n '3': {'a': 2.9162736326935907, 'c': 3.0, 'form': 'power', 'k': -172.39122246757617},\n '4': {'a': 1.151193060291421, 'c': -0.09999999999999987, 'form': 'power', 'k': 0.06338033084255867},\n '5': {'a': 3.8549582658324058, 'c': 0.08750000000000024, 'form': 'power', 'k': -0.7895243141113606}}\n\nDOMAINS = [1,2,3,4,5]\n\ndef _predict_domain(p: float, spec: dict) -> float:\n    # Guard against non-positive proportions for log and power\n    p = max(p, 1e-12)\n    form = spec.get('form')\n    if form == 'power':\n        return spec['a'] + spec['k'] * (p ** spec['c'])\n    elif form == 'log':\n        return spec['a'] + spec['b'] * math.log(p)\n    else:\n        # Should not happen; fall back to identity-like\n        return float('nan')\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: list of dicts with keys proportion_domain_1..5\n        group: group name. Functional form is same across groups (per-domain chosen from {power, log}); coefficients differ per group.\n\n    Returns:\n        list of dicts with keys loss_domain_1..5\n    \"\"\"\n    # Select group-specific params or fallback\n    spec = PARAMS.get(group, FALLBACK)\n    outputs = []\n    for row in input_data:\n        out = {}\n        for i in DOMAINS:\n            p = float(row.get(f'proportion_domain_{i}', 0.0))\n            out[f'loss_domain_{i}'] = _predict_domain(p, spec[str(i)])\n        outputs.append(out)\n    return outputs"}
{"task": "domain_mixture_scaling_law", "agent_name": "human", "model_name": "human", "reward_r2": 0.6711173709701893, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on the Domain Mixture Scaling Law.\n    Formula: L_i(r) = c_i + k_i * exp(sum_j(t_ij * r_j))\n    \"\"\"\n    PARAMS_BY_GROUP = {\n        '160M': [2.0856592097019804, 3.4111663782049946, 2.373130628164121, 1.115739002392498, 3.1434027050275315, 0.11985664504599375, -1.290497011833889, 0.8561546902084409, 0.0011385760402376783, 0.10773090897017512, -1.2957351800303398, 2.2746028871034647, 2.705623972785463, 2.316064861889176, 1.7981591134647914, 0.16002919107434516, -0.9036553611667323, -11.721095107645542, -10.97207094938961, -3.941965495927712, 0.3331004993016095, -0.8238306927391251, -7.9040599335438575, 0.22865320483207682, -0.015871079782087898, 6.356978958687983, 6.947613311543433, 6.828600154978058, 0.4033417288280921, 6.46043211334845, 1.1465685324225992, 4.224797824743394, -0.0873484493498842, 0.6586482763159163, -7.468434356093965],\n        '305M': [-0.406920519860249, -0.023482654768083127, 1.7584418964771162, 0.6696472032560989, 2.9621025231919225, 2.476937360825574, 3.942936207746321, 1.4901046642087359, 0.2742018286545925, 0.47308511801263603, -0.2365071111423701, 0.39095328391845285, 0.09758796539876204, 0.3506186812670231, 0.1972447482527336, -0.1387772197575137, -0.4345549940094546, -0.22027486679158964, -0.09838164990815944, -0.1903816656628244, 0.011876178764682731, -0.12147137554315933, -5.287503842834289, -0.0011619872771040376, -0.13970271686042884, 1.318799534922482, 1.5105795629915502, 1.456323279420494, -1.7786459949928333, 1.3304983402239023, 0.4281155226864253, 0.6804235220347148, -0.7530897211290078, -0.9933045226536323, -6.868138266660191],\n        '410M': [2.4216190161135236, -0.9949865207367925, 1.0485818312964124, 2.548675892146101, 2.7426928782936613, -1.9627602023940005, 1.402807376566214, 0.7417553689817263, -1.4145523926963819, 0.15508511422938762, -3.3903042944676276, -2.2610029749001375, -1.648920829689558, -2.5443645331878497, -8.53221429654514, 1.1289545539226238, 0.8980442879362072, 1.0234700780117505, 1.1650251900973105, 1.0860734000230958, 1.1534440167385025, 0.780721755668409, -2.212355347968287, 0.9803499956087594, 0.9766445881845902, -0.2419034983363214, -0.8751331448570513, -0.6202279643467626, 1.1901651070277095, -0.35615506056271157, 0.022752946896432296, 3.6721657643208983, -0.779545468876064, 1.776335708316458, -0.6357803386102799],\n        '70M': [1.397924353380805, 3.3600165973585336, 3.185343596637166, 0.4140345749996855, 2.686572212421311, 1.1495633562636562, 0.33362402415104286, 0.11913150425199642, 2.09383061022326, 0.7647608278389397, -0.5528008166747819, 0.7482607813260194, 0.3093620764866142, 0.6870023027892853, 0.40095800858226355, 0.2667574293187739, -0.6615453308993259, -2.4866382361709993, 1.0054990681290112, 0.10160933605146799, 2.165839466984036, -5.798339327373564, 0.914108528350298, 0.08116908389843563, 0.29052400025512765, -0.22540170902766082, -0.07188913043349504, -0.2619900024414107, -2.1812489268120494, -0.20593534390953777, 0.3037987618175764, 0.1976238605669583, 0.3676837669324082, 0.5677517222931325, -0.09463528681761862],\n    }\n\n    if group in PARAMS_BY_GROUP:\n        flat_params = np.array(PARAMS_BY_GROUP[group])\n    else:\n        flat_params = np.array(PARAMS_BY_GROUP['160M'])\n\n    num_domains = 5\n    c = flat_params[0:num_domains]\n    k = flat_params[num_domains:2*num_domains]\n    t = flat_params[2*num_domains:].reshape((num_domains, num_domains))\n    \n    predictions = []\n    for point in input_data:\n        r = np.array([point[f\"proportion_domain_{i+1}\"] for i in range(num_domains)])\n        \n        exponent = np.dot(r, t.T)\n        losses = c + k * np.exp(exponent)\n        \n        pred_dict = {f\"loss_domain_{i+1}\": losses[i] for i in range(num_domains)}\n        predictions.append(pred_dict)\n        \n    return predictions"}
