{"model_name": "gpt-5", "reward_r2": 0.9904279621411324, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The law models each domain's validation loss as the sum of:\n      - a group- and domain-specific intercept a_i,\n      - a group- and domain-specific coefficient b_i times log(p_i + eps), capturing\n        diminishing returns from allocating more mixture proportion to the same domain,\n      - plus a linear combination of the proportions of the other domains (j != i),\n        with group- and domain-specific coefficients c_{i,j}.\n\n    Mathematically, for domain i in {1..5}:\n        loss_i = a_i + b_i * log(p_i + eps) + sum_{j != i} c_{i,j} * p_j\n\n    where p_k are the mixture proportions (sum_k p_k = 1), and eps is a small constant\n    to handle zero proportions inside the logarithm.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: One of the experimental groups. The same functional form is used for all\n               groups, with coefficients differing per group.\n\n    Returns:\n        A list of dicts with keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n\n    # Small constant to avoid log(0)\n    EPS = 1e-6\n\n    # Coefficients fitted per group on the provided dataset (/app/data), using the\n    # model: loss_i = a_i + b_i * log(p_i + EPS) + sum_{j != i} c_{i,j} * p_j\n    # For convenience, linear coefficients are stored as a full 5-length vector per domain\n    # with 0.0 for the self-domain (j == i) entry.\n    COEFFS = {\n        \"70M\": {\n            1: {\"a\": 2.352400, \"b\": -0.041342, \"c\": [0.000000, 0.552302, 0.679733, 0.457510, 0.478500]},\n            2: {\"a\": 3.119185, \"b\": -0.005609, \"c\": [0.733329, 0.000000, 0.567223, 0.760307, 0.571576]},\n            3: {\"a\": 1.557687, \"b\": -0.029500, \"c\": [1.776484, 1.574088, 0.000000, 1.672027, 1.590520]},\n            4: {\"a\": 1.005729, \"b\": -0.040741, \"c\": [0.682161, 0.804593, 0.768164, 0.000000, 0.680742]},\n            5: {\"a\": 3.401418, \"b\": -0.019938, \"c\": [0.282951, 0.204621, 0.280657, 0.244292, 0.000000]},\n        },\n        \"160M\": {\n            1: {\"a\": 2.084419, \"b\": -0.039436, \"c\": [0.000000, 0.515541, 0.590549, 0.410446, 0.414215]},\n            2: {\"a\": 2.848965, \"b\": -0.005760, \"c\": [0.664815, 0.000000, 0.533358, 0.698111, 0.486927]},\n            3: {\"a\": 1.375788, \"b\": -0.028472, \"c\": [1.645880, 1.472320, 0.000000, 1.592583, 1.466833]},\n            4: {\"a\": 0.822570, \"b\": -0.036176, \"c\": [0.633280, 0.747330, 0.680942, 0.000000, 0.623930]},\n            5: {\"a\": 3.044954, \"b\": -0.020112, \"c\": [0.288934, 0.234711, 0.313982, 0.265677, 0.000000]},\n        },\n        \"305M\": {\n            1: {\"a\": 1.965386, \"b\": -0.039011, \"c\": [0.000000, 0.461256, 0.591688, 0.362942, 0.378769]},\n            2: {\"a\": 2.675656, \"b\": -0.004898, \"c\": [0.681773, 0.000000, 0.558797, 0.717652, 0.506549]},\n            3: {\"a\": 1.389474, \"b\": -0.030900, \"c\": [1.455301, 1.326467, 0.000000, 1.424874, 1.288538]},\n            4: {\"a\": 0.758123, \"b\": -0.034855, \"c\": [0.586244, 0.671620, 0.645107, 0.000000, 0.580221]},\n            5: {\"a\": 2.880988, \"b\": -0.021162, \"c\": [0.278675, 0.225879, 0.321137, 0.249162, 0.000000]},\n        },\n        \"410M\": {\n            1: {\"a\": 1.904173, \"b\": -0.038724, \"c\": [0.000000, 0.497929, 0.520547, 0.389682, 0.371875]},\n            2: {\"a\": 2.648743, \"b\": -0.005145, \"c\": [0.632228, 0.000000, 0.458498, 0.688205, 0.451025]},\n            3: {\"a\": 1.311117, \"b\": -0.031575, \"c\": [1.474932, 1.346313, 0.000000, 1.429078, 1.297670]},\n            4: {\"a\": 0.726224, \"b\": -0.033638, \"c\": [0.560347, 0.717670, 0.657147, 0.000000, 0.569629]},\n            5: {\"a\": 2.802291, \"b\": -0.021963, \"c\": [0.276436, 0.261534, 0.247464, 0.274675, 0.000000]},\n        },\n    }\n\n    # Fallback: if an unknown group is provided, use the closest available group\n    # by parameterization (default to the smallest model \"70M\").\n    params_by_group = COEFFS.get(group)\n    if params_by_group is None:\n        params_by_group = COEFFS[\"70M\"]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Read proportions in a fixed order\n        p = [float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)]\n        # Normalize defensively in case inputs are not perfectly normalized\n        s = sum(p)\n        if s > 0:\n            p = [pi / s for pi in p]\n\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            par = params_by_group[i]\n            a = par[\"a\"]\n            b = par[\"b\"]\n            c = par[\"c\"]  # length-5, zero at index i-1\n            log_term = math.log(max(p[i - 1], 0.0) + EPS)\n            linear_term = sum(c[j] * p[j] for j in range(5))\n            y = a + b * log_term + linear_term\n            pred[f\"loss_domain_{i}\"] = float(y)\n\n        outputs.append(pred)\n\n    return outputs", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.9894232423883015, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Fitted parameters for each group\n    # Model: L_i = A * (sum_j T_ij * p_j)^(-alpha) + C\n    PARAMS = {\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.4238451169174666,\n      \"alpha\": 0.09398513070233284,\n      \"C\": 1.1250493207278296,\n      \"T\": [\n        1.0,\n        0.0028653843430912034,\n        0.0,\n        0.006560914041313572,\n        0.007218091947616502\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.6028897145876133,\n      \"alpha\": 0.18249317423847652,\n      \"C\": 1.7098198790278034,\n      \"T\": [\n        0.13978044088307642,\n        1.0,\n        0.2888140021128257,\n        0.09238120511024654,\n        0.2702341876935682\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.3944974988308458,\n      \"alpha\": 0.07536985492525757,\n      \"C\": 1.4540031052227687,\n      \"T\": [\n        0.00042972756094378624,\n        0.004428392340713848,\n        1.0,\n        0.0015108566548276629,\n        0.004618708293329572\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.7156208007948365,\n      \"alpha\": 0.14819891555573603,\n      \"C\": 0.7188126729944071,\n      \"T\": [\n        0.004709723482930698,\n        0.0,\n        0.007159224662733469,\n        1.0,\n        0.006354517927118956\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.6903071983073346,\n      \"alpha\": 0.07574314873996338,\n      \"C\": 1.7459964167978976,\n      \"T\": [\n        0.0,\n        0.1294716976819937,\n        0.09092332659047013,\n        0.014567119409274644,\n        1.0\n      ]\n    }\n  },\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.167794250478621,\n      \"alpha\": 0.09893031045785339,\n      \"C\": 1.0967302669578558,\n      \"T\": [\n        1.0,\n        0.0,\n        0.0,\n        0.004702404391421327,\n        0.006374420575153449\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.520792165137527,\n      \"alpha\": 0.19100210139002716,\n      \"C\": 1.4717790036178258,\n      \"T\": [\n        0.14117673959388272,\n        1.0,\n        0.30984366873545993,\n        0.09323203270576641,\n        0.2930905074144338\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.1827621056082325,\n      \"alpha\": 0.08515955574666216,\n      \"C\": 1.3810689151399624,\n      \"T\": [\n        0.0004496159377778205,\n        0.005469210248664284,\n        1.0,\n        0.00014486145067144012,\n        0.005546525753692289\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.5950466323744031,\n      \"alpha\": 0.15657006642589474,\n      \"C\": 0.6274992666731508,\n      \"T\": [\n        0.0038211623746128476,\n        0.0,\n        0.004047765748103023,\n        1.0,\n        0.006932201209717277\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.5374892790861532,\n      \"alpha\": 0.08558831045269366,\n      \"C\": 1.546810504279026,\n      \"T\": [\n        0.010196510229647623,\n        0.04817027475788599,\n        0.0331566521815975,\n        0.07622206331237742,\n        1.0\n      ]\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.0636633714879822,\n      \"alpha\": 0.1022580547558815,\n      \"C\": 1.0643051121057456,\n      \"T\": [\n        1.0,\n        0.0020628543899588276,\n        0.0,\n        0.005055971810713113,\n        0.005517181906203229\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.5091813171571034,\n      \"alpha\": 0.21086460000369067,\n      \"C\": 1.317332273323408,\n      \"T\": [\n        0.1761437051777317,\n        1.0,\n        0.328313250063059,\n        0.12228845673566739,\n        0.3242993594399039\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.2930356452032414,\n      \"alpha\": 0.06407715343277973,\n      \"C\": 1.1862706116692965,\n      \"T\": [\n        0.00010662729851481288,\n        0.0016648522472207873,\n        1.0,\n        0.0,\n        0.002186394372089585\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.5311234622127226,\n      \"alpha\": 0.16396855241073996,\n      \"C\": 0.5956951080126597,\n      \"T\": [\n        0.0007484858832986562,\n        0.0,\n        0.0020986448436579866,\n        1.0,\n        0.0074776280923507365\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 1.4576330665204935,\n      \"alpha\": 0.08684367375738934,\n      \"C\": 1.4616723046222218,\n      \"T\": [\n        0.0,\n        0.04878929729092485,\n        0.027998535239229693,\n        0.07915718776179,\n        1.0\n      ]\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"A\": 1.0716651256430547,\n      \"alpha\": 0.0979215135041499,\n      \"C\": 1.0023484878829527,\n      \"T\": [\n        1.0,\n        0.0,\n        0.0,\n        0.003578328625604031,\n        0.00520215592199023\n      ]\n    },\n    \"loss_domain_2\": {\n      \"A\": 1.4082722528529894,\n      \"alpha\": 0.21260676990383326,\n      \"C\": 1.3641437773794949,\n      \"T\": [\n        0.16418103324714028,\n        1.0,\n        0.37481870992512745,\n        0.09587346431048725,\n        0.32578174634303375\n      ]\n    },\n    \"loss_domain_3\": {\n      \"A\": 1.3149711554737487,\n      \"alpha\": 0.062094311988635076,\n      \"C\": 1.1032193950712292,\n      \"T\": [\n        0.0,\n        0.001564258864235631,\n        1.0,\n        8.56970983439856e-05,\n        0.0018986297882148968\n      ]\n    },\n    \"loss_domain_4\": {\n      \"A\": 0.49842678668584145,\n      \"alpha\": 0.1778525216820956,\n      \"C\": 0.5822681110448701,\n      \"T\": [\n        0.0060231123550343905,\n        0.0007592880970631944,\n        0.0026719690465761368,\n        1.0,\n        0.0086213744689824\n      ]\n    },\n    \"loss_domain_5\": {\n      \"A\": 3.233311673093248,\n      \"alpha\": 0.04148885832669186,\n      \"C\": -0.39223243148328407,\n      \"T\": [\n        0.01291802719802647,\n        0.0,\n        0.3021193873339115,\n        0.0,\n        1.0\n      ]\n    }\n  }\n}\n\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n        \n    group_params = PARAMS[group]\n    predictions = []\n    \n    for item in input_data:\n        pred_item = {}\n        \n        # Extract proportions vector p\n        # Assuming keys are 'proportion_domain_1' to 'proportion_domain_5'\n        p = np.array([\n            item.get('proportion_domain_1', 0.0),\n            item.get('proportion_domain_2', 0.0),\n            item.get('proportion_domain_3', 0.0),\n            item.get('proportion_domain_4', 0.0),\n            item.get('proportion_domain_5', 0.0)\n        ])\n        \n        # Compute loss for each domain\n        for i in range(1, 6):\n            domain_key = f\"loss_domain_{i}\"\n            if domain_key in group_params:\n                params = group_params[domain_key]\n                A = params[\"A\"]\n                alpha = params[\"alpha\"]\n                C = params[\"C\"]\n                T = np.array(params[\"T\"])\n                \n                # Effective proportion\n                p_eff = np.dot(p, T)\n                # Handle effectively zero\n                p_eff = max(p_eff, 1e-9)\n                \n                loss_pred = A * (p_eff ** -alpha) + C\n                pred_item[domain_key] = loss_pred\n                \n        predictions.append(pred_item)\n        \n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9894230209470385, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import List, Dict\n\n# Fixed feature ordering\nPROP_COLS = [\n    \"proportion_domain_1\",\n    \"proportion_domain_2\",\n    \"proportion_domain_3\",\n    \"proportion_domain_4\",\n    \"proportion_domain_5\",\n]\nLOSS_COLS = [\n    \"loss_domain_1\",\n    \"loss_domain_2\",\n    \"loss_domain_3\",\n    \"loss_domain_4\",\n    \"loss_domain_5\",\n]\n\nEPS = 1e-6\n\n# Coefficients learned per group for the law:\n# loss_i = intercept[i] + sum_j coef_linear[i][j] * p_j + sum_j coef_log[i][j] * log(p_j + EPS)\nCOEFFS: Dict[str, Dict[str, list]] = {\n    \"160M\": {\n        \"intercept\": [\n            2.469311683337708,\n            3.3141620411008277,\n            2.5975875154705848,\n            1.3440867180535057,\n            3.2488739962835567,\n        ],\n        \"coef_linear\": [\n            [-0.39242870031019833, 0.1449840040105368, 0.20870621607378334, 0.012988956774962533, 0.02574952345091407],\n            [0.2073851738757125, -0.5034944849958123, 0.08099673956846185, 0.2119097462425706, 0.0032028253090607807],\n            [0.4222507474605135, 0.33048349239799873, -1.3184032511886987, 0.3397661471062194, 0.22590286422400102],\n            [0.1075280031010361, 0.3278752202366596, 0.018326424467131473, -0.5403846909284411, 0.08665504312361116],\n            [0.1224578633506513, -0.06992992306569604, 0.0648541522733341, 0.08654508830086936, -0.20392718085915945],\n        ],\n        \"coef_log\": [\n            [-0.039451752022555374, -0.0003854857497984469, -2.3239743517545694e-05, 9.268231255609287e-06, -0.0006293642768779598],\n            [-0.0015843455126219829, -0.00597505925571199, -0.00010878237745062993, -0.0007202157067082326, -0.0012285972839189082],\n            [-0.0009734332588850447, -0.001936822498506686, -0.027443305577813045, -0.00024645647285300213, -0.00019926772803499236],\n            [-0.0006024744943890134, -0.002147785787884586, 0.001399812773972361, -0.036472059131277504, 0.00012750772191223904],\n            [-0.001567815576140436, 0.0013055621917748808, 0.0002487312848513498, -0.0008614874408401778, -0.019870896443806487],\n        ],\n    },\n    \"305M\": {\n        \"intercept\": [\n            2.3392247012746834,\n            3.1651345666056483,\n            2.471987105632863,\n            1.2404678308980266,\n            3.0887017193916093,\n        ],\n        \"coef_linear\": [\n            [-0.3945995646360234, 0.04212797569256443, 0.3597852823915539, 0.004507385349434609, -0.011821078797535627],\n            [0.18765244585849242, -0.5607080638027755, 0.16385032928665508, 0.22772936777546302, -0.018524079117843765],\n            [0.36498559947643294, 0.36326950855260254, -1.247281529045098, 0.3544474379638183, 0.16457898305227767],\n            [0.11479489142933053, 0.2241274675743544, 0.07534052854957383, -0.4984903999878992, 0.08422751243463553],\n            [0.1034302676552572, -0.1442429588936119, 0.1542111292796102, 0.10260754874495605, -0.2160059867862149],\n        ],\n        \"coef_log\": [\n            [-0.0389843240976756, 0.0003898475662999871, -0.0012326552175473988, -0.0008170951320506675, -0.0006305864869774297],\n            [-0.0018382099319297328, -0.004966654576883016, -0.0008004862412949112, -0.0016726743862113481, -0.0014239105552697226],\n            [-0.0013605116194238868, -0.0029875971857020777, -0.029138080972677064, -0.0016317402099057068, 0.001163162472447215],\n            [-0.0011594434613557832, -0.0010890215347730992, 0.0008814829783619934, -0.035207303872518685, 0.00014797726343401387],\n            [-0.0015994149367300917, 0.002183961698325075, -0.0005510279268070304, -0.00175006550618083, -0.020723679414693993],\n        ],\n    },\n    \"410M\": {\n        \"intercept\": [\n            2.2845576475924543,\n            3.10221083581893,\n            2.4040537489237623,\n            1.2320388989073703,\n            3.0194029194493215,\n        ],\n        \"coef_linear\": [\n            [-0.40161868178180443, 0.04851096556048266, 0.37552617435827934, -0.007771674366947659, -0.014646783770016363],\n            [0.16564665878501697, -0.5418667012877614, 0.19196166965559713, 0.21461472108487065, -0.030356348237732297],\n            [0.3827566078799856, 0.34563333912754424, -1.207292188578679, 0.2962894338651403, 0.18261280770604105],\n            [0.054055822096378214, 0.18257490953749397, 0.25515869822947196, -0.5390509589695227, 0.04726152910616942],\n            [0.08444528786706501, -0.1235789613695045, 0.1680640099151793, 0.09513264245956578, -0.22406297887230905],\n        ],\n        \"coef_log\": [\n            [-0.03838578515244451, 0.0010474524802569906, -0.0020612475600514644, 0.0001902706294946067, -0.0012861227733191377],\n            [-0.0012829345286925373, -0.004688819508647834, -0.0016346691987556602, -0.0009769593878491815, -0.002091652532429498],\n            [-0.0012514981651361474, -0.0022513525226212174, -0.03034764820962916, -0.00021614146982995423, -0.00017869542804964955],\n            [-0.0010235048622236945, -7.088093693356411e-05, -0.0007898197931760238, -0.033703719578066345, -0.0007674227907133403],\n            [-0.001315884970387432, 0.0024028568720025913, -0.0013333912060313298, -0.0011109928776418308, -0.021811398074324508],\n        ],\n    },\n    \"70M\": {\n        \"intercept\": [\n            2.7857859114105317,\n            3.631804815517477,\n            2.8681805224896912,\n            1.5890762093073625,\n            3.585150303901379,\n        ],\n        \"coef_linear\": [\n            [-0.4416445868640977, 0.07076027431299382, 0.302121611856706, 0.021520016473356454, 0.04724268422103695],\n            [0.21834311045644716, -0.553392474651338, 0.04818153280501451, 0.24033451148528265, 0.04653331990458732],\n            [0.4651202418236946, 0.2657143234624336, -1.3670739924126776, 0.36683688818700816, 0.2694025389395758],\n            [0.0850252828881378, 0.2990024146623459, 0.16822113994472984, -0.6239023579571726, 0.07165352046195206],\n            [0.14123915472576296, -0.13497556494772603, 0.03364295958750099, 0.12770730941446795, -0.16761385878000656],\n        ],\n        \"coef_log\": [\n            [-0.041246477328631105, 0.0006531144880363961, -0.0006596475145338669, -0.00019599814522888677, -0.0015631188541267603],\n            [-0.0009803943328683558, -0.005672467237098692, -8.71136475631502e-05, -0.0009074144501494191, -0.0019413115294711764],\n            [-0.0006290227608540234, -0.0005120063062147314, -0.02905249764872596, -0.0007835652353273532, -0.000662736071510837],\n            [-0.0008408582373940847, -0.0019902435925866755, 0.00039188729846795716, -0.0409361614036341, -0.0005053380487605633],\n            [-0.0009055762689076869, 0.0025986071507895507, 0.00015711172405491434, -0.0011903203768379186, -0.019717110434476673],\n        ],\n    },\n}\n\n\ndef _predict_point(p: Dict[str, float], coeff: Dict[str, list]) -> Dict[str, float]:\n    # Build feature vectors in fixed order\n    P = [float(p.get(k, 0.0)) for k in PROP_COLS]\n    logP = [math.log(x + EPS) for x in P]\n\n    y = []\n    for i in range(5):\n        val = coeff[\"intercept\"][i]\n        # linear terms\n        for j in range(5):\n            val += coeff[\"coef_linear\"][i][j] * P[j]\n        # log terms\n        for j in range(5):\n            val += coeff[\"coef_log\"][i][j] * logP[j]\n        y.append(val)\n\n    return {LOSS_COLS[i]: y[i] for i in range(5)}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in COEFFS:\n        # Fallback: use the average of available groups if unknown label is passed\n        # This keeps functional form identical while remaining robust.\n        # Precompute simple average coefficients on the fly.\n        groups = list(COEFFS.values())\n        avg = {\n            \"intercept\": [sum(g[\"intercept\"][i] for g in groups) / len(groups) for i in range(5)],\n            \"coef_linear\": [\n                [sum(g[\"coef_linear\"][i][j] for g in groups) / len(groups) for j in range(5)]\n                for i in range(5)\n            ],\n            \"coef_log\": [\n                [sum(g[\"coef_log\"][i][j] for g in groups) / len(groups) for j in range(5)]\n                for i in range(5)\n            ],\n        }\n        coeff = avg\n    else:\n        coeff = COEFFS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        outputs.append(_predict_point(row, coeff))\n    return outputs", "agent_name": "opencode", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.9747667014004453, "solution": "from typing import List, Dict\n\n# Parameters for scaling law: loss(p) = c - a * p**b\n_PARAMS: Dict[str, Dict[int, Dict[str, float]]] = {\n    '70M': {\n        1: {'a': 0.9228, 'b': 0.2453, 'c': 3.4149},\n        2: {'a': 0.3726, 'b': 0.5065, 'c': 3.8184},\n        3: {'a': 0.7930, 'b': 0.2212, 'c': 3.6006},\n        4: {'a': 0.9436, 'b': 0.2406, 'c': 2.2663},\n        5: {'a': 0.5175, 'b': 0.3754, 'c': 3.9317},\n    },\n    '160M': {\n        1: {'a': 0.8432, 'b': 0.2285, 'c': 3.0604},\n        2: {'a': 0.3059, 'b': 0.4616, 'c': 3.4721},\n        3: {'a': 0.7277, 'b': 0.2081, 'c': 3.2856},\n        4: {'a': 0.8371, 'b': 0.2382, 'c': 1.9631},\n        5: {'a': 0.5291, 'b': 0.3623, 'c': 3.5949},\n    },\n    '305M': {\n        1: {'a': 0.8159, 'b': 0.2234, 'c': 2.8980},\n        2: {'a': 0.4262, 'b': 0.6940, 'c': 3.3062},\n        3: {'a': 0.7023, 'b': 0.1831, 'c': 3.1556},\n        4: {'a': 0.7988, 'b': 0.2365, 'c': 1.8330},\n        5: {'a': 0.5343, 'b': 0.3516, 'c': 3.4344},\n    },\n    '410M': {\n        1: {'a': 0.7997, 'b': 0.2158, 'c': 2.8319},\n        2: {'a': 0.3518, 'b': 0.6247, 'c': 3.2303},\n        3: {'a': 0.7099, 'b': 0.1805, 'c': 3.0983},\n        4: {'a': 0.7849, 'b': 0.2413, 'c': 1.7794},\n        5: {'a': 0.5501, 'b': 0.3404, 'c': 3.3746},\n    },\n}\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups,\n               but the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) under keys\n        'loss_domain_i'.\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n    group_params = _PARAMS[group]\n    results: List[Dict[str, float]] = []\n    # Compute prediction for each data point\n    for entry in input_data:\n        preds: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = entry.get(f'proportion_domain_{i}')\n            if p is None:\n                raise KeyError(f\"Missing proportion_domain_{i} in input data\")\n            a = group_params[i]['a']\n            b = group_params[i]['b']\n            c = group_params[i]['c']\n            # scaling law: loss = c - a * p**b\n            preds[f'loss_domain_{i}'] = c - a * (p ** b)\n        results.append(preds)\n    return results", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.974745, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts validation losses for each domain based on mixture proportions.\n\n    Uses a power-law decrease model per domain and group:\n        loss = L0 - alpha * proportion**beta\n\n    Args:\n        input_data: list of dicts with keys 'proportion_domain_i' for i in 1..5.\n        group: experimental group name ('70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_i' predictions.\n    \"\"\"\n    # Pre-fitted parameters per group and domain\n    params = {\n        '70M': {\n            'L0':   [3.415, 3.818, 3.601, 2.266, 3.932],\n            'alpha':[0.923, 0.373, 0.793, 0.944, 0.518],\n            'beta': [0.245, 0.506, 0.221, 0.241, 0.375],\n        },\n        '160M': {\n            'L0':   [3.060, 3.472, 3.286, 1.963, 3.595],\n            'alpha':[0.843, 0.306, 0.728, 0.837, 0.529],\n            'beta': [0.229, 0.462, 0.208, 0.238, 0.362],\n        },\n        '305M': {\n            'L0':   [2.898, 3.306, 3.156, 1.833, 3.434],\n            'alpha':[0.816, 0.426, 0.702, 0.799, 0.534],\n            'beta': [0.223, 0.694, 0.183, 0.236, 0.352],\n        },\n        '410M': {\n            'L0':   [2.832, 3.230, 3.098, 1.779, 3.375],\n            'alpha':[0.800, 0.352, 0.710, 0.785, 0.550],\n            'beta': [0.216, 0.625, 0.181, 0.241, 0.340],\n        },\n    }\n    if group not in params:\n        raise ValueError(f\"Unknown group '{group}'\")\n    gp = params[group]\n    results: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = row.get(f'proportion_domain_{i}', 0.0)\n            L0 = gp['L0'][i-1]\n            a = gp['alpha'][i-1]\n            b = gp['beta'][i-1]\n            pred[f'loss_domain_{i}'] = L0 - a * (p ** b)\n        results.append(pred)\n    return results", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5", "reward_r2": 0.9715978378387945, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each domain and group\n    # Formula: loss_domain_i = L_inf_i - a_i * (proportion_domain_i)^b_i\n    params = {\n        1: {\n            '70M': {'L_inf': 3.414908, 'a': 0.919426, 'b': 0.242859},\n            '160M': {'L_inf': 3.060407, 'a': 0.841478, 'b': 0.227141},\n            '305M': {'L_inf': 2.898031, 'a': 0.812498, 'b': 0.220675},\n            '410M': {'L_inf': 2.831881, 'a': 0.797224, 'b': 0.213714}\n        },\n        2: {\n            '70M': {'L_inf': 3.818429, 'a': 0.286092, 'b': 0.325881},\n            '160M': {'L_inf': 3.472137, 'a': 0.229089, 'b': 0.253344},\n            '305M': {'L_inf': 3.306184, 'a': 0.231382, 'b': 0.286684},\n            '410M': {'L_inf': 3.230276, 'a': 0.194197, 'b': 0.211294}\n        },\n        3: {\n            '70M': {'L_inf': 3.600640, 'a': 0.884553, 'b': 0.258117},\n            '160M': {'L_inf': 3.285555, 'a': 0.821255, 'b': 0.248967},\n            '305M': {'L_inf': 3.155623, 'a': 0.780708, 'b': 0.218846},\n            '410M': {'L_inf': 3.098252, 'a': 0.789583, 'b': 0.216252}\n        },\n        4: {\n            '70M': {'L_inf': 2.266335, 'a': 0.933792, 'b': 0.235431},\n            '160M': {'L_inf': 1.963058, 'a': 0.833632, 'b': 0.236056},\n            '305M': {'L_inf': 1.832974, 'a': 0.793849, 'b': 0.233302},\n            '410M': {'L_inf': 1.779367, 'a': 0.778080, 'b': 0.236914}\n        },\n        5: {\n            '70M': {'L_inf': 3.931742, 'a': 0.511339, 'b': 0.352486},\n            '160M': {'L_inf': 3.594913, 'a': 0.526879, 'b': 0.354306},\n            '305M': {'L_inf': 3.434413, 'a': 0.530724, 'b': 0.338928},\n            '410M': {'L_inf': 3.374611, 'a': 0.548299, 'b': 0.334021}\n        }\n    }\n\n    # Process each data point\n    results = []\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain_i in range(1, 6):\n            prop_key = f'proportion_domain_{domain_i}'\n            loss_key = f'loss_domain_{domain_i}'\n\n            # Get the proportion for this domain\n            proportion = data_point.get(prop_key, 0.0)\n\n            # Get parameters for this domain and group\n            domain_params = params[domain_i][group]\n            L_inf = domain_params['L_inf']\n            a = domain_params['a']\n            b = domain_params['b']\n\n            # Apply the scaling law: loss = L_inf - a * p^b\n            predicted_loss = L_inf - a * (proportion ** b)\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.971579, "solution": "from typing import List, Dict\n\n# Precomputed parameters for the scaling law per group and domain\n_PARAMS: Dict[str, Dict[str, Dict[str, float]]] = {\n    '70M': {\n        'domain_1': {'a': -0.9196, 'b': 0.2428, 'c': 3.4151},\n        'domain_2': {'a': -0.2861, 'b': 0.3259, 'c': 3.8184},\n        'domain_3': {'a': -0.8845, 'b': 0.2583, 'c': 3.6004},\n        'domain_4': {'a': -0.9339, 'b': 0.2354, 'c': 2.2665},\n        'domain_5': {'a': -0.5144, 'b': 0.3494, 'c': 3.9349},\n    },\n    '160M': {\n        'domain_1': {'a': -0.8416, 'b': 0.2271, 'c': 3.0605},\n        'domain_2': {'a': -0.2291, 'b': 0.2533, 'c': 3.4721},\n        'domain_3': {'a': -0.8212, 'b': 0.2491, 'c': 3.2853},\n        'domain_4': {'a': -0.8337, 'b': 0.2360, 'c': 1.9631},\n        'domain_5': {'a': -0.5281, 'b': 0.3531, 'c': 3.5961},\n    },\n    '305M': {\n        'domain_1': {'a': -0.8126, 'b': 0.2206, 'c': 2.8982},\n        'domain_2': {'a': -0.2314, 'b': 0.2867, 'c': 3.3062},\n        'domain_3': {'a': -0.7806, 'b': 0.2189, 'c': 3.1554},\n        'domain_4': {'a': -0.7939, 'b': 0.2333, 'c': 1.8330},\n        'domain_5': {'a': -0.5326, 'b': 0.3372, 'c': 3.4364},\n    },\n    '410M': {\n        'domain_1': {'a': -0.7973, 'b': 0.2137, 'c': 2.8320},\n        'domain_2': {'a': -0.1942, 'b': 0.2113, 'c': 3.2303},\n        'domain_3': {'a': -0.7895, 'b': 0.2163, 'c': 3.0981},\n        'domain_4': {'a': -0.7782, 'b': 0.2369, 'c': 1.7795},\n        'domain_5': {'a': -0.5493, 'b': 0.3332, 'c': 3.3756},\n    },\n}\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts per-domain validation loss based on mixture proportions using a power-law model.\n\n    The functional form is:\n        loss_domain_i = a * (proportion_domain_i ** b) + c\n\n    Args:\n        input_data: List of data points, each with keys 'proportion_domain_i'.\n        group: Experimental group key, must be one of the precomputed groups.\n\n    Returns:\n        List of dictionaries with predicted 'loss_domain_i' for each input point.\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}', available: {list(_PARAMS.keys())}\")\n    params_group = _PARAMS[group]\n    predictions: List[Dict[str, float]] = []\n    for point in input_data:\n        pred_point: Dict[str, float] = {}\n        for i in range(1, 6):\n            key_p = f'proportion_domain_{i}'\n            key_l = f'loss_domain_{i}'\n            p = point.get(key_p, 0.0)\n            a = params_group[f'domain_{i}']['a']\n            b = params_group[f'domain_{i}']['b']\n            c = params_group[f'domain_{i}']['c']\n            pred_point[key_l] = a * (p ** b) + c\n        predictions.append(pred_point)\n    return predictions", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9714759455458236, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Law: For each domain i in {1..5},\n        loss_domain_i = a_{group,i} + b_{group,i} * (proportion_domain_i) ** alpha_i\n    where the exponent alpha_i is domain-specific but shared across groups, and\n    (a_{group,i}, b_{group,i}) are fitted per group.\n    \"\"\"\n    # Domain-specific exponents shared across groups (fitted once)\n    alphas = {1: 0.226, 2: 0.272, 3: 0.236, 4: 0.235, 5: 0.343}\n\n    # Per-group coefficients a and b for each domain (fitted from the provided dataset)\n    coeffs = {\n        \"160M\": {\n            1: {\"a\": 3.0607589078884847, \"b\": -0.8406224674207222},\n            2: {\"a\": 3.471957561424479,  \"b\": -0.23709796451470122},\n            3: {\"a\": 3.2856010648519973, \"b\": -0.7919275425273328},\n            4: {\"a\": 1.9632078046951371, \"b\": -0.8321226336323998},\n            5: {\"a\": 3.600060737641489,  \"b\": -0.5302231304455584},\n        },\n        \"305M\": {\n            1: {\"a\": 2.896951436073815,  \"b\": -0.8170959564908562},\n            2: {\"a\": 3.306317389829822,  \"b\": -0.22521283957225652},\n            3: {\"a\": 3.155092174041798,  \"b\": -0.8182930011802386},\n            4: {\"a\": 1.8328824818924194, \"b\": -0.7963908513267552},\n            5: {\"a\": 3.4340665068448346, \"b\": -0.5313252100720468},\n        },\n        \"410M\": {\n            1: {\"a\": 2.8291888357597386, \"b\": -0.8073757705491997},\n            2: {\"a\": 3.2297361776335225, \"b\": -0.21719584738930717},\n            3: {\"a\": 3.097659192469288,  \"b\": -0.8335641687702692},\n            4: {\"a\": 1.779637332326639,  \"b\": -0.775555774148788},\n            5: {\"a\": 3.371561997175875,  \"b\": -0.5469883726664775},\n        },\n        \"70M\": {\n            1: {\"a\": 3.4193040905517047, \"b\": -0.9041352514360005},\n            2: {\"a\": 3.8189889954933474, \"b\": -0.25910738407437617},\n            3: {\"a\": 3.600895922417036,  \"b\": -0.8317098214628572},\n            4: {\"a\": 2.266520379741139,  \"b\": -0.9332890679011832},\n            5: {\"a\": 3.937342662537917,  \"b\": -0.5157344418970146},\n        },\n    }\n\n    # Fallback: if an unknown group is provided, use the average coefficients across known groups\n    if group not in coeffs:\n        groups = list(coeffs.keys())\n        avg = {}\n        for i in range(1, 6):\n            a_vals = [coeffs[g][i][\"a\"] for g in groups]\n            b_vals = [coeffs[g][i][\"b\"] for g in groups]\n            avg[i] = {\"a\": sum(a_vals) / len(a_vals), \"b\": sum(b_vals) / len(b_vals)}\n        coeffs[group] = avg\n\n    out = []\n    for row in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            a = coeffs[group][i][\"a\"]\n            b = coeffs[group][i][\"b\"]\n            alpha = alphas[i]\n            pred[f\"loss_domain_{i}\"] = a + b * (p ** alpha)\n        out.append(pred)\n    return out", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.971446, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Shared per-domain offset to make log well-defined at zero proportion.\n# Selected via cross-group grid search to maximize average R^2.\n_C_BY_DOMAIN: Dict[int, float] = {\n    1: 0.003125,\n    2: 0.0046875,\n    3: 0.0015625,\n    4: 0.003125,\n    5: 0.0234375,\n}\n\n# Per-group, per-domain coefficients for the law:\n#   loss_domain_i = a[g,i] + b[g,i] * log(proportion_domain_i + C[i])\n_COEFS_BY_GROUP: Dict[str, Dict[int, Dict[str, float]]] = {\n    \"160M\": {\n        1: {\"a\": 2.2424684059708717, \"b\": -0.1412039367794934},\n        2: {\"a\": 3.2541456054570035, \"b\": -0.0405858087415962},\n        3: {\"a\": 2.567113771294527, \"b\": -0.11102183824132524},\n        4: {\"a\": 1.167057464525046, \"b\": -0.1374903829439023},\n        5: {\"a\": 3.085896059150068, \"b\": -0.13693966564752397},\n    },\n    \"305M\": {\n        1: {\"a\": 2.101566717029658, \"b\": -0.137250480526824},\n        2: {\"a\": 3.0994046220334157, \"b\": -0.03855781421113323},\n        3: {\"a\": 2.4128405334920684, \"b\": -0.11468007010082179},\n        4: {\"a\": 1.0709224798061698, \"b\": -0.13158540389099785},\n        5: {\"a\": 2.918730301818813, \"b\": -0.13736039699677574},\n    },\n    \"410M\": {\n        1: {\"a\": 2.0433633841009002, \"b\": -0.1355817799554127},\n        2: {\"a\": 3.0302311023376722, \"b\": -0.03717104079844086},\n        3: {\"a\": 2.341569758388262, \"b\": -0.11681678106085443},\n        4: {\"a\": 1.0375379829435523, \"b\": -0.1281673168777551},\n        5: {\"a\": 2.841139982418687, \"b\": -0.14126939233946334},\n    },\n    \"70M\": {\n        1: {\"a\": 2.538957210154492, \"b\": -0.15195781604593908},\n        2: {\"a\": 3.5809102439444924, \"b\": -0.044369404452315006},\n        3: {\"a\": 2.8462143926943795, \"b\": -0.11662362761003567},\n        4: {\"a\": 1.3734832511282675, \"b\": -0.15423705944877764},\n        5: {\"a\": 3.4370247988143974, \"b\": -0.13346648082051366},\n    },\n}\n\n# Fallback coefficients (mean across groups) used if an unknown group is provided.\n_FALLBACK_COEFS: Dict[int, Dict[str, float]] = {\n    1: {\"a\": 2.231588929313981, \"b\": -0.1414985033269173},\n    2: {\"a\": 3.2411728934431463, \"b\": -0.040171017050871324},\n    3: {\"a\": 2.5419346139673094, \"b\": -0.11478557925325927},\n    4: {\"a\": 1.162250294600759, \"b\": -0.13787004079035822},\n    5: {\"a\": 3.0706977855504913, \"b\": -0.13725898395106917},\n}\n\n\ndef _predict_for_row(row: Dict[str, float], coefs: Dict[int, Dict[str, float]]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n        c = _C_BY_DOMAIN[i]\n        a = coefs[i][\"a\"]\n        b = coefs[i][\"b\"]\n        # Numerically safe log with small positive offset.\n        pred = a + b * math.log(max(p, 0.0) + c)\n        out[f\"loss_domain_{i}\"] = float(pred)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coefs = _COEFS_BY_GROUP.get(group, _FALLBACK_COEFS)\n    return [_predict_for_row(row, coefs) for row in input_data]", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9714345955231741, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fixed exponents (beta) for each domain, same for all groups\n    beta = {\n        1: 0.228,\n        2: 0.572,\n        3: 0.198,\n        4: 0.239,\n        5: 0.357\n    }\n\n    # Parameters for each group and domain: (baseline, A)\n    # baseline: loss when proportion = 0\n    # A: coefficient in the formula: loss = baseline - A * proportion^beta\n    params = {\n        '70M': {\n            1: (3.415, 0.896),\n            2: (3.818, 0.466),\n            3: (3.601, 0.745),\n            4: (2.266, 0.941),\n            5: (3.932, 0.512)\n        },\n        '160M': {\n            1: (3.060, 0.843),\n            2: (3.472, 0.432),\n            3: (3.286, 0.710),\n            4: (1.963, 0.839),\n            5: (3.595, 0.527)\n        },\n        '305M': {\n            1: (2.898, 0.822),\n            2: (3.306, 0.408),\n            3: (3.156, 0.736),\n            4: (1.833, 0.803),\n            5: (3.434, 0.536)\n        },\n        '410M': {\n            1: (2.832, 0.817),\n            2: (3.230, 0.398),\n            3: (3.098, 0.750),\n            4: (1.779, 0.781),\n            5: (3.375, 0.556)\n        }\n    }\n\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Supported groups: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    result = []\n    for point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            if proportion_key not in point:\n                # If proportion key is missing, we cannot predict loss for this domain\n                continue\n            p = point[proportion_key]\n            baseline, A = group_params[i]\n            b = beta[i]\n            if p == 0:\n                loss = baseline\n            else:\n                loss = baseline - A * (p ** b)\n            pred[f'loss_domain_{i}'] = loss\n        result.append(pred)\n\n    return result", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9711466282348672, "solution": "from __future__ import annotations\n\nimport math\nimport os\nfrom typing import Dict, List\n\n# Public API\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fit/load coefficients once (lazy on first invocation)\n    _ensure_fitted()\n    coeffs_for_group = _COEFFS.get(group, _COEFFS.get(_GLOBAL_KEY, _default_coeffs()))\n\n    results: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for d in _DOMAINS:\n            p = float(row.get(f\"proportion_domain_{d}\", 0.0))\n            pred[f\"loss_domain_{d}\"] = _predict_single(p, coeffs_for_group[d])\n        results.append(pred)\n    return results\n\n\n# ------------------------\n# Internal implementation\n# ------------------------\n\n# Model/Formula:\n# For each domain i in {1..5}, and for any group g:\n#     loss_domain_i = a_{g,i} + b_{g,i} * log(p_i + eps) + c_{g,i} * [log(p_i + eps)]^2\n# where p_i is the mixture proportion for domain i, eps = 1e-12.\n# This \"quadratic-in-log\" model captures a wide class of power-law-like curves\n# without requiring nonlinear optimization, improving stability and extrapolation.\n\n_EPS = 1e-12\n_DOMAINS = (1, 2, 3, 4, 5)\n_GLOBAL_KEY = \"__GLOBAL__\"\n\n# Coefficients structure:\n# _COEFFS[group][domain] = (a, b, c)\n_COEFFS: Dict[str, Dict[int, tuple[float, float, float]]] = {}\n\n# R^2 scores for reporting (per group/domain)\n_R2: Dict[str, Dict[int, float]] = {}\n\n# Guard for one-time fit\n_FITTED = False\n\n\ndef _predict_single(p: float, abc: tuple[float, float, float]) -> float:\n    a, b, c = abc\n    lp = math.log(max(p, _EPS))\n    return a + b * lp + c * (lp * lp)\n\n\ndef _default_coeffs() -> Dict[int, tuple[float, float, float]]:\n    # Neutral fallback: constant ~1.0 loss if fitting is unavailable\n    return {d: (1.0, 0.0, 0.0) for d in _DOMAINS}\n\n\ndef _ensure_fitted() -> None:\n    global _FITTED\n    if _FITTED:\n        return\n    try:\n        ds = _load_dataset(\"/app/data\")\n        if ds is None:\n            # Could not load dataset; use defaults\n            _COEFFS[_GLOBAL_KEY] = _default_coeffs()\n            _FITTED = True\n            _write_explain_file()\n            return\n\n        # Determine available groups\n        groups = _collect_groups(ds)\n        if not groups:\n            groups = {_GLOBAL_KEY}\n\n        # Fit per group\n        for g in groups:\n            rows = (r for r in ds if (g == _GLOBAL_KEY or r.get(\"group\") == g))\n            coeffs_g, r2_g = _fit_group(rows)\n            _COEFFS[g] = coeffs_g\n            _R2[g] = r2_g\n\n        # Also fit global across all data for robustness/fallback\n        rows_all = (r for r in ds)\n        coeffs_global, r2_global = _fit_group(rows_all)\n        _COEFFS[_GLOBAL_KEY] = coeffs_global\n        _R2[_GLOBAL_KEY] = r2_global\n\n    except Exception:\n        # Any failure => ensure safe defaults\n        _COEFFS[_GLOBAL_KEY] = _default_coeffs()\n    finally:\n        _FITTED = True\n        # Best-effort write explanation (ignore errors)\n        try:\n            _write_explain_file()\n        except Exception:\n            pass\n\n\ndef _load_dataset(path: str):\n    try:\n        from datasets import load_from_disk  # type: ignore\n    except Exception:\n        return None\n    if not os.path.exists(path):\n        return None\n    ds = load_from_disk(path)\n    # Support DatasetDict or Dataset\n    try:\n        # DatasetDict\n        if hasattr(ds, \"keys\"):\n            if \"train\" in ds:\n                ds_split = ds[\"train\"]\n            else:\n                # Pick the first available split\n                first_key = next(iter(ds.keys()))\n                ds_split = ds[first_key]\n        else:\n            ds_split = ds\n    except Exception:\n        ds_split = ds\n    return ds_split\n\n\ndef _collect_groups(ds) -> set:\n    groups = set()\n    try:\n        for r in ds:\n            g = r.get(\"group\")\n            if g is not None:\n                groups.add(g)\n    except Exception:\n        return set()\n    return groups\n\n\ndef _fit_group(rows_iter):\n    # Linear regression (ridge-regularized normal equations) for each domain\n    # y = a*1 + b*lp + c*lp^2  with lp = log(p + eps)\n    # We accumulate X^T X and X^T y in streaming fashion to avoid extra deps.\n    coeffs: Dict[int, tuple[float, float, float]] = {}\n    r2s: Dict[int, float] = {}\n\n    # Materialize rows for reuse (single pass needed for each domain)\n    rows = list(rows_iter)\n\n    for d in _DOMAINS:\n        # Initialize 3x3 matrix and 3x1 vector\n        xtx = [[0.0, 0.0, 0.0],\n               [0.0, 0.0, 0.0],\n               [0.0, 0.0, 0.0]]\n        xty = [0.0, 0.0, 0.0]\n\n        y_vals = []\n        f_list = []\n\n        for r in rows:\n            p = float(r.get(f\"proportion_domain_{d}\", 0.0))\n            y = r.get(f\"loss_domain_{d}\")\n            if y is None:\n                continue\n            y = float(y)\n            lp = math.log(max(p, _EPS))\n            f0 = 1.0\n            f1 = lp\n            f2 = lp * lp\n            f = (f0, f1, f2)\n            # Accumulate\n            xtx[0][0] += f0 * f0; xtx[0][1] += f0 * f1; xtx[0][2] += f0 * f2\n            xtx[1][0] += f1 * f0; xtx[1][1] += f1 * f1; xtx[1][2] += f1 * f2\n            xtx[2][0] += f2 * f0; xtx[2][1] += f2 * f1; xtx[2][2] += f2 * f2\n\n            xty[0] += f0 * y; xty[1] += f1 * y; xty[2] += f2 * y\n\n            y_vals.append(y)\n            f_list.append(f)\n\n        n = len(y_vals)\n        if n == 0:\n            coeffs[d] = (1.0, 0.0, 0.0)\n            r2s[d] = 0.0\n            continue\n\n        # Ridge regularization to stabilize\n        lam = 1e-8\n        xtx[0][0] += lam\n        xtx[1][1] += lam\n        xtx[2][2] += lam\n\n        a, b, c = _solve_3x3(xtx, xty)\n\n        coeffs[d] = (a, b, c)\n\n        # Compute R^2\n        y_mean = sum(y_vals) / n\n        ss_tot = sum((yy - y_mean) ** 2 for yy in y_vals) or 1e-12\n        ss_res = 0.0\n        for (f0, f1, f2), yy in zip(f_list, y_vals):\n            yhat = a + b * f1 + c * f2\n            ss_res += (yy - yhat) ** 2\n        r2s[d] = 1.0 - (ss_res / ss_tot)\n\n    return coeffs, r2s\n\n\ndef _solve_3x3(a: List[List[float]], b: List[float]) -> tuple[float, float, float]:\n    # Gaussian elimination with partial pivoting for 3x3\n    # Solve A x = b\n    A = [row[:] for row in a]\n    x = [0.0, 0.0, 0.0]\n    rhs = b[:]\n\n    # Forward elimination\n    for i in range(3):\n        # Pivot\n        pivot = i\n        max_abs = abs(A[i][i])\n        for r in range(i + 1, 3):\n            if abs(A[r][i]) > max_abs:\n                max_abs = abs(A[r][i])\n                pivot = r\n        if max_abs < 1e-18:\n            # Ill-conditioned; fallback identity\n            return (0.0, 0.0, 0.0)\n        if pivot != i:\n            A[i], A[pivot] = A[pivot], A[i]\n            rhs[i], rhs[pivot] = rhs[pivot], rhs[i]\n\n        # Normalize and eliminate\n        piv = A[i][i]\n        for r in range(i + 1, 3):\n            if A[r][i] == 0.0:\n                continue\n            f = A[r][i] / piv\n            rhs[r] -= f * rhs[i]\n            for c in range(i, 3):\n                A[r][c] -= f * A[i][c]\n\n    # Back substitution\n    for i in reversed(range(3)):\n        s = rhs[i]\n        for c in range(i + 1, 3):\n            s -= A[i][c] * x[c]\n        if abs(A[i][i]) < 1e-18:\n            x[i] = 0.0\n        else:\n            x[i] = s / A[i][i]\n\n    return (x[0], x[1], x[2])\n\n\ndef _write_explain_file() -> None:\n    # Write a detailed explanation with fitted coefficients to /app/explain.md\n    lines: List[str] = []\n    lines.append(\"# Discovered Scaling Law for Domain Mixture\\n\")\n    lines.append(\"This document is auto-generated by /app/law.py when imported or first used.\\n\")\n    lines.append(\"## Formula\\n\")\n    lines.append(\n        \"For each domain i in {1,2,3,4,5}, and for any experimental group G, the validation loss is modeled as:\\n\"\n    )\n    lines.append(\n        \"    loss_domain_i = a_{G,i} + b_{G,i} * log(proportion_domain_i + 1e-12) + c_{G,i} * [log(proportion_domain_i + 1e-12)]^2\\n\"\n    )\n    lines.append(\n        \"This quadratic-in-log model approximates power-law behavior with a smooth curvature term and is fit via linear regression (normal equations with a small ridge regularizer).\\n\"\n    )\n    lines.append(\"\\n## Methodology\\n\")\n    lines.append(\n        \"- Loaded the dataset from /app/data using datasets.load_from_disk.\\n\"\n        \"- For each group and each domain, constructed features [1, log(p+1e-12), (log(p+1e-12))^2].\\n\"\n        \"- Solved for coefficients (a,b,c) with closed-form least squares per domain.\\n\"\n        \"- Report R\u00b2 per fit to indicate goodness-of-fit. If a group is unknown at inference time, a global fit over all groups is used.\\n\"\n    )\n    lines.append(\"\\n## Fitted Coefficients by Group and Domain\\n\")\n\n    if not _COEFFS:\n        lines.append(\"\\nNo coefficients available; using defaults (1.0, 0.0, 0.0).\\n\")\n    else:\n        for g in sorted(_COEFFS.keys()):\n            lines.append(f\"\\n### Group: {g}\\n\")\n            lines.append(\"| Domain | a | b | c | R^2 |\\n\")\n            lines.append(\"|---:|---:|---:|---:|---:|\\n\")\n            for d in _DOMAINS:\n                a, b, c = _COEFFS[g][d]\n                r2 = _R2.get(g, {}).get(d, float('nan'))\n                lines.append(f\"| {d} | {a:.6f} | {b:.6f} | {c:.6f} | {r2:.4f} |\\n\")\n\n    path = \"/app/explain.md\"\n    try:\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.writelines(lines)\n    except Exception:\n        # Swallow IO errors to avoid breaking runtime\n        pass", "agent_name": "aider", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.971145, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Log-scaling law per group and domain:\n# loss_domain_i = a + b * log(proportion_domain_i + eps)\n# The functional form is identical across groups; only (a, b, eps) differ.\n\nCOEFFS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"160M\": {\n        \"loss_domain_1\": {\"a\": 2.2532615943868906, \"b\": -0.1337057136545849, \"eps\": 0.0023901343745956435},\n        \"loss_domain_2\": {\"a\": 3.2631919363951556, \"b\": -0.03558508713341269, \"eps\": 0.0028180383737978713},\n        \"loss_domain_3\": {\"a\": 2.617689579679002, \"b\": -0.09252884608244097, \"eps\": 0.000733714154677849},\n        \"loss_domain_4\": {\"a\": 1.1948582730723922, \"b\": -0.12312317609847837, \"eps\": 0.00195132901468823},\n        \"loss_domain_5\": {\"a\": 3.0846493236033896, \"b\": -0.14154552771058287, \"eps\": 0.027307052357303876},\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\"a\": 2.1166886286314748, \"b\": -0.12677944713435335, \"eps\": 0.002106041895388586},\n        \"loss_domain_2\": {\"a\": 3.0993803822936004, \"b\": -0.0385714100210861, \"eps\": 0.00469352643305276},\n        \"loss_domain_3\": {\"a\": 2.498051621939985, \"b\": -0.08371626119596315, \"eps\": 0.0003880182070929469},\n        \"loss_domain_4\": {\"a\": 1.0998534285937238, \"b\": -0.11664800642521109, \"eps\": 0.001864178447017992},\n        \"loss_domain_5\": {\"a\": 2.918795681535315, \"b\": -0.13713177186385297, \"eps\": 0.023253743472940992},\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\"a\": 2.0637659220766347, \"b\": -0.12150735957421108, \"eps\": 0.0017973751820540681},\n        \"loss_domain_2\": {\"a\": 3.0488279748294658, \"b\": -0.027049899121057236, \"eps\": 0.0012212145195230296},\n        \"loss_domain_3\": {\"a\": 2.431248081266068, \"b\": -0.08424573100842837, \"eps\": 0.00036447069913629604},\n        \"loss_domain_4\": {\"a\": 1.0617583927188603, \"b\": -0.11564152762782738, \"eps\": 0.0020181562979846223},\n        \"loss_domain_5\": {\"a\": 2.8417472866298, \"b\": -0.13918681081923803, \"eps\": 0.02183585057099095},\n    },\n    \"70M\": {\n        \"loss_domain_1\": {\"a\": 2.536901150373465, \"b\": -0.1533997450030275, \"eps\": 0.0032688441418023727},\n        \"loss_domain_2\": {\"a\": 3.568955616082981, \"b\": -0.05120827266924541, \"eps\": 0.007660059062331851},\n        \"loss_domain_3\": {\"a\": 2.886179438825381, \"b\": -0.10197118388034185, \"eps\": 0.0009064844324661106},\n        \"loss_domain_4\": {\"a\": 1.4039456278566775, \"b\": -0.13849029661959558, \"eps\": 0.0019748899726039115},\n        \"loss_domain_5\": {\"a\": 3.435925433424547, \"b\": -0.1375046497033619, \"eps\": 0.02690444281470573},\n    },\n}\n\n_PROP_COLS = [\n    \"proportion_domain_1\",\n    \"proportion_domain_2\",\n    \"proportion_domain_3\",\n    \"proportion_domain_4\",\n    \"proportion_domain_5\",\n]\n_LOSS_COLS = [\n    \"loss_domain_1\",\n    \"loss_domain_2\",\n    \"loss_domain_3\",\n    \"loss_domain_4\",\n    \"loss_domain_5\",\n]\n\n\ndef _predict_for_group(row: Dict[str, float], group: str) -> Dict[str, float]:\n    coeffs = COEFFS.get(group)\n    if coeffs is None:\n        raise ValueError(f\"Unknown group '{group}'. Known groups: {sorted(COEFFS)}\")\n    out: Dict[str, float] = {}\n    for i, loss_key in enumerate(_LOSS_COLS):\n        prop_key = _PROP_COLS[i]\n        p = float(row.get(prop_key, 0.0))\n        a = coeffs[loss_key][\"a\"]\n        b = coeffs[loss_key][\"b\"]\n        eps = coeffs[loss_key][\"eps\"]\n        # Guard against tiny negatives from numerical issues\n        val = a + b * math.log(max(p + eps, 1e-12))\n        out[loss_key] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    return [_predict_for_group(row, group) for row in input_data]", "agent_name": "opencode", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5", "reward_r2": 0.971145, "solution": "import math\n\n# Fitted parameters for each domain and group\n# Structure: PARAMS[domain][group] = {'a': ..., 'p0': ..., 'c': ...}\nPARAMS = {\n    'domain_1': {\n        '160M': {'a': -0.133706, 'p0': 0.002390, 'c': 2.253262},\n        '305M': {'a': -0.126779, 'p0': 0.002106, 'c': 2.116689},\n        '410M': {'a': -0.121507, 'p0': 0.001797, 'c': 2.063766},\n        '70M': {'a': -0.153400, 'p0': 0.003269, 'c': 2.536901}\n    },\n    'domain_2': {\n        '160M': {'a': -0.035585, 'p0': 0.002818, 'c': 3.263192},\n        '305M': {'a': -0.038571, 'p0': 0.004694, 'c': 3.099380},\n        '410M': {'a': -0.027050, 'p0': 0.001221, 'c': 3.048828},\n        '70M': {'a': -0.051208, 'p0': 0.007660, 'c': 3.568956}\n    },\n    'domain_3': {\n        '160M': {'a': -0.092529, 'p0': 0.000734, 'c': 2.617690},\n        '305M': {'a': -0.083716, 'p0': 0.000388, 'c': 2.498053},\n        '410M': {'a': -0.084246, 'p0': 0.000364, 'c': 2.431248},\n        '70M': {'a': -0.101971, 'p0': 0.000906, 'c': 2.886180}\n    },\n    'domain_4': {\n        '160M': {'a': -0.123123, 'p0': 0.001951, 'c': 1.194858},\n        '305M': {'a': -0.116648, 'p0': 0.001864, 'c': 1.099853},\n        '410M': {'a': -0.115642, 'p0': 0.002018, 'c': 1.061758},\n        '70M': {'a': -0.138490, 'p0': 0.001975, 'c': 1.403946}\n    },\n    'domain_5': {\n        '160M': {'a': -0.141545, 'p0': 0.027307, 'c': 3.084649},\n        '305M': {'a': -0.137132, 'p0': 0.023254, 'c': 2.918796},\n        '410M': {'a': -0.139187, 'p0': 0.021836, 'c': 2.841747},\n        '70M': {'a': -0.137505, 'p0': 0.026904, 'c': 3.435925}\n    }\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    results = []\n\n    for data_point in input_data:\n        predictions = {}\n\n        # For each domain, predict the loss using the logarithmic scaling law\n        # L = a * log(p + p0) + c\n        for domain_idx in range(1, 6):\n            prop_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n            domain_key = f'domain_{domain_idx}'\n\n            # Get the proportion for this domain\n            proportion = data_point[prop_key]\n\n            # Get the fitted parameters for this domain and group\n            params = PARAMS[domain_key][group]\n            a = params['a']\n            p0 = params['p0']\n            c = params['c']\n\n            # Apply the scaling law: L = a * log(p + p0) + c\n            predicted_loss = a * math.log(proportion + p0) + c\n\n            predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5", "reward_r2": 0.971145, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each group and domain\n    # Formula: loss_domain_i = c - a * log(proportion_domain_i + b)\n    params = {\n        \"160M\": {\n            1: {\"a\": 0.13370571385297636, \"b\": 0.002390134393468617, \"c\": 2.2532615941125935},\n            2: {\"a\": 0.03558508709506605, \"b\": 0.002818038362083297, \"c\": 3.2631919364668476},\n            3: {\"a\": 0.09252859186605873, \"b\": 0.0007337050789613676, \"c\": 2.6176902832037365},\n            4: {\"a\": 0.12312317552374129, \"b\": 0.0019513289742103528, \"c\": 1.1948582742030367},\n            5: {\"a\": 0.14154544729126772, \"b\": 0.02730698179138869, \"c\": 3.0846493439463316}\n        },\n        \"305M\": {\n            1: {\"a\": 0.12677944537739508, \"b\": 0.0021060417483921446, \"c\": 2.116688631238799},\n            2: {\"a\": 0.03857137198216823, \"b\": 0.004693509918912462, \"c\": 3.0993804511546172},\n            3: {\"a\": 0.08371624093149321, \"b\": 0.0003880177278893411, \"c\": 2.498051678487977},\n            4: {\"a\": 0.1166480072254707, \"b\": 0.0018641785034444133, \"c\": 1.0998534270479912},\n            5: {\"a\": 0.1371317714952529, \"b\": 0.02325374313900293, \"c\": 2.9187956816183247}\n        },\n        \"410M\": {\n            1: {\"a\": 0.12150735155719182, \"b\": 0.001797374580772529, \"c\": 2.0637659341196875},\n            2: {\"a\": 0.02704989730064647, \"b\": 0.0012212141601325463, \"c\": 3.0488279783306345},\n            3: {\"a\": 0.08424583253617433, \"b\": 0.00036447295242515113, \"c\": 2.4312477954997536},\n            4: {\"a\": 0.11564152660339765, \"b\": 0.0020181562198990236, \"c\": 1.0617583947352045},\n            5: {\"a\": 0.13918678570181242, \"b\": 0.021835831641187086, \"c\": 2.841747294132424}\n        },\n        \"70M\": {\n            1: {\"a\": 0.15339974058406491, \"b\": 0.0032688436937777696, \"c\": 2.536901156656863},\n            2: {\"a\": 0.05120827004111036, \"b\": 0.007660058657766675, \"c\": 3.568955621338834},\n            3: {\"a\": 0.10197068274747872, \"b\": 0.0009064652901137906, \"c\": 2.8861808195815533},\n            4: {\"a\": 0.13849029277376568, \"b\": 0.001974889729256156, \"c\": 1.4039456354099615},\n            5: {\"a\": 0.1375046566797978, \"b\": 0.02690444916275532, \"c\": 3.435925431698058}\n        }\n    }\n\n    # Get parameters for the specified group\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Available groups: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    # Generate predictions\n    predictions = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # For each domain, predict the loss\n        for domain in range(1, 6):\n            prop_key = f\"proportion_domain_{domain}\"\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get the proportion for this domain\n            if prop_key not in data_point:\n                raise ValueError(f\"Missing input variable: {prop_key}\")\n\n            proportion = data_point[prop_key]\n\n            # Get parameters for this domain\n            a = group_params[domain][\"a\"]\n            b = group_params[domain][\"b\"]\n            c = group_params[domain][\"c\"]\n\n            # Apply the scaling law: loss = c - a * log(proportion + b)\n            predicted_loss = c - a * math.log(proportion + b)\n\n            prediction[loss_key] = predicted_loss\n\n        predictions.append(prediction)\n\n    return predictions", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9711402923865737, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Scaling law (same functional form for all groups and domains):\n        loss_domain_i = a_{g,i} + b_{g,i} * ln(proportion_domain_i + eps_{g,i})\n\n    If the provided group is unknown, a sensible fallback using the average\n    coefficients across known groups is used (per-domain averages of a, b, eps).\n    \"\"\"\n    import math\n\n    # Parameters per (group, domain): loss_i = a + b * ln(p_i + eps)\n    _PARAMS = {\n        '160M': {\n            1: {'a': 2.2531838390, 'b': -0.1337589930, 'eps': 0.0023949075},\n            2: {'a': 3.2636809068, 'b': -0.0353192223, 'eps': 0.0027325293},\n            3: {'a': 2.6179153875, 'b': -0.0924472969, 'eps': 0.0007308092},\n            4: {'a': 1.1944779835, 'b': -0.1233168892, 'eps': 0.0019650546},\n            5: {'a': 3.0846022090, 'b': -0.1417331522, 'eps': 0.0274723768},\n        },\n        '305M': {\n            1: {'a': 2.1168108211, 'b': -0.1266964846, 'eps': 0.0020990011},\n            2: {'a': 3.0996329693, 'b': -0.0384297964, 'eps': 0.0046309399},\n            3: {'a': 2.4992568493, 'b': -0.0832862054, 'eps': 0.0003779407},\n            4: {'a': 1.1005269460, 'b': -0.1163058210, 'eps': 0.0018396558},\n            5: {'a': 2.9190563483, 'b': -0.1362346592, 'eps': 0.0225414637},\n        },\n        '410M': {\n            1: {'a': 2.0629628567, 'b': -0.1220476872, 'eps': 0.0018396558},\n            2: {'a': 3.0486794047, 'b': -0.0271283688, 'eps': 0.0012385352},\n            3: {'a': 2.4325992818, 'b': -0.0837640232, 'eps': 0.0003538226},\n            4: {'a': 1.0631159333, 'b': -0.1149495630, 'eps': 0.0019650546},\n            5: {'a': 2.8414721314, 'b': -0.1401158009, 'eps': 0.0225414637},\n        },\n        '70M': {\n            1: {'a': 2.5360441935, 'b': -0.1540029694, 'eps': 0.0033302662},\n            2: {'a': 3.5682949842, 'b': -0.0515955155, 'eps': 0.0078482616},\n            3: {'a': 2.8873239058, 'b': -0.1015557222, 'eps': 0.0008906727},\n            4: {'a': 1.4042502135, 'b': -0.1383351096, 'eps': 0.0019650546},\n            5: {'a': 3.4357662053, 'b': -0.1381350881, 'eps': 0.0274723768},\n        },\n    }\n\n    # Build per-domain average fallback in case of unknown group\n    if group not in _PARAMS:\n        # compute averages across known groups for each domain\n        avg_params = {}\n        for i in range(1, 6):\n            a_vals = [gparams[i]['a'] for gparams in _PARAMS.values()]\n            b_vals = [gparams[i]['b'] for gparams in _PARAMS.values()]\n            eps_vals = [gparams[i]['eps'] for gparams in _PARAMS.values()]\n            avg_params[i] = {\n                'a': sum(a_vals) / len(a_vals),\n                'b': sum(b_vals) / len(b_vals),\n                'eps': sum(eps_vals) / len(eps_vals),\n            }\n        params = avg_params\n    else:\n        params = _PARAMS[group]\n\n    predictions: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            # numerical safety for log at extremely small or slightly negative due to noise\n            if p < 0.0:\n                p = 0.0\n            a = params[i]['a']\n            b = params[i]['b']\n            eps = params[i]['eps']\n            y = a + b * math.log(p + eps)\n            out[f\"loss_domain_{i}\"] = float(y)\n        predictions.append(out)\n\n    return predictions", "agent_name": "goose", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.971092, "solution": "# Discovered scaling law (shared functional form across groups):\n#   loss_domain_k = a_{g,k} + b_{g,k} * (proportion_domain_k + c_{g,k}) ** d_{g,k}\n# where g indexes the experimental group and k in {1..5} indexes the domain.\n\n\n# Fitted parameters per group and per domain (k = 1..5).\n# Values were obtained by least squares fitting on the provided dataset.\n_PARAMS = {\n    \"160M\": {\n        \"domain_1\": {\"a\": 3.118296598901631, \"b\": -0.8877441483834596, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.3016069560703,   \"b\":  0.0018261061960447534, \"c\": 0.09183755102040818, \"d\": -1.9},\n        \"domain_3\": {\"a\": 3.312678979117116, \"b\": -0.8785517989680411, \"c\": 1e-06, \"d\": 0.25},\n        \"domain_4\": {\"a\": 1.0034194693515974, \"b\": 0.24243472606360897, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 4.118698802652438, \"b\": -1.040736728854194,  \"c\": 0.010205061224489796, \"d\": 0.15},\n    },\n    \"305M\": {\n        \"domain_1\": {\"a\": 2.952929420274062, \"b\": -0.8629898487399583, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.127318377480758, \"b\":  0.007202777918698585, \"c\": 0.061225367346938786, \"d\": -1.15},\n        \"domain_3\": {\"a\": 3.207800159763377, \"b\": -0.8322110176673488, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_4\": {\"a\": 0.9142458490282026, \"b\": 0.23205050091886964, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 0.2308748803753971, \"b\": 2.689570473211351,  \"c\": 0.030613183673469394, \"d\": -0.05},\n    },\n    \"410M\": {\n        \"domain_1\": {\"a\": 2.884545514793264, \"b\": -0.852803939078441,  \"c\": 1e-06, \"d\": 0.2},\n        \"domain_2\": {\"a\": 3.0790530221595462, \"b\": 0.001307985468446201, \"c\": 0.07142942857142857, \"d\": -1.8},\n        \"domain_3\": {\"a\": 3.151353366782936, \"b\": -0.8477484549673475, \"c\": 1e-06, \"d\": 0.2},\n        \"domain_4\": {\"a\": 0.884948385285892, \"b\": 0.22601551215198246, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 4.3056179059111725, \"b\": -1.4673725683803218, \"c\": 0.010205061224489796, \"d\": 0.1},\n    },\n    \"70M\": {\n        \"domain_1\": {\"a\": 2.00061995681349,  \"b\": 0.5657494904538474,  \"c\": 0.010205061224489796, \"d\": -0.2},\n        \"domain_2\": {\"a\": 3.6141195606157925, \"b\": 0.00530072540335944, \"c\": 0.10204161224489797, \"d\": -1.6},\n        \"domain_3\": {\"a\": 3.6293616626237855, \"b\": -0.9227693166043902, \"c\": 1e-06, \"d\": 0.25},\n        \"domain_4\": {\"a\": 1.1898035159399545, \"b\": 0.2720101134529132, \"c\": 0.010205061224489796, \"d\": -0.3},\n        \"domain_5\": {\"a\": 3.249380658214184,  \"b\": 0.2049354477567703,  \"c\": 0.11224567346938777, \"d\": -0.55},\n    },\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Select parameters for the group; if unseen, use average across known groups.\n    if group in _PARAMS:\n        params = _PARAMS[group]\n    else:\n        domains = [f\"domain_{i}\" for i in range(1, 6)]\n        params = {}\n        for d in domains:\n            acc = {\"a\": 0.0, \"b\": 0.0, \"c\": 0.0, \"d\": 0.0}\n            for g in _PARAMS.values():\n                pd = g[d]\n                acc[\"a\"] += pd[\"a\"]\n                acc[\"b\"] += pd[\"b\"]\n                acc[\"c\"] += pd[\"c\"]\n                acc[\"d\"] += pd[\"d\"]\n            n = float(len(_PARAMS))\n            params[d] = {k: v / n for k, v in acc.items()}\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        for k in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{k}\", 0.0))\n            par = params[f\"domain_{k}\"]\n            a, b, c, d = par[\"a\"], par[\"b\"], par[\"c\"], par[\"d\"]\n            # Ensure numerical stability for zero proportions\n            pred = a + b * (p + c) ** d\n            out[f\"loss_domain_{k}\"] = float(pred)\n        outputs.append(out)\n    return outputs", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.971072, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups, but\n                the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Discovered scaling law parameters per group and domain\n    # Model: loss_domain_i = a + b * ln(proportion_domain_i + c)\n    # Coefficients were fitted on the provided dataset.\n    COEFFS: dict[str, dict[int, dict[str, float]]] = {\n        \"160M\": {\n            1: {\"a\": 2.255463034657955, \"b\": -0.1322014387664249, \"c\": 0.0022579423766059038},\n            2: {\"a\": 3.2627500668798435, \"b\": -0.035825707556857135, \"c\": 0.002896657450965321},\n            3: {\"a\": 2.6244424847304453, \"b\": -0.09009357911542895, \"c\": 0.0006498175348279338},\n            4: {\"a\": 1.2003470092201818, \"b\": -0.12033532343430565, \"c\": 0.0017600644406102247},\n            5: {\"a\": 3.0846622790213734, \"b\": -0.14149413638165387, \"c\": 0.027261874278143278},\n        },\n        \"305M\": {\n            1: {\"a\": 2.114124254258875, \"b\": -0.12852626852708746, \"c\": 0.0022579423766059038},\n            2: {\"a\": 3.099085405020726, \"b\": -0.03873697421213334, \"c\": 0.004767225673466724},\n            3: {\"a\": 2.4972468378996573, \"b\": -0.08400352953211676, \"c\": 0.0003948415562753178},\n            4: {\"a\": 1.1027537337452233, \"b\": -0.1151761426543334, \"c\": 0.0017600644406102247},\n            5: {\"a\": 2.919559840422695, \"b\": -0.13456190779289226, \"c\": 0.021250611175238732},\n        },\n        \"410M\": {\n            1: {\"a\": 2.0644864248236448, \"b\": -0.1210234418135825, \"c\": 0.0017600644406102247},\n            2: {\"a\": 3.0475764758867196, \"b\": -0.027711888083369603, \"c\": 0.0013719689515536608},\n            3: {\"a\": 2.4275273847120293, \"b\": -0.08557331062431346, \"c\": 0.0003948415562753178},\n            4: {\"a\": 1.0558984541830227, \"b\": -0.11864017408189217, \"c\": 0.0022579423766059038},\n            5: {\"a\": 2.8419851362063593, \"b\": -0.13840170983545286, \"c\": 0.021250611175238732},\n        },\n        \"70M\": {\n            1: {\"a\": 2.5423740680906737, \"b\": -0.14957785135641832, \"c\": 0.002896657450965321},\n            2: {\"a\": 3.568303753809257, \"b\": -0.0515903682298029, \"c\": 0.007845746694759024},\n            3: {\"a\": 2.891555035928987, \"b\": -0.1000216033928399, \"c\": 0.0008336345619486921},\n            4: {\"a\": 1.4108355902865883, \"b\": -0.13498993068037493, \"c\": 0.0017600644406102247},\n            5: {\"a\": 3.435824595629432, \"b\": -0.1379023492163919, \"c\": 0.027261874278143278},\n        },\n    }\n    DEFAULT_GROUP = \"70M\"\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        pred: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            g = group if group in COEFFS else DEFAULT_GROUP\n            params = COEFFS[g][i]\n            a = params[\"a\"]\n            b = params[\"b\"]\n            c = params[\"c\"]\n            # Ensure numerical stability for very small/negative p\n            if p < 0:\n                p = 0.0\n            y = a + b * math.log(p + c)\n            pred[f\"loss_domain_{i}\"] = float(y)\n        outputs.append(pred)\n    return outputs", "agent_name": "goose", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.971, "solution": "from __future__ import annotations\nfrom typing import Dict, List\n\n# Discovered scaling law (same functional form across groups):\n#   loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + c_{g,i}) ** (-alpha_{g,i})\n# Parameters (a, b, alpha, c) are fitted per group g and domain i.\n\n_PARAMS: Dict[str, Dict[int, Dict[str, float]]] = {\n    \"160M\": {\n        1: {\"a\": -0.11147193199439992, \"b\": 2.3787691982958217, \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.278500369142649,   \"b\": 0.01306711991811282, \"alpha\": 0.8034482758620689, \"c\": 0.034895833333333334},\n        3: {\"a\": 0.8506733951924391,  \"b\": 1.7622754024333271, \"alpha\": 0.05, \"c\": 0.0015625},\n        4: {\"a\": -1.072627406051137,  \"b\": 2.2748208460279304, \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 0.3144198553952366,  \"b\": 2.772096693493694,  \"alpha\": 0.05, \"c\": 0.034895833333333334},\n    },\n    \"305M\": {\n        1: {\"a\": -0.18733739807046243, \"b\": 2.312924189576437,  \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.0583843406593463,  \"b\": 0.05867924234335276, \"alpha\": 0.3672413793103448, \"c\": 0.019791666666666666},\n        3: {\"a\": 2.037098981893403,   \"b\": 0.48486293207181885, \"alpha\": 0.1293103448275862, \"c\": 0.0015625},\n        4: {\"a\": -1.0727458525278075, \"b\": 2.177270392291824,  \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 1.465292570110724,   \"b\": 1.4569415752353443, \"alpha\": 0.0896551724137931, \"c\": 0.034895833333333334},\n    },\n    \"410M\": {\n        1: {\"a\": -0.21860503605523499, \"b\": 2.2855798194974324, \"alpha\": 0.05, \"c\": 0.003125},\n        2: {\"a\": 3.068706883350695,    \"b\": 0.005605907282164662, \"alpha\": 1.0017241379310344, \"c\": 0.034895833333333334},\n        3: {\"a\": 1.958790095134566,    \"b\": 0.4939182316775028,  \"alpha\": 0.1293103448275862, \"c\": 0.0015625},\n        4: {\"a\": -1.050236886559304,   \"b\": 2.1205339628269058, \"alpha\": 0.05, \"c\": 0.003125},\n        5: {\"a\": 1.3466574766335897,   \"b\": 1.4981064795012045,  \"alpha\": 0.0896551724137931, \"c\": 0.034895833333333334},\n    },\n    \"70M\": {\n        1: {\"a\": 2.1270757115611185,  \"b\": 0.4395199897525094,  \"alpha\": 0.2482758620689655, \"c\": 0.013194444444444443},\n        2: {\"a\": 3.552025532207962,   \"b\": 0.04562819235160885, \"alpha\": 0.5258620689655172, \"c\": 0.034895833333333334},\n        3: {\"a\": 1.0433037320725291,  \"b\": 1.8510839638322976,  \"alpha\": 0.05, \"c\": 0.0015625},\n        4: {\"a\": 1.3203779743559676,  \"b\": 0.16265793539818846, \"alpha\": 0.4068965517241379, \"c\": 0.013194444444444443},\n        5: {\"a\": 3.192722657529413,   \"b\": 0.2586205571225181,  \"alpha\": 0.446551724137931, \"c\": 0.09531249999999998},\n    },\n}\n\n_DOMAIN_KEYS = {\n    1: (\"proportion_domain_1\", \"loss_domain_1\"),\n    2: (\"proportion_domain_2\", \"loss_domain_2\"),\n    3: (\"proportion_domain_3\", \"loss_domain_3\"),\n    4: (\"proportion_domain_4\", \"loss_domain_4\"),\n    5: (\"proportion_domain_5\", \"loss_domain_5\"),\n}\n\n_DEF_EPS = 1e-8  # numerical floor to avoid zero to negative-power\n\n\ndef _predict_for_group_row(row: Dict[str, float], params_g: Dict[int, Dict[str, float]]) -> Dict[str, float]:\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p_key, y_key = _DOMAIN_KEYS[i]\n        p = float(row.get(p_key, 0.0))\n        par = params_g[i]\n        # Apply shifted power law with small numerical floor\n        val = par[\"a\"] + par[\"b\"] * ((max(p, 0.0) + max(par[\"c\"], _DEF_EPS)) ** (-par[\"alpha\"]))\n        out[y_key] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Known: {sorted(_PARAMS.keys())}\")\n    params_g = _PARAMS[group]\n    return [_predict_for_group_row(row, params_g) for row in input_data]", "agent_name": "opencode", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2", "reward_r2": 0.9709959950056923, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Model parameters discovered from data analysis\n    # Model: loss_i = A_i * (proportion_i + delta_i)^beta_i\n    # Where i = domain index (1-5)\n    \n    # Parameters for each domain and group\n    # Format: {group: {domain: {'A': A, 'beta': beta, 'delta': delta}}}\n    params = {\n        '70M': {\n            1: {'A': 2.544863, 'beta': -0.055547, 'delta': 0.005023},\n            2: {'A': 3.570086, 'beta': -0.014014, 'delta': 0.008241},\n            3: {'A': 2.900582, 'beta': -0.031946, 'delta': 0.001151},\n            4: {'A': 1.423500, 'beta': -0.083270, 'delta': 0.003754},\n            5: {'A': 3.437510, 'beta': -0.039108, 'delta': 0.031957}\n        },\n        '160M': {\n            1: {'A': 2.260306, 'beta': -0.054464, 'delta': 0.003835},\n            2: {'A': 3.263859, 'beta': -0.010693, 'delta': 0.003073},\n            3: {'A': 2.631247, 'beta': -0.031889, 'delta': 0.000946},\n            4: {'A': 1.213212, 'beta': -0.086238, 'delta': 0.003772},\n            5: {'A': 3.086537, 'beta': -0.044702, 'delta': 0.033190}\n        },\n        '305M': {\n            1: {'A': 2.123389, 'beta': -0.054917, 'delta': 0.003471},\n            2: {'A': 3.100167, 'beta': -0.012180, 'delta': 0.005080},\n            3: {'A': 2.510346, 'beta': -0.030294, 'delta': 0.000525},\n            4: {'A': 1.117573, 'beta': -0.088355, 'delta': 0.003698},\n            5: {'A': 2.920654, 'beta': -0.045656, 'delta': 0.028746}\n        },\n        '410M': {\n            1: {'A': 2.070173, 'beta': -0.054007, 'delta': 0.003024},\n            2: {'A': 3.049263, 'beta': -0.008725, 'delta': 0.001348},\n            3: {'A': 2.444041, 'beta': -0.031212, 'delta': 0.000501},\n            4: {'A': 1.079474, 'beta': -0.090464, 'delta': 0.003987},\n            5: {'A': 2.843735, 'beta': -0.047495, 'delta': 0.027361}\n        }\n    }\n    \n    # Check if group is valid\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(params.keys())}\")\n    \n    results = []\n    \n    for data_point in input_data:\n        # Create prediction dictionary\n        prediction = {}\n        \n        # For each domain (1-5)\n        for domain_idx in range(1, 6):\n            prop_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n            \n            # Get proportion value (default to 0 if not present)\n            proportion = data_point.get(prop_key, 0.0)\n            \n            # Get parameters for this domain and group\n            domain_params = params[group][domain_idx]\n            A = domain_params['A']\n            beta = domain_params['beta']\n            delta = domain_params['delta']\n            \n            # Apply scaling law: loss = A * (proportion + delta)^beta\n            # Handle very small values to avoid numerical issues\n            if proportion + delta <= 0:\n                # This shouldn't happen with delta > 0, but just in case\n                predicted_loss = A * (1e-10) ** beta\n            else:\n                predicted_loss = A * ((proportion + delta) ** beta)\n            \n            # Store prediction\n            prediction[loss_key] = predicted_loss\n        \n        results.append(prediction)\n    \n    return results", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.970854, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Parameters fitted: L = c0 + c1 * (p + c2)^(-c3)\n    # params structure: group -> domain -> [c0, c1, c2, c3]\n    params = {\n    '70M': {\n        'loss_domain_1': [9.049791064506775e-16, 2.544863255075162, 0.005023335215394096, 0.055546740330062044],\n        'loss_domain_2': [2.7486656945851666, 0.8246899154131252, 0.010080775042703558, 0.05659733424323028],\n        'loss_domain_3': [5.985319537346353e-07, 2.9005808574645533, 0.001151070479839812, 0.031946298634857095],\n        'loss_domain_4': [1.2590153991726705, 0.21220778772640178, 0.011700299738268685, 0.3501422357655192],\n        'loss_domain_5': [3.2198538792667297, 0.23277138740838701, 0.10173862328204047, 0.48926338453846824],\n    },\n    '160M': {\n        'loss_domain_1': [7.921177091134831e-11, 2.2603061159513467, 0.0038345630816795984, 0.05446415324935802],\n        'loss_domain_2': [2.4461406598589988, 0.8195058967138309, 0.0038249028744246735, 0.04037177211421319],\n        'loss_domain_3': [7.510940516965377e-07, 2.631246343363198, 0.00094573078880027, 0.03188902494410328],\n        'loss_domain_4': [8.674987985071811e-15, 1.2132115807783543, 0.003771920704982149, 0.0862384287195669],\n        'loss_domain_5': [4.666662783752345e-17, 3.0865366503919907, 0.03318950890733831, 0.04470214262740694],\n    },\n    '305M': {\n        'loss_domain_1': [4.63396739841804e-14, 2.123388567522197, 0.0034713501822073336, 0.05491740702600626],\n        'loss_domain_2': [2.3349069954135566, 0.7673802246129325, 0.006222133135954493, 0.04638693162532186],\n        'loss_domain_3': [6.593427727187525e-13, 2.510346003736797, 0.00052546156955814, 0.030293831005327836],\n        'loss_domain_4': [4.9747859075972676e-08, 1.117572915518561, 0.003698166118962751, 0.08835480936682777],\n        'loss_domain_5': [1.2908709077742297, 1.6311412445258724, 0.03310495146187288, 0.08014352579523258],\n    },\n    '410M': {\n        'loss_domain_1': [6.45159383764858e-15, 2.0701727493297715, 0.0030243803942285714, 0.05400686896572349],\n        'loss_domain_2': [2.2870399938680213, 0.7634132483462176, 0.0017345344382877009, 0.033273045391567696],\n        'loss_domain_3': [3.4557544039691014e-07, 2.4440410784149362, 0.0005010731785675117, 0.03121240344669886],\n        'loss_domain_4': [0.7411064514872984, 0.35914458865064114, 0.007467419826885272, 0.21677194222721097],\n        'loss_domain_5': [4.776331691474593e-12, 2.8437345217824648, 0.0273606303463639, 0.047494879619303806],\n    },\n}\n    \n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}\")\n        \n    group_params = params[group]\n    predictions = []\n    \n    for row in input_data:\n        pred_row = {}\n        for d in range(1, 6):\n            input_key = f\"proportion_domain_{d}\"\n            output_key = f\"loss_domain_{d}\"\n            \n            p = row.get(input_key, 0.0)\n            \n            # Retrieve parameters for this domain\n            c0, c1, c2, c3 = group_params[output_key]\n            \n            # Calculate L = c0 + c1 * (p + c2)^(-c3)\n            # Use math.pow for single scalar power\n            loss = c0 + c1 * math.pow(p + c2, -c3)\n            \n            pred_row[output_key] = loss\n            \n        predictions.append(pred_row)\n        \n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-flash-preview", "reward_r2": 0.970853986879495, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Discovered parameters\n    params = {\n      \"1\": {\n        \"k\": 0.003926356360468923,\n        \"alpha\": 0.05718810672412049,\n        \"groups\": {\n          \"70M\": {\"c\": 0.25035470891403067, \"a\": 2.3081440577674486},\n          \"160M\": {\"c\": 0.11473631324372671, \"a\": 2.145732347753247},\n          \"305M\": {\"c\": 0.032892265162742906, \"a\": 2.0860959948872986},\n          \"410M\": {\"c\": 0.0, \"a\": 2.0606024013061623}\n        }\n      },\n      \"2\": {\n        \"k\": 0.005348734990404739,\n        \"alpha\": 0.04849930034177744,\n        \"groups\": {\n          \"70M\": {\"c\": 2.788938903014082, \"a\": 0.7992339295329967},\n          \"160M\": {\"c\": 2.5294393042337133, \"a\": 0.7313132160229274},\n          \"305M\": {\"c\": 2.4110293528397078, \"a\": 0.6946680409496713},\n          \"410M\": {\"c\": 2.366352895383652, \"a\": 0.6699095227161864}\n        }\n      },\n      \"3\": {\n        \"k\": 0.0007633629218726617,\n        \"alpha\": 0.03416674736689901,\n        \"groups\": {\n          \"70M\": {\"c\": 0.5096484002350068, \"a\": 2.41908212577788},\n          \"160M\": {\"c\": 0.3422949311162433, \"a\": 2.3033031203456074},\n          \"305M\": {\"c\": 0.11331781391577726, \"a\": 2.3803966281988505},\n          \"410M\": {\"c\": 0.0, \"a\": 2.424065471843669}\n        }\n      },\n      \"4\": {\n        \"k\": 0.005278641910570982,\n        \"alpha\": 0.1450495769891357,\n        \"groups\": {\n          \"70M\": {\"c\": 0.7098001982822185, \"a\": 0.7275091490945351},\n          \"160M\": {\"c\": 0.5755162600091049, \"a\": 0.6484659339778274},\n          \"305M\": {\"c\": 0.504693250857682, \"a\": 0.6206758323529236},\n          \"410M\": {\"c\": 0.4860887527396757, \"a\": 0.6045069362922936}\n        }\n      },\n      \"5\": {\n        \"k\": 0.030736480866075007,\n        \"alpha\": 0.04878579210761627,\n        \"groups\": {\n          \"70M\": {\"c\": 0.7516912881140767, \"a\": 2.686857307527944},\n          \"160M\": {\"c\": 0.33125206249053835, \"a\": 2.756227753016214},\n          \"305M\": {\"c\": 0.15563495035933986, \"a\": 2.7646844004467073},\n          \"410M\": {\"c\": 0.0, \"a\": 2.8427953616069908}\n        }\n      }\n    }\n\n    predictions = []\n    for data_point in input_data:\n        pred_point = {}\n        for d in range(1, 6):\n            d_str = str(d)\n            p = data_point.get(f'proportion_domain_{d}', 0.0)\n            d_params = params[d_str]\n            g_params = d_params['groups'][group]\n            \n            # L_i = c_i + a_i * (p_i + k_i)^{-alpha_i}\n            loss = g_params['c'] + g_params['a'] * (p + d_params['k'])**(-d_params['alpha'])\n            pred_point[f'loss_domain_{d}'] = loss\n        predictions.append(pred_point)\n    \n    return predictions", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5", "reward_r2": 0.9708404106349446, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law discovered is a log-linear relationship:\n    loss_domain_i = a_i + b_i * log(proportion_domain_i)\n\n    where a_i and b_i are fitted coefficients that depend on the domain and group.\n\n    For zero proportions, a baseline loss value is used.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values (proportion_domain_i).\n        group: The name of the experimental group for which to make predictions.\n               Supported groups: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) (loss_domain_i).\n    \"\"\"\n    import math\n\n    # Fitted parameters: loss_domain_i = a_i + b_i * log(proportion_domain_i)\n    # Format: {domain_i: {group: {a: float, b: float}}}\n    params = {\n        'domain_1': {\n            '70M': {'a': 2.538716023062174, 'b': -0.15014064985318967},\n            '160M': {'a': 2.2544843272662405, 'b': -0.1315809083635399},\n            '305M': {'a': 2.1175968264069764, 'b': -0.12507426813350242},\n            '410M': {'a': 2.0645520693878363, 'b': -0.1200838190624766},\n        },\n        'domain_2': {\n            '70M': {'a': 3.5716664015145185, 'b': -0.04836401240653225},\n            '160M': {'a': 3.263924262467963, 'b': -0.034830492162166084},\n            '305M': {'a': 3.1006743096532867, 'b': -0.03722876491120557},\n            '410M': {'a': 3.0490737355109614, 'b': -0.02679818184395711},\n        },\n        'domain_3': {\n            '70M': {'a': 2.893348244102368, 'b': -0.09886102692916138},\n            '160M': {'a': 2.623560530670634, 'b': -0.09003128254977677},\n            '305M': {'a': 2.501230421480594, 'b': -0.08239118209742197},\n            '410M': {'a': 2.4342910737037524, 'b': -0.08297977891663602},\n        },\n        'domain_4': {\n            '70M': {'a': 1.4060660023243017, 'b': -0.1362756449783236},\n            '160M': {'a': 1.1970241661817773, 'b': -0.12103109712578866},\n            '305M': {'a': 1.101723420311251, 'b': -0.11479764855541283},\n            '410M': {'a': 1.0636521193056327, 'b': -0.11371123168518989},\n        },\n        'domain_5': {\n            '70M': {'a': 3.434597631728354, 'b': -0.12648654647382065},\n            '160M': {'a': 3.0839186234824836, 'b': -0.12887711537787372},\n            '305M': {'a': 2.9180204995169983, 'b': -0.1269094532480054},\n            '410M': {'a': 2.8412870151502942, 'b': -0.12892583253832435},\n        },\n    }\n\n    # Baseline losses at p=0 (computed from training data)\n    baseline_losses = {\n        'domain_1': {\n            '70M': 3.4149081168601354,\n            '160M': 3.060407302873726,\n            '305M': 2.8980309491789353,\n            '410M': 2.831881281851642,\n        },\n        'domain_2': {\n            '70M': 3.818428775380711,\n            '160M': 3.4721373995346867,\n            '305M': 3.3061837391603213,\n            '410M': 3.230276246563029,\n        },\n        'domain_3': {\n            '70M': 3.60063959478022,\n            '160M': 3.2855554601648356,\n            '305M': 3.1556234171102338,\n            '410M': 3.0982517116672392,\n        },\n        'domain_4': {\n            '70M': 2.266334699876238,\n            '160M': 1.9630575752887789,\n            '305M': 1.83297445467203,\n            '410M': 1.7793668136344885,\n        },\n        'domain_5': {\n            '70M': 3.931742488662131,\n            '160M': 3.594912574404762,\n            '305M': 3.4344130881519273,\n            '410M': 3.374610814377834,\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        output = {}\n\n        # Predict loss for each domain\n        for domain_idx in range(1, 6):\n            domain_key = f'domain_{domain_idx}'\n            proportion_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n\n            # Get the proportion for this domain\n            proportion = data_point.get(proportion_key, 0.0)\n\n            # Get parameters for this domain and group\n            if domain_key in params and group in params[domain_key]:\n                # For zero proportion, use baseline loss\n                if proportion == 0:\n                    if domain_key in baseline_losses and group in baseline_losses[domain_key]:\n                        loss = baseline_losses[domain_key][group]\n                    else:\n                        loss = 3.0  # fallback value\n                else:\n                    # Apply the log-linear model: loss = a + b*log(proportion)\n                    a = params[domain_key][group]['a']\n                    b = params[domain_key][group]['b']\n                    loss = a + b * math.log(proportion)\n\n                output[loss_key] = loss\n\n        results.append(output)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5", "reward_r2": 0.970491, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters for each group and domain\n    # Structure: params[group][domain] = {'L_0': ..., 'L_inf': ..., 'C': ..., 'alpha': ...}\n    params = {\n        '70M': {\n            1: {'L_0': 3.414908, 'L_inf': 0.100000, 'C': 2.447942, 'alpha': 0.055697},\n            2: {'L_0': 3.818429, 'L_inf': 3.293314, 'C': 0.289679, 'alpha': 0.128688},\n            3: {'L_0': 3.600640, 'L_inf': 0.100000, 'C': 2.809106, 'alpha': 0.031725},\n            4: {'L_0': 2.266335, 'L_inf': 1.184513, 'C': 0.285163, 'alpha': 0.269767},\n            5: {'L_0': 3.931742, 'L_inf': 2.131576, 'C': 1.306624, 'alpha': 0.087635},\n        },\n        '160M': {\n            1: {'L_0': 3.060407, 'L_inf': 0.100000, 'C': 2.162561, 'alpha': 0.055281},\n            2: {'L_0': 3.472137, 'L_inf': 2.968259, 'C': 0.301832, 'alpha': 0.095188},\n            3: {'L_0': 3.285555, 'L_inf': 0.100000, 'C': 2.538393, 'alpha': 0.031901},\n            4: {'L_0': 1.963058, 'L_inf': 0.100000, 'C': 1.118286, 'alpha': 0.089544},\n            5: {'L_0': 3.594913, 'L_inf': 0.100000, 'C': 2.985780, 'alpha': 0.041148},\n        },\n        '305M': {\n            1: {'L_0': 2.898031, 'L_inf': 0.100000, 'C': 2.025209, 'alpha': 0.056080},\n            2: {'L_0': 3.306184, 'L_inf': 2.808446, 'C': 0.299215, 'alpha': 0.101365},\n            3: {'L_0': 3.155623, 'L_inf': 0.100000, 'C': 2.414588, 'alpha': 0.030772},\n            4: {'L_0': 1.832974, 'L_inf': 0.100000, 'C': 1.022308, 'alpha': 0.092419},\n            5: {'L_0': 3.434413, 'L_inf': 0.100000, 'C': 2.819827, 'alpha': 0.042872},\n        },\n        '410M': {\n            1: {'L_0': 2.831881, 'L_inf': 0.100000, 'C': 1.971811, 'alpha': 0.055358},\n            2: {'L_0': 3.230276, 'L_inf': 2.748866, 'C': 0.304027, 'alpha': 0.075641},\n            3: {'L_0': 3.098252, 'L_inf': 0.100000, 'C': 2.348189, 'alpha': 0.031759},\n            4: {'L_0': 1.779367, 'L_inf': 0.524991, 'C': 0.569721, 'alpha': 0.146568},\n            5: {'L_0': 3.374611, 'L_inf': 0.100000, 'C': 2.743305, 'alpha': 0.044620},\n        },\n    }\n\n    def predict_loss(proportion: float, domain: int, group: str) -> float:\n        \"\"\"\n        Predict the validation loss for a domain given its proportion in training data.\n\n        Scaling law:\n            loss_i = L_0_i                          if proportion_i = 0\n            loss_i = L_inf_i + C_i * proportion_i^(-alpha_i)   if proportion_i > 0\n\n        where:\n            - L_0_i is the loss when domain i has zero proportion (intrinsic difficulty)\n            - L_inf_i is the asymptotic loss as proportion_i \u2192 \u221e\n            - C_i is a scaling coefficient\n            - alpha_i is the scaling exponent (how fast loss decreases with proportion)\n        \"\"\"\n        p = params[group][domain]\n\n        if proportion == 0:\n            return p['L_0']\n        else:\n            return p['L_inf'] + p['C'] * (proportion ** (-p['alpha']))\n\n    # Generate predictions for each input data point\n    results = []\n\n    for data_point in input_data:\n        predictions = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            proportion_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            if proportion_key in data_point:\n                proportion = data_point[proportion_key]\n                predicted_loss = predict_loss(proportion, domain, group)\n                predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.970459, "solution": "from typing import List, Dict\nimport math\n\n# Offset power-law with asymptote per group and domain:\n# loss_domain_i = a + b * (proportion_domain_i + d) ** c\n# Parameters were fitted on the provided dataset.\n_PARAMS = {\n    \"70M\": {\n        1: {\"a\": 2.1328, \"b\": 0.4344, \"c\": -0.25, \"d\": 0.01320},\n        2: {\"a\": 3.5859, \"b\": 0.0212, \"c\": -0.80, \"d\": 0.05000},\n        3: {\"a\": 2.6248, \"b\": 0.3145, \"c\": -0.20, \"d\": 0.00349},\n        4: {\"a\": 1.3108, \"b\": 0.1693, \"c\": -0.40, \"d\": 0.01320},\n        5: {\"a\": 2.8438, \"b\": 0.6006, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"160M\": {\n        1: {\"a\": 1.7899, \"b\": 0.4896, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.2759, \"b\": 0.0149, \"c\": -0.75, \"d\": 0.03208},\n        3: {\"a\": 2.4432, \"b\": 0.2486, \"c\": -0.20, \"d\": 0.00224},\n        4: {\"a\": 0.9249, \"b\": 0.3149, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.4777, \"b\": 0.6157, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"305M\": {\n        1: {\"a\": 1.6614, \"b\": 0.4760, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.1208, \"b\": 0.0108, \"c\": -0.95, \"d\": 0.05000},\n        3: {\"a\": 2.3548, \"b\": 0.2163, \"c\": -0.20, \"d\": 0.00143},\n        4: {\"a\": 0.8390, \"b\": 0.3014, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.3085, \"b\": 0.6178, \"c\": -0.20, \"d\": 0.05000},\n    },\n    \"410M\": {\n        1: {\"a\": 1.6083, \"b\": 0.4704, \"c\": -0.20, \"d\": 0.00847},\n        2: {\"a\": 3.0372, \"b\": 0.0275, \"c\": -0.45, \"d\": 0.01320},\n        3: {\"a\": 2.2824, \"b\": 0.2203, \"c\": -0.20, \"d\": 0.00143},\n        4: {\"a\": 0.8117, \"b\": 0.2936, \"c\": -0.25, \"d\": 0.00847},\n        5: {\"a\": 2.2138, \"b\": 0.6352, \"c\": -0.20, \"d\": 0.05000},\n    },\n}\n\n# Fallback parameters: average across known groups per domain.\n# This is used if an unknown group is requested.\n_avg_params = {}\nfor dom in range(1, 6):\n    vals = [g[dom] for g in _PARAMS.values()]\n    _avg_params[dom] = {\n        \"a\": sum(v[\"a\"] for v in vals) / len(vals),\n        \"b\": sum(v[\"b\"] for v in vals) / len(vals),\n        \"c\": sum(v[\"c\"] for v in vals) / len(vals),\n        \"d\": sum(v[\"d\"] for v in vals) / len(vals),\n    }\n\n\ndef _predict_for_group(row: Dict[str, float], group: str) -> Dict[str, float]:\n    params = _PARAMS.get(group, _avg_params)\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n        if p < 0.0:\n            p = 0.0\n        pr = params[i]\n        a = pr[\"a\"]\n        b = pr[\"b\"]\n        c = pr[\"c\"]\n        d = pr[\"d\"]\n        y = a + b * ((p + d) ** c)\n        out[f\"loss_domain_{i}\"] = float(y)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    return [_predict_for_group(row, group) for row in input_data]", "agent_name": "opencode", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.970426, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Discovered functional form (per-domain, per-group parameters):\n#   loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + c_{g,i}) ** (-alpha_{g,i})\n# The same functional form is used for all groups g; parameters differ by group and domain.\n#\n# We fitted these parameters on the provided dataset. For the four observed groups\n# ('70M','160M','305M','410M'), we hard-code the best parameters found via a coarse grid\n# over (alpha, c) and least-squares fit for (a, b). For unseen groups, we linearly\n# regress each parameter versus log(model_size) and use that trend to extrapolate.\n\n# Parameters per observed group (a, b, alpha, c) per domain index 1..5\n_PARAMS_BY_GROUP: Dict[str, Dict[int, Dict[str, float]]] = {\n    '70M': {\n        1: {'a': 1.9887972995594814, 'b': 0.5747539023729472, 'alpha': 0.2, 'c': 0.010628090822653227},\n        2: {'a': 3.478375647232149,  'b': 0.10946818216563689,'alpha': 0.29743589743589743, 'c': 0.022132450145006925},\n        3: {'a': 2.662255623173122,  'b': 0.29249248967351243,'alpha': 0.2, 'c': 0.002944084977655257},\n        4: {'a': 1.3106586089315084, 'b': 0.17087362129489647,'alpha': 0.3948717948717949, 'c': 0.012767292740982497},\n        5: {'a': 3.269825262023636,  'b': 0.18572717232438435,'alpha': 0.5897435897435898, 'c': 0.11529793501972682},\n    },\n    '160M': {\n        1: {'a': 1.7790306985292368, 'b': 0.49777281762832476,'alpha': 0.2, 'c': 0.008847319226258555},\n        2: {'a': 3.147643888465916,  'b': 0.12605528549680592,'alpha': 0.2, 'c': 0.008847319226258555},\n        3: {'a': 2.426971402916676,  'b': 0.25800433609650775,'alpha': 0.2, 'c': 0.002450793850108051},\n        4: {'a': 0.7868481183561111, 'b': 0.4401977738333642, 'alpha': 0.2, 'c': 0.007364921771696289},\n        5: {'a': 2.453268819807029,  'b': 0.6393110153314099, 'alpha': 0.2, 'c': 0.05536652819843693},\n    },\n    '305M': {\n        1: {'a': 1.650844002886236,  'b': 0.48398936723908575,'alpha': 0.2, 'c': 0.008847319226258555},\n        2: {'a': 3.145345522432937,  'b': 0.000842624257026407,'alpha': 2.9282051282051285, 'c': 0.16638288403323945},\n        3: {'a': 2.329970918889095,  'b': 0.2305617062841119, 'alpha': 0.2, 'c': 0.0016983205437168204},\n        4: {'a': 0.7519113608459367, 'b': 0.39047606538528545,'alpha': 0.2, 'c': 0.006130904889496624},\n        5: {'a': 2.3271583641121616, 'b': 0.5999351396187924, 'alpha': 0.2, 'c': 0.046089684177061926},\n    },\n    '410M': {\n        1: {'a': 1.6401646612679839, 'b': 0.4464062192995915, 'alpha': 0.2, 'c': 0.007364921771696289},\n        2: {'a': 3.0569561734470225, 'b': 0.012635254264443395,'alpha': 0.6871794871794872, 'c': 0.022132450145006925},\n        3: {'a': 2.257122316003787,  'b': 0.23487158519383375,'alpha': 0.2, 'c': 0.0016983205437168204},\n        4: {'a': 0.6830677895878933, 'b': 0.4103719579323839, 'alpha': 0.2, 'c': 0.007364921771696289},\n        5: {'a': 2.2329632735094207, 'b': 0.6168094915316678, 'alpha': 0.2, 'c': 0.046089684177061926},\n    },\n}\n\n# Linear-in-log(size) trend for unseen groups: parameter ~= u + v * log(model_size)\n# model_size parsed from group string (e.g., '70M' -> 70e6). Values fitted from the same training data.\n# Structure: FITS[domain][param] = (u, v)\n_FITS_BY_LOGSIZE: Dict[int, Dict[str, tuple[float, float]]] = {\n    1: {\n        'a': (5.660934950865894, -0.20419887240098242),\n        'b': (1.7767893971925035, -0.0668774826289396),\n        'alpha': (0.2, 0.0),\n        'c': (0.03922594390798102, -0.0015882162200686668),\n    },\n    2: {\n        'a': (7.408699678416423, -0.22020436827726178),\n        'b': (1.3613732788033166, -0.06808625950778105),\n        'alpha': (-14.612320116647743, 0.8197106109208487),\n        'c': (-0.6519810182884438, 0.03704583868478927),\n    },\n    3: {\n        'a': (6.678219554420149, -0.2232189634307364),\n        'b': (0.9269627425670832, -0.035270492078040415),\n        'alpha': (0.2, 0.0),\n        'c': (0.016804595335682817, -0.0007655292505396046),\n    },\n    4: {\n        'a': (7.3750527374798045, -0.34023825047309186),\n        'b': (-2.0178969199781505, 0.12425623940963766),\n        'alpha': (2.3019722108868343, -0.10760983274344801),\n        'c': (0.07115201468643739, -0.0032884283026312243),\n    },\n    5: {\n        'a': (13.48467303818107, -0.5719893785046406),\n        'b': (-3.9077155402955737, 0.2315531989231151),\n        'alpha': (4.4039444217736685, -0.21521966548689597),\n        'c': (0.8131752563511551, -0.039174158612878644),\n    },\n}\n\n\ndef _parse_group_size(group: str) -> float | None:\n    \"\"\"Parse group string like '70M', '1.3B' into a numeric size in tokens.\n\n    Returns None if parsing fails.\n    \"\"\"\n    if not isinstance(group, str) or not group:\n        return None\n    s = group.strip().upper()\n    try:\n        if s.endswith('B'):\n            return float(s[:-1]) * 1e9\n        if s.endswith('M'):\n            return float(s[:-1]) * 1e6\n        if s.endswith('K'):\n            return float(s[:-1]) * 1e3\n        # Fallback: raw number\n        return float(s)\n    except Exception:\n        return None\n\n\ndef _get_params_for_group(group: str) -> Dict[int, Dict[str, float]]:\n    # If we have exact parameters for this group, return them.\n    if group in _PARAMS_BY_GROUP:\n        return _PARAMS_BY_GROUP[group]\n\n    # Otherwise, extrapolate/interpolate using linear fit versus log(size).\n    size = _parse_group_size(group)\n    if size is None or size <= 0:\n        # Fallback to median known group parameters (use 160M as a reasonable default)\n        return _PARAMS_BY_GROUP['160M']\n\n    logn = math.log(size)\n    params: Dict[int, Dict[str, float]] = {}\n    for d, fits in _FITS_BY_LOGSIZE.items():\n        a_u, a_v = fits['a']\n        b_u, b_v = fits['b']\n        al_u, al_v = fits['alpha']\n        c_u, c_v = fits['c']\n        a = a_u + a_v * logn\n        b = b_u + b_v * logn\n        alpha = al_u + al_v * logn\n        c = c_u + c_v * logn\n        # Guardrails: keep parameters in reasonable ranges\n        alpha = max(0.05, float(alpha))\n        c = max(1e-6, float(c))\n        b = max(1e-8, float(b))\n        params[d] = {'a': float(a), 'b': float(b), 'alpha': float(alpha), 'c': float(c)}\n    return params\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys: 'proportion_domain_1'..'_5'.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s):\n        'loss_domain_1'..'_5'.\n    \"\"\"\n    params_by_domain = _get_params_for_group(group)\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        out: Dict[str, float] = {}\n        for d in range(1, 6):\n            p = float(row.get(f'proportion_domain_{d}', 0.0))\n            par = params_by_domain[d]\n            a = par['a']\n            b = par['b']\n            alpha = par['alpha']\n            c = par['c']\n            # Ensure numerical stability\n            x = max(0.0, p) + c\n            y = a + b * (x ** (-alpha))\n            out[f'loss_domain_{d}'] = float(y)\n        outputs.append(out)\n    return outputs", "agent_name": "goose", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5", "reward_r2": 0.968671, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # The discovered scaling law: loss_domain_i = baseline_i + coeff_i * proportion_domain_i^alpha\n    # Where alpha is a universal exponent that applies to all domains and groups\n\n    ALPHA = 0.2724489795918367\n\n    # Fitted parameters for each domain and group\n    PARAMETERS = {\n        1: {\n            '70M': {'baseline': 3.406631, 'coeff': -0.944830},\n            '160M': {'baseline': 3.048356, 'coeff': -0.877175},\n            '305M': {'baseline': 2.884640, 'coeff': -0.852095},\n            '410M': {'baseline': 2.816754, 'coeff': -0.841401},\n        },\n        2: {\n            '70M': {'baseline': 3.818985, 'coeff': -0.259324},\n            '160M': {'baseline': 3.471953, 'coeff': -0.237293},\n            '305M': {'baseline': 3.306313, 'coeff': -0.225400},\n            '410M': {'baseline': 3.229731, 'coeff': -0.217373},\n        },\n        3: {\n            '70M': {'baseline': 3.600080, 'coeff': -0.919650},\n            '160M': {'baseline': 3.284799, 'coeff': -0.875579},\n            '305M': {'baseline': 3.154179, 'coeff': -0.904445},\n            '410M': {'baseline': 3.096721, 'coeff': -0.921298},\n        },\n        4: {\n            '70M': {'baseline': 2.261856, 'coeff': -0.996885},\n            '160M': {'baseline': 1.959060, 'coeff': -0.888851},\n            '305M': {'baseline': 1.828874, 'coeff': -0.850592},\n            '410M': {'baseline': 1.775782, 'coeff': -0.828452},\n        },\n        5: {\n            '70M': {'baseline': 3.963172, 'coeff': -0.528051},\n            '160M': {'baseline': 3.626490, 'coeff': -0.542729},\n            '305M': {'baseline': 3.461341, 'coeff': -0.544839},\n            '410M': {'baseline': 3.399876, 'coeff': -0.561194},\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            proportion_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            if proportion_key in data_point:\n                proportion = data_point[proportion_key]\n\n                # Get the fitted parameters for this domain and group\n                baseline = PARAMETERS[domain][group]['baseline']\n                coeff = PARAMETERS[domain][group]['coeff']\n\n                # Apply the scaling law: loss = baseline + coeff * proportion^alpha\n                if proportion == 0:\n                    # When proportion is 0, loss equals baseline\n                    predicted_loss = baseline\n                else:\n                    predicted_loss = baseline + coeff * (proportion ** ALPHA)\n\n                prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5", "reward_r2": 0.968336, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Fitted parameters for each (group, domain) pair\n    # Model: loss_domain_i = a + b / (proportion_domain_i + eps)\n    params = {\n        '70M': {\n            1: {'a': 2.5662, 'b': 0.0488, 'eps': 0.0576},\n            2: {'a': 3.5963, 'b': 0.0141, 'eps': 0.0636},\n            3: {'a': 3.0578, 'b': 0.0084, 'eps': 0.0155},\n            4: {'a': 1.5025, 'b': 0.0288, 'eps': 0.0377},\n            5: {'a': 3.3529, 'b': 0.1127, 'eps': 0.1952},\n        },\n        '160M': {\n            1: {'a': 2.2834, 'b': 0.0402, 'eps': 0.0519},\n            2: {'a': 3.2866, 'b': 0.0083, 'eps': 0.0445},\n            3: {'a': 2.7769, 'b': 0.0073, 'eps': 0.0143},\n            4: {'a': 1.2831, 'b': 0.0255, 'eps': 0.0375},\n            5: {'a': 2.9952, 'b': 0.1205, 'eps': 0.2034},\n        },\n        '305M': {\n            1: {'a': 2.1469, 'b': 0.0374, 'eps': 0.0498},\n            2: {'a': 3.1226, 'b': 0.0097, 'eps': 0.0528},\n            3: {'a': 2.6482, 'b': 0.0059, 'eps': 0.0117},\n            4: {'a': 1.1838, 'b': 0.0240, 'eps': 0.0370},\n            5: {'a': 2.8383, 'b': 0.1097, 'eps': 0.1856},\n        },\n        '410M': {\n            1: {'a': 2.0943, 'b': 0.0350, 'eps': 0.0476},\n            2: {'a': 3.0684, 'b': 0.0057, 'eps': 0.0351},\n            3: {'a': 2.5829, 'b': 0.0059, 'eps': 0.0115},\n            4: {'a': 1.1439, 'b': 0.0241, 'eps': 0.0379},\n            5: {'a': 2.7604, 'b': 0.1109, 'eps': 0.1828},\n        },\n    }\n\n    # Get parameters for the specified group\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(params.keys())}\")\n\n    group_params = params[group]\n\n    # Generate predictions\n    results = []\n    for data_point in input_data:\n        predictions = {}\n\n        # Predict loss for each domain\n        for domain_idx in range(1, 6):\n            proportion_key = f'proportion_domain_{domain_idx}'\n            loss_key = f'loss_domain_{domain_idx}'\n\n            # Get the proportion value\n            if proportion_key not in data_point:\n                raise ValueError(f\"Missing input key: {proportion_key}\")\n\n            proportion = data_point[proportion_key]\n\n            # Apply the scaling law: loss = a + b / (proportion + eps)\n            p = group_params[domain_idx]\n            predicted_loss = p['a'] + p['b'] / (proportion + p['eps'])\n\n            predictions[loss_key] = predicted_loss\n\n        results.append(predictions)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.9683293523927846, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Fitted parameters (A, B, C) for each group and domain\n    # Structure: fitted_params[group][domain_key] = {'A': A_val, 'B': B_val, 'C': C_val}\n    fitted_params = {\n        '70M': {\n            'domain_1': {'A': 0.0488, 'B': 0.0576, 'C': 2.5662},\n            'domain_2': {'A': 0.0141, 'B': 0.0636, 'C': 3.5963},\n            'domain_3': {'A': 0.0084, 'B': 0.0155, 'C': 3.0578},\n            'domain_4': {'A': 0.0288, 'B': 0.0377, 'C': 1.5025},\n            'domain_5': {'A': 0.1127, 'B': 0.1952, 'C': 3.3529},\n        },\n        '160M': {\n            'domain_1': {'A': 0.0402, 'B': 0.0519, 'C': 2.2834},\n            'domain_2': {'A': 0.0083, 'B': 0.0445, 'C': 3.2866},\n            'domain_3': {'A': 0.0073, 'B': 0.0143, 'C': 2.7768},\n            'domain_4': {'A': 0.0255, 'B': 0.0375, 'C': 1.2831},\n            'domain_5': {'A': 0.1205, 'B': 0.2034, 'C': 2.9952},\n        },\n        '305M': {\n            'domain_1': {'A': 0.0374, 'B': 0.0498, 'C': 2.1469},\n            'domain_2': {'A': 0.0097, 'B': 0.0528, 'C': 3.1226},\n            'domain_3': {'A': 0.0059, 'B': 0.0117, 'C': 2.6482},\n            'domain_4': {'A': 0.0240, 'B': 0.0370, 'C': 1.1838},\n            'domain_5': {'A': 0.1097, 'B': 0.1856, 'C': 2.8383},\n        },\n        '410M': {\n            'domain_1': {'A': 0.0350, 'B': 0.0476, 'C': 2.0943},\n            'domain_2': {'A': 0.0057, 'B': 0.0351, 'C': 3.0684},\n            'domain_3': {'A': 0.0059, 'B': 0.0115, 'C': 2.5829},\n            'domain_4': {'A': 0.0241, 'B': 0.0379, 'C': 1.1439},\n            'domain_5': {'A': 0.1109, 'B': 0.1828, 'C': 2.7604},\n        },\n    }\n\n    predictions = []\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            prop_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            domain_key = f'domain_{i}'\n\n            if prop_key in data_point and group in fitted_params and domain_key in fitted_params[group]:\n                proportion = data_point[prop_key]\n                params = fitted_params[group][domain_key]\n                A, B, C = params['A'], params['B'], params['C']\n\n                # Apply the scaling law: Loss = C + A / (Proportion + B)\n                # Ensure Proportion + B is not zero; B is fitted to be positive, so this should be safe.\n                predicted_loss = C + A / (proportion + B)\n                predicted_losses[loss_key] = predicted_loss\n            else:\n                # If proportion data is missing or group/domain parameters are not found,\n                # we cannot make a prediction for this loss.\n                predicted_losses[loss_key] = float('nan') # or raise an error, or a default value\n\n        predictions.append(predicted_losses)\n\n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9682354213902405, "solution": "import json\nimport math\n\n# Parameters for the inverse model: loss_i = a_i / (proportion_i + d_i) + b_i\n# Fitted for each group (70M, 160M, 305M, 410M) and each domain (1-5)\n# Format: {group: {domain: [a, d, b], ...}, ...}\n_PARAMETERS = {\n    \"70M\": {\n        1: [0.04878599086138536, 0.05761346405158714, 2.5662012376540178],\n        2: [0.014135387206822326, 0.06364850513781467, 3.596343634872201],\n        3: [0.008387904751418776, 0.015461968754208568, 3.05781474423163],\n        4: [0.028801775868037463, 0.03771563383219453, 1.5025024666563043],\n        5: [0.11267690590215215, 0.19519809588743528, 3.3529296445854784]\n    },\n    \"160M\": {\n        1: [0.04024460778235155, 0.051905757515631736, 2.283446663607906],\n        2: [0.008264021316061512, 0.044544788433985835, 3.286615779366045],\n        3: [0.007256837974328765, 0.014274056969858308, 2.77684652780886],\n        4: [0.025481918408022042, 0.037492446926947996, 1.2830889217612353],\n        5: [0.12054804290383295, 0.2034031192808916, 2.9951524979661333]\n    },\n    \"305M\": {\n        1: [0.03737314553894349, 0.04983513099255469, 2.1468583883982597],\n        2: [0.009696865197410729, 0.05283332111654462, 3.12264681601252],\n        3: [0.00592020572808425, 0.011673405159810744, 2.6482433111606776],\n        4: [0.024006354429815788, 0.03699452722522798, 1.1838139275097523],\n        5: [0.10974681269658985, 0.18557303638024017, 2.8382615981606016]\n    },\n    \"410M\": {\n        1: [0.03501902535540624, 0.04755839242291932, 2.0943034559232316],\n        2: [0.005683702629988877, 0.03511169823166188, 3.068401346850442],\n        3: [0.005902396531300411, 0.011459054884304518, 2.5829423357570143],\n        4: [0.024105740877550415, 0.03794729822983249, 1.1439330683530762],\n        5: [0.11091769605008381, 0.18275848984108722, 2.7603882181164185]\n    }\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Check if group is known\n    if group not in _PARAMETERS:\n        raise ValueError(f\"Unknown group: {group}. Available groups: {list(_PARAMETERS.keys())}\")\n\n    # Get parameters for this group\n    group_params = _PARAMETERS[group]\n\n    # For each data point, compute predictions for each domain\n    predictions = []\n    for data_point in input_data:\n        pred = {}\n        for domain in range(1, 6):\n            # Input key\n            prop_key = f\"proportion_domain_{domain}\"\n            # Output key\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get proportion value (should be present)\n            proportion = data_point.get(prop_key, 0.0)  # default 0 if missing\n\n            # Get parameters for this domain\n            a, d, b = group_params[domain]\n\n            # Apply the inverse model: loss = a / (proportion + d) + b\n            # To avoid division by zero, we use proportion + d (d is positive)\n            loss = a / (proportion + d) + b\n\n            # Store prediction\n            pred[loss_key] = loss\n\n        predictions.append(pred)\n\n    return predictions\n\n\n# Example usage (for testing only)\nif __name__ == \"__main__\":\n    # Example input\n    example_input = [\n        {\"proportion_domain_1\": 0.5, \"proportion_domain_2\": 0.1875,\n         \"proportion_domain_3\": 0.0625, \"proportion_domain_4\": 0.0625,\n         \"proportion_domain_5\": 0.1875},\n        {\"proportion_domain_1\": 0.0, \"proportion_domain_2\": 0.25,\n         \"proportion_domain_3\": 0.25, \"proportion_domain_4\": 0.25,\n         \"proportion_domain_5\": 0.25}\n    ]\n    example_group = \"70M\"\n    result = law(example_input, example_group)\n    print(\"Example predictions:\")\n    for i, pred in enumerate(result):\n        print(f\"  Data point {i}: {pred}\")", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.968235, "solution": "import json\nimport os\n\n# Load the fitted parameters from the JSON file\ndef load_fitted_params():\n    try:\n        # Assuming fitted_params.json is in the same directory as law.py or /app\n        # During testing, the law.py might be in a different context, so we try to be robust.\n        current_dir = os.path.dirname(__file__)\n        param_path_local = os.path.join(current_dir, 'fitted_params.json')\n        param_path_app = os.path.join('/app', 'fitted_params.json')\n\n        if os.path.exists(param_path_local):\n            with open(param_path_local, 'r') as f:\n                return json.load(f)\n        elif os.path.exists(param_path_app):\n            with open(param_path_app, 'r') as f:\n                return json.load(f)\n        else:\n            raise FileNotFoundError(\"fitted_params.json not found in current directory or /app.\")\n    except Exception as e:\n        print(f\"Error loading fitted parameters: {e}\")\n        # Fallback to empty parameters if loading fails, or raise a more specific error\n        return {}\n\nFITTED_PARAMS = load_fitted_params()\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    predictions = []\n    group_params = FITTED_PARAMS.get(group)\n\n    if not group_params:\n        # Handle cases where the group is not found in the fitted parameters.\n        # This could return default values, raise an error, or return None/NaN.\n        # For now, let's return None for all predictions.\n        print(f\"Warning: Group '{group}' not found in fitted parameters. Returning None for losses.\")\n        for _ in input_data:\n            predicted_losses = {}\n            for i in range(1, 6):\n                predicted_losses[f'loss_domain_{i}'] = None\n            predictions.append(predicted_losses)\n        return predictions\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            \n            proportion = data_point.get(proportion_key)\n            domain_params = group_params.get(f'domain_{i}')\n\n            if proportion is None or domain_params is None or any(p is None for p in domain_params.values()):\n                predicted_losses[loss_key] = None\n                continue\n            \n            A = domain_params['A']\n            k = domain_params['k']\n            B = domain_params['B']\n            \n            # Apply the scaling law: loss = A / (proportion + k) + B\n            # Ensure k is not extremely small if proportion is 0 or very small to avoid overflow\n            # We've bounded k to be >= 0.0001 during fitting.\n            predicted_loss = A / (proportion + k) + B\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n        \n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.966399, "solution": "from __future__ import annotations\nimport math\nfrom typing import Dict, List\n\n# Discovered functional form (same for all groups):\n#   loss_domain_i = a[group][i] + b[group][i] * log(proportion_domain_i + EPS)\n# A small epsilon handles zero proportions.\nEPS = 0.003125  # min positive proportion observed / 10\n\n# Fitted coefficients (a, b) per group and domain i in {1..5}\nCOEFS: Dict[str, Dict[int, tuple[float, float]]] = {\n    \"160M\": {\n        1: (2.2424684059708717, -0.1412039367794934),\n        2: (3.2615055117992546, -0.036505329124803795),\n        3: (2.503204375028289, -0.13515973441168078),\n        4: (1.167057464525046, -0.1374903829439023),\n        5: (3.1050702723961345, -0.09354145265121051),\n    },\n    \"305M\": {\n        1: (2.101566717029658, -0.137250480526824),\n        2: (3.1064301933569487, -0.034670424237437165),\n        3: (2.347069418836081, -0.1395498701909293),\n        4: (1.0709224798061698, -0.13158540389099785),\n        5: (2.9376949827197802, -0.09412677108369148),\n    },\n    \"410M\": {\n        1: (2.0433633841009002, -0.1355817799554127),\n        2: (3.036930564759394, -0.033447198672712085),\n        3: (2.2745949072179825, -0.14214431175511827),\n        4: (1.0375379829435523, -0.1281673168777551),\n        5: (2.860506493994419, -0.09695843157676008),\n    },\n    \"70M\": {\n        1: (2.538957210154492, -0.15195781604593908),\n        2: (3.589039351229478, -0.03988167092985459),\n        3: (2.7789903294958576, -0.1420028367096475),\n        4: (1.3734832511282675, -0.15423705944877764),\n        5: (3.455746153156199, -0.09113184327935854),\n    },\n}\n\n# Fallback coefficients: average across known groups per domain (used if group not found)\nAVG_COEFS = {\n    i: (\n        sum(COEFS[g][i][0] for g in COEFS) / len(COEFS),\n        sum(COEFS[g][i][1] for g in COEFS) / len(COEFS),\n    )\n    for i in range(1, 6)\n}\n\n\ndef _resolve_group(group: str) -> Dict[int, tuple[float, float]]:\n    if group in COEFS:\n        return COEFS[group]\n    # Try numeric nearest match like '300M' -> closest of known keys\n    import re\n    m = re.search(r\"(\\d+(?:\\.\\d+)?)\", group)\n    if m:\n        target = float(m.group(1))\n        def num(k: str) -> float:\n            mk = re.search(r\"(\\d+(?:\\.\\d+)?)\", k)\n            return float(mk.group(1)) if mk else float(\"inf\")\n        nearest = min(COEFS.keys(), key=lambda k: abs(num(k) - target))\n        return COEFS[nearest]\n    return AVG_COEFS  # last-resort fallback\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups, while\n                coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coefs = _resolve_group(group)\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        out: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            a, b = coefs[i]\n            out[f\"loss_domain_{i}\"] = a + b * math.log(p + EPS)\n        outputs.append(out)\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5", "reward_r2": 0.96542, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law follows a power law model: loss_domain_i = a_i * (proportion_domain_i)^(-b_i)\n    where a_i and b_i are domain-specific coefficients that vary by model size group.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys are 'proportion_domain_1'\n                    through 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n                Should be one of: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted loss_domain_1 through loss_domain_5.\n    \"\"\"\n\n    # Fitted parameters for power law model: loss = a * (proportion)^(-b)\n    # These were fitted using scipy.optimize.curve_fit on the experimental dataset\n    parameters = {\n        \"160M\": {\n            1: {\"a\": 2.2622394738070954, \"b\": 0.05305230055807545},\n            2: {\"a\": 3.2646400859561724, \"b\": 0.010446160735262215},\n            3: {\"a\": 2.6379005393307238, \"b\": 0.030809881360294254},\n            4: {\"a\": 1.2168896281087662, \"b\": 0.08337725170858164},\n            5: {\"a\": 3.0857224560131016, \"b\": 0.039875164268842474},\n        },\n        \"305M\": {\n            1: {\"a\": 2.124886418748417, \"b\": 0.053673219712720203},\n            2: {\"a\": 3.101532158667605, \"b\": 0.011722348365702549},\n            3: {\"a\": 2.5141204041736103, \"b\": 0.029664627417788678},\n            4: {\"a\": 1.1208478064340652, \"b\": 0.08553463709842556},\n            5: {\"a\": 2.9197997341340862, \"b\": 0.04145492785494071},\n        },\n        \"410M\": {\n            1: {\"a\": 2.071494574419323, \"b\": 0.052918397685703625},\n            2: {\"a\": 3.04952958026863, \"b\": 0.008635628449930979},\n            3: {\"a\": 2.447692223080657, \"b\": 0.03058843279314549},\n            4: {\"a\": 1.0827937621785138, \"b\": 0.0874293859289332},\n            5: {\"a\": 2.8432374398519316, \"b\": 0.04312786024778481},\n        },\n        \"70M\": {\n            1: {\"a\": 2.547611674057732, \"b\": 0.05370607557144707},\n            2: {\"a\": 3.5729182354113966, \"b\": 0.013180493524131429},\n            3: {\"a\": 2.908629500537471, \"b\": 0.030739009328856415},\n            4: {\"a\": 1.4271407005367267, \"b\": 0.08068857938614302},\n            5: {\"a\": 3.436049097361814, \"b\": 0.03538256136605673},\n        },\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        for domain in range(1, 6):\n            prop_key = f\"proportion_domain_{domain}\"\n            loss_key = f\"loss_domain_{domain}\"\n\n            # Get proportion for this domain\n            proportion = data_point[prop_key]\n\n            # Get parameters for this domain and group\n            params = parameters[group][domain]\n            a = params[\"a\"]\n            b = params[\"b\"]\n\n            # Apply power law: loss = a * (proportion)^(-b)\n            if proportion > 0:\n                predicted_loss = a * (proportion ** (-b))\n            else:\n                # When proportion is 0, the loss is theoretically undefined (approaches infinity)\n                # For practical purposes, we use the baseline value (approaching when prop -> 0)\n                # which is approximated by the maximum observed baseline\n                predicted_loss = a * (0.001 ** (-b))  # Use a small value as proxy\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.965021, "solution": "from __future__ import annotations\n\nfrom typing import Dict, List\n\n# Discovered scaling law (same functional form for all groups):\n#   loss_domain_i(p) = a_i + b_i * p + c_i * sqrt(p_eps) + d_i / sqrt(p_eps)\n# where p_eps = max(p, eps) with eps = 1e-8 for numerical stability at p=0.\n# Coefficients (a_i, b_i, c_i, d_i) vary by experimental group and by domain i.\n# These were fit via least squares on the provided dataset.\n\nimport math\n\n# Group- and domain-specific parameters (a, b, c, d)\n_PARAMS: Dict[str, Dict[int, List[float]]] = {\n    # 160M\n    \"160M\": {\n        1: [2.852643507112996, 0.5079373888086807, -1.0796605253398814, 2.0787176420883635e-05],\n        2: [3.354383873226113, -0.37869509907848437, 0.0896600728386695, 1.177445603007801e-05],\n        3: [2.347377598500148, -13.805640202637328, 5.641670151072003, 9.376136946540265e-05],\n        4: [1.8042334785143834, 0.8353298815978365, -1.2981302396786871, 1.589539097983332e-05],\n        5: [3.5849119562787277, 0.2559696004923059, -0.758695672520524, 1.0076550902818928e-06],\n    },\n    # 305M\n    \"305M\": {\n        1: [2.706834215254821, 0.5880677740198962, -1.1276437000774089, 1.9130955209367118e-05],\n        2: [3.2017362691741527, -0.37187187552261475, 0.07149899259874355, 1.0444032008682824e-05],\n        3: [2.169244558185138, -14.140803471052271, 5.865028311622053, 9.857923560960747e-05],\n        4: [1.7004746672414206, 0.9689402903601732, -1.3627948240241132, 1.326360669137494e-05],\n        5: [3.4565779916529773, 0.38064813646972745, -0.9095505284674135, -2.207395814670825e-06],\n    },\n    # 410M\n    \"410M\": {\n        1: [2.6309647984255844, 0.5673263318126346, -1.085609650668732, 2.010250371618536e-05],\n        2: [3.112610624866361, -0.3369700553653706, 0.10271603621695127, 1.1765535009316992e-05],\n        3: [2.090987095821702, -14.41194746580547, 5.986201940650726, 0.00010066659956643521],\n        4: [1.6814138568367192, 1.1507555128736286, -1.4921179761773755, 9.810216859529726e-06],\n        5: [3.3436836877501346, 0.2595236072021162, -0.7633518960486015, 3.100351755000551e-06],\n    },\n    # 70M\n    \"70M\": {\n        1: [3.232089856516487, 0.6342384359826007, -1.284813531062777, 1.8294673741495497e-05],\n        2: [3.7081016310626413, -0.444398338713182, 0.06425723222437253, 1.1032071859421705e-05],\n        3: [2.727368677530126, -12.547994180570411, 4.978112989985035, 8.727731059550263e-05],\n        4: [2.1695959734275863, 1.5585707751928726, -1.9218402484504162, 9.693091047463545e-06],\n        5: [4.009115069855263, 0.48733821145363176, -1.042620301077381, -7.726820572537514e-06],\n    },\n}\n\n# Fallback parameters if group not found: fit across all groups jointly per domain.\n_DEFAULT_PARAMS: Dict[int, List[float]] = {\n    1: [2.8556330943274784, 0.5743924826559886, -1.1444318517872365, 1.9578827271952478e-05],\n    2: [3.3442080995823162, -0.3829838421699124, 0.08203308346968709, 1.1254023726860987e-05],\n    3: [2.3337444824927815, -13.726596330795772, 5.617753348542388, 9.507112880974419e-05],\n    4: [1.838929494005038, 1.1283991150062023, -1.5187208220827102, 1.2165576394408536e-05],\n    5: [3.5985721763843057, 0.34586988890451575, -0.8685545995285723, -1.4565523854500717e-06],\n}\n\n\ndef _predict_loss_for_domain(p: float, coeffs: List[float]) -> float:\n    eps = 1e-8\n    p_clamped = max(min(float(p), 1.0), 0.0)\n    r = math.sqrt(max(p_clamped, eps))\n    a, b, c, d = coeffs\n    return a + b * p_clamped + c * r + d / r\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s):\n        loss_domain_1 .. loss_domain_5.\n    \"\"\"\n    group_params = _PARAMS.get(group, None)\n    if group_params is None:\n        group_params = _DEFAULT_PARAMS\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = row.get(f\"proportion_domain_{i}\")\n            if p is None:\n                # If missing, assume zero contribution for that domain.\n                p = 0.0\n            coeffs = group_params.get(i, _DEFAULT_PARAMS[i])\n            pred[f\"loss_domain_{i}\"] = _predict_loss_for_domain(p, coeffs)\n        outputs.append(pred)\n    return outputs", "agent_name": "goose", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2", "reward_r2": 0.9603660325813097, "solution": "import numpy as np\nfrom typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    \n    The scaling law for domain mixture in language model pre-training follows an\n    exponential decay form:\n        loss_domain_i = \u03b1_i * exp(-\u03b2_i * proportion_domain_i) + \u03b3_i\n    \n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    \n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Parameters fitted from the training data for each group and domain\n    # Format: {group: {domain: (alpha, beta, gamma)}}\n    parameters = {\n        '70M': {\n            1: (0.739473, 12.863625, 2.666348),\n            2: (0.177182, 14.641798, 3.641247),\n            3: (0.456612, 50.442102, 3.143153),\n            4: (0.654308, 19.872508, 1.610290),\n            5: (0.461188, 4.603233, 3.458564)\n        },\n        '160M': {\n            1: (0.682762, 13.822854, 2.369861),\n            2: (0.155643, 17.604643, 3.316494),\n            3: (0.430615, 53.729461, 2.854183),\n            4: (0.582318, 19.938110, 1.378787),\n            5: (0.469066, 4.358060, 3.104265)\n        },\n        '305M': {\n            1: (0.662690, 14.266620, 2.228801),\n            2: (0.150461, 16.158770, 3.155723),\n            3: (0.437894, 61.560421, 2.717248),\n            4: (0.556845, 20.098877, 1.274425),\n            5: (0.474134, 4.678997, 2.942271)\n        },\n        '410M': {\n            1: (0.653111, 14.678035, 2.172276),\n            2: (0.139821, 19.700209, 3.090455),\n            3: (0.445505, 62.332274, 2.652279),\n            4: (0.543846, 19.799034, 1.233952),\n            5: (0.486017, 4.649797, 2.865121)\n        }\n    }\n    \n    # Check if group is valid\n    if group not in parameters:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(parameters.keys())}\")\n    \n    # Get parameters for this group\n    group_params = parameters[group]\n    \n    # Initialize result list\n    results = []\n    \n    for data_point in input_data:\n        # Initialize prediction dictionary\n        prediction = {}\n        \n        # Predict loss for each domain\n        for domain in range(1, 6):\n            prop_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n            \n            # Get proportion for this domain\n            if prop_key not in data_point:\n                raise ValueError(f\"Missing input variable: {prop_key}\")\n            \n            proportion = data_point[prop_key]\n            \n            # Get parameters for this domain\n            alpha, beta, gamma = group_params[domain]\n            \n            # Apply exponential decay law\n            # Handle edge case where proportion might be exactly 0\n            if proportion <= 0:\n                # When proportion is 0, use the limit as p->0: alpha + gamma\n                predicted_loss = alpha + gamma\n            else:\n                predicted_loss = alpha * np.exp(-beta * proportion) + gamma\n            \n            prediction[loss_key] = float(predicted_loss)\n        \n        results.append(prediction)\n    \n    return results", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o3", "reward_r2": 0.9379072323150286, "solution": "import math\n\neps = 1e-4  # small constant to avoid log(0)\n\n# Pre-computed coefficients for each model size (group)\n# loss_domain_i = intercept[group][i] + slope[group][i] * ln(proportion_domain_i + eps)\n_slope = {\n    '70M':   {1: -0.08576965184491259, 2: -0.02112006507699999, 3: -0.0670701925472496, 4: -0.08177119680651854, 5: -0.05565421575888993},\n    '160M':  {1: -0.07999971655626487, 2: -0.019391734511526593, 3: -0.06387334934699097, 4: -0.07289970938551527, 5: -0.05717848070226565},\n    '305M':  {1: -0.07786837968301963, 2: -0.018390798367162706, 3: -0.06604426750330716, 4: -0.06978964405667937, 5: -0.057705526180705076},\n    '410M':  {1: -0.07705237903612913, 2: -0.017799580108894552, 3: -0.06728070485198959, 4: -0.06794024380520877, 5: -0.05954798320878324},\n}\n\n_intercept = {\n    '70M':   {1: 2.642865350100738, 2: 3.624938704129457, 3: 2.984082886764596, 4: 1.5197999684082721, 5: 3.4777450913291093},\n    '160M':  {1: 2.3379482012968085, 2: 3.2941184797908094, 3: 2.698234126707024, 4: 1.29746010289275, 5: 3.127593940602042},\n    '305M':  {1: 2.1939843086827238, 2: 3.137512552401725, 3: 2.5479442702720028, 4: 1.1956432975415627, 5: 2.9601767597640576},\n    '410M':  {1: 2.1341887819076444, 2: 3.0666775401979516, 3: 2.4791606926460794, 4: 1.159161359690975, 5: 2.8835495923809824},\n}\n\n\ndef _predict_single(x: dict[str, float], g: str) -> dict[str, float]:\n    \"\"\"Predict losses for the five domains for one observation.\"\"\"\n    if g not in _slope:\n        raise ValueError(f\"Unknown group '{g}'. Known groups: {list(_slope)}\")\n    out = {}\n    for i in range(1, 5 + 1):\n        prop_key = f\"proportion_domain_{i}\"\n        loss_key = f\"loss_domain_{i}\"\n        prop = x.get(prop_key)\n        if prop is None:\n            raise ValueError(f\"Missing key '{prop_key}' in input data point\")\n        out[loss_key] = _intercept[g][i] + _slope[g][i] * math.log(prop + eps)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts domain-specific validation losses from domain mixture proportions using the discovered scaling law.\n\n    The law is identical for all groups: a linear relationship between the loss for a domain and the natural\n    logarithm of that domain's proportion in the training mix. Each group (model size) possesses its own\n    set of slope and intercept parameters.\n    \"\"\"\n    return [_predict_single(row, group) for row in input_data]", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5.2", "reward_r2": 0.9331370854792963, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Scaling law discovered from /app/data.\n# Functional form (same for all groups and domains):\n#   loss_j = base_{g,j} - sum_{k=1..5} b_{g,j,k} * sqrt(p_k)\n# where p_k is the mixture proportion of training domain k.\n#\n# Parameters below were fit per group (g) and per target domain (j)\n# using ordinary least squares on sqrt(proportion) features.\n_PARAMS: Dict[str, Dict[str, Dict[str, object]]] = {\n    \"160M\": {\n        \"1\": {\n            \"base\": 3.4983868111735097,\n            \"b\": [-1.322244801503619, -0.05648236883892693, -0.1945208834476242, -0.07961426797704284, -0.48684961017065925],\n        },\n        \"2\": {\n            \"base\": 3.5901800975050886,\n            \"b\": [0.020259593136957545, -0.4261579533746848, -0.030500964720705733, 0.03195749881367703, -0.15925155887706166],\n        },\n        \"3\": {\n            \"base\": 3.404587658123985,\n            \"b\": [0.03082205478652992, -0.070071859031754, -1.636082759861348, -0.007193427564357435, -0.16048151875641523],\n        },\n        \"4\": {\n            \"base\": 2.229735317270052,\n            \"b\": [-0.13253328937666387, -0.0683463072293981, -0.06043866341791307, -1.4101893804572472, -0.2745437017330747],\n        },\n        \"5\": {\n            \"base\": 3.758978235206044,\n            \"b\": [-0.12040510823857463, -0.10138882666075966, -0.009625053520307786, -0.09995476483700354, -0.6624104061161924],\n        },\n    },\n    \"305M\": {\n        \"1\": {\n            \"base\": 3.368983334496183,\n            \"b\": [-1.3045025214887618, -0.07366983178813807, -0.19351469275990807, -0.1032498767800255, -0.5115484434058833],\n        },\n        \"2\": {\n            \"base\": 3.4650847175005275,\n            \"b\": [-0.0011777777393013559, -0.42534878260827746, -0.031637315796954495, 0.007651237757482249, -0.189691856112857],\n        },\n        \"3\": {\n            \"base\": 3.292456822440978,\n            \"b\": [0.016558822514137258, -0.06880890106063497, -1.692086053867289, -0.02043102024195373, -0.17469071753225673],\n        },\n        \"4\": {\n            \"base\": 2.1117242275989283,\n            \"b\": [-0.14209291141875355, -0.08185652172224422, -0.05933113785945075, -1.3567444624497598, -0.2795164803421837],\n        },\n        \"5\": {\n            \"base\": 3.6389244724608583,\n            \"b\": [-0.14463627940451954, -0.11811233876990179, -0.012096699259550073, -0.12940575716222572, -0.6983566433599756],\n        },\n    },\n    \"410M\": {\n        \"1\": {\n            \"base\": 3.2842119964419947,\n            \"b\": [-1.2888843000766006, -0.048995574255384367, -0.21360402703448367, -0.07761669368320008, -0.5026384873856817],\n        },\n        \"2\": {\n            \"base\": 3.378529873283224,\n            \"b\": [0.00731609765505886, -0.4046999067248046, -0.049914824967428884, 0.026006973552861305, -0.18578313165497676],\n        },\n        \"3\": {\n            \"base\": 3.240001825430652,\n            \"b\": [0.01628141300127803, -0.06568083623469262, -1.7274618646873994, -0.018326862157837145, -0.18278824660933443],\n        },\n        \"4\": {\n            \"base\": 2.057463260748564,\n            \"b\": [-0.15491308151495475, -0.05840055193974876, -0.06049410956879981, -1.3202941320593231, -0.28063147396807486],\n        },\n        \"5\": {\n            \"base\": 3.568172168062507,\n            \"b\": [-0.14806051645884583, -0.10343029066915999, -0.03544970070841352, -0.11508274131132587, -0.7078376382372219],\n        },\n    },\n    \"70M\": {\n        \"1\": {\n            \"base\": 3.902118280566893,\n            \"b\": [-1.425775576227664, -0.0833917420961342, -0.19694968669320845, -0.10889703541483507, -0.524387171419523],\n        },\n        \"2\": {\n            \"base\": 3.9510586938545873,\n            \"b\": [0.00835098938085932, -0.46316767641389445, -0.03510385309121029, 0.0115013961440507, -0.16308414687557324],\n        },\n        \"3\": {\n            \"base\": 3.745961102254981,\n            \"b\": [0.026773593153325063, -0.08530223577932154, -1.715458399253737, -0.04218357993957652, -0.17722638893079976],\n        },\n        \"4\": {\n            \"base\": 2.605542887734501,\n            \"b\": [-0.17483150982599954, -0.0974811357922756, -0.06917847354633115, -1.5953611426115695, -0.33815426297860407],\n        },\n        \"5\": {\n            \"base\": 4.106409751948681,\n            \"b\": [-0.11776391265174756, -0.10968999915729594, -0.013571098407638444, -0.11195553430775985, -0.6537211226224747],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts domain validation losses from domain mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group {group!r}. Known groups: {sorted(_PARAMS)}\")\n\n    out: List[Dict[str, float]] = []\n    for row in input_data:\n        p = [float(row.get(f\"proportion_domain_{k}\", 0.0)) for k in range(1, 6)]\n        # Numerical safety: proportions are in [0,1] and sum to 1 in training data;\n        # clamp to avoid negative due to noise.\n        sp = [math.sqrt(max(0.0, x)) for x in p]\n\n        pred: Dict[str, float] = {}\n        for j in range(1, 6):\n            par = _PARAMS[group][str(j)]\n            base = float(par[\"base\"])\n            b = par[\"b\"]  # type: ignore[assignment]\n            y = base\n            for k in range(5):\n                y += float(b[k]) * sp[k]\n            pred[f\"loss_domain_{j}\"] = float(y)\n        out.append(pred)\n\n    return out", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5.2", "reward_r2": 0.933137085479296, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Scaling law discovered from the provided dataset.\n#\n# For each group g and validation domain j:\n#   loss_domain_j = a[g][j] + sum_k b[g][j][k] * sqrt(proportion_domain_k)\n#\n# Coefficients were fit with ordinary least squares on the training dataset.\n_PARAMS: dict[str, dict[int, dict[str, list[float] | float]]] = {\n    \"70M\": {\n        1: {\n            \"intercept\": 3.902118280566895,\n            \"b\": [\n                -1.425775576227666,\n                -0.08339174209613366,\n                -0.19694968669320942,\n                -0.10889703541483398,\n                -0.5243871714195225,\n            ],\n        },\n        2: {\n            \"intercept\": 3.951058693854588,\n            \"b\": [\n                0.008350989380858986,\n                -0.4631676764138939,\n                -0.035103853091210474,\n                0.011501396144051866,\n                -0.1630841468755727,\n            ],\n        },\n        3: {\n            \"intercept\": 3.745961102254981,\n            \"b\": [\n                0.02677359315332495,\n                -0.08530223577932106,\n                -1.715458399253737,\n                -0.04218357993957588,\n                -0.17722638893079964,\n            ],\n        },\n        4: {\n            \"intercept\": 2.605542887734503,\n            \"b\": [\n                -0.1748315098260007,\n                -0.09748113579227549,\n                -0.06917847354633108,\n                -1.5953611426115701,\n                -0.33815426297860457,\n            ],\n        },\n        5: {\n            \"intercept\": 4.106409751948681,\n            \"b\": [\n                -0.11776391265174813,\n                -0.10968999915729538,\n                -0.013571098407638814,\n                -0.11195553430775876,\n                -0.6537211226224754,\n            ],\n        },\n    },\n    \"160M\": {\n        1: {\n            \"intercept\": 3.4983868111735092,\n            \"b\": [\n                -1.3222448015036194,\n                -0.05648236883892562,\n                -0.19452088344762403,\n                -0.07961426797704164,\n                -0.4868496101706582,\n            ],\n        },\n        2: {\n            \"intercept\": 3.5901800975050886,\n            \"b\": [\n                0.02025959313695644,\n                -0.4261579533746849,\n                -0.030500964720706167,\n                0.03195749881367785,\n                -0.1592515588770622,\n            ],\n        },\n        3: {\n            \"intercept\": 3.404587658123986,\n            \"b\": [\n                0.030822054786528774,\n                -0.0700718590317547,\n                -1.6360827598613485,\n                -0.007193427564357451,\n                -0.16048151875641617,\n            ],\n        },\n        4: {\n            \"intercept\": 2.2297353172700536,\n            \"b\": [\n                -0.13253328937666442,\n                -0.06834630722939852,\n                -0.06043866341791265,\n                -1.410189380457248,\n                -0.2745437017330755,\n            ],\n        },\n        5: {\n            \"intercept\": 3.758978235206044,\n            \"b\": [\n                -0.12040510823857553,\n                -0.10138882666075939,\n                -0.009625053520308261,\n                -0.09995476483700287,\n                -0.6624104061161932,\n            ],\n        },\n    },\n    \"305M\": {\n        1: {\n            \"intercept\": 3.3689833344961833,\n            \"b\": [\n                -1.3045025214887638,\n                -0.07366983178813784,\n                -0.19351469275990896,\n                -0.10324987678002452,\n                -0.5115484434058839,\n            ],\n        },\n        2: {\n            \"intercept\": 3.4650847175005275,\n            \"b\": [\n                -0.0011777777393015554,\n                -0.4253487826082773,\n                -0.03163731579695457,\n                0.007651237757482987,\n                -0.18969185611285697,\n            ],\n        },\n        3: {\n            \"intercept\": 3.2924568224409776,\n            \"b\": [\n                0.01655882251413715,\n                -0.06880890106063427,\n                -1.6920860538672895,\n                -0.020431020241952865,\n                -0.17469071753225646,\n            ],\n        },\n        4: {\n            \"intercept\": 2.1117242275989287,\n            \"b\": [\n                -0.14209291141875402,\n                -0.08185652172224457,\n                -0.05933113785945045,\n                -1.3567444624497598,\n                -0.27951648034218374,\n            ],\n        },\n        5: {\n            \"intercept\": 3.638924472460857,\n            \"b\": [\n                -0.14463627940451984,\n                -0.11811233876990086,\n                -0.012096699259550365,\n                -0.12940575716222455,\n                -0.6983566433599759,\n            ],\n        },\n    },\n    \"410M\": {\n        1: {\n            \"intercept\": 3.2842119964419956,\n            \"b\": [\n                -1.2888843000766026,\n                -0.04899557425538448,\n                -0.21360402703448442,\n                -0.0776166936831994,\n                -0.5026384873856827,\n            ],\n        },\n        2: {\n            \"intercept\": 3.3785298732832234,\n            \"b\": [\n                0.007316097655057974,\n                -0.40469990672480444,\n                -0.049914824967429404,\n                0.02600697355286197,\n                -0.18578313165497698,\n            ],\n        },\n        3: {\n            \"intercept\": 3.240001825430652,\n            \"b\": [\n                0.016281413001278167,\n                -0.06568083623469226,\n                -1.7274618646873992,\n                -0.01832686215783651,\n                -0.18278824660933396,\n            ],\n        },\n        4: {\n            \"intercept\": 2.0574632607485643,\n            \"b\": [\n                -0.1549130815149553,\n                -0.058400551939748885,\n                -0.060494109568799366,\n                -1.3202941320593231,\n                -0.2806314739680751,\n            ],\n        },\n        5: {\n            \"intercept\": 3.568172168062505,\n            \"b\": [\n                -0.14806051645884644,\n                -0.10343029066915954,\n                -0.03544970070841392,\n                -0.11508274131132501,\n                -0.7078376382372229,\n            ],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts domain validation losses from domain mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group {group!r}. Known groups: {sorted(_PARAMS)}\")\n\n    out: list[dict[str, float]] = []\n\n    for row in input_data:\n        s = [\n            math.sqrt(max(0.0, float(row.get(f\"proportion_domain_{k}\", 0.0))))\n            for k in range(1, 6)\n        ]\n\n        pred: dict[str, float] = {}\n        for j in range(1, 6):\n            intercept = float(_PARAMS[group][j][\"intercept\"])  # type: ignore[index]\n            b = _PARAMS[group][j][\"b\"]  # type: ignore[index]\n            val = intercept\n            for k in range(5):\n                val += float(b[k]) * s[k]\n            pred[f\"loss_domain_{j}\"] = float(val)\n\n        out.append(pred)\n\n    return out", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o3", "reward_r2": 0.9022243523590913, "solution": "import math\n\n# Pre-computed coefficients (\u03b1 intercept and \u03b2 slope) for each domain within each experimental group.\n# These values were obtained by ordinary least-squares fitting on the provided training split.\n_COEFFS = {\n    '70M':  {\n        1: {'alpha': 2.699187282650469, 'beta': -0.05301834148938922},\n        2: {'alpha': 3.641245514435248, 'beta': -0.012884478287480516},\n        3: {'alpha': 3.064817132296827, 'beta': -0.03887742050158055},\n        4: {'alpha': 1.5883178939323077, 'beta': -0.0495375219255563},\n        5: {'alpha': 3.4914427487846966, 'beta': -0.03541574159783459},\n    },\n    '160M': {\n        1: {'alpha': 2.3900234534690616, 'beta': -0.049548319842154825},\n        2: {'alpha': 3.308997240661732,  'beta': -0.01184703921853561},\n        3: {'alpha': 2.7750603405001533, 'beta': -0.03703330098796484},\n        4: {'alpha': 1.3585320071946527, 'beta': -0.044165574068738556},\n        5: {'alpha': 3.141635642438281,  'beta': -0.03640944805519766},\n    },\n    '305M': {\n        1: {'alpha': 2.244509680794173,  'beta': -0.04826257363962712},\n        2: {'alpha': 3.1516643395018273, 'beta': -0.011228110004842098},\n        3: {'alpha': 2.627225671804749,  'beta': -0.038315259040671304},\n        4: {'alpha': 1.2540806183376128, 'beta': -0.0422871397587127},\n        5: {'alpha': 2.974256168413441,  'beta': -0.03681503714017356},\n    },\n    '410M': {\n        1: {'alpha': 2.1839855870092384, 'beta': -0.047798856964779556},\n        2: {'alpha': 3.0802841205472316, 'beta': -0.010883493163969749},\n        3: {'alpha': 2.5599124453127016, 'beta': -0.03903464689969438},\n        4: {'alpha': 1.2161032438866797, 'beta': -0.04115602093820591},\n        5: {'alpha': 2.898019428647134,  'beta': -0.03803558851478692},\n    },\n}\n\n_EPS = 1e-6  # Numerical guard against log(0)\n\n\ndef _predict_single(sample: dict[str, float], coeffs: dict[int, dict[str, float]]) -> dict[str, float]:\n    \"\"\"Predict losses for one sample given coefficient set.\"\"\"\n    output: dict[str, float] = {}\n    for i in range(1, 6):\n        p = sample.get(f'proportion_domain_{i}', 0.0)\n        # Guard against log(0).  Values extremely close to zero do not impact loss materially.\n        p_clamped = max(p, _EPS)\n        alpha = coeffs[i]['alpha']\n        beta = coeffs[i]['beta']\n        output[f'loss_domain_{i}'] = float(alpha + beta * math.log(p_clamped))\n    return output\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts validation loss for each domain in the domain-mixture pre-training setting.\n\n    The discovered scaling relationship is, for every domain *i* (1-5):\n        loss_domain_i = \u03b1_{group,i} + \u03b2_{group,i} \u00b7 ln(proportion_domain_i)\n\n    where (\u03b1, \u03b2) are constants determined by least-squares fitting on the\n    provided training data and differ per experimental *group* and *domain*.\n\n    Args:\n        input_data: list of samples, each containing keys\n            'proportion_domain_1' \u2026 'proportion_domain_5'.\n        group: experimental group identifier (e.g. '70M', '160M', \u2026).\n\n    Returns:\n        List of dictionaries mirroring *input_data* order, each populated with\n        predictions under keys 'loss_domain_1' \u2026 'loss_domain_5'.\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Known groups: {list(_COEFFS)}\")\n\n    coeffs = _COEFFS[group]\n    return [_predict_single(sample, coeffs) for sample in input_data]", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9022243523590899, "solution": "# Auto-generated scaling law function\n# Formula: loss_domain_i = a_{group,i} + b_{group,i} * log(1 / (proportion_domain_i + EPS))\n# EPS helps handle zero or tiny proportions.\nimport math\nfrom typing import List, Dict\n\nEPS = 1e-06\n\nCOEFFS = {\n  \"70M\": {\n    \"domain_1\": {\n      \"a\": 2.69918728265047,\n      \"b\": 0.05301834148938907\n    },\n    \"domain_2\": {\n      \"a\": 3.6412455144352482,\n      \"b\": 0.012884478287480374\n    },\n    \"domain_3\": {\n      \"a\": 3.064817132296828,\n      \"b\": 0.03887742050158043\n    },\n    \"domain_4\": {\n      \"a\": 1.5883178939323082,\n      \"b\": 0.049537521925556285\n    },\n    \"domain_5\": {\n      \"a\": 3.4914427487846966,\n      \"b\": 0.03541574159783458\n    }\n  },\n  \"160M\": {\n    \"domain_1\": {\n      \"a\": 2.390023453469063,\n      \"b\": 0.04954831984215471\n    },\n    \"domain_2\": {\n      \"a\": 3.3089972406617325,\n      \"b\": 0.011847039218535506\n    },\n    \"domain_3\": {\n      \"a\": 2.775060340500154,\n      \"b\": 0.03703330098796477\n    },\n    \"domain_4\": {\n      \"a\": 1.3585320071946532,\n      \"b\": 0.044165574068738486\n    },\n    \"domain_5\": {\n      \"a\": 3.141635642438281,\n      \"b\": 0.036409448055197705\n    }\n  },\n  \"305M\": {\n    \"domain_1\": {\n      \"a\": 2.244509680794174,\n      \"b\": 0.048262573639627\n    },\n    \"domain_2\": {\n      \"a\": 3.151664339501828,\n      \"b\": 0.011228110004841948\n    },\n    \"domain_3\": {\n      \"a\": 2.6272256718047498,\n      \"b\": 0.03831525904067123\n    },\n    \"domain_4\": {\n      \"a\": 1.2540806183376132,\n      \"b\": 0.04228713975871266\n    },\n    \"domain_5\": {\n      \"a\": 2.9742561684134405,\n      \"b\": 0.03681503714017352\n    }\n  },\n  \"410M\": {\n    \"domain_1\": {\n      \"a\": 2.1839855870092397,\n      \"b\": 0.04779885696477952\n    },\n    \"domain_2\": {\n      \"a\": 3.0802841205472307,\n      \"b\": 0.010883493163969587\n    },\n    \"domain_3\": {\n      \"a\": 2.559912445312702,\n      \"b\": 0.0390346468996943\n    },\n    \"domain_4\": {\n      \"a\": 1.2161032438866803,\n      \"b\": 0.04115602093820587\n    },\n    \"domain_5\": {\n      \"a\": 2.8980194286471335,\n      \"b\": 0.038035588514786826\n    }\n  },\n  \"GLOBAL\": {\n    \"domain_1\": {\n      \"a\": 2.379426500980737,\n      \"b\": 0.04965702298398756\n    },\n    \"domain_2\": {\n      \"a\": 3.2955478037865107,\n      \"b\": 0.0117107801687068\n    },\n    \"domain_3\": {\n      \"a\": 2.756753897478608,\n      \"b\": 0.03831515685747756\n    },\n    \"domain_4\": {\n      \"a\": 1.354258440837814,\n      \"b\": 0.04428656417280338\n    },\n    \"domain_5\": {\n      \"a\": 3.1263384970708885,\n      \"b\": 0.03666895382699818\n    }\n  }\n}\n\ndef _select_group_key(group: str) -> str:\n    if isinstance(group, str) and group in COEFFS:\n        return group\n    if isinstance(group, str):\n        gl = group.lower()\n        for k in COEFFS.keys():\n            if k.lower() == gl:\n                return k\n    return \"GLOBAL\" if \"GLOBAL\" in COEFFS else list(COEFFS.keys())[0]\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    gkey = _select_group_key(group)\n    params = COEFFS[gkey]\n    outputs: list[dict[str, float]] = []\n    domain_keys = sorted(params.keys(), key=lambda k: int(''.join(ch for ch in k if ch.isdigit())) if any(ch.isdigit() for ch in k) else 9999)\n    for row in input_data:\n        out: dict[str, float] = {}\n        for dom in domain_keys:\n            idx = ''.join(ch for ch in dom if ch.isdigit())\n            p_key = \"proportion_domain_\" + idx\n            y_key = \"loss_domain_\" + idx\n            p = row.get(p_key, None)\n            if p is None:\n                # try \"proportion_domain{idx}\" without underscore (legacy variant)\n                p = row.get(\"proportion_domain\" + idx, None)\n            if p is None:\n                p = 0.0\n            try:\n                p = float(p)\n            except Exception:\n                p = 0.0\n            a = float(params[dom].get(\"a\", 0.0))\n            b = float(params[dom].get(\"b\", 0.0))\n            val = a + b * math.log(1.0 / max(p, EPS))\n            out[y_key] = float(val)\n        outputs.append(out)\n    return outputs", "agent_name": "mini-swe-agent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.902224, "solution": "import json\nimport numpy as np\n\n# Global variable to store loaded parameters\n_FITTED_PARAMETERS = None\n\ndef _load_parameters():\n    global _FITTED_PARAMETERS\n    if _FITTED_PARAMETERS is None:\n        try:\n            with open('/app/fitted_parameters.json', 'r') as f:\n                _FITTED_PARAMETERS = json.load(f)\n        except FileNotFoundError:\n            print(\"Error: fitted_parameters.json not found. Please run analyze_data.py first.\")\n            _FITTED_PARAMETERS = {}\n    return _FITTED_PARAMETERS\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = _load_parameters()\n\n    if group not in params:\n        # Fallback if group not found, e.g., use a default group or raise an error.\n        # For now, let's return an empty prediction or average parameters.\n        # The problem statement implies group will be one of the known groups.\n        # If unseen group, this would be an extrapolation case.\n        print(f\"Warning: Group '{group}' not found in fitted parameters. Cannot make predictions.\")\n        return [{'loss_domain_1': 0.0, 'loss_domain_2': 0.0, 'loss_domain_3': 0.0, 'loss_domain_4': 0.0, 'loss_domain_5': 0.0} for _ in input_data]\n\n    group_params = params[group]\n    predictions = []\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6):\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            domain_key = f'domain_{i}'\n\n            if proportion_key not in data_point:\n                predicted_losses[loss_key] = 0.0 # Or some default/error handling\n                continue\n\n            proportion = data_point[proportion_key]\n\n            if domain_key in group_params:\n                A = group_params[domain_key]['A']\n                B = group_params[domain_key]['B']\n                \n                # Apply the logarithmic scaling law\n                # Use 1e-6 as epsilon for proportions close to zero or exactly zero\n                predicted_loss = A - B * np.log(proportion + 1e-6)\n                predicted_losses[loss_key] = predicted_loss\n            else:\n                predicted_losses[loss_key] = 0.0 # Fallback if domain parameters not found\n        predictions.append(predicted_losses)\n\n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.9022238569876261, "solution": "from math import log\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\"\"\"\n    EPS = 1e-06\n    COEFFS = {'160M': {'loss_domain_1': {'a': 2.3900239026095846, 'b': -0.0495482846415819},\n          'loss_domain_2': {'a': 3.3089973473750915, 'b': -0.011847031339016876},\n          'loss_domain_3': {'a': 2.7750612243262176, 'b': -0.03703323688495387},\n          'loss_domain_4': {'a': 1.3585325213280746, 'b': -0.04416553521395201},\n          'loss_domain_5': {'a': 3.1416357474005943, 'b': -0.03640943279009211}},\n '305M': {'loss_domain_1': {'a': 2.2445101169537405, 'b': -0.048262539632469084},\n          'loss_domain_2': {'a': 3.1516644411817394, 'b': -0.011228102438934438},\n          'loss_domain_3': {'a': 2.627226581493613, 'b': -0.03831519342481427},\n          'loss_domain_4': {'a': 1.254081110356706, 'b': -0.042287102605490595},\n          'loss_domain_5': {'a': 2.974256274101213, 'b': -0.03681502204362127}},\n '410M': {'loss_domain_1': {'a': 2.183986016421472, 'b': -0.04779882382417481},\n          'loss_domain_2': {'a': 3.0802842179137, 'b': -0.010883486046165937},\n          'loss_domain_3': {'a': 2.5599133716532463, 'b': -0.039034580115764335},\n          'loss_domain_4': {'a': 1.216103723749701, 'b': -0.04115598457974984},\n          'loss_domain_5': {'a': 2.898019537149145, 'b': -0.038035573443952336}},\n '70M': {'loss_domain_1': {'a': 2.6991877691852117, 'b': -0.05301830256954046},\n         'loss_domain_2': {'a': 3.6412456318395994, 'b': -0.012884469474280064},\n         'loss_domain_3': {'a': 3.064818062764666, 'b': -0.038877352813920324},\n         'loss_domain_4': {'a': 1.5883184714718916, 'b': -0.04953747817222867},\n         'loss_domain_5': {'a': 3.491442851657023, 'b': -0.035415726158268684}}}\n    # Determine coeff set for group; fallback to first available group if not found\n    group_coeffs = COEFFS.get(group)\n    if group_coeffs is None:\n        if COEFFS:\n            group_coeffs = COEFFS[sorted(COEFFS.keys())[0]]\n        else:\n            group_coeffs = {}\n    outputs = []\n    for row in input_data:\n        out = {}\n        # For each loss key we know, compute using corresponding proportion\n        for loss_key, ab in group_coeffs.items():\n            # Infer proportion key by replacing loss_ with proportion_ in the key name\n            prop_key = loss_key.replace('loss_', 'proportion_')\n            p = float(row.get(prop_key, 0.0))\n            a = float(ab.get('a', 0.0))\n            b = float(ab.get('b', 0.0))\n            pred = a + b * log(p + EPS)\n            out[loss_key] = float(pred)\n        outputs.append(out)\n    return outputs", "agent_name": "terminus-2", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-4.1", "reward_r2": 0.9022238561044323, "solution": "import numpy as np\n\ngroup_params = {\n    '70M': [\n        (-0.0530183, 2.69918777),\n        (-0.01288447, 3.64124563),\n        (-0.03887735, 3.06481806),\n        (-0.04953748, 1.58831847),\n        (-0.03541573, 3.49144285),\n    ],\n    '160M': [\n        (-0.04954828, 2.3900239),\n        (-0.01184703, 3.30899735),\n        (-0.03703324, 2.77506122),\n        (-0.04416554, 1.35853252),\n        (-0.03640943, 3.14163575),\n    ],\n    '305M': [\n        (-0.04826254, 2.24451012),\n        (-0.0112281, 3.15166444),\n        (-0.03831519, 2.62722658),\n        (-0.0422871, 1.25408111),\n        (-0.03681502, 2.97425627),\n    ],\n    '410M': [\n        (-0.04779882, 2.18398602),\n        (-0.01088349, 3.08028422),\n        (-0.03903458, 2.55991337),\n        (-0.04115598, 1.21610372),\n        (-0.03803557, 2.89801954),\n    ],\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = group_params[group]\n    eps = 1e-6\n    results = []\n    for row in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = row[f'proportion_domain_{i}']\n            b, c = params[i-1]\n            pred[f'loss_domain_{i}'] = b * np.log(p + eps) + c\n        results.append(pred)\n    return results", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.9021493866194279, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters for the scaling law: loss_i = a_i + b_i * log(proportion_i + epsilon)\n    # Fitted parameters for each group and domain\n    epsilon = 1e-6\n    \n    # Group: 70M, 160M, 305M, 410M\n    # Domain: 1 to 5\n    parameters = {\n        '70M': {\n            'a': [2.6992, 3.6412, 3.0648, 1.5883, 3.4914],\n            'b': [-0.0530, -0.0129, -0.0389, -0.0495, -0.0354]\n        },\n        '160M': {\n            'a': [2.3900, 3.3090, 2.7751, 1.3585, 3.1416],\n            'b': [-0.0495, -0.0118, -0.0370, -0.0442, -0.0364]\n        },\n        '305M': {\n            'a': [2.2445, 3.1517, 2.6272, 1.2541, 2.9743],\n            'b': [-0.0483, -0.0112, -0.0383, -0.0423, -0.0368]\n        },\n        '410M': {\n            'a': [2.1840, 3.0803, 2.5599, 1.2161, 2.8980],\n            'b': [-0.0478, -0.0109, -0.0390, -0.0412, -0.0380]\n        }\n    }\n    \n    # Check if group is valid\n    if group not in parameters:\n        raise ValueError(f\"Unknown group: {group}. Valid groups are: {list(parameters.keys())}\")\n    \n    # Get parameters for the specified group\n    group_params = parameters[group]\n    a_coeffs = group_params['a']\n    b_coeffs = group_params['b']\n    \n    # Prepare the output list\n    output_data = []\n    \n    for data_point in input_data:\n        # We expect the input data point to have keys: proportion_domain_1, ..., proportion_domain_5\n        # However, we can be flexible and look for any key that starts with 'proportion_domain_'\n        # and has a domain number.\n        predicted_losses = {}\n        \n        for domain_idx in range(5):  # Domains 1 to 5\n            domain_num = domain_idx + 1\n            prop_key = f'proportion_domain_{domain_num}'\n            \n            if prop_key not in data_point:\n                # If the exact key is not found, try to find any proportion key for this domain?\n                # Alternatively, we could skip, but for now we assume the keys are present.\n                raise KeyError(f\"Input data point missing expected key: {prop_key}\")\n            \n            proportion = data_point[prop_key]\n            a = a_coeffs[domain_idx]\n            b = b_coeffs[domain_idx]\n            \n            # Apply the scaling law: loss = a + b * log(proportion + epsilon)\n            # Note: if proportion is exactly 0, we use epsilon to avoid log(0)\n            loss = a + b * math.log(proportion + epsilon)\n            predicted_losses[f'loss_domain_{domain_num}'] = loss\n        \n        output_data.append(predicted_losses)\n    \n    return output_data", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.899569, "solution": "from typing import List, Dict\n\n# Quadratic scaling law in the five mixture proportions.\n# For group g and domain k (k in 1..5):\n#   loss_k = b_k(g) + sum_i A_{k,i}(g) * p_i + sum_{i<=j} C_{k,ij}(g) * p_i * p_j\n# The feature order for the quadratic terms matches sklearn PolynomialFeatures(degree=2, include_bias=False):\n# [p1, p2, p3, p4, p5,\n#  p1^2, p1*p2, p1*p3, p1*p4, p1*p5,\n#  p2^2, p2*p3, p2*p4, p2*p5,\n#  p3^2, p3*p4, p3*p5,\n#  p4^2, p4*p5,\n#  p5^2]\n\n# Group-specific coefficients (5 outputs x 20 features) and intercepts (5,)\nCOEFFS: Dict[str, Dict[str, list]] = {\n    '70M': {\n        'intercept': [2.67722332, 3.61228621, 2.31848257, 1.43615424, 3.64312409],\n        'coef': [\n            [-1.93535793e+00,  9.21334406e-01,  6.07166334e-01,  1.86436004e-01,  2.20421182e-01,\n              4.17217173e+00, -2.50227074e+00, -8.71756745e-01, -1.45116852e+00, -1.28233366e+00,\n              7.50528746e-01,  1.88319583e+00,  7.09251012e-01,  8.06295581e-02,  1.85529330e-01,\n             -4.30245275e-01, -1.59556803e-01,  1.88349071e-01,  1.17024971e+00,  4.11432370e-01],\n            [ 1.87287409e-01, -5.24704674e-01,  8.19666923e-02,  2.31939372e-01,  2.35112001e-02,\n              1.93535049e-01, -6.08606198e-01,  1.18874357e-01,  3.03342030e-01,  1.80142171e-01,\n              8.76312565e-01,  2.24295717e-02, -4.31900052e-01, -3.82940560e-01,  4.86317090e-03,\n             -4.36420286e-03, -5.98362049e-02,  2.27452590e-01,  1.37409008e-01,  1.48736786e-01],\n            [ 6.06770406e-01,  1.02165804e+00, -3.42499870e+00,  8.48286144e-01,  9.48284111e-01,\n              1.17883254e+00,  9.85326844e-02, -2.30473008e+00,  7.43287853e-01,  8.90847414e-01,\n              2.23385663e+00, -5.42165653e-01, -6.21380452e-01, -1.47185169e-01,  8.79046349e-01,\n             -1.00148149e+00, -4.55667827e-01,  1.32253646e+00,  4.05323777e-01,  2.54965916e-01],\n            [ 2.25923880e-01,  5.66280635e-01,  3.05380370e-01, -1.18219021e+00,  8.46053253e-02,\n              2.43370013e-02,  8.12183458e-01, -5.58959006e-01, -1.43199391e+00,  1.38035634e+00,\n              5.92080565e-01,  1.49390820e+00, -3.14016004e+00,  8.08268452e-01,  1.55579420e-01,\n             -1.08989614e+00,  3.04747891e-01,  7.59681480e+00, -3.11695492e+00,  7.08187563e-01],\n            [ 9.59227049e-02,  2.87475915e-01,  7.95688772e-02,  2.43121301e-01, -7.06088798e-01,\n              8.03086961e-02, -2.38179581e-01,  4.09505305e-02,  3.70796209e-01, -1.57953149e-01,\n              4.77055325e-01,  3.25008377e-01,  2.93142645e-01, -5.69550850e-01, -2.33055409e-02,\n             -7.62471460e-02, -1.86837343e-01,  1.05339029e-01, -4.49909436e-01,  6.58161980e-01],\n        ],\n    },\n    '160M': {\n        'intercept': [2.38262016, 3.30071035, 2.07434882, 1.22527058, 3.31341378],\n        'coef': [\n            [-1.89040168e+00,  9.35907232e-01,  5.60322596e-01,  1.88444025e-01,  2.05727823e-01,\n              4.04736833e+00, -2.51535118e+00, -8.90374549e-01, -1.39350885e+00, -1.13853543e+00,\n              7.41695287e-01,  1.84860556e+00,  8.63861779e-01, -2.90421544e-03,  1.62855446e-01,\n             -4.13720211e-01, -1.47043649e-01, -1.33076166e-02,  1.14511892e+00,  3.49092199e-01],\n            [ 1.06414210e-01, -4.42355222e-01,  1.03243547e-01,  2.22326118e-01,  1.03713464e-02,\n              2.92828097e-01, -7.03779930e-01,  8.34271237e-03,  2.86433970e-01,  2.22589362e-01,\n              8.46832622e-01,  1.48524491e-01, -2.84465358e-01, -4.49467047e-01,  2.13811575e-02,\n             -2.55730882e-02, -4.94317254e-02,  6.67143569e-02,  1.79216238e-01,  1.07464519e-01],\n            [ 5.10457626e-01,  1.02965945e+00, -3.28863673e+00,  8.27651985e-01,  9.20867673e-01,\n              1.21104590e+00, -1.00467440e-01, -2.27536431e+00,  7.92543737e-01,  8.82699738e-01,\n              2.29667733e+00, -4.60952149e-01, -4.56400842e-01, -2.49197452e-01,  8.27965658e-01,\n             -1.01341930e+00, -3.66866637e-01,  1.04265170e+00,  4.62276688e-01,  1.91955335e-01],\n            [ 1.85394175e-01,  5.25641652e-01,  2.36807484e-01, -1.05531359e+00,  1.07470277e-01,\n              4.73960405e-02,  6.76640181e-01, -4.99158421e-01, -1.30202159e+00,  1.26253796e+00,\n              4.71471468e-01,  1.33050402e+00, -2.67005203e+00,  7.17078014e-01,  9.60652672e-02,\n             -1.01089455e+00,  3.20291164e-01,  6.69954159e+00, -2.77188702e+00,  5.79450152e-01],\n            [ 4.30213821e-03,  3.81322064e-01,  1.04070391e-01,  2.40299130e-01, -7.29993724e-01,\n              2.03468508e-01, -3.88921326e-01, -6.90314075e-02,  3.50874845e-01, -9.20884817e-02,\n              5.96457642e-01,  4.54420685e-01,  4.50055879e-01, -7.30690815e-01, -9.18767274e-03,\n             -1.08473047e-01, -1.63658167e-01, -5.80983095e-02, -3.94060239e-01,  6.50503979e-01],\n        ],\n    },\n    '305M': {\n        'intercept': [2.20308636, 3.13378774, 2.0099542, 1.21090615, 3.12985208],\n        'coef': [\n            [-1.36612527e+00,  5.08760751e-01,  2.58565850e-01,  1.92426844e-01,  4.06371822e-01,\n              3.02406051e+00, -1.36341416e+00, -5.63422763e-01, -9.77163560e-01, -1.48618530e+00,\n              3.33590741e-01,  7.44731825e-01,  2.29382605e-01,  5.64469737e-01,  1.08830884e-01,\n             -2.29650547e-01,  1.98076451e-01,  1.70915773e-01,  9.98942574e-01,  1.31068355e-01],\n            [ 1.42155086e-01, -4.77001278e-01,  5.52264024e-02,  2.48408466e-01,  3.12113236e-02,\n              2.58768506e-01, -3.66936157e-01, -1.32405322e-02,  1.23555478e-01,  1.40007791e-01,\n              2.60734738e-01,  5.87994884e-02, -1.60567442e-01, -2.69031906e-01,  2.22863705e-02,\n             -7.35552021e-03, -5.26340404e-03,  2.24335488e-01,  6.84404625e-02,  9.70583802e-02],\n            [ 6.18506100e-01,  8.68693819e-01, -3.08914591e+00,  7.89619410e-01,  8.12326585e-01,\n              1.02643703e+00,  1.14784592e-01, -1.46267084e+00,  1.80033336e-01,  7.59921974e-01,\n              1.17239480e+00, -3.63996838e-01, -2.03911468e-01,  1.49422736e-01,  1.90418054e-01,\n             -5.68054905e-01, -8.84841389e-01,  8.13849714e-01,  5.67702734e-01,  2.20120530e-01],\n            [ 2.12633425e-01,  4.83463627e-01,  2.22311810e-01, -8.90787050e-01, -2.76218129e-02,\n             -9.29330545e-02,  1.99613262e-01, -1.14370686e-01, -7.24874326e-01,  9.45198229e-01,\n              1.74912693e-01,  6.25137716e-01, -1.05562696e+00,  5.39426918e-01,  8.52526881e-02,\n             -3.88847751e-01,  1.51398435e-02,  3.43073712e+00, -2.15217513e+00,  6.24788323e-01],\n            [ 8.99898182e-02,  2.46945404e-01,  7.24765027e-02,  2.55936575e-01, -6.65348300e-01,\n              1.05642777e-01, -1.69323103e-02, -2.45232236e-02,  2.31633418e-01, -2.05830842e-01,\n              2.57824563e-01,  1.81182032e-01,  3.00383115e-01, -4.75511996e-01,  1.31255309e-02,\n              1.56611021e-03, -9.88739467e-02,  2.04976089e-01, -4.82622158e-01,  5.97490643e-01],\n        ],\n    },\n    '410M': {\n        'intercept': [2.1439425, 3.06228202, 1.93134234, 1.17352651, 3.0537303],\n        'coef': [\n            [-1.36744907e+00,  5.27110822e-01,  2.03069104e-01,  2.31281112e-01,  4.05988034e-01,\n              3.00741578e+00, -1.41296472e+00, -5.90698287e-01, -9.03407996e-01, -1.46779385e+00,\n              3.36508765e-01,  7.13154207e-01,  3.03122142e-01,  5.87290423e-01,  1.12114917e-01,\n             -2.17566149e-01,  1.86064416e-01,  6.33336092e-02,  9.85799506e-01,  1.14627539e-01],\n            [ 1.31827459e-01, -4.52442821e-01,  5.13599152e-03,  2.81922129e-01,  3.35572415e-02,\n              2.56204289e-01, -4.04028535e-01, -4.49805105e-02,  1.62119123e-01,  1.62513091e-01,\n              2.84260949e-01,  2.83929538e-02, -1.02570552e-01, -2.58497637e-01,  2.72658790e-02,\n             -3.64530850e-03, -1.89702227e-03,  1.75991829e-01,  5.00270374e-02,  8.14117726e-02],\n            [ 6.14742174e-01,  8.65449930e-01, -3.13247704e+00,  8.30315141e-01,  8.21969797e-01,\n              1.11981200e+00, -7.51476148e-02, -1.43672744e+00,  2.46278198e-01,  7.60527037e-01,\n              1.21696703e+00, -4.08532829e-01, -8.73310035e-02,  2.19494346e-01,  1.95911154e-01,\n             -5.50356121e-01, -9.32771804e-01,  6.76874073e-01,  5.44849994e-01,  2.29870224e-01],\n            [ 1.90124684e-01,  4.83340554e-01,  1.97559385e-01, -8.65266788e-01, -5.75783540e-03,\n             -7.48247755e-02,  1.37556138e-01, -1.54467045e-01, -5.85386297e-01,  8.67246663e-01,\n              1.95357715e-01,  6.05246418e-01, -1.03503337e+00,  5.80213650e-01,  9.95312192e-02,\n             -4.27655041e-01,  7.49038343e-02,  3.29049485e+00, -2.10768693e+00,  5.79564950e-01],\n            [ 7.34423141e-02,  2.79758269e-01,  1.55572901e-02,  2.98213415e-01, -6.66971288e-01,\n              1.21456923e-01, -7.12563637e-02, -7.33905276e-02,  2.86905077e-01, -1.90272795e-01,\n              2.93628007e-01,  1.67735765e-01,  3.62290116e-01, -4.72639254e-01,  1.73275864e-02,\n              8.76400932e-04, -9.69919342e-02,  1.48793013e-01, -5.00651192e-01,  5.93583887e-01],\n        ],\n    },\n}\n\nNAME_ORDER = [\n    'proportion_domain_1', 'proportion_domain_2', 'proportion_domain_3',\n    'proportion_domain_4', 'proportion_domain_5'\n]\n\n\ndef _poly2_features(vals: List[float]) -> List[float]:\n    \"\"\"Build polynomial features up to degree 2 for five inputs.\n\n    Order matches sklearn PolynomialFeatures(deg=2, include_bias=False).\n    \"\"\"\n    x1, x2, x3, x4, x5 = vals\n    feats = [\n        x1, x2, x3, x4, x5,\n        x1*x1, x1*x2, x1*x3, x1*x4, x1*x5,\n        x2*x2, x2*x3, x2*x4, x2*x5,\n        x3*x3, x3*x4, x3*x5,\n        x4*x4, x4*x5,\n        x5*x5,\n    ]\n    return feats\n\n\ndef _choose_group(group: str) -> str:\n    if group in COEFFS:\n        return group\n    # Fallback: map to nearest known parameterized size by numeric value\n    import re\n    m = re.search(r\"(\\d+)\", str(group))\n    if not m:\n        return '160M'\n    val = int(m.group(1))\n    def gnum(g: str) -> int:\n        mg = re.search(r\"(\\d+)\", g)\n        return int(mg.group(1)) if mg else 0\n    return min(COEFFS.keys(), key=lambda g: abs(gnum(g) - val))\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    grp = _choose_group(group)\n    params = COEFFS[grp]\n    W = params['coef']  # 5 x 20\n    b = params['intercept']  # 5\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        vals = [float(row.get(k, 0.0)) for k in NAME_ORDER]\n        feats = _poly2_features(vals)\n        y = [b[k] + sum(W[k][j] * feats[j] for j in range(20)) for k in range(5)]\n        outputs.append({f'loss_domain_{i+1}': y[i] for i in range(5)})\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n\n# Per-group, per-target coefficients fitted on /app/data using OLS.\n# Model: loss_k = b0 + sum_j b_j * log(proportion_j + eps)\n_EPS = 1e-6\n_COEFS: Dict[str, Dict[int, List[float]]] = {\n    \"160M\": {\n        1: [\n            2.4042025803406553,\n            -0.04878456902733,\n            -0.001374141990715709,\n            0.00425143168670042,\n            -0.004036425589256114,\n            0.007660516801797778,\n        ],\n        2: [\n            3.329935868696654,\n            0.002613273350161564,\n            -0.011891996557419415,\n            2.5572674116154508e-05,\n            0.003256629390122456,\n            -0.005952205705195364,\n        ],\n        3: [\n            2.8006343825165314,\n            0.0038463371713819104,\n            0.0010271048724917586,\n            -0.03834514983761013,\n            0.003370871701657902,\n            -0.004992444549614571,\n        ],\n        4: [\n            1.3842254371292473,\n            0.0001770125184695076,\n            0.0019008052563317707,\n            0.00040985897229494966,\n            -0.04321725706040489,\n            0.005203775943861618,\n        ],\n        5: [\n            3.22070848376419,\n            0.005681276339156933,\n            0.004826630941776033,\n            -0.00156340623248917,\n            0.005473022220321205,\n            -0.030039287316337895,\n        ],\n    },\n    \"305M\": {\n        1: [\n            2.254097761686427,\n            -0.047618308465165196,\n            -0.0013550046237553216,\n            0.004003192579882288,\n            -0.0042909223510698554,\n            0.006758159630738282,\n        ],\n        2: [\n            3.166848403701224,\n            0.0023474329227610603,\n            -0.011422287483401284,\n            4.890168638858063e-06,\n            0.0026987505976661923,\n            -0.006498741023938888,\n        ],\n        3: [\n            2.6500359123770685,\n            0.0035548451771600445,\n            0.0011888369505355075,\n            -0.03964115398330717,\n            0.0028959099742507494,\n            -0.004828358464409514,\n        ],\n        4: [\n            1.2751944568968825,\n            -0.00024923215104993846,\n            0.0015693015233153626,\n            0.0004352251902864513,\n            -0.041431801189773,\n            0.00487564464080058,\n        ],\n        5: [\n            3.0477226649403506,\n            0.005424541586794049,\n            0.004752807485933067,\n            -0.0015473014680650466,\n            0.004775541162988269,\n            -0.030879417992197157,\n        ],\n    },\n    \"410M\": {\n        1: [\n            2.1969109104720936,\n            -0.04711053566759858,\n            -0.0005929598868873091,\n            0.0032935545744610275,\n            -0.0034299883497122844,\n            0.006245418652814477,\n        ],\n        2: [\n            3.096774846113359,\n            0.002683578481767577,\n            -0.01079532012122322,\n            -0.0006061709329291659,\n            0.003289915492216653,\n            -0.007059325448456723,\n        ],\n        3: [\n            2.5854698484676923,\n            0.0036493802056689617,\n            0.0014981486567392278,\n            -0.04057579585320029,\n            0.003422309711002145,\n            -0.005304641068008599,\n        ],\n        4: [\n            1.2374283294627488,\n            -0.0007019754734980677,\n            0.0023220834389344196,\n            0.00022483249867104974,\n            -0.040255057354183485,\n            0.0043998574943246395,\n        ],\n        5: [\n            2.9734625677861892,\n            0.005474979456664107,\n            0.005292433569369652,\n            -0.0021983339298417347,\n            0.005328130514437292,\n            -0.031848404298810445,\n        ],\n    },\n    \"70M\": {\n        1: [\n            2.709753097020149,\n            -0.05223448519596348,\n            -0.002096832291136182,\n            0.0049309629806633245,\n            -0.005121637241168704,\n            0.008582633603168494,\n        ],\n        2: [\n            3.6589940901202946,\n            0.0024594854221674217,\n            -0.013037306540415955,\n            0.00018013301869309603,\n            0.0025694674548725075,\n            -0.0055192991657138785,\n        ],\n        3: [\n            3.0882519411499176,\n            0.004138978755961604,\n            0.000999612740187218,\n            -0.040138664176247195,\n            0.002610113875372775,\n            -0.00489486237526904,\n        ],\n        4: [\n            1.6123830068952152,\n            -0.0002626923872205353,\n            0.001840626589019361,\n            0.0004871656033250259,\n            -0.048580221686945084,\n            0.0053741041526707105,\n        ],\n        5: [\n            3.567736623439916,\n            0.005906206376564464,\n            0.00463119283202685,\n            -0.0015851069219014631,\n            0.004999662780701288,\n            -0.029220774259506147,\n        ],\n    },\n}\n\n\ndef _predict_row(props: List[float], coefs: List[float]) -> float:\n    # coefs: [b0, b1..b5], props: [p1..p5]\n    x = [1.0] + [math.log(max(p, 0.0) + _EPS) for p in props]\n    return sum(c * xi for c, xi in zip(coefs, x))\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Choose coefficients for the requested group; if unseen, fall back to the\n    # average of known groups to remain robust.\n    if group in _COEFS:\n        group_coefs = _COEFS[group]\n    else:\n        # Average coefficients across groups\n        group_coefs = {}\n        for k in range(1, 6):\n            # Collect coefs per group for this k\n            mats = [v[k] for v in _COEFS.values()]\n            avg = [sum(col) / len(mats) for col in zip(*mats)]\n            group_coefs[k] = avg\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        props = [\n            float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)\n        ]\n        pred = {}\n        for k in range(1, 6):\n            yk = _predict_row(props, group_coefs[k])\n            pred[f\"loss_domain_{k}\"] = float(yk)\n        outputs.append(pred)\n    return outputs", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from __future__ import annotations\n\nimport math\nfrom typing import Dict, List\n\n# Small epsilon to avoid log(0) when a domain proportion is zero\n_EPS = 1e-6\n\n# Per-group, per-output coefficients for the multi-log linear law.\n# For each group and each output loss_domain_k, the list is:\n#   [intercept, b1, b2, b3, b4, b5]\n_COEFS: Dict[str, Dict[str, List[float]]] = {\n    \"160M\": {\n        \"loss_domain_1\": [\n            2.4042025803406553,\n            -0.04878456902733,\n            -0.001374141990715709,\n            0.00425143168670042,\n            -0.004036425589256114,\n            0.007660516801797778,\n        ],\n        \"loss_domain_2\": [\n            3.329935868696654,\n            0.002613273350161564,\n            -0.011891996557419415,\n            2.5572674116154508e-05,\n            0.003256629390122456,\n            -0.005952205705195364,\n        ],\n        \"loss_domain_3\": [\n            2.8006343825165314,\n            0.0038463371713819104,\n            0.0010271048724917586,\n            -0.03834514983761013,\n            0.003370871701657902,\n            -0.004992444549614571,\n        ],\n        \"loss_domain_4\": [\n            1.3842254371292473,\n            0.0001770125184695076,\n            0.0019008052563317707,\n            0.00040985897229494966,\n            -0.04321725706040489,\n            0.005203775943861618,\n        ],\n        \"loss_domain_5\": [\n            3.22070848376419,\n            0.005681276339156933,\n            0.004826630941776033,\n            -0.00156340623248917,\n            0.005473022220321205,\n            -0.030039287316337895,\n        ],\n    },\n    \"305M\": {\n        \"loss_domain_1\": [\n            2.254097761686427,\n            -0.047618308465165196,\n            -0.0013550046237553216,\n            0.004003192579882288,\n            -0.0042909223510698554,\n            0.006758159630738282,\n        ],\n        \"loss_domain_2\": [\n            3.166848403701224,\n            0.0023474329227610603,\n            -0.011422287483401284,\n            4.890168638858063e-06,\n            0.0026987505976661923,\n            -0.006498741023938888,\n        ],\n        \"loss_domain_3\": [\n            2.6500359123770685,\n            0.0035548451771600445,\n            0.0011888369505355075,\n            -0.03964115398330717,\n            0.0028959099742507494,\n            -0.004828358464409514,\n        ],\n        \"loss_domain_4\": [\n            1.2751944568968825,\n            -0.00024923215104993846,\n            0.0015693015233153626,\n            0.0004352251902864513,\n            -0.041431801189773,\n            0.00487564464080058,\n        ],\n        \"loss_domain_5\": [\n            3.0477226649403506,\n            0.005424541586794049,\n            0.004752807485933067,\n            -0.0015473014680650466,\n            0.004775541162988269,\n            -0.030879417992197157,\n        ],\n    },\n    \"410M\": {\n        \"loss_domain_1\": [\n            2.1969109104720936,\n            -0.04711053566759858,\n            -0.0005929598868873091,\n            0.0032935545744610275,\n            -0.0034299883497122844,\n            0.006245418652814477,\n        ],\n        \"loss_domain_2\": [\n            3.096774846113359,\n            0.002683578481767577,\n            -0.01079532012122322,\n            -0.0006061709329291659,\n            0.003289915492216653,\n            -0.007059325448456723,\n        ],\n        \"loss_domain_3\": [\n            2.5854698484676923,\n            0.0036493802056689617,\n            0.0014981486567392278,\n            -0.04057579585320029,\n            0.003422309711002145,\n            -0.005304641068008599,\n        ],\n        \"loss_domain_4\": [\n            1.2374283294627488,\n            -0.0007019754734980677,\n            0.0023220834389344196,\n            0.00022483249867104974,\n            -0.040255057354183485,\n            0.0043998574943246395,\n        ],\n        \"loss_domain_5\": [\n            2.9734625677861892,\n            0.005474979456664107,\n            0.005292433569369652,\n            -0.0021983339298417347,\n            0.005328130514437292,\n            -0.031848404298810445,\n        ],\n    },\n    \"70M\": {\n        \"loss_domain_1\": [\n            2.709753097020149,\n            -0.05223448519596348,\n            -0.002096832291136182,\n            0.0049309629806633245,\n            -0.005121637241168704,\n            0.008582633603168494,\n        ],\n        \"loss_domain_2\": [\n            3.6589940901202946,\n            0.0024594854221674217,\n            -0.013037306540415955,\n            0.00018013301869309603,\n            0.0025694674548725075,\n            -0.0055192991657138785,\n        ],\n        \"loss_domain_3\": [\n            3.0882519411499176,\n            0.004138978755961604,\n            0.000999612740187218,\n            -0.040138664176247195,\n            0.002610113875372775,\n            -0.00489486237526904,\n        ],\n        \"loss_domain_4\": [\n            1.6123830068952152,\n            -0.0002626923872205353,\n            0.001840626589019361,\n            0.0004871656033250259,\n            -0.048580221686945084,\n            0.0053741041526707105,\n        ],\n        \"loss_domain_5\": [\n            3.567736623439916,\n            0.005906206376564464,\n            0.00463119283202685,\n            -0.0015851069219014631,\n            0.004999662780701288,\n            -0.029220774259506147,\n        ],\n    },\n}\n\n\ndef _predict_one(input_point: Dict[str, float], beta: List[float]) -> float:\n    # Build feature vector: [1, log(p1+eps), ..., log(p5+eps)]\n    logs = [math.log(max(input_point.get(f\"proportion_domain_{i}\", 0.0), 0.0) + _EPS) for i in range(1, 6)]\n    return beta[0] + sum(b * x for b, x in zip(beta[1:], logs))\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _COEFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {sorted(_COEFS.keys())}\")\n\n    group_coefs = _COEFS[group]\n    outputs: List[Dict[str, float]] = []\n    for point in input_data:\n        pred: Dict[str, float] = {}\n        for k in range(1, 6):\n            key = f\"loss_domain_{k}\"\n            beta = group_coefs[key]\n            pred[key] = _predict_one(point, beta)\n        outputs.append(pred)\n    return outputs", "agent_name": "opencode", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.899201, "solution": "from typing import List, Dict\nimport math\n\n# Fitted coefficients for the linear-in-log proportions model:\n# loss_domain_i = intercept_i + sum_j beta_ij * log(proportion_domain_j + eps)\nCOEFS = {\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.4042025803406553,\n      \"betas\": {\n        \"proportion_domain_1\": -0.04878456902733,\n        \"proportion_domain_2\": -0.001374141990715709,\n        \"proportion_domain_3\": 0.00425143168670042,\n        \"proportion_domain_4\": -0.004036425589256114,\n        \"proportion_domain_5\": 0.007660516801797778\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.329935868696654,\n      \"betas\": {\n        \"proportion_domain_1\": 0.002613273350161564,\n        \"proportion_domain_2\": -0.011891996557419415,\n        \"proportion_domain_3\": 2.5572674116154508e-05,\n        \"proportion_domain_4\": 0.003256629390122456,\n        \"proportion_domain_5\": -0.005952205705195364\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8006343825165314,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0038463371713819104,\n        \"proportion_domain_2\": 0.0010271048724917586,\n        \"proportion_domain_3\": -0.03834514983761013,\n        \"proportion_domain_4\": 0.003370871701657902,\n        \"proportion_domain_5\": -0.004992444549614571\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.3842254371292473,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0001770125184695076,\n        \"proportion_domain_2\": 0.0019008052563317707,\n        \"proportion_domain_3\": 0.00040985897229494966,\n        \"proportion_domain_4\": -0.04321725706040489,\n        \"proportion_domain_5\": 0.005203775943861618\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.22070848376419,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005681276339156933,\n        \"proportion_domain_2\": 0.004826630941776033,\n        \"proportion_domain_3\": -0.00156340623248917,\n        \"proportion_domain_4\": 0.005473022220321205,\n        \"proportion_domain_5\": -0.030039287316337895\n      }\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.254097761686427,\n      \"betas\": {\n        \"proportion_domain_1\": -0.047618308465165196,\n        \"proportion_domain_2\": -0.0013550046237553216,\n        \"proportion_domain_3\": 0.004003192579882288,\n        \"proportion_domain_4\": -0.0042909223510698554,\n        \"proportion_domain_5\": 0.006758159630738282\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.166848403701224,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0023474329227610603,\n        \"proportion_domain_2\": -0.011422287483401284,\n        \"proportion_domain_3\": 4.890168638858063e-06,\n        \"proportion_domain_4\": 0.0026987505976661923,\n        \"proportion_domain_5\": -0.006498741023938888\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.6500359123770685,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0035548451771600445,\n        \"proportion_domain_2\": 0.0011888369505355075,\n        \"proportion_domain_3\": -0.03964115398330717,\n        \"proportion_domain_4\": 0.0028959099742507494,\n        \"proportion_domain_5\": -0.004828358464409514\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.2751944568968825,\n      \"betas\": {\n        \"proportion_domain_1\": -0.00024923215104993846,\n        \"proportion_domain_2\": 0.0015693015233153626,\n        \"proportion_domain_3\": 0.0004352251902864513,\n        \"proportion_domain_4\": -0.041431801189773,\n        \"proportion_domain_5\": 0.00487564464080058\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.0477226649403506,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005424541586794049,\n        \"proportion_domain_2\": 0.004752807485933067,\n        \"proportion_domain_3\": -0.0015473014680650466,\n        \"proportion_domain_4\": 0.004775541162988269,\n        \"proportion_domain_5\": -0.030879417992197157\n      }\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.1969109104720936,\n      \"betas\": {\n        \"proportion_domain_1\": -0.04711053566759858,\n        \"proportion_domain_2\": -0.0005929598868873091,\n        \"proportion_domain_3\": 0.0032935545744610275,\n        \"proportion_domain_4\": -0.0034299883497122844,\n        \"proportion_domain_5\": 0.006245418652814477\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.096774846113359,\n      \"betas\": {\n        \"proportion_domain_1\": 0.002683578481767577,\n        \"proportion_domain_2\": -0.01079532012122322,\n        \"proportion_domain_3\": -0.0006061709329291659,\n        \"proportion_domain_4\": 0.003289915492216653,\n        \"proportion_domain_5\": -0.007059325448456723\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.5854698484676923,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0036493802056689617,\n        \"proportion_domain_2\": 0.0014981486567392278,\n        \"proportion_domain_3\": -0.04057579585320029,\n        \"proportion_domain_4\": 0.003422309711002145,\n        \"proportion_domain_5\": -0.005304641068008599\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.2374283294627488,\n      \"betas\": {\n        \"proportion_domain_1\": -0.0007019754734980677,\n        \"proportion_domain_2\": 0.0023220834389344196,\n        \"proportion_domain_3\": 0.00022483249867104974,\n        \"proportion_domain_4\": -0.040255057354183485,\n        \"proportion_domain_5\": 0.0043998574943246395\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 2.9734625677861892,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005474979456664107,\n        \"proportion_domain_2\": 0.005292433569369652,\n        \"proportion_domain_3\": -0.0021983339298417347,\n        \"proportion_domain_4\": 0.005328130514437292,\n        \"proportion_domain_5\": -0.031848404298810445\n      }\n    }\n  },\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.709753097020149,\n      \"betas\": {\n        \"proportion_domain_1\": -0.05223448519596348,\n        \"proportion_domain_2\": -0.002096832291136182,\n        \"proportion_domain_3\": 0.0049309629806633245,\n        \"proportion_domain_4\": -0.005121637241168704,\n        \"proportion_domain_5\": 0.008582633603168494\n      }\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.6589940901202946,\n      \"betas\": {\n        \"proportion_domain_1\": 0.0024594854221674217,\n        \"proportion_domain_2\": -0.013037306540415955,\n        \"proportion_domain_3\": 0.00018013301869309603,\n        \"proportion_domain_4\": 0.0025694674548725075,\n        \"proportion_domain_5\": -0.0055192991657138785\n      }\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 3.0882519411499176,\n      \"betas\": {\n        \"proportion_domain_1\": 0.004138978755961604,\n        \"proportion_domain_2\": 0.000999612740187218,\n        \"proportion_domain_3\": -0.040138664176247195,\n        \"proportion_domain_4\": 0.002610113875372775,\n        \"proportion_domain_5\": -0.00489486237526904\n      }\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.6123830068952152,\n      \"betas\": {\n        \"proportion_domain_1\": -0.0002626923872205353,\n        \"proportion_domain_2\": 0.001840626589019361,\n        \"proportion_domain_3\": 0.0004871656033250259,\n        \"proportion_domain_4\": -0.048580221686945084,\n        \"proportion_domain_5\": 0.0053741041526707105\n      }\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.567736623439916,\n      \"betas\": {\n        \"proportion_domain_1\": 0.005906206376564464,\n        \"proportion_domain_2\": 0.00463119283202685,\n        \"proportion_domain_3\": -0.0015851069219014631,\n        \"proportion_domain_4\": 0.004999662780701288,\n        \"proportion_domain_5\": -0.029220774259506147\n      }\n    }\n  }\n}\n\nPROP_COLS = ['proportion_domain_1', 'proportion_domain_2', 'proportion_domain_3', 'proportion_domain_4', 'proportion_domain_5']\nLOSS_COLS = ['loss_domain_1', 'loss_domain_2', 'loss_domain_3', 'loss_domain_4', 'loss_domain_5']\nEPS = 1e-6\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Functional form (same across groups):\n        For each loss_domain_k:\n            loss_k = intercept_k(group) + sum_j beta_kj(group) * log(proportion_domain_j + EPS)\n\n    Args:\n        input_data: List of dicts with keys including all 'proportion_domain_*' inputs.\n        group: The experimental group for which to make predictions.\n               Coefficients differ per group but the functional form is fixed.\n\n    Returns:\n        List of dicts with keys equal to LOSS_COLS and predicted float values.\n    \"\"\"\n    if group not in COEFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(COEFS.keys())}\")\n    group_coefs = COEFS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Build predictions for each loss column\n        out = {}\n        for loss_key in LOSS_COLS:\n            params = group_coefs[loss_key]\n            val = float(params[\"intercept\"])\n            for p in PROP_COLS:\n                x = float(row.get(p, 0.0))\n                val += float(params[\"betas\"][p]) * math.log(max(x, 0.0) + EPS)\n            out[loss_key] = val\n        outputs.append(out)\n    return outputs", "agent_name": "mini-swe-agent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5.2", "reward_r2": 0.8992005438971138, "solution": "import math\nfrom typing import Dict, List\n\n\n# Discovered scaling law:\n#   loss_domain_i = b_i(group) + sum_j w_{i,j}(group) * ln(proportion_domain_j + eps)\n# where eps is a small constant to safely handle zero proportions.\n\n_EPS = 1e-6\n\n_PARAMS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"70M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.7097530970201484,\n            \"proportion_domain_1\": -0.0522344851959636,\n            \"proportion_domain_2\": -0.002096832291136152,\n            \"proportion_domain_3\": 0.004930962980663215,\n            \"proportion_domain_4\": -0.005121637241168656,\n            \"proportion_domain_5\": 0.008582633603168563,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.658994090120295,\n            \"proportion_domain_1\": 0.0024594854221674004,\n            \"proportion_domain_2\": -0.013037306540415919,\n            \"proportion_domain_3\": 0.00018013301869302894,\n            \"proportion_domain_4\": 0.0025694674548724715,\n            \"proportion_domain_5\": -0.005519299165713886,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 3.088251941149917,\n            \"proportion_domain_1\": 0.004138978755961478,\n            \"proportion_domain_2\": 0.0009996127401871256,\n            \"proportion_domain_3\": -0.04013866417624731,\n            \"proportion_domain_4\": 0.0026101138753726574,\n            \"proportion_domain_5\": -0.004894862375269067,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.6123830068952154,\n            \"proportion_domain_1\": -0.0002626923872205609,\n            \"proportion_domain_2\": 0.0018406265890194163,\n            \"proportion_domain_3\": 0.00048716560332494747,\n            \"proportion_domain_4\": -0.04858022168694516,\n            \"proportion_domain_5\": 0.00537410415267081,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.567736623439917,\n            \"proportion_domain_1\": 0.0059062063765644435,\n            \"proportion_domain_2\": 0.00463119283202682,\n            \"proportion_domain_3\": -0.001585106921901497,\n            \"proportion_domain_4\": 0.004999662780701258,\n            \"proportion_domain_5\": -0.029220774259506154,\n        },\n    },\n    \"160M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.404202580340655,\n            \"proportion_domain_1\": -0.04878456902733004,\n            \"proportion_domain_2\": -0.0013741419907157414,\n            \"proportion_domain_3\": 0.004251431686700347,\n            \"proportion_domain_4\": -0.004036425589256161,\n            \"proportion_domain_5\": 0.007660516801797868,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.329935868696656,\n            \"proportion_domain_1\": 0.0026132733501615223,\n            \"proportion_domain_2\": -0.011891996557419387,\n            \"proportion_domain_3\": 2.55726741161342e-05,\n            \"proportion_domain_4\": 0.003256629390122431,\n            \"proportion_domain_5\": -0.005952205705195353,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.8006343825165327,\n            \"proportion_domain_1\": 0.0038463371713819065,\n            \"proportion_domain_2\": 0.0010271048724917568,\n            \"proportion_domain_3\": -0.03834514983761017,\n            \"proportion_domain_4\": 0.003370871701657851,\n            \"proportion_domain_5\": -0.004992444549614443,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.3842254371292473,\n            \"proportion_domain_1\": 0.00017701251846946727,\n            \"proportion_domain_2\": 0.0019008052563317963,\n            \"proportion_domain_3\": 0.00040985897229483783,\n            \"proportion_domain_4\": -0.04321725706040493,\n            \"proportion_domain_5\": 0.005203775943861722,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.22070848376419,\n            \"proportion_domain_1\": 0.005681276339156912,\n            \"proportion_domain_2\": 0.004826630941775963,\n            \"proportion_domain_3\": -0.0015634062324892195,\n            \"proportion_domain_4\": 0.005473022220321154,\n            \"proportion_domain_5\": -0.030039287316337846,\n        },\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.254097761686427,\n            \"proportion_domain_1\": -0.04761830846516523,\n            \"proportion_domain_2\": -0.0013550046237552692,\n            \"proportion_domain_3\": 0.004003192579882179,\n            \"proportion_domain_4\": -0.004290922351069889,\n            \"proportion_domain_5\": 0.006758159630738393,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.166848403701226,\n            \"proportion_domain_1\": 0.0023474329227610503,\n            \"proportion_domain_2\": -0.011422287483401315,\n            \"proportion_domain_3\": 4.890168638751796e-06,\n            \"proportion_domain_4\": 0.0026987505976661533,\n            \"proportion_domain_5\": -0.006498741023938811,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.65003591237707,\n            \"proportion_domain_1\": 0.003554845177160056,\n            \"proportion_domain_2\": 0.001188836950535425,\n            \"proportion_domain_3\": -0.03964115398330725,\n            \"proportion_domain_4\": 0.0028959099742506974,\n            \"proportion_domain_5\": -0.004828358464409561,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.2751944568968825,\n            \"proportion_domain_1\": -0.0002492321510499463,\n            \"proportion_domain_2\": 0.0015693015233153589,\n            \"proportion_domain_3\": 0.0004352251902864149,\n            \"proportion_domain_4\": -0.04143180118977307,\n            \"proportion_domain_5\": 0.004875644640800658,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 3.0477226649403515,\n            \"proportion_domain_1\": 0.0054245415867939665,\n            \"proportion_domain_2\": 0.00475280748593301,\n            \"proportion_domain_3\": -0.001547301468065229,\n            \"proportion_domain_4\": 0.004775541162988245,\n            \"proportion_domain_5\": -0.030879417992197206,\n        },\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\n            \"intercept\": 2.196910910472094,\n            \"proportion_domain_1\": -0.04711053566759865,\n            \"proportion_domain_2\": -0.0005929598868872977,\n            \"proportion_domain_3\": 0.0032935545744609243,\n            \"proportion_domain_4\": -0.003429988349712316,\n            \"proportion_domain_5\": 0.006245418652814499,\n        },\n        \"loss_domain_2\": {\n            \"intercept\": 3.09677484611336,\n            \"proportion_domain_1\": 0.0026835784817675957,\n            \"proportion_domain_2\": -0.010795320121223216,\n            \"proportion_domain_3\": -0.0006061709329293224,\n            \"proportion_domain_4\": 0.00328991549221664,\n            \"proportion_domain_5\": -0.007059325448456602,\n        },\n        \"loss_domain_3\": {\n            \"intercept\": 2.5854698484676923,\n            \"proportion_domain_1\": 0.0036493802056689595,\n            \"proportion_domain_2\": 0.0014981486567391834,\n            \"proportion_domain_3\": -0.040575795853200314,\n            \"proportion_domain_4\": 0.003422309711002071,\n            \"proportion_domain_5\": -0.005304641068008607,\n        },\n        \"loss_domain_4\": {\n            \"intercept\": 1.2374283294627486,\n            \"proportion_domain_1\": -0.0007019754734981021,\n            \"proportion_domain_2\": 0.0023220834389343996,\n            \"proportion_domain_3\": 0.00022483249867098906,\n            \"proportion_domain_4\": -0.04025505735418355,\n            \"proportion_domain_5\": 0.004399857494324675,\n        },\n        \"loss_domain_5\": {\n            \"intercept\": 2.973462567786192,\n            \"proportion_domain_1\": 0.005474979456664085,\n            \"proportion_domain_2\": 0.0052924335693696496,\n            \"proportion_domain_3\": -0.002198333929841836,\n            \"proportion_domain_4\": 0.005328130514437295,\n            \"proportion_domain_5\": -0.03184840429881037,\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n            point containing input variable names as keys and their corresponding values.\n        group: The name of the experimental group for which to make predictions.\n            The functional form of the law is shared across groups, but coefficients\n            differ.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group: {group}. Known groups: {sorted(_PARAMS.keys())}\")\n\n    group_params = _PARAMS[group]\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        # Precompute log proportions (with epsilon for safety at p=0)\n        logp = {}\n        for j in range(1, 6):\n            k = f\"proportion_domain_{j}\"\n            v = float(row.get(k, 0.0))\n            if v < 0:\n                v = 0.0\n            logp[k] = math.log(v + _EPS)\n\n        pred_row: Dict[str, float] = {}\n        for i in range(1, 6):\n            outk = f\"loss_domain_{i}\"\n            p = group_params[outk]\n            val = p[\"intercept\"]\n            for j in range(1, 6):\n                ink = f\"proportion_domain_{j}\"\n                val += p[ink] * logp[ink]\n            pred_row[outk] = float(val)\n\n        outputs.append(pred_row)\n\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5.2", "reward_r2": 0.8992005438971136, "solution": "import math\nfrom typing import Dict, List\n\n# Scaling law discovered from the dataset:\n# For each group g and each domain d (1..5):\n#   loss_domain_d = b[g,d] + sum_{k=1..5} a[g,d,k] * log(proportion_domain_k + eps)\n# with a small eps to handle zero proportions.\n\n_EPS = 1e-6\n\n_PARAMS: Dict[str, Dict[str, Dict[str, List[float] | float]]] = {\n    \"70M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.7097530970201484,\n            \"a\": [-0.0522344851959636, -0.002096832291136152, 0.004930962980663215, -0.005121637241168656, 0.008582633603168563],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.658994090120295,\n            \"a\": [0.0024594854221674004, -0.013037306540415919, 0.00018013301869302894, 0.0025694674548724715, -0.005519299165713886],\n        },\n        \"loss_domain_3\": {\n            \"b\": 3.088251941149917,\n            \"a\": [0.004138978755961478, 0.0009996127401871256, -0.04013866417624731, 0.0026101138753726574, -0.004894862375269067],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.6123830068952154,\n            \"a\": [-0.0002626923872205609, 0.0018406265890194163, 0.00048716560332494747, -0.04858022168694516, 0.00537410415267081],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.567736623439917,\n            \"a\": [0.0059062063765644435, 0.00463119283202682, -0.001585106921901497, 0.004999662780701258, -0.029220774259506154],\n        },\n    },\n    \"160M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.404202580340655,\n            \"a\": [-0.04878456902733004, -0.0013741419907157414, 0.004251431686700347, -0.004036425589256161, 0.007660516801797868],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.329935868696656,\n            \"a\": [0.0026132733501615223, -0.011891996557419387, 2.55726741161342e-05, 0.003256629390122431, -0.005952205705195353],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.8006343825165327,\n            \"a\": [0.0038463371713819065, 0.0010271048724917568, -0.03834514983761017, 0.003370871701657851, -0.004992444549614443],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.3842254371292473,\n            \"a\": [0.00017701251846946727, 0.0019008052563317963, 0.00040985897229483783, -0.04321725706040493, 0.005203775943861722],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.22070848376419,\n            \"a\": [0.005681276339156912, 0.004826630941775963, -0.0015634062324892195, 0.005473022220321154, -0.030039287316337846],\n        },\n    },\n    \"305M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.254097761686427,\n            \"a\": [-0.04761830846516523, -0.0013550046237552692, 0.004003192579882179, -0.004290922351069889, 0.006758159630738393],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.166848403701226,\n            \"a\": [0.0023474329227610503, -0.011422287483401315, 4.890168638751796e-06, 0.0026987505976661533, -0.006498741023938811],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.65003591237707,\n            \"a\": [0.003554845177160056, 0.001188836950535425, -0.03964115398330725, 0.0028959099742506974, -0.004828358464409561],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.2751944568968825,\n            \"a\": [-0.0002492321510499463, 0.0015693015233153589, 0.0004352251902864149, -0.04143180118977307, 0.004875644640800658],\n        },\n        \"loss_domain_5\": {\n            \"b\": 3.0477226649403515,\n            \"a\": [0.0054245415867939665, 0.00475280748593301, -0.001547301468065229, 0.004775541162988245, -0.030879417992197206],\n        },\n    },\n    \"410M\": {\n        \"loss_domain_1\": {\n            \"b\": 2.196910910472094,\n            \"a\": [-0.04711053566759865, -0.0005929598868872977, 0.0032935545744609243, -0.003429988349712316, 0.006245418652814499],\n        },\n        \"loss_domain_2\": {\n            \"b\": 3.09677484611336,\n            \"a\": [0.0026835784817675957, -0.010795320121223216, -0.0006061709329293224, 0.00328991549221664, -0.007059325448456602],\n        },\n        \"loss_domain_3\": {\n            \"b\": 2.5854698484676923,\n            \"a\": [0.0036493802056689595, 0.0014981486567391834, -0.040575795853200314, 0.003422309711002071, -0.005304641068008607],\n        },\n        \"loss_domain_4\": {\n            \"b\": 1.2374283294627486,\n            \"a\": [-0.0007019754734981021, 0.0023220834389343996, 0.00022483249867098906, -0.04025505735418355, 0.004399857494324675],\n        },\n        \"loss_domain_5\": {\n            \"b\": 2.973462567786192,\n            \"a\": [0.005474979456664085, 0.0052924335693696496, -0.002198333929841836, 0.005328130514437295, -0.03184840429881037],\n        },\n    },\n}\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"Predicts validation loss per domain from training mixture proportions.\"\"\"\n\n    if group not in _PARAMS:\n        raise KeyError(f\"Unknown group '{group}'. Known groups: {sorted(_PARAMS.keys())}\")\n\n    group_params = _PARAMS[group]\n    outputs: List[Dict[str, float]] = []\n\n    for row in input_data:\n        props = [\n            float(row.get(f\"proportion_domain_{i}\", 0.0)) for i in range(1, 6)\n        ]\n        logp = [math.log(p + _EPS) for p in props]\n\n        pred: Dict[str, float] = {}\n        for d in range(1, 6):\n            key = f\"loss_domain_{d}\"\n            b = float(group_params[key][\"b\"])  # type: ignore[index]\n            a = group_params[key][\"a\"]  # type: ignore[index]\n            pred[key] = b + sum(float(ai) * li for ai, li in zip(a, logp))\n\n        outputs.append(pred)\n\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.8992, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Implementation notes:\n    # - Functional form (shared across groups):\n    #     For each domain k:\n    #         loss_domain_k = a_k + sum_j w_{k,j} * log(proportion_domain_j + eps)\n    #   where eps is a small constant to stabilize log near zero.\n    # - Coefficients (a_k, w_{k,j}) are fitted per `group` by ridge regression\n    #   on the dataset loaded from /app/data (if available).\n    # - Fitted parameters are cached on the function object (law._params)\n    #   and also written to /app/fit.json for inspection.\n    import re\n    import os\n    import json\n    import math\n\n    # Lazy imports to avoid hard dependency unless actually called.\n    try:\n        import numpy as _np\n    except Exception as _e:\n        raise RuntimeError(\"numpy is required to run the scaling law\") from _e\n\n    # Create persistent cache on the function object\n    if not hasattr(law, \"_params\"):\n        law._params = {}\n\n    def _load_dataset():\n        try:\n            from datasets import load_from_disk as _load_from_disk\n            return _load_from_disk(\"/app/data\")\n        except Exception:\n            return None\n\n    def _extract_columns(ds):\n        # Determine proportion and loss columns from dataset schema\n        def _cols_from_split(split_ds):\n            # datasets.Dataset has attribute column_names\n            return list(getattr(split_ds, \"column_names\", []))\n        if hasattr(ds, \"keys\"):\n            # DatasetDict\n            first_key = next(iter(ds.keys()))\n            cols = _cols_from_split(ds[first_key])\n        else:\n            cols = _cols_from_split(ds)\n        prop_cols = [c for c in cols if c.startswith(\"proportion_domain_\")]\n        loss_cols = [c for c in cols if c.startswith(\"loss_domain_\")]\n\n        def _idx(c):\n            m = re.search(r\"(\\d+)$\", c)\n            return int(m.group(1)) if m else 10**9\n\n        prop_cols = sorted(prop_cols, key=_idx)\n        loss_cols = sorted(loss_cols, key=_idx)\n        group_col = \"group\" if \"group\" in cols else None\n        return prop_cols, loss_cols, group_col\n\n    def _iter_train_rows(ds):\n        # Normalize to a list of dict rows, preferring 'train' if present\n        if hasattr(ds, \"keys\"):\n            # DatasetDict\n            if \"train\" in ds:\n                return list(ds[\"train\"])\n            # else merge all splits\n            rows = []\n            for sp in ds.keys():\n                rows.extend(ds[sp])\n            return rows\n        # Single Dataset\n        rows = list(ds)\n        if rows and isinstance(rows[0], dict) and \"split\" in rows[0]:\n            rows = [r for r in rows if r.get(\"split\") == \"train\"]\n        return rows\n\n    def _fit_for_group(g: str):\n        ds = _load_dataset()\n        eps = 1e-6\n        reg = 1e-3  # L2 regularization to mitigate collinearity (proportions sum to 1)\n\n        # If dataset is unavailable, create a degenerate model based on input_data structure.\n        if ds is None:\n            # Infer columns from input_data if possible\n            prop_cols = []\n            loss_cols = []\n            if input_data:\n                keys = list(input_data[0].keys())\n                prop_cols = sorted([k for k in keys if k.startswith(\"proportion_domain_\")],\n                                   key=lambda c: int(re.search(r\"(\\d+)$\", c).group(1)) if re.search(r\"(\\d+)$\", c) else 10**9)\n                # Default order for losses mirrors proportions\n                loss_cols = [f\"loss_domain_{i+1}\" for i in range(len(prop_cols))]\n            beta = _np.zeros((len(prop_cols) + 1, len(loss_cols) if loss_cols else 0), dtype=float)\n            return {\"beta\": beta, \"prop_cols\": prop_cols, \"loss_cols\": loss_cols, \"eps\": eps}\n\n        prop_cols, loss_cols, group_col = _extract_columns(ds)\n        rows = _iter_train_rows(ds)\n\n        # Filter by group if a group column is present; otherwise use all rows\n        if group_col is not None:\n            grp_rows = [r for r in rows if r.get(group_col) == g]\n            if len(grp_rows) == 0:\n                # Fallback to all rows if the requested group does not exist\n                grp_rows = rows\n        else:\n            grp_rows = rows\n\n        # Assemble design matrix X (log proportions) and targets Y (losses)\n        X_list = []\n        Y_list = []\n        for r in grp_rows:\n            try:\n                xrow = [math.log(float(r.get(c, 0.0)) + eps) for c in prop_cols]\n                yrow = [float(r[c]) for c in loss_cols if c in r]\n            except Exception:\n                continue\n            if len(yrow) != len(loss_cols):\n                continue\n            if any(math.isinf(v) or math.isnan(v) for v in xrow):\n                continue\n            X_list.append(xrow)\n            Y_list.append(yrow)\n\n        if len(X_list) == 0:\n            beta = _np.zeros((len(prop_cols) + 1, len(loss_cols)), dtype=float)\n        else:\n            X = _np.asarray(X_list, dtype=float)\n            Y = _np.asarray(Y_list, dtype=float)\n            # Add bias column\n            Xb = _np.concatenate([_np.ones((X.shape[0], 1), dtype=float), X], axis=1)\n            XtX = Xb.T @ Xb\n            I = _np.eye(XtX.shape[0], dtype=float)\n            I[0, 0] = 0.0  # do not regularize intercept\n            A = XtX + reg * I\n            XtY = Xb.T @ Y\n            try:\n                beta = _np.linalg.solve(A, XtY)\n            except _np.linalg.LinAlgError:\n                beta = _np.linalg.pinv(A) @ XtY\n\n        params = {\"beta\": beta, \"prop_cols\": prop_cols, \"loss_cols\": loss_cols, \"eps\": eps}\n\n        # Persist for inspection\n        try:\n            serial = {\n                \"prop_cols\": prop_cols,\n                \"loss_cols\": loss_cols,\n                \"eps\": eps,\n                \"beta\": beta.tolist(),\n            }\n            fit_path = \"/app/fit.json\"\n            blob = {}\n            if os.path.exists(fit_path):\n                try:\n                    with open(fit_path, \"r\") as f:\n                        blob = json.load(f) or {}\n                except Exception:\n                    blob = {}\n            blob[g] = serial\n            with open(fit_path, \"w\") as f:\n                json.dump(blob, f, indent=2)\n        except Exception:\n            # Best-effort; do not fail predictions because of I/O issues.\n            pass\n\n        return params\n\n    # Ensure we have fitted parameters for this group\n    if group not in law._params:\n        law._params[group] = _fit_for_group(group)\n\n    params = law._params[group]\n    beta = params[\"beta\"]\n    prop_cols = params[\"prop_cols\"]\n    loss_cols = params[\"loss_cols\"]\n    eps = params[\"eps\"]\n\n    # Produce predictions\n    results: list[dict[str, float]] = []\n    for row in input_data:\n        # If dataset unavailable and we couldn't infer loss columns, return empty predictions\n        if not loss_cols:\n            results.append({})\n            continue\n        x = [_np.log(float(row.get(c, 0.0)) + eps) for c in prop_cols]\n        xb = _np.concatenate([_np.array([1.0], dtype=float), _np.asarray(x, dtype=float)])\n        y = xb @ beta  # shape (num_losses,)\n        pred = {loss_cols[j]: float(y[j]) for j in range(len(loss_cols))}\n        results.append(pred)\n\n    return results", "agent_name": "aider", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-flash-preview", "reward_r2": 0.8946454997391851, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters fitted for each group and domain\n    # Format: {group: {domain_index: [a, b, c]}}\n    # Formula: loss = a * (proportion + 1e-6)**b + c\n    params = {\n      \"70M\": {\n        1: [2.710378086839277, -0.017029560929862225, 6.122502542461153e-14],\n        2: [3.641822387148273, -0.0034420417203390283, 3.664705437636036e-05],\n        3: [3.0734642770039438, -0.011482268379146534, 5.0279160217933075e-14],\n        4: [1.6059732325280927, -0.025094792442143306, 5.6645199147913316e-14],\n        5: [3.4932889198309134, -0.00937863408600572, 2.2025811669049076e-16]\n      },\n      \"160M\": {\n        1: [2.40081495799688, -0.017856712579836093, 1.465625882756652e-12],\n        2: [3.309549242165178, -0.003481852718743377, 1.2595135178484399e-05],\n        3: [2.7836616124660836, -0.012021613474412426, 5.855820258273959e-08],\n        4: [1.3747713978038434, -0.025950009122439683, 2.5488849307414626e-15],\n        5: [3.143771112943483, -0.010605607268231104, 1.8784228785426135e-13]\n      },\n      \"305M\": {\n        1: [2.255287703195163, -0.01843295718759498, 1.2051934351546453e-12],\n        2: [3.1521871451134604, -0.0034650075599279586, 1.233711008468711e-05],\n        3: [2.636809538515421, -0.013020916869252688, 9.041291266491046e-09],\n        4: [1.2700474990878887, -0.026723137945954697, 3.622469806622208e-16],\n        5: [2.976521908043489, -0.011269643911003876, 1.18352489559479e-13]\n      },\n      \"410M\": {\n        1: [2.1947499199085096, -0.018720868396927052, 7.767189923855914e-14],\n        2: [3.080781816845867, -0.003437582676076585, 1.1255468841608477e-05],\n        3: [2.5700550678168232, -0.013549146350127213, 2.6549168297135425e-17],\n        4: [1.2317060817465844, -0.02679817668535512, 5.066308580862643e-13],\n        5: [2.9004711222062993, -0.011890602184807265, 4.2759352333656383e-14]\n      }\n    }\n\n    group_params = params.get(group)\n    if not group_params:\n        # Default to 410M if group not found\n        group_params = params[\"410M\"]\n\n    predictions = []\n    for data_point in input_data:\n        prediction = {}\n        for i in range(1, 6):\n            proportion = data_point.get(f\"proportion_domain_{i}\", 0.0)\n            a, b, c = group_params[i]\n            prediction[f\"loss_domain_{i}\"] = a * (proportion + 1e-6)**b + c\n        predictions.append(prediction)\n    \n    return predictions", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.880748, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nEPS = 1e-06\nPARAMS = {\n  \"70M\": {\n    \"domain_1\": {\n      \"a\": 2.0220104987927257,\n      \"b\": 0.7051338858670713,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.478057239579727,\n      \"b\": 0.17092206236157978,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.59254285900522,\n      \"b\": 0.5058051486225027,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.9657272567112907,\n      \"b\": 0.6545542978911262,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 3.024670907672095,\n      \"b\": 0.47441444506610303,\n      \"alpha\": 0.05\n    }\n  },\n  \"160M\": {\n    \"domain_1\": {\n      \"a\": 1.7563840140531553,\n      \"b\": 0.6595784179449191,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.1588187107783474,\n      \"b\": 0.15725443050159807,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.3251342372559862,\n      \"b\": 0.48184980806156213,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.8034472891804151,\n      \"b\": 0.583579912689387,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.661628446598834,\n      \"b\": 0.487853772515834,\n      \"alpha\": 0.05\n    }\n  },\n  \"305M\": {\n    \"domain_1\": {\n      \"a\": 1.6270228147234236,\n      \"b\": 0.6426830441112606,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 3.0093884970464932,\n      \"b\": 0.14899746540873754,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.1615746228783017,\n      \"b\": 0.4986333285709002,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.7225593285828501,\n      \"b\": 0.5587932312372937,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.4882874593817577,\n      \"b\": 0.49385627236422996,\n      \"alpha\": 0.05\n    }\n  },\n  \"410M\": {\n    \"domain_1\": {\n      \"a\": 1.5720931564397607,\n      \"b\": 0.6367651459833956,\n      \"alpha\": 0.05\n    },\n    \"domain_2\": {\n      \"a\": 2.9422498586244026,\n      \"b\": 0.14451569098864436,\n      \"alpha\": 0.05\n    },\n    \"domain_3\": {\n      \"a\": 2.0855052306255972,\n      \"b\": 0.5080046213375579,\n      \"alpha\": 0.05\n    },\n    \"domain_4\": {\n      \"a\": 0.6988734728738473,\n      \"b\": 0.5437907496465303,\n      \"alpha\": 0.05\n    },\n    \"domain_5\": {\n      \"a\": 2.395591013031327,\n      \"b\": 0.5105511796136086,\n      \"alpha\": 0.05\n    }\n  }\n}\n\ndef _predict_one(sample: Dict[str, float], group: str) -> Dict[str, float]:\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group: {group}\")\n    out: Dict[str, float] = {}\n    gparams = PARAMS[group]\n    for key, coeffs in gparams.items():\n        # key looks like 'domain_i' from fitting stage\n        d_idx = ''.join(ch for ch in key if ch.isdigit())\n        p_key = f'proportion_domain_{d_idx}'\n        y_key = f'loss_domain_{d_idx}'\n        p = float(sample.get(p_key, 0.0))\n        a = coeffs[\"a\"]; b = coeffs[\"b\"]; alpha = coeffs[\"alpha\"]\n        x = (p + EPS) ** (-alpha) if alpha != 0.0 else p\n        y = a + b * x\n        out[y_key] = float(y)\n    return out\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: List of dicts, each containing at least proportion_domain_i keys.\n        group: Name of the experimental group for which to make predictions.\n    Returns:\n        List of dicts with predicted loss_domain_i for i=1..5 (as available).\n    \"\"\"\n    return [_predict_one(sample, group) for sample in input_data]", "agent_name": "terminus-2", "task": "domain_mixture_scaling_law"}
{"model_name": "o3", "reward_r2": 0.873244167574921, "solution": "import math\n\n# Pre-fitted coefficients (A, B) for each experimental group and domain.\n# The relationship is:  loss_domain_i = A + B * ln(proportion_domain_i + eps)\n# where eps is a very small constant to avoid log(0).\n_COEFFS = {\n    \"70M\": {\n        1: (2.73390692, -0.03347999),\n        2: (3.65073552, -0.00812046),\n        3: (3.10822759, -0.02380902),\n        4: (1.62816371, -0.03102932),\n        5: (3.50046709, -0.02257996),\n    },\n    \"160M\": {\n        1: (2.42222454, -0.03132683),\n        2: (3.31767555, -0.00747288),\n        3: (2.81638464, -0.02268263),\n        4: (1.39405071, -0.02766535),\n        5: (3.15089490, -0.02322456),\n    },\n    \"305M\": {\n        1: (2.27578800, -0.03052741),\n        2: (3.15991008, -0.00707973),\n        3: (2.66991027, -0.02347551),\n        4: (1.28807431, -0.02649081),\n        5: (2.98356485, -0.02351572),\n    },\n    \"410M\": {\n        1: (2.21485658, -0.03025062),\n        2: (3.08823094, -0.00686847),\n        3: (2.60339221, -0.02391696),\n        4: (1.24921386, -0.02577835),\n        5: (2.90760213, -0.02431623),\n    },\n}\n\n_EPS = 1e-9\n\ndef _predict_single(point: dict[str, float], coeffs: dict[int, tuple[float, float]]) -> dict[str, float]:\n    \"\"\"Apply the scaling law to a single data point.\"\"\"\n    out: dict[str, float] = {}\n    for i in range(1, 6):\n        p = point.get(f\"proportion_domain_{i}\")\n        if p is None:\n            raise KeyError(f\"Input missing 'proportion_domain_{i}'\")\n        A, B = coeffs[i]\n        out[f\"loss_domain_{i}\"] = A + B * math.log(p + _EPS)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is shared across groups, while the\n                coefficients differ.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {sorted(_COEFFS.keys())}\")\n\n    group_coeffs = _COEFFS[group]\n    return [_predict_single(pt, group_coeffs) for pt in input_data]", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.873244, "solution": "# Auto-generated scaling law implementation\nfrom __future__ import annotations\nimport math\nfrom typing import Dict, List\n\nEPS = 1e-9\nMODEL_FAMILY = \"log\"\n\n# Per-group, per-domain coefficients (domain indices 1..5)\nPARAMS: dict[str, dict[int, dict[str, float]]] = {\n  \"70M\": {\n    \"1\": {\n      \"a\": 2.73390692,\n      \"b\": -0.03347999\n    },\n    \"2\": {\n      \"a\": 3.65073552,\n      \"b\": -0.00812046\n    },\n    \"3\": {\n      \"a\": 3.10822759,\n      \"b\": -0.02380902\n    },\n    \"4\": {\n      \"a\": 1.62816371,\n      \"b\": -0.03102932\n    },\n    \"5\": {\n      \"a\": 3.50046709,\n      \"b\": -0.02257996\n    }\n  },\n  \"160M\": {\n    \"1\": {\n      \"a\": 2.42222453,\n      \"b\": -0.03132683\n    },\n    \"2\": {\n      \"a\": 3.31767555,\n      \"b\": -0.00747288\n    },\n    \"3\": {\n      \"a\": 2.81638464,\n      \"b\": -0.02268263\n    },\n    \"4\": {\n      \"a\": 1.39405071,\n      \"b\": -0.02766535\n    },\n    \"5\": {\n      \"a\": 3.1508949,\n      \"b\": -0.02322456\n    }\n  },\n  \"305M\": {\n    \"1\": {\n      \"a\": 2.275788,\n      \"b\": -0.03052741\n    },\n    \"2\": {\n      \"a\": 3.15991008,\n      \"b\": -0.00707973\n    },\n    \"3\": {\n      \"a\": 2.66991027,\n      \"b\": -0.02347551\n    },\n    \"4\": {\n      \"a\": 1.28807431,\n      \"b\": -0.02649081\n    },\n    \"5\": {\n      \"a\": 2.98356485,\n      \"b\": -0.02351572\n    }\n  },\n  \"410M\": {\n    \"1\": {\n      \"a\": 2.21485658,\n      \"b\": -0.03025062\n    },\n    \"2\": {\n      \"a\": 3.08823094,\n      \"b\": -0.00686847\n    },\n    \"3\": {\n      \"a\": 2.60339221,\n      \"b\": -0.02391696\n    },\n    \"4\": {\n      \"a\": 1.24921386,\n      \"b\": -0.02577835\n    },\n    \"5\": {\n      \"a\": 2.90760213,\n      \"b\": -0.02431623\n    }\n  }\n}\n\n# Fallback median parameters across groups (for unseen groups)\nDEFAULT_PARAMS: dict[int, dict[str, float]] = {\n  \"1\": {\n    \"a\": 2.34900627,\n    \"b\": -0.03092712\n  },\n  \"2\": {\n    \"a\": 3.23879282,\n    \"b\": -0.0072763\n  },\n  \"3\": {\n    \"a\": 2.74314745,\n    \"b\": -0.02364227\n  },\n  \"4\": {\n    \"a\": 1.34106251,\n    \"b\": -0.02707808\n  },\n  \"5\": {\n    \"a\": 3.06722987,\n    \"b\": -0.02337014\n  }\n}\n\ndef _normalize_coefs(coefs_any) -> dict[int, dict[str, float]]:\n    try:\n        return {int(k): v for k, v in coefs_any.items()}\n    except Exception:\n        return coefs_any\n\ndef _predict_one(proportions: dict[str, float], coefs: dict[int, dict[str, float]]) -> dict[str, float]:\n    out: dict[str, float] = {}\n    default_norm = _normalize_coefs(DEFAULT_PARAMS)\n    for d in range(1, 6):\n        p = float(proportions.get(f\"proportion_domain_{d}\", 0.0))\n        p_eff = p if p > EPS else EPS\n        dc = coefs.get(d)\n        if dc is None:\n            dc = default_norm.get(d, {})\n        if MODEL_FAMILY == \"power\":\n            c0 = float(dc.get(\"c0\", 0.0)); c1 = float(dc.get(\"c1\", 0.0)); a = float(dc.get(\"alpha\", 0.0))\n            y = c0 + c1 * (p_eff ** (-a))\n        else:\n            A = float(dc.get(\"a\", 0.0)); B = float(dc.get(\"b\", 0.0))\n            y = A + B * math.log(p_eff)\n        out[f\"loss_domain_{d}\"] = float(y)\n    return out\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    raw = PARAMS.get(group, DEFAULT_PARAMS)\n    coefs = _normalize_coefs(raw)\n    return [_predict_one(x, coefs) for x in input_data]", "agent_name": "mini-swe-agent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.873244, "solution": "from __future__ import annotations\nimport math\n\n# Single functional form across all groups:\n# L_i = A_{g,i} + B_{g,i} * log(p_i) with clipping p_i -> max(p_i, 1e-9)\nCOEFS = {\n  \"160M\": {\n    \"0\": [\n      2.4222245349880027,\n      -0.03132683285487464\n    ],\n    \"1\": [\n      3.317675549031874,\n      -0.007472875400866049\n    ],\n    \"2\": [\n      2.816384638275675,\n      -0.02268262544964051\n    ],\n    \"3\": [\n      1.3940507054184053,\n      -0.027665347428548207\n    ],\n    \"4\": [\n      3.150894901274346,\n      -0.023224558453472387\n    ]\n  },\n  \"305M\": {\n    \"0\": [\n      2.2757879996895203,\n      -0.03052740882148673\n    ],\n    \"1\": [\n      3.1599100833879104,\n      -0.007079726627925801\n    ],\n    \"2\": [\n      2.6699102690836165,\n      -0.02347551348122601\n    ],\n    \"3\": [\n      1.2880743105690569,\n      -0.02649081104120577\n    ],\n    \"4\": [\n      2.9835648450347945,\n      -0.023515715591365914\n    ]\n  },\n  \"410M\": {\n    \"0\": [\n      2.214856577818689,\n      -0.030250619653697765\n    ],\n    \"1\": [\n      3.0882309384163564,\n      -0.006868468175494051\n    ],\n    \"2\": [\n      2.6033922128748963,\n      -0.023916963654028817\n    ],\n    \"3\": [\n      1.2492138560279822,\n      -0.025778353486428242\n    ],\n    \"4\": [\n      2.907602130692238,\n      -0.024316234178981166\n    ]\n  },\n  \"70M\": {\n    \"0\": [\n      2.733906922836672,\n      -0.033479990654170164\n    ],\n    \"1\": [\n      3.6507355235720613,\n      -0.008120459606531918\n    ],\n    \"2\": [\n      3.108227590265197,\n      -0.02380902449916114\n    ],\n    \"3\": [\n      1.6281637129068052,\n      -0.03102931954295701\n    ],\n    \"4\": [\n      3.5004670876544726,\n      -0.022579959393990854\n    ]\n  }\n}\n\ndef _predict_log(p: float, A: float, B: float) -> float:\n    if p <= 0.0:\n        p = 1e-9\n    return A + B * math.log(p)\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"Predicts output variables based on input variables according to a discovered scaling law.\"\"\"\n    g = group if group in COEFS else (next(iter(COEFS.keys())) if COEFS else group)\n    out_list = []\n    for row in input_data:\n        out = {}\n        for i in range(5):\n            pc = f\"proportion_domain_{i+1}\"\n            lc = f\"loss_domain_{i+1}\"\n            p = float(row.get(pc, 0.0))\n            A,B = COEFS[g].get(str(i), [0.0, 0.0])\n            out[lc] = float(_predict_log(p, A, B))\n        out_list.append(out)\n    return out_list", "agent_name": "terminus-2", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.865747, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The discovered law relates each domain's validation loss to its training mixture\n    proportion via an offset power-decay form shared across groups:\n        loss_domain_i = L_{g,i} + C_{g,i} * (proportion_domain_i + eps) ** (-a_{g,i})\n    where g is the experimental group and i in {1..5} is the domain index.\n\n    Coefficients (L, C, a) are group- and domain-specific, fitted from the provided dataset.\n    For unknown groups, coefficients are obtained by size-aware interpolation (based on\n    the numeric model size parsed from the group string), falling back to a cross-group\n    average if size cannot be parsed.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: Group name (e.g., '70M', '160M', '305M', '410M'). The functional form is\n               shared across groups; parameters differ per group.\n\n    Returns:\n        List of dicts, each containing keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n    # Fitted parameters per group and domain (pow-decay): y = L + C * (p + eps)^(-a)\n    # Derived from analysis over /app/data.\n    PARAMS = {\n        '70M': {\n            1: {'L': 2.3407861901608458, 'C': 0.3990228222403043, 'a': 0.07229469195307019},\n            2: {'L': 3.5878039240958115, 'C': 0.06619054334605488, 'a': 0.0904125332037067},\n            3: {'L': 3.0397589847616344, 'C': 0.09422712368752768, 'a': 0.12920486135303147},\n            4: {'L': 1.367615184081782,  'C': 0.2717420228432777,  'a': 0.08687316390039175},\n            5: {'L': 3.3039469037556852, 'C': 0.20054244651774714, 'a': 0.08493556412221587},\n        },\n        '160M': {\n            1: {'L': 2.110715267629515,  'C': 0.3198800255123924,  'a': 0.07933154901483508},\n            2: {'L': 3.1803738201935277, 'C': 0.13638680248291965, 'a': 0.05512158489827569},\n            3: {'L': 2.6798939752704323, 'C': 0.15139248008383838, 'a': 0.10036697294631616},\n            4: {'L': 1.1773152518677579, 'C': 0.2282095425768926,  'a': 0.08971340404690013},\n            5: {'L': 2.946602702302544,  'C': 0.20862088187761854, 'a': 0.08441408092133072},\n        },\n        '305M': {\n            1: {'L': 1.9852814769362541, 'C': 0.2993480669803183,  'a': 0.08124887473179328},\n            2: {'L': 3.023213559842957,  'C': 0.13550118870333341, 'a': 0.053401380352092805},\n            3: {'L': 2.5983613840895425, 'C': 0.09639191662897745, 'a': 0.12705347977640008},\n            4: {'L': 1.0610600002430857, 'C': 0.2366422762032485,  'a': 0.08578219015376275},\n            5: {'L': 2.7799528735574452, 'C': 0.2083464473165256,  'a': 0.08504953874822715},\n        },\n        '410M': {\n            1: {'L': 1.9365715595185018, 'C': 0.2874401995772295,  'a': 0.08276413468982288},\n            2: {'L': 2.9481641120717006, 'C': 0.13858188362142976, 'a': 0.05155133454157204},\n            3: {'L': 2.5316764731368075, 'C': 0.0972583597842447,  'a': 0.1275873334591975},\n            4: {'L': 1.0267251504383115, 'C': 0.23166028198558963, 'a': 0.08550863921076982},\n            5: {'L': 2.700651481039514,  'C': 0.21075075625956133, 'a': 0.0863440646491576},\n        },\n    }\n\n    # Cross-group per-domain averages (fallback when group is unknown and size cannot be parsed)\n    AVG = {\n        1: {'L': 2.093338623561279, 'C': 0.32642277857756113, 'a': 0.07890981259738035},\n        2: {'L': 3.1848888540509996, 'C': 0.11916510453843443, 'a': 0.0626217082489118},\n        3: {'L': 2.712422704314604, 'C': 0.10981747004614704, 'a': 0.1210531618837363},\n        4: {'L': 1.1581788966577342, 'C': 0.24206353090225213, 'a': 0.08696934932795611},\n        5: {'L': 2.932788490163797, 'C': 0.20706513299286314, 'a': 0.08518581211023284},\n    }\n\n    # Parse numeric model size (in millions) from group string like '70M', '1.3B', '410M'\n    def _parse_size_millions(g: str):\n        if not isinstance(g, str):\n            return None\n        s = g.strip().upper()\n        num = ''\n        unit = ''\n        for ch in s:\n            if (ch.isdigit() or ch == '.' or ch == '+') and unit == '':\n                num += ch\n            elif ch.isalpha():\n                unit += ch\n            # stop at first non-alnum/decimal\n        try:\n            val = float(num) if num else None\n        except Exception:\n            val = None\n        if val is None:\n            return None\n        if 'B' in unit:\n            return val * 1000.0\n        if 'M' in unit or unit == '':\n            return val\n        return None\n\n    # Retrieve parameters for a group, with size-aware interpolation when needed\n    def _get_params_for_group(g: str):\n        if g in PARAMS:\n            return PARAMS[g]\n        # Try interpolation based on parsed size\n        known = sorted(((k, v) for k, v in PARAMS.items()), key=lambda kv: _parse_size_millions(kv[0]) or float('inf'))\n        sizes = [\n            (_parse_size_millions(k) if _parse_size_millions(k) is not None else float('inf'))\n            for k, _ in known\n        ]\n        size = _parse_size_millions(g)\n        if size is None or any(x == float('inf') for x in sizes):\n            return AVG\n        # Clamp to range if outside\n        if size <= sizes[0]:\n            return known[0][1]\n        if size >= sizes[-1]:\n            return known[-1][1]\n        # Find enclosing bracket for interpolation\n        lo_idx = 0\n        for i in range(len(sizes) - 1):\n            if sizes[i] <= size <= sizes[i + 1]:\n                lo_idx = i\n                break\n        hi_idx = lo_idx + 1\n        s0, s1 = sizes[lo_idx], sizes[hi_idx]\n        t = (size - s0) / (s1 - s0) if s1 > s0 else 0.0\n        # Linear interpolation of each parameter per domain\n        interp = {}\n        for di in range(1, 6):\n            L0 = known[lo_idx][1][di]['L']; L1 = known[hi_idx][1][di]['L']\n            C0 = known[lo_idx][1][di]['C']; C1 = known[hi_idx][1][di]['C']\n            a0 = known[lo_idx][1][di]['a']; a1 = known[hi_idx][1][di]['a']\n            interp[di] = {\n                'L': L0 + (L1 - L0) * t,\n                'C': C0 + (C1 - C0) * t,\n                'a': a0 + (a1 - a0) * t,\n            }\n        return interp\n\n    coeffs = _get_params_for_group(group)\n\n    # Small epsilon to regularize p=0 behavior; chosen to be tiny relative to [0,1]\n    eps = 1e-6\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out_row: dict[str, float] = {}\n        for i in range(1, 6):\n            p = float(row.get(f'proportion_domain_{i}', 0.0))\n            L = float(coeffs[i]['L'])\n            C = float(coeffs[i]['C'])\n            a = float(coeffs[i]['a'])\n            # Ensure non-negative proportion and stable power\n            if p < 0.0:\n                p = 0.0\n            y = L + C * (p + eps) ** (-a)\n            out_row[f'loss_domain_{i}'] = float(y)\n        outputs.append(out_row)\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.862092, "solution": "# Auto-generated scaling law implementation\n# Functional form (shared across groups):\n#   For each k in 1..5:\n#     loss_domain_k = intercept_k(group) + sum_j weights_kj(group) * log(proportion_domain_j + EPS)\n# Where EPS = 1e-09 for numerical stability when proportions are very small.\n\nfrom math import log\nfrom typing import List, Dict\n\nEPS = 1e-09\n\nCOEFFS = {\n  \"160M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.4300949275826182,\n      \"weights\": [\n        -0.031068969226910582,\n        -0.001251970877444727,\n        0.0030660145955268667,\n        -0.0031923386958230627,\n        0.005666961729262434\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.337559043041566,\n      \"weights\": [\n        0.0016673078859499708,\n        -0.0074170755968738225,\n        3.951373748949835e-05,\n        0.0021209041901585806,\n        -0.003650890207385123\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8401818458556174,\n      \"weights\": [\n        0.002460792554312405,\n        0.0007625714631167231,\n        -0.023536811266176717,\n        0.002221882872273453,\n        -0.0031186174972866886\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.416827910017676,\n      \"weights\": [\n        -8.308604862155428e-06,\n        0.0012190503004574596,\n        0.00032399676863148686,\n        -0.027078844363725037,\n        0.004010845050737586\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.224543906049842,\n      \"weights\": [\n        0.003905427350717464,\n        0.0034498008479645842,\n        -0.0011438176738371758,\n        0.0038638961855052053,\n        -0.019328119858345046\n      ]\n    }\n  },\n  \"305M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.280243138043643,\n      \"weights\": [\n        -0.030323002210805715,\n        -0.001211700929338305,\n        0.0028947830670476163,\n        -0.003324913376723421,\n        0.005113800264772562\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.1750876801913237,\n      \"weights\": [\n        0.00150916925021668,\n        -0.007107484565596503,\n        2.2128083411678632e-05,\n        0.0017728620875118904,\n        -0.003972454524642374\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.6913398939739,\n      \"weights\": [\n        0.0022708344859972973,\n        0.0008607850265603164,\n        -0.024331792555795703,\n        0.001910605212900081,\n        -0.0029724526398083326\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.3069649592755754,\n      \"weights\": [\n        -0.0002767577345750335,\n        0.0010150273630255805,\n        0.0003398702171026176,\n        -0.02596334582661601,\n        0.003801676322267153\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.0528011051041433,\n      \"weights\": [\n        0.0037594659546520076,\n        0.0034195441188783746,\n        -0.0011368975113422175,\n        0.0034318689601916725,\n        -0.01984609557908483\n      ]\n    }\n  },\n  \"410M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.222420776701192,\n      \"weights\": [\n        -0.0299923842986247,\n        -0.0007219779189471512,\n        0.002445903709102556,\n        -0.0027711721407406966,\n        0.004740901934744721\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.104708939329123,\n      \"weights\": [\n        0.0017336451412636902,\n        -0.006704834426433428,\n        -0.00036440290744305196,\n        0.0021542694049179936,\n        -0.004373253420089363\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.627413222019661,\n      \"weights\": [\n        0.0023403919816331487,\n        0.0010679528945524257,\n        -0.02491463877855555,\n        0.002264951898769409,\n        -0.003300544791425321\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.268317554857684,\n      \"weights\": [\n        -0.0005477525270926719,\n        0.001495543684942191,\n        0.00019725923029861728,\n        -0.025223453889896486,\n        0.0034567257553997934\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 2.9787362389662326,\n      \"weights\": [\n        0.0038069030391143168,\n        0.003769753651834665,\n        -0.0015455123055104936,\n        0.003789696705093632,\n        -0.020506176044717026\n      ]\n    }\n  },\n  \"70M\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.738245966324205,\n      \"weights\": [\n        -0.033247355909198334,\n        -0.0017297973305077973,\n        0.0035157990300376508,\n        -0.003918220474936535,\n        0.006317934127290131\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.6680087213236505,\n      \"weights\": [\n        0.0015828220455031585,\n        -0.008139958599724776,\n        0.00014447856565839406,\n        0.0016881696288695383,\n        -0.0033595783088106986\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 3.1304945178100705,\n      \"weights\": [\n        0.0026532896643926083,\n        0.0007600084197338211,\n        -0.024637174874146756,\n        0.001757289151853728,\n        -0.0030248438725573442\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.6498551367668615,\n      \"weights\": [\n        -0.0002889005529800089,\n        0.001194603021950033,\n        0.00037667659236133267,\n        -0.030432861224971082,\n        0.004231626123725879\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.5718538704762426,\n      \"weights\": [\n        0.004053652875855752,\n        0.0033280287851346077,\n        -0.0011542215280026637,\n        0.0035637709733213644,\n        -0.01878431722025504\n      ]\n    }\n  },\n  \"DEFAULT\": {\n    \"loss_domain_1\": {\n      \"intercept\": 2.417751202162915,\n      \"weights\": [\n        -0.03115792791138483,\n        -0.001228861764059495,\n        0.0029806251004286725,\n        -0.0033016611720559285,\n        0.005459899514017462\n      ]\n    },\n    \"loss_domain_2\": {\n      \"intercept\": 3.3213410959714156,\n      \"weights\": [\n        0.0016232360807333748,\n        -0.007342338297157132,\n        -3.957063022087023e-05,\n        0.0019340513278645007,\n        -0.00383904411523189\n      ]\n    },\n    \"loss_domain_3\": {\n      \"intercept\": 2.8223573699148123,\n      \"weights\": [\n        0.002431327171583865,\n        0.0008628294509908216,\n        -0.024355104368668678,\n        0.0020386822839491676,\n        -0.003104114700269422\n      ]\n    },\n    \"loss_domain_4\": {\n      \"intercept\": 1.4104913902294491,\n      \"weights\": [\n        -0.00028042985487746743,\n        0.001231056092593816,\n        0.0003094507020985136,\n        -0.027174626326302152,\n        0.003875218313032603\n      ]\n    },\n    \"loss_domain_5\": {\n      \"intercept\": 3.2069837801491152,\n      \"weights\": [\n        0.003881362305084885,\n        0.0034917818509530582,\n        -0.0012451122546731377,\n        0.0036623082060279685,\n        -0.019616177175600488\n      ]\n    }\n  }\n}\n\nPROP_ORDER = [\"proportion_domain_1\", \"proportion_domain_2\", \"proportion_domain_3\", \"proportion_domain_4\", \"proportion_domain_5\"]\nLOSS_ORDER = [\"loss_domain_1\", \"loss_domain_2\", \"loss_domain_3\", \"loss_domain_4\", \"loss_domain_5\"]\n\ndef _select_group(group: str) -> Dict[str, Dict[str, float]]:\n    # Use exact match if available; otherwise fallback to DEFAULT\n    if group in COEFFS:\n        return COEFFS[group]\n    # Try case-insensitive match\n    for g in COEFFS:\n        if g.lower() == (group or \"\").lower():\n            return COEFFS[g]\n    return COEFFS[\"DEFAULT\"]\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups, while\n               coefficients differ by group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) for keys in LOSS_ORDER.\n    \"\"\"\n    coeffs = _select_group(group)\n    outputs = []\n    for row in input_data:\n        # Build feature vector in canonical order\n        logs = [log(max(float(row.get(name, 0.0)), 0.0) + EPS) for name in PROP_ORDER]\n        pred = {}\n        for lkey in LOSS_ORDER:\n            c = coeffs[lkey]\n            y = c[\"intercept\"] + sum(w * x for w, x in zip(c[\"weights\"], logs))\n            pred[lkey] = float(y)\n        outputs.append(pred)\n    return outputs", "agent_name": "mini-swe-agent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.85774, "solution": "from __future__ import annotations\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups, but the\n               coefficients are learned per-group from /app/data (cached).\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    import math\n\n    # Initialize caches as function attributes (keeps module surface minimal: single function)\n    if not hasattr(law, \"_coeff_cache\"):\n        law._coeff_cache = {}  # type: ignore[attr-defined]\n    if not hasattr(law, \"_domains_by_group\"):\n        law._domains_by_group = {}  # type: ignore[attr-defined]\n    if not hasattr(law, \"_fit_attempted\"):\n        law._fit_attempted = set()  # type: ignore[attr-defined]\n\n    def _safe_log(p: float, eps: float = 1e-12) -> float:\n        # Avoid log(0) while keeping behavior stable for tiny proportions.\n        return math.log(p if p > eps else eps)\n\n    def _discover_domain_indices_from_columns(columns: list[str]) -> list[int]:\n        idxs = []\n        prefix = \"proportion_domain_\"\n        for c in columns:\n            if c.startswith(prefix):\n                try:\n                    i = int(c[len(prefix):])\n                    idxs.append(i)\n                except ValueError:\n                    continue\n        return sorted(set(idxs))\n\n    def _fit_group_coeffs(_group: str) -> None:\n        # Skip refit if already attempted (prevents repeated I/O on missing env)\n        if _group in law._fit_attempted:  # type: ignore[attr-defined]\n            return\n        law._fit_attempted.add(_group)  # type: ignore[attr-defined]\n\n        # Defaults if dataset is unavailable: reasonable monotone log response.\n        default_coeffs = {}\n        # If we can infer domain ids from incoming data later, we'll update per call.\n        try:\n            # Lazy import to avoid hard dependency if environment lacks datasets\n            from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore\n            import numpy as np  # type: ignore\n\n            ds_any = load_from_disk(\"/app/data\")\n            if isinstance(ds_any, DatasetDict):\n                # Prefer 'train' split when available; otherwise choose the first split.\n                if \"train\" in ds_any:\n                    ds = ds_any[\"train\"]\n                else:\n                    first_split = list(ds_any.keys())[0]\n                    ds = ds_any[first_split]\n            else:\n                ds = ds_any  # Already a Dataset\n\n            colnames = list(getattr(ds, \"column_names\", []))\n            domain_idxs = _discover_domain_indices_from_columns(colnames)\n\n            # Filter by group if the column exists, else fit globally.\n            use_ds = ds\n            if \"group\" in colnames and _group not in (\"\", None):\n                try:\n                    use_ds = ds.filter(lambda x: x.get(\"group\") == _group)\n                    # If filter produced empty set, fall back to global\n                    if len(use_ds) == 0:\n                        use_ds = ds\n                except Exception:\n                    use_ds = ds\n\n            coeffs = {}\n            # Ordinary Least Squares per domain: y = a + b * ln(p)\n            for i in domain_idxs:\n                p_col = f\"proportion_domain_{i}\"\n                y_col = f\"loss_domain_{i}\"\n                if p_col not in colnames or y_col not in colnames:\n                    continue\n                try:\n                    p_vals = use_ds[p_col]\n                    y_vals = use_ds[y_col]\n                except Exception:\n                    continue\n\n                # Convert to numpy arrays and robustly filter invalid/edge entries\n                try:\n                    x = np.array([_safe_log(float(p)) for p in p_vals], dtype=float)\n                    y = np.array([float(v) for v in y_vals], dtype=float)\n                    mask = np.isfinite(x) & np.isfinite(y)\n                    x = x[mask]\n                    y = y[mask]\n                except Exception:\n                    x = []\n                    y = []\n\n                # If insufficient variance or data, fall back to defaults\n                if len(x) < 2 or float(np.var(x)) <= 1e-18:\n                    # Reasonable fallback: a ~ mean(y), b ~ 0 if no variance; else mild slope\n                    if len(y) >= 1 and all(np.isfinite(y)):\n                        a = float(np.mean(y))\n                    else:\n                        a = 2.5\n                    b = -0.2  # gentle improvement with more data\n                else:\n                    # Closed-form OLS for simple linear regression\n                    x_mean = float(np.mean(x))\n                    y_mean = float(np.mean(y))\n                    cov = float(np.mean((x - x_mean) * (y - y_mean)))\n                    var = float(np.mean((x - x_mean) ** 2))\n                    if var <= 1e-18:\n                        b = 0.0\n                        a = y_mean\n                    else:\n                        b = cov / var\n                        a = y_mean - b * x_mean\n\n                coeffs[i] = (a, b)\n\n            law._coeff_cache[_group] = coeffs  # type: ignore[attr-defined]\n            law._domains_by_group[_group] = sorted(coeffs.keys())  # type: ignore[attr-defined]\n\n        except Exception:\n            # Dataset not available or fitting failed; set a permissive default.\n            # Assume 5 domains if unseen; coefficients encourage modest loss decrease with higher p.\n            for i in range(1, 6):\n                default_coeffs[i] = (2.5, -0.2)\n            law._coeff_cache[_group] = default_coeffs  # type: ignore[attr-defined]\n            law._domains_by_group[_group] = sorted(default_coeffs.keys())  # type: ignore[attr-defined]\n\n    # Ensure coefficients for this group are fitted/cached\n    _fit_group_coeffs(group)\n\n    # Inference: for each input point, predict loss_domain_i for available proportion_domain_i\n    coeffs_for_group = law._coeff_cache.get(group, {})  # type: ignore[attr-defined]\n    results: list[dict[str, float]] = []\n    for row in input_data:\n        out: dict[str, float] = {}\n        # Use domains inferred from the row's keys; if empty, fall back to known domains for the group\n        keys = [k for k in row.keys() if k.startswith(\"proportion_domain_\")]\n        if not keys:\n            # Predict for all known domains for this group with p assumed ~0 (minimal exposure)\n            for i in law._domains_by_group.get(group, []):  # type: ignore[attr-defined]\n                a, b = coeffs_for_group.get(i, (2.5, -0.2))\n                p = 0.0\n                out[f\"loss_domain_{i}\"] = a + b * _safe_log(p)\n        else:\n            for k in keys:\n                try:\n                    i = int(k.split(\"proportion_domain_\")[1])\n                except Exception:\n                    continue\n                a, b = coeffs_for_group.get(i, (2.5, -0.2))\n                p_val = float(row.get(k, 0.0))\n                out[f\"loss_domain_{i}\"] = a + b * _safe_log(p_val)\n        results.append(out)\n\n    return results", "agent_name": "aider", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.852129, "solution": "from typing import List, Dict\n\n_PARAMS = {\n  \"160M\": {\n    \"1\": {\n      \"a\": 0.4777718023660796,\n      \"b\": 1.9651028132012958,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.853979648026824,\n      \"b\": 0.46913461080072405,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.4416433577788708,\n      \"b\": 1.3992301424068143,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.31200145102844623,\n      \"b\": 1.7281252544418098,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.6988970818463138,\n      \"b\": 1.458160686737703,\n      \"alpha\": 0.01\n    }\n  },\n  \"305M\": {\n    \"1\": {\n      \"a\": 0.380355700173621,\n      \"b\": 1.9155046248750955,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.720727222497648,\n      \"b\": 0.4443470722501475,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.2468028929418933,\n      \"b\": 1.4484164085003954,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.34563740615899546,\n      \"b\": 1.6548398159870399,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.5118433832803935,\n      \"b\": 1.4779286413154575,\n      \"alpha\": 0.01\n    }\n  },\n  \"410M\": {\n    \"1\": {\n      \"a\": 0.3358801860263219,\n      \"b\": 1.8988049175912776,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 2.6618934312385854,\n      \"b\": 0.43132199370582547,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.153496029332803,\n      \"b\": 1.475677952459072,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.3403973481604199,\n      \"b\": 1.6101852758013921,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 1.3848194753051277,\n      \"b\": 1.5291803128611205,\n      \"alpha\": 0.01\n    }\n  },\n  \"70M\": {\n    \"1\": {\n      \"a\": 0.6576096958255063,\n      \"b\": 2.098519217677623,\n      \"alpha\": 0.01\n    },\n    \"2\": {\n      \"a\": 3.14714983348316,\n      \"b\": 0.5095246197489386,\n      \"alpha\": 0.01\n    },\n    \"3\": {\n      \"a\": 1.6653425516285103,\n      \"b\": 1.4686043699822906,\n      \"alpha\": 0.01\n    },\n    \"4\": {\n      \"a\": -0.2852957436802397,\n      \"b\": 1.9382200618934766,\n      \"alpha\": 0.01\n    },\n    \"5\": {\n      \"a\": 2.089249143779692,\n      \"b\": 1.4172201529137438,\n      \"alpha\": 0.01\n    }\n  }\n}\n\ndef _get_params(group: str):\n    return _PARAMS.get(group, _PARAMS.get(str(group), None)) or next(iter(_PARAMS.values()))\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n               The functional form of the law is the same for all groups,\n               but parameters can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    params = _get_params(group)\n    outputs = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            p = float(row.get('proportion_domain_%d' % i, 0.0))\n            if p <= 0:\n                p = 1e-12\n            dom = params[str(i)]\n            a = dom['a']; b = dom['b']; alpha = dom['alpha']\n            out['loss_domain_%d' % i] = float(a + b * (p ** (-alpha)))\n        outputs.append(out)\n    return outputs", "agent_name": "terminus-2", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.845752, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts domain losses based on mixture proportions using a linear scaling law.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_i'.\n        group: Experimental group identifier ('70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with predicted 'loss_domain_i' values.\n    \"\"\"\n    # Coefficients from linear regression: loss = a + b * proportion\n    coefs = {\n        '70M': {\n            1: (3.1806393062811864, -1.2285149798567838),\n            2: (3.801253923348644, -0.8613783672356026),\n            3: (3.5566666364458976, -5.9220601237842345),\n            4: (2.092666552495471, -2.465721152611366),\n            5: (3.7712578339232965, -0.4114998066410033),\n        },\n        '160M': {\n            1: (2.835641724877016, -1.125053454723479),\n            2: (3.4544639388150142, -0.7759431938007308),\n            3: (3.243292438159674, -5.633806849579989),\n            4: (1.8086663359915747, -2.2029709658709042),\n            5: (3.430618715766743, -0.4253675130276537),\n        },\n        '305M': {\n            1: (2.6765745376811547, -1.0852461829640883),\n            2: (3.290256899590436, -0.7424407706785378),\n            3: (3.1100613611088073, -5.787743320347023),\n            4: (1.684474243911805, -2.103498009394809),\n            5: (3.2619262695047944, -0.4221154025882421),\n        },\n        '410M': {\n            1: (2.609996558045746, -1.0646721511212967),\n            2: (3.2130253217275206, -0.7041613598604771),\n            3: (3.0516738117194397, -5.89283215213306),\n            4: (1.6354678769114128, -2.0519200859166604),\n            5: (3.194356949340593, -0.4345763744862935),\n        },\n    }\n    if group not in coefs:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = coefs[group]\n    predictions: List[Dict[str, float]] = []\n    for point in input_data:\n        out: Dict[str, float] = {}\n        for i in range(1, 6):\n            p = point.get(f'proportion_domain_{i}', 0.0)\n            a, b = params[i]\n            out[f'loss_domain_{i}'] = a + b * p\n        predictions.append(out)\n    return predictions", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2-reasoning", "reward_r2": 0.8457519810005265, "solution": "from typing import List, Dict\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Linear coefficients per group per domain: intercept and slope\n    # These were fitted on the provided dataset.\n    coefficients = {\n        '70M': {\n            'intercept_domain_1': 3.180639,\n            'slope_domain_1': -1.228515,\n            'intercept_domain_2': 3.801254,\n            'slope_domain_2': -0.861378,\n            'intercept_domain_3': 3.556667,\n            'slope_domain_3': -5.922060,\n            'intercept_domain_4': 2.092667,\n            'slope_domain_4': -2.465721,\n            'intercept_domain_5': 3.771258,\n            'slope_domain_5': -0.411500,\n        },\n        '160M': {\n            'intercept_domain_1': 2.835642,\n            'slope_domain_1': -1.125053,\n            'intercept_domain_2': 3.454464,\n            'slope_domain_2': -0.775943,\n            'intercept_domain_3': 3.243292,\n            'slope_domain_3': -5.633807,\n            'intercept_domain_4': 1.808666,\n            'slope_domain_4': -2.202971,\n            'intercept_domain_5': 3.430619,\n            'slope_domain_5': -0.425368,\n        },\n        '305M': {\n            'intercept_domain_1': 2.676575,\n            'slope_domain_1': -1.085246,\n            'intercept_domain_2': 3.290257,\n            'slope_domain_2': -0.742441,\n            'intercept_domain_3': 3.110061,\n            'slope_domain_3': -5.787743,\n            'intercept_domain_4': 1.684474,\n            'slope_domain_4': -2.103498,\n            'intercept_domain_5': 3.261926,\n            'slope_domain_5': -0.422115,\n        },\n        '410M': {\n            'intercept_domain_1': 2.609997,\n            'slope_domain_1': -1.064672,\n            'intercept_domain_2': 3.213025,\n            'slope_domain_2': -0.704161,\n            'intercept_domain_3': 3.051674,\n            'slope_domain_3': -5.892832,\n            'intercept_domain_4': 1.635468,\n            'slope_domain_4': -2.051920,\n            'intercept_domain_5': 3.194357,\n            'slope_domain_5': -0.434576,\n        }\n    }\n\n    # Check if the group is known\n    if group not in coefficients:\n        raise ValueError(f\"Unknown group: {group}\")\n\n    group_coeffs = coefficients[group]\n\n    result = []\n    for point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            prop_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            # If the proportion key is missing, we cannot predict. Assume it's present.\n            prop = point[prop_key]\n            intercept = group_coeffs[f'intercept_domain_{i}']\n            slope = group_coeffs[f'slope_domain_{i}']\n            pred[loss_key] = intercept + slope * prop\n        result.append(pred)\n\n    return result", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.842023, "solution": "from __future__ import annotations\n\nfrom math import log\nfrom typing import Dict, List\n\n# Discovered scaling law:\n#   For each output loss_domain_i, predict as a linear function of the\n#   logarithms of all five domain proportions (with a tiny epsilon for stability):\n#       \\hat{L}_{g,i} = bias_{g,i} + sum_j w_{g,i,j} * log(p_j + eps)\n#   The functional form is fixed; parameters (biases and weights) depend on group g.\n#   Coefficients were fit by ordinary least squares on the provided dataset.\n\nEPS = 1e-12\n\n# Per-group, per-output coefficients learned on the provided dataset.\n# Keys: group -> domain_{i} -> {bias, log_p1..log_p5}\nCOEFFS: Dict[str, Dict[str, Dict[str, float]]] = {\n    \"160M\": {\n        \"domain_1\": {\n            \"bias\": 2.4422650614387416,\n            \"log_p1\": -0.02277638164592572,\n            \"log_p2\": -0.0010526071930561662,\n            \"log_p3\": 0.0023643145622796475,\n            \"log_p4\": -0.0025583225480046595,\n            \"log_p5\": 0.004378832847069199,\n        },\n        \"domain_2\": {\n            \"bias\": 3.3408696428701075,\n            \"log_p1\": 0.0012198254051165147,\n            \"log_p2\": -0.005388376734279845,\n            \"log_p3\": 3.717373734748179e-05,\n            \"log_p4\": 0.001569209092862663,\n            \"log_p5\": -0.002615879540461004,\n        },\n        \"domain_3\": {\n            \"bias\": 2.8574312884633413,\n            \"log_p1\": 0.0018047356370065415,\n            \"log_p2\": 0.0005929693186934749,\n            \"log_p3\": -0.016974557643524405,\n            \"log_p4\": 0.0016523205850350147,\n            \"log_p5\": -0.0022616680413822805,\n        },\n        \"domain_4\": {\n            \"bias\": 1.4313732131696744,\n            \"log_p1\": -5.1765954939435764e-05,\n            \"log_p2\": 0.00089014156596828,\n            \"log_p3\": 0.00025768087959735246,\n            \"log_p4\": -0.019712789545137503,\n            \"log_p5\": 0.0031631020272182907,\n        },\n        \"domain_5\": {\n            \"bias\": 3.2257864919233756,\n            \"log_p1\": 0.0029557696510542136,\n            \"log_p2\": 0.0026611908853740054,\n            \"log_p3\": -0.0008885188514808986,\n            \"log_p4\": 0.002963471212841412,\n            \"log_p5\": -0.014216518710413713,\n        },\n    },\n    \"305M\": {\n        \"domain_1\": {\n            \"bias\": 2.2924869661791267,\n            \"log_p1\": -0.022229067565166443,\n            \"log_p2\": -0.001013806799899392,\n            \"log_p3\": 0.00223467186554643,\n            \"log_p4\": -0.002645873902860267,\n            \"log_p5\": 0.003982864991197694,\n        },\n        \"domain_2\": {\n            \"bias\": 3.178656522099369,\n            \"log_p1\": 0.001107302828752052,\n            \"log_p2\": -0.005157973767038715,\n            \"log_p3\": 2.3050110124257784e-05,\n            \"log_p4\": 0.001316251689675477,\n            \"log_p5\": -0.0028425410208344046,\n        },\n        \"domain_3\": {\n            \"bias\": 2.709373598108862,\n            \"log_p1\": 0.0016635063654583334,\n            \"log_p2\": 0.0006628818961311013,\n            \"log_p3\": -0.017547370822364454,\n            \"log_p4\": 0.0014206959628518868,\n            \"log_p5\": -0.002139388852229081,\n        },\n        \"domain_4\": {\n            \"bias\": 1.3211315997161879,\n            \"log_p1\": -0.00024768872185789104,\n            \"log_p2\": 0.0007428895039710641,\n            \"log_p3\": 0.0002693184497085781,\n            \"log_p4\": -0.01890218970791502,\n            \"log_p5\": 0.0030105938208213993,\n        },\n        \"domain_5\": {\n            \"bias\": 3.0545845875446473,\n            \"log_p1\": 0.002854476470176697,\n            \"log_p2\": 0.0026446139943498434,\n            \"log_p3\": -0.0008847006391603586,\n            \"log_p4\": 0.002650748867952872,\n            \"log_p5\": -0.014590961474216001,\n        },\n    },\n    \"410M\": {\n        \"domain_1\": {\n            \"bias\": 2.2343717924971283,\n            \"log_p1\": -0.021983874135919367,\n            \"log_p2\": -0.0006526879839640017,\n            \"log_p3\": 0.0019063557364627178,\n            \"log_p4\": -0.0022375323165275815,\n            \"log_p5\": 0.003693691571213775,\n        },\n        \"domain_2\": {\n            \"bias\": 3.1081386488108924,\n            \"log_p1\": 0.0012757792324108383,\n            \"log_p2\": -0.004861060292668727,\n            \"log_p3\": -0.00025971561825861964,\n            \"log_p4\": 0.0015980855298738693,\n            \"log_p5\": -0.003151617877548417,\n        },\n        \"domain_3\": {\n            \"bias\": 2.6457079548318867,\n            \"log_p1\": 0.0017179901900188285,\n            \"log_p2\": 0.0008184656194115004,\n            \"log_p3\": -0.017971019588506255,\n            \"log_p4\": 0.0016870680564514456,\n            \"log_p5\": -0.0023896692822175354,\n        },\n        \"domain_4\": {\n            \"bias\": 1.2820975851894605,\n            \"log_p1\": -0.00043994437500341244,\n            \"log_p2\": 0.0010961729632694766,\n            \"log_p3\": 0.00016179126609633246,\n            \"log_p4\": -0.01836235643857456,\n            \"log_p5\": 0.0027422260133483827,\n        },\n        \"domain_5\": {\n            \"bias\": 2.9806037129271403,\n            \"log_p1\": 0.0028948032957862636,\n            \"log_p2\": 0.002904176943938192,\n            \"log_p3\": -0.0011826414336436638,\n            \"log_p4\": 0.00291564957025589,\n            \"log_p5\": -0.015090031561988756,\n        },\n    },\n    \"70M\": {\n        \"domain_1\": {\n            \"bias\": 2.7516102214862785,\n            \"log_p1\": -0.024366773302687862,\n            \"log_p2\": -0.00140949302483008,\n            \"log_p3\": 0.0026992835128934697,\n            \"log_p4\": -0.0031027937470278375,\n            \"log_p5\": 0.0048722418014892025,\n        },\n        \"domain_2\": {\n            \"bias\": 3.6719317859686043,\n            \"log_p1\": 0.0011625721914211736,\n            \"log_p2\": -0.005916469283232795,\n            \"log_p3\": 0.0001159058459700631,\n            \"log_p4\": 0.001253484647821796,\n            \"log_p5\": -0.0023991782312973043,\n        },\n        \"domain_3\": {\n            \"bias\": 3.148901142003107,\n            \"log_p1\": 0.0019476114702619417,\n            \"log_p2\": 0.0005957023862280053,\n            \"log_p3\": -0.01776813563794145,\n            \"log_p4\": 0.0013179243483677264,\n            \"log_p5\": -0.002183184626543552,\n        },\n        \"domain_4\": {\n            \"bias\": 1.6665569458710474,\n            \"log_p1\": -0.00025847451148676925,\n            \"log_p2\": 0.0008758521651152874,\n            \"log_p3\": 0.00029735558927713497,\n            \"log_p4\": -0.022152540972955964,\n            \"log_p5\": 0.0033608606565960417,\n        },\n        \"domain_5\": {\n            \"bias\": 3.5732020332438177,\n            \"log_p1\": 0.003066036404538771,\n            \"log_p2\": 0.002572530514690887,\n            \"log_p3\": -0.0008951939451058563,\n            \"log_p4\": 0.0027436364335349703,\n            \"log_p5\": -0.013811286335446366,\n        },\n    },\n}\n\n# Fallback pooled coefficients (across all groups) in case an unknown group is requested.\nFALLBACK: Dict[str, Dict[str, float]] = {\n    \"domain_1\": {\n        \"bias\": 2.4301835104003184,\n        \"log_p1\": -0.02283902416242487,\n        \"log_p2\": -0.001032148750437415,\n        \"log_p3\": 0.002301156419295567,\n        \"log_p4\": -0.0026361306286050916,\n        \"log_p5\": 0.004231907802742471,\n    },\n    \"domain_2\": {\n        \"bias\": 3.3248991499372433,\n        \"log_p1\": 0.0011913699144251472,\n        \"log_p2\": -0.005330970019305012,\n        \"log_p3\": -2.0896481204204573e-05,\n        \"log_p4\": 0.001434257740058452,\n        \"log_p5\": -0.0027523041675352787,\n    },\n    \"domain_3\": {\n        \"bias\": 2.8403534958517995,\n        \"log_p1\": 0.0017834609156864088,\n        \"log_p2\": 0.00066750480511602,\n        \"log_p3\": -0.01756527092308412,\n        \"log_p4\": 0.0015195022381765178,\n        \"log_p5\": -0.0022434777005931115,\n    },\n    \"domain_4\": {\n        \"bias\": 1.4252898359865924,\n        \"log_p1\": -0.0002494683908218775,\n        \"log_p2\": 0.0009012640495810252,\n        \"log_p3\": 0.0002465365461698426,\n        \"log_p4\": -0.019782469166145775,\n        \"log_p5\": 0.003069195629496027,\n    },\n    \"domain_5\": {\n        \"bias\": 3.208544206409745,\n        \"log_p1\": 0.0029427714553889924,\n        \"log_p2\": 0.0026956280845882408,\n        \"log_p3\": -0.0009627637173476962,\n        \"log_p4\": 0.0028183765211462894,\n        \"log_p5\": -0.014427199520516213,\n    },\n}\n\n\ndef _predict_one(row: Dict[str, float], coeffs: Dict[str, Dict[str, float]]) -> Dict[str, float]:\n    # Build log-proportion features with numerical stability for zeros\n    logs = [log(float(row.get(f\"proportion_domain_{j}\", 0.0)) + EPS) for j in range(1, 6)]\n    out: Dict[str, float] = {}\n    for i in range(1, 6):\n        c = coeffs.get(f\"domain_{i}\")\n        if c is None:\n            # If missing, fall back to pooled per-domain coefficients\n            c = FALLBACK[f\"domain_{i}\"]\n        val = c[\"bias\"]\n        for j in range(1, 6):\n            val += c[f\"log_p{j}\"] * logs[j - 1]\n        out[f\"loss_domain_{i}\"] = float(val)\n    return out\n\n\ndef law(input_data: List[Dict[str, float]], group: str) -> List[Dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s): loss_domain_1..5.\n    \"\"\"\n    # Select coefficients for the provided group (fallback if unknown)\n    coeffs = COEFFS.get(group)\n    if coeffs is None:\n        # Unknown group: use pooled (group-agnostic) coefficients for all domains\n        coeffs = FALLBACK\n    return [_predict_one(row, coeffs) for row in input_data]", "agent_name": "goose", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.840301, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Lazy-fit parameters on first call and cache them on the function object.\n    # We keep all helper logic enclosed to satisfy the \"single function\" constraint.\n    import math\n\n    # Small numerical constants\n    EPS_P = 1e-12   # floor for proportions (avoids log(0))\n    EPS_Y = 1e-12   # floor for losses in fitting\n    L2 = 1e-6       # small ridge regularization for stability\n\n    # Initialize cache containers if missing\n    if not hasattr(law, \"_fitted\"):\n        law._fitted = False\n    if not hasattr(law, \"_model\"):\n        law._model = {}\n    if not hasattr(law, \"_names\"):\n        law._names = {\"prop\": [], \"loss\": []}\n\n    def _int_suffix(s: str) -> int:\n        try:\n            return int(s.rsplit(\"_\", 1)[-1])\n        except Exception:\n            return 1 << 30  # push non-numeric to the end deterministically\n\n    def _ensure_fit():\n        if law._fitted:\n            return\n        try:\n            from datasets import load_from_disk\n            import numpy as np\n        except Exception:\n            # If required libs are unavailable, fall back to a neutral, constant model.\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Load dataset\n        rows = []\n        prop_cols = []\n        loss_cols = []\n        group_col = None\n        try:\n            ds_any = load_from_disk(\"/app/data\")\n        except Exception:\n            # Dataset missing; use fallback\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Iterate rows from either a DatasetDict or a single Dataset\n        def _iter_rows(ds):\n            try:\n                ds = ds.with_format(\"python\")\n            except Exception:\n                pass\n            for r in ds:\n                yield r\n\n        if hasattr(ds_any, \"items\"):\n            # DatasetDict\n            for _, ds in ds_any.items():\n                for r in _iter_rows(ds):\n                    rows.append(r)\n        else:\n            # Single split\n            for r in _iter_rows(ds_any):\n                rows.append(r)\n\n        if not rows:\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Detect columns\n        all_cols = set()\n        for r in rows:\n            all_cols.update(r.keys())\n\n        prop_cols = sorted(\n            [c for c in all_cols if c.startswith(\"proportion_domain_\")],\n            key=_int_suffix,\n        )\n        loss_cols = sorted(\n            [c for c in all_cols if c.startswith(\"loss_domain_\")],\n            key=_int_suffix,\n        )\n        for cand in (\"group\", \"Group\", \"group_name\", \"group_id\", \"subset\", \"task\", \"exp_group\"):\n            if cand in all_cols:\n                group_col = cand\n                break\n\n        if not prop_cols or not loss_cols:\n            # No usable columns; fallback to constant model\n            law._model = {\"groups\": {\"ALL\": {}}, \"is_fallback\": True}\n            law._names = {\"prop\": [], \"loss\": []}\n            law._fitted = True\n            return\n\n        # Group rows\n        by_group = {}\n        for r in rows:\n            g = r.get(group_col, \"ALL\") if group_col else \"ALL\"\n            by_group.setdefault(str(g), []).append(r)\n\n        # Utility: fit per group and per loss target using log-linear regression with ridge\n        def _fit_group(g_rows):\n            # Returns: dict loss_col -> (intercept, [beta_j for each prop_col])\n            params = {}\n            # Prepare X rows per record (log-proportions)\n            X_logp = []\n            mask_valid = []\n            for r in g_rows:\n                # Build log-proportion vector with safe normalization and floors\n                vals = [r.get(c, None) for c in prop_cols]\n                present = [(i, v) for i, v in enumerate(vals) if v is not None]\n                m = len(prop_cols)\n                if not present:\n                    p = [1.0 / m] * m\n                else:\n                    # Normalize present proportions to sum 1; add tiny floor to all dims and renormalize\n                    p = [0.0] * m\n                    s = 0.0\n                    for i, v in present:\n                        try:\n                            fv = float(v)\n                        except Exception:\n                            fv = 0.0\n                        if not math.isfinite(fv) or fv < 0:\n                            fv = 0.0\n                        p[i] = fv\n                        s += fv\n                    if s <= 0.0:\n                        p = [1.0 / m] * m\n                    else:\n                        p = [pi / s for pi in p]\n                        # add small floor and renormalize\n                        p = [pi + EPS_P for pi in p]\n                        tot = sum(p)\n                        p = [pi / tot for pi in p]\n                X_logp.append([math.log(max(pi, EPS_P)) for pi in p])\n                mask_valid.append(True)\n            # Fit a separate model for each loss dimension\n            import numpy as np\n            X = np.asarray(X_logp, dtype=float)\n            n = X.shape[0]\n            d = X.shape[1]\n            ones = np.ones((n, 1), dtype=float)\n            X_aug = np.concatenate([ones, X], axis=1)  # [1, log p_1, ..., log p_d]\n            XtX = X_aug.T @ X_aug\n            XtX += L2 * np.eye(d + 1)\n\n            for loss_col in loss_cols:\n                y_list = []\n                idx_list = []\n                for idx, r in enumerate(g_rows):\n                    v = r.get(loss_col, None)\n                    if v is None:\n                        continue\n                    try:\n                        fv = float(v)\n                    except Exception:\n                        continue\n                    if not math.isfinite(fv) or fv <= 0:\n                        continue\n                    y_list.append(math.log(max(fv, EPS_Y)))\n                    idx_list.append(idx)\n                if len(y_list) < d + 1:\n                    # Not enough data; fall back to neutral params\n                    params[loss_col] = (0.0, [0.0] * d)\n                    continue\n                y = np.asarray(y_list, dtype=float)\n                Xa = X_aug[idx_list, :]\n                A = Xa.T @ Xa + L2 * np.eye(d + 1)\n                b = Xa.T @ y\n                try:\n                    w = np.linalg.solve(A, b)\n                except np.linalg.LinAlgError:\n                    w = np.linalg.lstsq(Xa, y, rcond=None)[0]\n                intercept = float(w[0])\n                betas = [float(bi) for bi in w[1:].tolist()]\n                params[loss_col] = (intercept, betas)\n            return params\n\n        groups_params = {}\n        for g, g_rows in by_group.items():\n            groups_params[g] = _fit_group(g_rows)\n\n        # Also fit a global \"ALL\" group over every row for fallback/extrapolation\n        if \"ALL\" not in groups_params or len(groups_params) > 1:\n            all_rows = rows\n            groups_params[\"ALL\"] = _fit_group(all_rows)\n\n        law._model = {\"groups\": groups_params, \"is_fallback\": False}\n        law._names = {\"prop\": prop_cols, \"loss\": loss_cols}\n        law._fitted = True\n\n    # Ensure model is fitted\n    _ensure_fit()\n\n    # If input_data is empty, allow callers to trigger fitting/caching without predicting.\n    if not input_data:\n        return []\n\n    # Choose group parameters; if unknown, fall back to \"ALL\" then any available group\n    groups = law._model.get(\"groups\", {})\n    group_key = str(group)\n    if group_key not in groups:\n        if \"ALL\" in groups:\n            group_key = \"ALL\"\n        elif groups:\n            group_key = next(iter(groups.keys()))\n        else:\n            group_key = \"ALL\"\n    params_for_group = groups.get(group_key, {})\n    prop_cols = law._names.get(\"prop\", [])\n    loss_cols = law._names.get(\"loss\", [])\n\n    # Prediction helper: build log-proportions vector aligned to fitted prop_cols\n    def _row_logp(row):\n        m = len(prop_cols)\n        if m == 0:\n            return []\n        present_vals = []\n        for c in prop_cols:\n            v = row.get(c, None)\n            if v is None:\n                continue\n            try:\n                fv = float(v)\n            except Exception:\n                continue\n            if not math.isfinite(fv) or fv < 0:\n                continue\n            present_vals.append((c, fv))\n        if not present_vals:\n            # Default to uniform if nothing provided\n            p = [1.0 / m] * m\n        else:\n            # Start with zeros, set present, normalize to sum 1\n            p = [0.0] * m\n            s = sum(v for _, v in present_vals)\n            if s <= 0:\n                p = [1.0 / m] * m\n            else:\n                for idx, c in enumerate(prop_cols):\n                    val = row.get(c, None)\n                    if val is None:\n                        continue\n                    try:\n                        fv = float(val)\n                    except Exception:\n                        fv = 0.0\n                    if not math.isfinite(fv) or fv < 0:\n                        fv = 0.0\n                    p[idx] = fv\n                # normalize and apply tiny floor to avoid zero\n                s = sum(p)\n                if s > 0:\n                    p = [pi / s for pi in p]\n                else:\n                    p = [1.0 / m] * m\n                p = [pi + EPS_P for pi in p]\n                total = sum(p)\n                p = [pi / total for pi in p]\n        return [math.log(max(pi, EPS_P)) for pi in p]\n\n    # Produce predictions\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        logp = _row_logp(row)\n        # If no fitted names, return empty dict\n        if not loss_cols or not prop_cols:\n            outputs.append({})\n            continue\n        pred = {}\n        for lc in loss_cols:\n            intercept, betas = params_for_group.get(lc, (0.0, [0.0] * len(prop_cols)))\n            # If group didn't have params for this target, try ALL\n            if (intercept, betas) == (0.0, [0.0] * len(prop_cols)) and \"ALL\" in groups:\n                intercept, betas = groups[\"ALL\"].get(lc, (0.0, [0.0] * len(prop_cols)))\n            s = intercept\n            for bj, lj in zip(betas, logp):\n                s += bj * lj\n            yhat = math.exp(s)\n            pred[lc] = float(yhat)\n        outputs.append(pred)\n    return outputs", "agent_name": "aider", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.834132, "solution": "from __future__ import annotations\nimport math\nfrom typing import Dict, List\n\n\n# Coefficients fitted on the provided dataset using\n# least squares for the model:\n#   loss_domain_i = c_{g,i} + b_{g,i} * log(1 / (proportion_domain_i + eps))\n# where g is the experimental group.\n_COEFFS: Dict[str, Dict[int, tuple[float, float]]] = {\n    \"160M\": {\n        1: (2.437447, 0.022863),\n        2: (3.321687, 0.005456),\n        3: (2.834681, 0.016343),\n        4: (1.410384, 0.020125),\n        5: (3.155392, 0.016970),\n    },\n    \"305M\": {\n        1: (2.290583, 0.022284),\n        2: (3.163720, 0.005168),\n        3: (2.688818, 0.016917),\n        4: (1.303708, 0.019271),\n        5: (2.988093, 0.017195),\n    },\n    \"410M\": {\n        1: (2.229470, 0.022088),\n        2: (3.091907, 0.005016),\n        3: (2.622653, 0.017235),\n        4: (1.264438, 0.018752),\n        5: (2.912269, 0.017788),\n    },\n    \"70M\": {\n        1: (2.750293, 0.024420),\n        2: (3.655117, 0.005927),\n        3: (3.127444, 0.017154),\n        4: (1.646486, 0.022572),\n        5: (3.504848, 0.016495),\n    },\n}\n\n\ndef _get_group_coeffs(group: str) -> Dict[int, tuple[float, float]]:\n    \"\"\"Return per-domain (c, b) coefficients for the given group.\n\n    If the group is unknown, fall back to the average of known groups.\n    \"\"\"\n    if group in _COEFFS:\n        return _COEFFS[group]\n    # Fallback: average coefficients across all known groups\n    avg: Dict[int, List[float]] = {i: [0.0, 0.0] for i in range(1, 6)}\n    n = float(len(_COEFFS))\n    for g in _COEFFS.values():\n        for i in range(1, 6):\n            c, b = g[i]\n            avg[i][0] += c / n\n            avg[i][1] += b / n\n    return {i: (avg[i][0], avg[i][1]) for i in range(1, 6)}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    coeffs = _get_group_coeffs(group)\n    eps = 1e-9\n\n    outputs: List[Dict[str, float]] = []\n    for row in input_data:\n        pred: Dict[str, float] = {}\n        for i in range(1, 6):\n            q = float(row.get(f\"proportion_domain_{i}\", 0.0))\n            c, b = coeffs[i]\n            pred[f\"loss_domain_{i}\"] = c + b * math.log(1.0 / (q + eps))\n        outputs.append(pred)\n    return outputs", "agent_name": "codex", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2", "reward_r2": 0.8339644854356709, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Model parameters discovered from data analysis\n    # Model: loss_domain_i = slope_i * proportion_domain_i + alpha_i * (model_size)^(beta_i)\n    \n    # Map group names to model sizes (in millions of parameters)\n    group_sizes = {\n        '70M': 70,\n        '160M': 160,\n        '305M': 305,\n        '410M': 410\n    }\n    \n    # Get model size for this group\n    if group not in group_sizes:\n        raise ValueError(f\"Unknown group: {group}. Expected one of {list(group_sizes.keys())}\")\n    \n    size = group_sizes[group]\n    \n    # Domain-specific parameters (optimized from data)\n    # Format: [slope, alpha, beta] for each domain 1-5\n    domain_params = [\n        # Domain 1\n        [-1.1256, 4.9466, -0.1069],\n        # Domain 2\n        [-0.7693, 5.5977, -0.0929],\n        # Domain 3\n        [-5.8083, 5.1105, -0.0869],\n        # Domain 4\n        [-2.2043, 3.5415, -0.1287],\n        # Domain 5\n        [-0.4231, 5.6630, -0.0965]\n    ]\n    \n    # Initialize result list\n    results = []\n    \n    for data_point in input_data:\n        # Create prediction dictionary for this data point\n        prediction = {}\n        \n        # Predict loss for each domain\n        for i in range(1, 6):\n            # Get proportion for this domain\n            prop_key = f'proportion_domain_{i}'\n            if prop_key not in data_point:\n                # If proportion not provided, assume 0\n                proportion = 0.0\n            else:\n                proportion = data_point[prop_key]\n            \n            # Get parameters for this domain\n            slope, alpha, beta = domain_params[i-1]\n            \n            # Apply scaling law: loss = slope * proportion + alpha * size^beta\n            loss = slope * proportion + alpha * (size ** beta)\n            \n            # Store prediction\n            prediction[f'loss_domain_{i}'] = loss\n        \n        results.append(prediction)\n    \n    return results", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5", "reward_r2": 0.830099, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law models the validation loss for each domain as a linear function of:\n    1. The proportion of that domain in the training mixture\n    2. The logarithm of the model size (in millions of parameters)\n\n    Functional form: loss_domain_i = a_i * proportion_domain_i + b_i * log(M) + c_i\n    where M is the model size in millions.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys are 'proportion_domain_1'\n                    through 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n               Must be one of: '70M', '160M', '305M', '410M'\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variables 'loss_domain_1'\n        through 'loss_domain_5'.\n    \"\"\"\n\n    # Model size in millions for each group\n    group_sizes = {\n        '70M': 70,\n        '160M': 160,\n        '305M': 305,\n        '410M': 410\n    }\n\n    # Fitted parameters: {domain: {'a': coef_prop, 'b': coef_log_model_size, 'c': intercept}}\n    params = {\n        1: {'a': -1.125872, 'b': -0.304742, 'c': 4.430192},\n        2: {'a': -0.770981, 'b': -0.321885, 'c': 5.134486},\n        3: {'a': -5.809111, 'b': -0.283582, 'c': 4.733491},\n        4: {'a': -2.206028, 'b': -0.234356, 'c': 3.039213},\n        5: {'a': -0.423390, 'b': -0.332244, 'c': 5.163817},\n    }\n\n    # Get model size for the group\n    model_size = group_sizes[group]\n    log_model_size = math.log(model_size)\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Predict loss for each domain\n        for domain in range(1, 6):\n            prop_key = f'proportion_domain_{domain}'\n            loss_key = f'loss_domain_{domain}'\n\n            proportion = data_point[prop_key]\n\n            # Apply the scaling law formula\n            p = params[domain]\n            predicted_loss = p['a'] * proportion + p['b'] * log_model_size + p['c']\n\n            prediction[loss_key] = predicted_loss\n\n        results.append(prediction)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.825888, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: list of dicts containing keys 'proportion_domain_1'..'proportion_domain_5'.\n        group: group name string selecting the parameter set.\n\n    Returns:\n        list of dicts with keys 'loss_domain_1'..'loss_domain_5'.\n    \"\"\"\n    # Fitted parameters per group and domain for the power-law model:\n    # loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + EPS)^(-alpha_{g,i})\n    EPS = 1e-06\n    _PARAMS = {'160M': {'domain_1': {'a': 2.447836676230049, 'b': 0.01947604243193669, 'alpha': 0.25}, 'domain_2': {'a': 3.324865267668977, 'b': 0.0046611055397499865, 'alpha': 0.25}, 'domain_3': {'a': 2.8536617392943797, 'b': 0.013668198149884528, 'alpha': 0.25}, 'domain_4': {'a': 1.4245257489216498, 'b': 0.017073641554700338, 'alpha': 0.25}, 'domain_5': {'a': 3.1503851621943806, 'b': 0.014361051003233668, 'alpha': 0.25}}, '305M': {'domain_1': {'a': 2.300638607022169, 'b': 0.01899092355571077, 'alpha': 0.25}, 'domain_2': {'a': 3.1667447973834975, 'b': 0.004413698745158715, 'alpha': 0.25}, 'domain_3': {'a': 2.7084277465750337, 'b': 0.014151032173366376, 'alpha': 0.25}, 'domain_4': {'a': 1.3172396091096554, 'b': 0.016350509608112452, 'alpha': 0.25}, 'domain_5': {'a': 2.9829538687037322, 'b': 0.014576042006844646, 'alpha': 0.25}}, '410M': {'domain_1': {'a': 2.239353613802408, 'b': 0.018832858223664272, 'alpha': 0.25}, 'domain_2': {'a': 3.0948104946214414, 'b': 0.004286808779512701, 'alpha': 0.25}, 'domain_3': {'a': 2.6426284303607965, 'b': 0.014417587188864664, 'alpha': 0.25}, 'domain_4': {'a': 1.2776219014512213, 'b': 0.015907819042873157, 'alpha': 0.25}, 'domain_5': {'a': 2.9069132394663955, 'b': 0.01509328547538018, 'alpha': 0.25}}, '70M': {'domain_1': {'a': 2.761596918863867, 'b': 0.020779709248785288, 'alpha': 0.25}, 'domain_2': {'a': 3.6586060682487, 'b': 0.005059595461040137, 'alpha': 0.25}, 'domain_3': {'a': 3.1473798666330657, 'b': 0.014345011881448757, 'alpha': 0.25}, 'domain_4': {'a': 1.6623493783999272, 'b': 0.019149169215257496, 'alpha': 0.25}, 'domain_5': {'a': 3.4999972282096126, 'b': 0.013952976677258505, 'alpha': 0.25}}}\n    if group not in _PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(_PARAMS.keys())}\")\n    group_params = _PARAMS[group]\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            pkey = f'proportion_domain_{i}'\n            lkey = f'loss_domain_{i}'\n            if pkey not in row:\n                raise KeyError(f\"Missing required input key: {pkey}\")\n            p = float(row[pkey])\n            par = group_params[f'domain_{i}']\n            a = float(par['a']); b = float(par['b']); alpha = float(par['alpha'])\n            pred = a + b * (max(p, 0.0) + EPS) ** (-alpha)\n            out[lkey] = float(pred)\n        outputs.append(out)\n    return outputs", "agent_name": "mini-swe-agent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.809195, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Fitted parameters (A and B) for each group and domain\n    fitted_parameters = {\n        \"70M\": {\n            \"domain_1\": {\"A\": 2.7954155842226505, \"B\": 6.194945290313312e-07},\n            \"domain_2\": {\"A\": 3.666992583441158, \"B\": 1.5143629378910318e-07},\n            \"domain_3\": {\"A\": 3.1770876336775062, \"B\": 4.2355238205837483e-07},\n            \"domain_4\": {\"A\": 1.6957346873000834, \"B\": 5.706010522162532e-07},\n            \"domain_5\": {\"A\": 3.5171284546533235, \"B\": 4.146173044780857e-07},\n        },\n        \"160M\": {\n            \"domain_1\": {\"A\": 2.4794515045367107, \"B\": 5.809575446413638e-07},\n            \"domain_2\": {\"A\": 3.3325754438215687, \"B\": 1.3956202936091392e-07},\n            \"domain_3\": {\"A\": 2.8819574855777637, \"B\": 4.035983480572172e-07},\n            \"domain_4\": {\"A\": 1.4542888945325934, \"B\": 5.087695900137449e-07},\n            \"domain_5\": {\"A\": 3.168009255701514, \"B\": 4.2690655326847836e-07},\n        },\n        \"305M\": {\n            \"domain_1\": {\"A\": 2.331439324565376, \"B\": 5.665932970655655e-07},\n            \"domain_2\": {\"A\": 3.174052635323413, \"B\": 1.3213118365722513e-07},\n            \"domain_3\": {\"A\": 2.737698767011762, \"B\": 4.179249834595783e-07},\n            \"domain_4\": {\"A\": 1.3457382302771388, \"B\": 4.872370934063391e-07},\n            \"domain_5\": {\"A\": 3.000827223421394, \"B\": 4.3358908846628955e-07},\n        },\n        \"410M\": {\n            \"domain_1\": {\"A\": 2.269862140426057, \"B\": 5.620207432145631e-07},\n            \"domain_2\": {\"A\": 3.101893008310117, \"B\": 1.2838329492243058e-07},\n            \"domain_3\": {\"A\": 2.6724486393426763, \"B\": 4.2580340959834756e-07},\n            \"domain_4\": {\"A\": 1.3053577148261022, \"B\": 4.74009961564549e-07},\n            \"domain_5\": {\"A\": 2.925409380072483, \"B\": 4.492046744951497e-07},\n        },\n    }\n\n    epsilon = 1e-6\n    predictions = []\n\n    if group not in fitted_parameters:\n        raise ValueError(f\"Unknown group: {group}\")\n\n    for data_point in input_data:\n        predicted_losses = {}\n        for i in range(1, 6): # Domains 1 to 5\n            proportion_key = f'proportion_domain_{i}'\n            loss_key = f'loss_domain_{i}'\n            \n            if proportion_key not in data_point:\n                # If a proportion is missing, we cannot predict its loss.\n                # Or, based on problem, assume it's 0 if not present, or raise error.\n                # For now, let's assume all proportions are present.\n                # If a proportion for a domain is 0 and it's explicitly in input_data,\n                # the epsilon handles it.\n                continue \n\n            proportion = data_point[proportion_key]\n            \n            domain_params = fitted_parameters[group][f'domain_{i}']\n            A = domain_params['A']\n            B = domain_params['B']\n            \n            # Apply the scaling law\n            # Using np.maximum to prevent division by an extremely small or zero value, consistent with fitting.\n            predicted_loss = A + B / np.maximum(proportion + epsilon, 1e-10)\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n    \n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-4.1", "reward_r2": 0.7922160496910731, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Coefficients and intercepts for each group and domain\n    params = {\n        '70M': [\n            (2.9696, [-1.2024, 0.7226, -0.2714, 0.6117, 0.1395]),\n            (3.6191, [0.3115, -0.7697, -0.0298, 0.3476, 0.1404]),\n            (2.3841, [1.3538, 1.0329, -4.6770, 1.1539, 1.1364]),\n            (1.6263, [0.3999, 0.7476, 0.4032, -1.9906, 0.4400]),\n            (3.7121, [0.0403, 0.0147, 0.1869, 0.1151, -0.3571]),\n        ],\n        '160M': [\n            (2.6457, [-1.1195, 0.7054, -0.2893, 0.5850, 0.1183]),\n            (3.2983, [0.2957, -0.7264, -0.0158, 0.3383, 0.1082]),\n            (2.1324, [1.2790, 0.9910, -4.4730, 1.1336, 1.0695]),\n            (1.3894, [0.3669, 0.6810, 0.3411, -1.7833, 0.3944]),\n            (3.3748, [0.0278, 0.0268, 0.2030, 0.1190, -0.3766]),\n        ],\n        '305M': [\n            (2.4976, [-1.0844, 0.6722, -0.2556, 0.5587, 0.1091]),\n            (3.1454, [0.2803, -0.7053, 0.0042, 0.3240, 0.0969]),\n            (1.9688, [1.2989, 1.0459, -4.6125, 1.1685, 1.0992]),\n            (1.2834, [0.3504, 0.6285, 0.3385, -1.6974, 0.3799]),\n            (3.2108, [0.0211, 0.0243, 0.2216, 0.1120, -0.3790]),\n        ],\n        '410M': [\n            (2.4321, [-1.0761, 0.7076, -0.3202, 0.5842, 0.1045]),\n            (3.0703, [0.2823, -0.6691, -0.0522, 0.3466, 0.0925]),\n            (1.8897, [1.3286, 1.0731, -4.6999, 1.1806, 1.1176]),\n            (1.2548, [0.3111, 0.6544, 0.3396, -1.6598, 0.3546]),\n            (3.1335, [0.0202, 0.0634, 0.1552, 0.1434, -0.3823]),\n        ],\n    }\n    group_params = params[group]\n    output = []\n    for row in input_data:\n        props = [row[f'proportion_domain_{i+1}'] for i in range(5)]\n        pred = {}\n        for i, (intercept, coefs) in enumerate(group_params):\n            pred[f'loss_domain_{i+1}'] = intercept + sum(c*p for c, p in zip(coefs, props))\n        output.append(pred)\n    return output", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.792215, "solution": "from math import log10\nfrom typing import Dict, List\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The law is a linear model for each domain's validation loss as a function of\n    the five domain mixture proportions. The functional form is the same across\n    experimental groups, but coefficients differ by group.\n\n    For domain k in {1..5} and proportions p_j that sum to 1:\n        loss_domain_k = intercept[g][k] + sum_j coef[g][k,j] * proportion_domain_j\n\n    To generalize to unseen groups, the group-specific parameters are also\n    modeled as linear functions of log10(model_size) fitted from the observed\n    groups. If a group string encodes a size (e.g., \"550M\", \"1.3B\"), we use this\n    mapping to synthesize parameters for that group; otherwise we fall back to\n    the average of known-group parameters.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values. Expected keys: 'proportion_domain_1' .. 'proportion_domain_5'.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law is the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s) with keys\n        'loss_domain_1' .. 'loss_domain_5'.\n    \"\"\"\n    # Learned coefficients from the provided dataset (one linear model per loss per known group)\n    COEFS: Dict[str, Dict] = {\n        \"160M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.645699436335381,\n                \"loss_domain_2\": 3.2982774153184558,\n                \"loss_domain_3\": 2.132434044675029,\n                \"loss_domain_4\": 1.3893657641467703,\n                \"loss_domain_5\": 3.3747611697919497,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.119495690625232,\n                    \"proportion_domain_2\": 0.7054294745261317,\n                    \"proportion_domain_3\": -0.28927157238419526,\n                    \"proportion_domain_4\": 0.584990034524239,\n                    \"proportion_domain_5\": 0.11834775395905708,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.2957017578513571,\n                    \"proportion_domain_2\": -0.7264047971224965,\n                    \"proportion_domain_3\": -0.015757968287745085,\n                    \"proportion_domain_4\": 0.338268574484209,\n                    \"proportion_domain_5\": 0.10819243307467594,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.2789627462923447,\n                    \"proportion_domain_2\": 0.9909852370273848,\n                    \"proportion_domain_3\": -4.473015910337291,\n                    \"proportion_domain_4\": 1.1335508900914408,\n                    \"proportion_domain_5\": 1.0695170369261213,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.366883293836219,\n                    \"proportion_domain_2\": 0.6809673154560535,\n                    \"proportion_domain_3\": 0.3411068917199623,\n                    \"proportion_domain_4\": -1.7833344940309523,\n                    \"proportion_domain_5\": 0.3943769930187157,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.02781842295027701,\n                    \"proportion_domain_2\": 0.02677570609375618,\n                    \"proportion_domain_3\": 0.20298408706514612,\n                    \"proportion_domain_4\": 0.1189855046814393,\n                    \"proportion_domain_5\": -0.37656372079061845,\n                },\n            },\n        },\n        \"305M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.497559209494426,\n                \"loss_domain_2\": 3.14536997892604,\n                \"loss_domain_3\": 1.968805837824704,\n                \"loss_domain_4\": 1.2833765608574346,\n                \"loss_domain_5\": 3.210803311891907,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.0843715656873325,\n                    \"proportion_domain_2\": 0.6721539878426451,\n                    \"proportion_domain_3\": -0.25559189326785264,\n                    \"proportion_domain_4\": 0.5586611040857702,\n                    \"proportion_domain_5\": 0.1091483670267703,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.2802593171012234,\n                    \"proportion_domain_2\": -0.705345666382081,\n                    \"proportion_domain_3\": 0.004212615597317157,\n                    \"proportion_domain_4\": 0.32402103174261404,\n                    \"proportion_domain_5\": 0.0968527019409263,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.2989242757705795,\n                    \"proportion_domain_2\": 1.0459184205941086,\n                    \"proportion_domain_3\": -4.612543266518341,\n                    \"proportion_domain_4\": 1.1685296372435943,\n                    \"proportion_domain_5\": 1.0991709329100599,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.35041793228110724,\n                    \"proportion_domain_2\": 0.628523569343145,\n                    \"proportion_domain_3\": 0.33852456736125136,\n                    \"proportion_domain_4\": -1.6973603028352418,\n                    \"proportion_domain_5\": 0.37989423384973636,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.021137176480833044,\n                    \"proportion_domain_2\": 0.024298666101822186,\n                    \"proportion_domain_3\": 0.22155472081459832,\n                    \"proportion_domain_4\": 0.11202244079079128,\n                    \"proportion_domain_5\": -0.37901300418804473,\n                },\n            },\n        },\n        \"410M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.432148695798992,\n                \"loss_domain_2\": 3.070321096782969,\n                \"loss_domain_3\": 1.889657943760822,\n                \"loss_domain_4\": 1.254798446077109,\n                \"loss_domain_5\": 3.1335037706175104,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.0761151307100763,\n                    \"proportion_domain_2\": 0.7075633674705835,\n                    \"proportion_domain_3\": -0.32021933049462503,\n                    \"proportion_domain_4\": 0.584248540833674,\n                    \"proportion_domain_5\": 0.10452255290044439,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.28229210571466595,\n                    \"proportion_domain_2\": -0.6691020114749978,\n                    \"proportion_domain_3\": -0.05223402368178615,\n                    \"proportion_domain_4\": 0.3465500387546539,\n                    \"proportion_domain_5\": 0.09249389068746447,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.3285868291288925,\n                    \"proportion_domain_2\": 1.0730826822922905,\n                    \"proportion_domain_3\": -4.69986390494191,\n                    \"proportion_domain_4\": 1.1805807813503215,\n                    \"proportion_domain_5\": 1.1176136121704066,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.3110943108385622,\n                    \"proportion_domain_2\": 0.6544173298915442,\n                    \"proportion_domain_3\": 0.339609109190628,\n                    \"proportion_domain_4\": -1.6597557356277692,\n                    \"proportion_domain_5\": 0.3546349857070334,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.02023592248594768,\n                    \"proportion_domain_2\": 0.06340878588569265,\n                    \"proportion_domain_3\": 0.15519736526922107,\n                    \"proportion_domain_4\": 0.14343010613675986,\n                    \"proportion_domain_5\": -0.38227217977762123,\n                },\n            },\n        },\n        \"70M\": {\n            \"intercepts\": {\n                \"loss_domain_1\": 2.9696038495486428,\n                \"loss_domain_2\": 3.619059870697084,\n                \"loss_domain_3\": 2.3841489786591965,\n                \"loss_domain_4\": 1.6263083817663921,\n                \"loss_domain_5\": 3.712126062067348,\n            },\n            \"coefs\": {\n                \"loss_domain_1\": {\n                    \"proportion_domain_1\": -1.2024018889698118,\n                    \"proportion_domain_2\": 0.7225748684796697,\n                    \"proportion_domain_3\": -0.2714083488994125,\n                    \"proportion_domain_4\": 0.6116971390006383,\n                    \"proportion_domain_5\": 0.1395382303889172,\n                },\n                \"loss_domain_2\": {\n                    \"proportion_domain_1\": 0.31154794402259256,\n                    \"proportion_domain_2\": -0.7696911526036795,\n                    \"proportion_domain_3\": -0.029834711619449426,\n                    \"proportion_domain_4\": 0.3475522996421775,\n                    \"proportion_domain_5\": 0.14042562055835917,\n                },\n                \"loss_domain_3\": {\n                    \"proportion_domain_1\": 1.3538168492045202,\n                    \"proportion_domain_2\": 1.0328737163751711,\n                    \"proportion_domain_3\": -4.676968208089681,\n                    \"proportion_domain_4\": 1.1539207687720698,\n                    \"proportion_domain_5\": 1.1363568737379204,\n                },\n                \"loss_domain_4\": {\n                    \"proportion_domain_1\": 0.3998862788060258,\n                    \"proportion_domain_2\": 0.747594278597924,\n                    \"proportion_domain_3\": 0.40318411868033077,\n                    \"proportion_domain_4\": -1.9906253596015544,\n                    \"proportion_domain_5\": 0.439960683517272,\n                },\n                \"loss_domain_5\": {\n                    \"proportion_domain_1\": 0.040341011044849134,\n                    \"proportion_domain_2\": 0.014732615064055155,\n                    \"proportion_domain_3\": 0.18686886153522414,\n                    \"proportion_domain_4\": 0.11511773826952129,\n                    \"proportion_domain_5\": -0.3570602259136497,\n                },\n            },\n        },\n    }\n\n    # Parameter scaling w.r.t. model size: each parameter theta is fit as\n    # theta = beta0 + beta1 * log10(model_size)\n    PARAM_MAP = {\n        'intercepts': {\n            'loss_domain_1': {'beta0': 8.410693789482913, 'beta1': -0.6968428684065728},\n            'loss_domain_2': {'beta0': 9.159453070486071, 'beta1': -0.709122366641981},\n            'loss_domain_3': {'beta0': 7.408200748969857, 'beta1': -0.64133116180932},\n            'loss_domain_4': {'beta0': 5.432810327260841, 'beta1': -0.4880602455582854},\n            'loss_domain_5': {'beta0': 9.564414879713457, 'beta1': -0.7489965370842978},\n        },\n        'coefs': {\n            'loss_domain_1': {\n                'proportion_domain_1': {'beta0': -2.497615657116547, 'beta1': 0.16617474851983763},\n                'proportion_domain_2': {'beta0': 1.022994012405379, 'beta1': -0.03874502688716343},\n                'proportion_domain_3': {'beta0': -0.013900448056208633, 'beta1': -0.03260965165867218},\n                'proportion_domain_4': {'beta0': 0.9963146912448967, 'beta1': -0.04964843319473607},\n                'proportion_domain_5': {'beta0': 0.4922074015224826, 'beta1': -0.0451716367792662},\n            },\n            'loss_domain_2': {\n                'proportion_domain_1': {'beta0': 0.6366841433930845, 'beta1': -0.041541148709937525},\n                'proportion_domain_2': {'beta0': -1.7192214869138862, 'beta1': 0.12086845627811361},\n                'proportion_domain_3': {'beta0': 0.01801791154655993, 'beta1': -0.004998619018912918},\n                'proportion_domain_4': {'beta0': 0.43967439538852, 'beta1': -0.012137270708032335},\n                'proportion_domain_5': {'beta0': 0.6248450365857227, 'beta1': -0.06219141784123091},\n            },\n            'loss_domain_3': {\n                'proportion_domain_1': {'beta0': 1.623227750301966, 'beta1': -0.03718726484996544},\n                'proportion_domain_2': {'beta0': 0.5786824938578662, 'beta1': 0.05515336511355352},\n                'proportion_domain_3': {'beta0': -4.340004860354267, 'beta1': -0.0332577630477254},\n                'proportion_domain_4': {'beta0': 0.842562300321158, 'beta1': 0.03820434891706093},\n                'proportion_domain_5': {'beta0': 1.2955323158732615, 'beta1': -0.022912686132921745},\n            },\n            'loss_domain_4': {\n                'proportion_domain_1': {'beta0': 1.2168426320766839, 'beta1': -0.10375482434593292},\n                'proportion_domain_2': {'beta0': 1.8340679378772013, 'beta1': -0.1395259506615147},\n                'proportion_domain_3': {'beta0': 1.0335256104698727, 'beta1': -0.08180936075274063},\n                'proportion_domain_4': {'beta0': -5.334693091404859, 'beta1': 0.428635948410881},\n                'proportion_domain_5': {'beta0': 1.2502569109811015, 'beta1': -0.10354581265069283},\n            },\n            'loss_domain_5': {\n                'proportion_domain_1': {'beta0': 0.24988780248626472, 'beta1': -0.026851221126731266},\n                'proportion_domain_2': {'beta0': -0.3681011545143189, 'beta1': 0.048319731256692484},\n                'proportion_domain_3': {'beta0': 0.30625861824469436, 'beta1': -0.013830485288663432},\n                'proportion_domain_4': {'beta0': -0.07581595507286405, 'beta1': 0.023918795432970087},\n                'proportion_domain_5': {'beta0': -0.11222931114377548, 'beta1': -0.03155682027426793},\n            },\n        },\n    }\n\n    def _parse_group_size(g: str) -> float | None:\n        if not isinstance(g, str):\n            return None\n        s = g.strip().upper()\n        try:\n            if s.endswith('B'):\n                return float(s[:-1]) * 1e9\n            if s.endswith('M'):\n                return float(s[:-1]) * 1e6\n            if s.isdigit():\n                return float(s)\n        except ValueError:\n            return None\n        return None\n\n    def _params_from_size(model_size: float) -> Dict[str, Dict[str, float]]:\n        logS = log10(model_size)\n        losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n        props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n        out = {\"intercepts\": {}, \"coefs\": {}}\n        for l in losses:\n            b = PARAM_MAP['intercepts'][l]\n            out[\"intercepts\"][l] = b['beta0'] + b['beta1'] * logS\n            out[\"coefs\"][l] = {}\n            for p in props:\n                bp = PARAM_MAP['coefs'][l][p]\n                out[\"coefs\"][l][p] = bp['beta0'] + bp['beta1'] * logS\n        return out\n\n    # Choose parameters for this group\n    if group in COEFS:\n        group_params = COEFS[group]\n    else:\n        size = _parse_group_size(group)\n        if size is not None and size > 0:\n            group_params = _params_from_size(size)\n        else:\n            # Fallback: average coefficients across known groups\n            groups = list(COEFS.keys())\n            losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n            props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n            avg = {\"intercepts\": {}, \"coefs\": {}}\n            for loss in losses:\n                avg[\"intercepts\"][loss] = sum(COEFS[g][\"intercepts\"][loss] for g in groups) / len(groups)\n                avg[\"coefs\"][loss] = {p: sum(COEFS[g][\"coefs\"][loss][p] for g in groups) / len(groups) for p in props}\n            group_params = avg\n\n    losses = [f\"loss_domain_{i}\" for i in range(1, 6)]\n    props = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n\n    outputs: list[dict[str, float]] = []\n    for row in input_data:\n        pred: dict[str, float] = {}\n        for loss in losses:\n            val = group_params[\"intercepts\"][loss]\n            for p in props:\n                val += group_params[\"coefs\"][loss][p] * float(row.get(p, 0.0))\n            pred[loss] = float(val)\n        outputs.append(pred)\n\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5", "reward_r2": 0.792215, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n\n    # Coefficients for each group, domain, and proportion\n    # Discovered through linear regression: loss_i = intercept_i + sum_j(coeff_ij * proportion_j)\n    coefficients = {\n        \"160M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.2047495302794857,\n                \"proportions\": [-0.6785457845693336, 1.1463793805820295, 0.15167833367170308, 1.025939940580135, 0.559297660014955]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.7485645127653795,\n                \"proportions\": [0.8454146604044328, -0.17669189456942216, 0.5339549342653283, 0.8879814770372856, 0.657905335627755]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.777028370562523,\n                \"proportions\": [1.6343684204048485, 1.3463909111398886, -4.1176102362247855, 1.4889565642039464, 1.4249227110386287]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.157804803455643,\n                \"proportions\": [0.5984442545273441, 0.9125282761471838, 0.5726678524110912, -1.5517735333398215, 0.6259379537098435]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.8123009748266243,\n                \"proportions\": [0.5902786179156017, 0.5892359010590805, 0.7654442820304688, 0.6814456996467663, 0.18589647417470898]\n            }\n        },\n        \"305M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.0812993412453547,\n                \"proportions\": [-0.6681116974382604, 1.0884138560917158, 0.16066797498121796, 0.9749209723348417, 0.5254082352758425]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.6211416491050334,\n                \"proportions\": [0.8044876469222303, -0.18111733656107531, 0.528440945418323, 0.8482493615636207, 0.6210810317619353]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.6406715315205858,\n                \"proportions\": [1.6270585820746972, 1.3740527268982263, -4.2844089602142255, 1.4966639435477096, 1.4273052392141794]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.0694804673811962,\n                \"proportions\": [0.5643140257573426, 0.8424196628193863, 0.55242066083749, -1.4834642093589991, 0.5937903273259748]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.675669426576589,\n                \"proportions\": [0.5562710617961506, 0.5594325514171398, 0.7566886061299141, 0.647156326106111, 0.15612088112727496]\n            }\n        },\n        \"410M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.026790579832494,\n                \"proportions\": [-0.6707570147435764, 1.1129214834370829, 0.0851387854718725, 0.9896066568001738, 0.5098806688669443]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 2.5586009139858072,\n                \"proportions\": [0.7940122885118273, -0.157381828677838, 0.45948615911537244, 0.8582702215518173, 0.6042140734846292]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.574714953134018,\n                \"proportions\": [1.6435298197556953, 1.3880256729190952, -4.3849209143151056, 1.4955237719771226, 1.4325566027972132]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.0456653717309252,\n                \"proportions\": [0.5202273851847438, 0.863550404237731, 0.5487421835368134, -1.450622661281582, 0.5637680600532177]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 2.611253142181258,\n                \"proportions\": [0.5424865509221992, 0.5856594143219438, 0.6774479937054709, 0.665680734573013, 0.13997844865863204]\n            }\n        },\n        \"70M\": {\n            \"loss_domain_1\": {\n                \"intercept\": 2.47466987462387,\n                \"proportions\": [-0.7074679140450373, 1.2175088434044439, 0.22352562602536008, 1.106631113925414, 0.6344722053136926]\n            },\n            \"loss_domain_2\": {\n                \"intercept\": 3.0158832255809034,\n                \"proportions\": [0.9147245891387732, -0.16651450748750043, 0.5733419334967286, 0.9507289447583598, 0.7436022656745435]\n            },\n            \"loss_domain_3\": {\n                \"intercept\": 1.9867908155493292,\n                \"proportions\": [1.7511750123143863, 1.4302318794850386, -4.279610044979821, 1.5512789318819353, 1.53371503684779]\n            },\n            \"loss_domain_4\": {\n                \"intercept\": 1.355256984805328,\n                \"proportions\": [0.6709376757670867, 1.0186456755589912, 0.674235515641396, -1.7195739626404851, 0.7110120804783369]\n            },\n            \"loss_domain_5\": {\n                \"intercept\": 3.093438385056124,\n                \"proportions\": [0.6590286880560735, 0.63342029207528, 0.805556538546447, 0.7338054152807485, 0.2616274510975776]\n            }\n        }\n    }\n\n    results = []\n\n    for data_point in input_data:\n        prediction = {}\n\n        # Get coefficients for this group\n        group_coeffs = coefficients[group]\n\n        # Predict each domain's loss\n        for domain_idx in range(1, 6):\n            domain_key = f\"loss_domain_{domain_idx}\"\n            domain_params = group_coeffs[domain_key]\n\n            # Calculate: intercept + sum(coeff_j * proportion_j)\n            intercept = domain_params[\"intercept\"]\n            proportions = domain_params[\"proportions\"]\n\n            loss = intercept\n            for j in range(5):\n                proportion_key = f\"proportion_domain_{j+1}\"\n                if proportion_key in data_point:\n                    loss += proportions[j] * data_point[proportion_key]\n\n            prediction[domain_key] = loss\n\n        results.append(prediction)\n\n    return results", "agent_name": "claude-code", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-4.1", "reward_r2": 0.7922146059149126, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Coefficients for each group/domain: [const, c1, c2, c3, c4, c5]\n    coeffs = {\n        '70M': {\n            1: [2.47466987, -0.70746791, 1.21750884, 0.22352563, 1.10663111, 0.63447221],\n            2: [3.01588323, 0.91472459, -0.16651451, 0.57334193, 0.95072894, 0.74360227],\n            3: [1.98679082, 1.75117501, 1.43023188, -4.27961004, 1.55127893, 1.53371504],\n            4: [1.35525698, 0.67093768, 1.01864568, 0.67423552, -1.71957396, 0.71101208],\n            5: [3.09343839, 0.65902869, 0.63342029, 0.80555654, 0.73380542, 0.26162745],\n        },\n        '160M': {\n            1: [2.20474953, -0.67854578, 1.14637938, 0.15167833, 1.02593994, 0.55929766],\n            2: [2.74856451, 0.84541466, -0.17669189, 0.53395493, 0.88798148, 0.65790534],\n            3: [1.77702837, 1.63436842, 1.34639091, -4.11761024, 1.48895656, 1.42492271],\n            4: [1.1578048, 0.59844425, 0.91252828, 0.57266785, -1.55177353, 0.62593795],\n            5: [2.81230097, 0.59027862, 0.5892359, 0.76544428, 0.6814457, 0.18589647],\n        },\n        '305M': {\n            1: [2.08129934, -0.6681117, 1.08841386, 0.16066797, 0.97492097, 0.52540824],\n            2: [2.62114165, 0.80448765, -0.18111734, 0.52844095, 0.84824936, 0.62108103],\n            3: [1.64067153, 1.62705858, 1.37405273, -4.28440896, 1.49666394, 1.42730524],\n            4: [1.06948047, 0.56431403, 0.84241966, 0.55242066, -1.48346421, 0.59379033],\n            5: [2.67566943, 0.55627106, 0.55943255, 0.75668861, 0.64715633, 0.15612088],\n        },\n        '410M': {\n            1: [2.02679058, -0.67075701, 1.11292148, 0.08513879, 0.98960666, 0.50988067],\n            2: [2.55860091, 0.79401229, -0.15738183, 0.45948616, 0.85827022, 0.60421407],\n            3: [1.57471495, 1.64352982, 1.38802567, -4.38492091, 1.49552377, 1.4325566],\n            4: [1.04566537, 0.52022739, 0.8635504, 0.54874218, -1.45062266, 0.56376806],\n            5: [2.61125314, 0.54248655, 0.58565941, 0.67744799, 0.66568073, 0.13997845],\n        },\n    }\n    out = []\n    for row in input_data:\n        pred = {}\n        for i in range(1,6):\n            c = coeffs[group][i]\n            x = [1.0] + [row[f'proportion_domain_{j}'] for j in range(1,6)]\n            pred[f'loss_domain_{i}'] = sum(ci*xi for ci,xi in zip(c,x))\n        out.append(pred)\n    return out", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.7922146054081852, "solution": "# Auto-generated scaling law implementation\n# Coefficients were fitted via linear regression per group and domain\n\n# Mapping from group to parameters for each loss_domain\n# Each parameter list has [intercept, coef_p1, coef_p2, coef_p3, coef_p4, coef_p5]\nGROUP_PARAMS = {\n    '70M': {\n        'loss_domain_1': [2.47466987462387, -0.7074679140450373, 1.2175088434044439, 0.22352562602536008, 1.106631113925414, 0.6344722053136926],\n        'loss_domain_2': [3.0158832255809034, 0.9147245891387732, -0.16651450748750043, 0.5733419334967286, 0.9507289447583598, 0.7436022656745435],\n        'loss_domain_3': [1.9867908155493292, 1.7511750123143863, 1.4302318794850386, -4.279610044979821, 1.5512789318819353, 1.53371503684779],\n        'loss_domain_4': [1.355256984805328, 0.6709376757670867, 1.0186456755589912, 0.674235515641396, -1.7195739626404851, 0.7110120804783369],\n        'loss_domain_5': [3.093438385056124, 0.6590286880560735, 0.63342029207528, 0.805556538546447, 0.7338054152807485, 0.2616274510975776],\n    },\n    '160M': {\n        'loss_domain_1': [2.2047495302794857, -0.6785457845693336, 1.1463793805820295, 0.15167833367170308, 1.025939940580135, 0.559297660014955],\n        'loss_domain_2': [2.7485645127653795, 0.8454146604044328, -0.17669189456942216, 0.5339549342653283, 0.8879814770372856, 0.657905335627755],\n        'loss_domain_3': [1.777028370562523, 1.6343684204048485, 1.3463909111398886, -4.1176102362247855, 1.4889565642039464, 1.4249227110386287],\n        'loss_domain_4': [1.157804803455643, 0.5984442545273441, 0.9125282761471838, 0.5726678524110912, -1.5517735333398215, 0.6259379537098435],\n        'loss_domain_5': [2.8123009748266243, 0.5902786179156017, 0.5892359010590805, 0.7654442820304688, 0.6814456996467663, 0.18589647417470898],\n    },\n    '305M': {\n        'loss_domain_1': [2.0812993412453547, -0.6681116974382604, 1.0884138560917158, 0.16066797498121796, 0.9749209723348417, 0.5254082352758425],\n        'loss_domain_2': [2.6211416491050334, 0.8044876469222303, -0.18111733656107531, 0.528440945418323, 0.8482493615636207, 0.6210810317619353],\n        'loss_domain_3': [1.6406715315205858, 1.6270585820746972, 1.3740527268982263, -4.2844089602142255, 1.4966639435477096, 1.4273052392141794],\n        'loss_domain_4': [1.0694804673811962, 0.5643140257573426, 0.8424196628193863, 0.55242066083749, -1.4834642093589991, 0.5937903273259748],\n        'loss_domain_5': [2.675669426576589, 0.5562710617961506, 0.5594325514171398, 0.7566886061299141, 0.647156326106111, 0.15612088112727496],\n    },\n    '410M': {\n        'loss_domain_1': [2.026790579832494, -0.6707570147435764, 1.1129214834370829, 0.0851387854718725, 0.9896066568001738, 0.5098806688669443],\n        'loss_domain_2': [2.5586009139858072, 0.7940122885118273, -0.157381828677838, 0.45948615911537244, 0.8582702215518173, 0.6042140734846292],\n        'loss_domain_3': [1.574714953134018, 1.6435298197556953, 1.3880256729190952, -4.3849209143151056, 1.4955237719771226, 1.4325566027972132],\n        'loss_domain_4': [1.0456653717309252, 0.5202273851847438, 0.863550404237731, 0.5487421835368134, -1.450622661281582, 0.5637680600532177],\n        'loss_domain_5': [2.611253142181258, 0.5424865509221992, 0.5856594143219438, 0.6774479937054709, 0.665680734573013, 0.13997844865863204],\n    },\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to the discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, each containing keys 'proportion_domain_1' through 'proportion_domain_5'.\n        group: The experimental group key corresponding to pre-fitted parameters.\n\n    Returns:\n        A list of dictionaries, each with predicted 'loss_domain_1' through 'loss_domain_5'.\n    \"\"\"\n    if group not in GROUP_PARAMS:\n        raise KeyError(f\"Unknown group: {group}\")\n    preds = []\n    params = GROUP_PARAMS[group]\n    # Feature keys in order\n    feat_keys = [f\"proportion_domain_{i}\" for i in range(1,6)]\n    out_keys = [f\"loss_domain_{i}\" for i in range(1,6)]\n    for entry in input_data:\n        # Build feature vector\n        x = [1.0] + [entry[k] for k in feat_keys]\n        # Compute predicted losses\n        out = {}\n        for key in out_keys:\n            theta = params[key]\n            # dot product\n            pred = sum(t * xi for t, xi in zip(theta, x))\n            out[key] = pred\n        preds.append(out)\n    return preds", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-4.1", "reward_r2": 0.7922146054081847, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    COEFS = {\n        '70M': {\n            'loss_domain_1': {'coef': [-0.7074679140450348, 1.2175088434044405, 0.223525626025359, 1.1066311139254104, 0.6344722053136927], 'intercept': 2.47466987462387},\n            'loss_domain_2': {'coef': [0.9147245891387733, -0.16651450748750252, 0.5733419334967271, 0.950728944758362, 0.7436022656745442], 'intercept': 3.0158832255809047},\n            'loss_domain_3': {'coef': [1.7511750123143854, 1.4302318794850444, -4.279610044979821, 1.5512789318819356, 1.5337150368477888], 'intercept': 1.986790815549331},\n            'loss_domain_4': {'coef': [0.6709376757670921, 1.0186456755589888, 0.6742355156413925, -1.7195739626404885, 0.7110120804783404], 'intercept': 1.3552569848053264},\n            'loss_domain_5': {'coef': [0.6590286880560785, 0.6334202920752747, 0.805556538546449, 0.7338054152807434, 0.2616274510975761], 'intercept': 3.0934383850561247},\n        },\n        '160M': {\n            'loss_domain_1': {'coef': [-0.6785457845693319, 1.146379380582026, 0.15167833367169947, 1.0259399405801335, 0.5592976600149551], 'intercept': 2.204749530279485},\n            'loss_domain_2': {'coef': [0.8454146604044344, -0.1766918945694198, 0.5339549342653316, 0.8879814770372834, 0.6579053356277531], 'intercept': 2.7485645127653826},\n            'loss_domain_3': {'coef': [1.6343684204048525, 1.3463909111398857, -4.11761023622479, 1.4889565642039502, 1.424922711038629], 'intercept': 1.7770283705625238},\n            'loss_domain_4': {'coef': [0.5984442545273474, 0.9125282761471805, 0.5726678524110904, -1.551773533339824, 0.6259379537098458], 'intercept': 1.1578048034556419},\n            'loss_domain_5': {'coef': [0.5902786179156042, 0.5892359010590775, 0.7654442820304671, 0.6814456996467655, 0.18589647417470864], 'intercept': 2.8123009748266248},\n        },\n        '305M': {\n            'loss_domain_1': {'coef': [-0.6681116974382557, 1.0884138560917078, 0.1606679749812158, 0.9749209723348401, 0.5254082352758443], 'intercept': 2.0812993412453538},\n            'loss_domain_2': {'coef': [0.804487646922234, -0.18111733656107631, 0.5284409454183221, 0.8482493615636183, 0.621081031761935], 'intercept': 2.6211416491050334},\n            'loss_domain_3': {'coef': [1.627058582074697, 1.374052726898228, -4.284408960214225, 1.4966639435477105, 1.427305239214179], 'intercept': 1.6406715315205878},\n            'loss_domain_4': {'coef': [0.5643140257573457, 0.8424196628193856, 0.5524206608374863, -1.4834642093590011, 0.5937903273259771], 'intercept': 1.0694804673811953},\n            'loss_domain_5': {'coef': [0.5562710617961523, 0.5594325514171392, 0.7566886061299118, 0.6471563261061084, 0.15612088112727607], 'intercept': 2.6756694265765897},\n        },\n        '410M': {\n            'loss_domain_1': {'coef': [-0.6707570147435775, 1.1129214834370798, 0.08513878547187051, 0.9896066568001731, 0.5098806688669455], 'intercept': 2.0267905798324937},\n            'loss_domain_2': {'coef': [0.7940122885118303, -0.1573818286778379, 0.4594861591153763, 0.8582702215518133, 0.6042140734846265], 'intercept': 2.558600913985809},\n            'loss_domain_3': {'coef': [1.643529819755696, 1.3880256729190958, -4.384920914315108, 1.4955237719771257, 1.4325566027972132], 'intercept': 1.574714953134019},\n            'loss_domain_4': {'coef': [0.5202273851847482, 0.8635504042377276, 0.5487421835368104, -1.450622661281584, 0.5637680600532203], 'intercept': 1.045665371730924},\n            'loss_domain_5': {'coef': [0.5424865509221987, 0.5856594143219445, 0.6774479937054712, 0.6656807345730104, 0.13997844865863152], 'intercept': 2.6112531421812584},\n        },\n    }\n    \n    dom_props = [f'proportion_domain_{i}' for i in range(1,6)]\n    out = []\n    for row in input_data:\n        pred = {}\n        for dom in range(1,6):\n            key = f'loss_domain_{dom}'\n            c = COEFS[group][key]['coef']\n            b = COEFS[group][key]['intercept']\n            x = [row.get(p, 0.0) for p in dom_props]\n            pred[key] = float(sum(ci*xi for ci,xi in zip(c,x)) + b)\n        out.append(pred)\n    return out", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2", "reward_r2": 0.7766522374268384, "solution": "import math\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Convert group name to model size in millions\n    model_sizes = {\n        '70M': 70.0,\n        '160M': 160.0,\n        '305M': 305.0,\n        '410M': 410.0\n    }\n    \n    if group not in model_sizes:\n        raise ValueError(f\"Unknown group: {group}. Expected one of {list(model_sizes.keys())}\")\n    \n    N = model_sizes[group]\n    logN = math.log(N)\n    \n    # Model parameters obtained from fitting the dataset\n    # Formula: loss_i = a_i + b_i * log(N) + \u03a3_{j=1}^5 c_ij * p_j\n    # Where p_j are the mixture proportions\n    \n    # Parameters for each domain (i = 1..5)\n    params = {\n        1: {\n            'a': 3.533943,\n            'b': -0.304742,\n            'c': [-0.413807, 1.408719, 0.422666, 1.291688, 0.824678]\n        },\n        2: {\n            'a': 4.148328,\n            'b': -0.321885,\n            'c': [1.122116, 0.112030, 0.806262, 1.168764, 0.939157]\n        },\n        3: {\n            'a': 2.989025,\n            'b': -0.283582,\n            'c': [1.912878, 1.633520, -4.017793, 1.756950, 1.703470]\n        },\n        4: {\n            'a': 2.185297,\n            'b': -0.234356,\n            'c': [0.794130, 1.114935, 0.792666, -1.345710, 0.829276]\n        },\n        5: {\n            'a': 4.255896,\n            'b': -0.332244,\n            'c': [0.878562, 0.883483, 1.042831, 0.973568, 0.477452]\n        }\n    }\n    \n    results = []\n    \n    for data_point in input_data:\n        # Extract proportions\n        p1 = data_point.get('proportion_domain_1', 0.0)\n        p2 = data_point.get('proportion_domain_2', 0.0)\n        p3 = data_point.get('proportion_domain_3', 0.0)\n        p4 = data_point.get('proportion_domain_4', 0.0)\n        p5 = data_point.get('proportion_domain_5', 0.0)\n        \n        # Create vector of proportions\n        p_vec = [p1, p2, p3, p4, p5]\n        \n        # Compute predictions for each domain\n        pred_dict = {}\n        \n        for i in range(1, 6):\n            param = params[i]\n            \n            # Compute dot product: \u03a3 c_ij * p_j\n            dot_product = sum(c * p for c, p in zip(param['c'], p_vec))\n            \n            # Compute prediction: a + b*log(N) + dot_product\n            prediction = param['a'] + param['b'] * logN + dot_product\n            \n            pred_dict[f'loss_domain_{i}'] = prediction\n        \n        results.append(pred_dict)\n    \n    return results", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o3", "reward_r2": 0.7543921987682078, "solution": "import math\n\n# Pre-computed coefficients (a, b) for each group and domain.\n_COEFFS = {\n    \"70M\": {\n        1: (2.6991877691852126, -0.05301830256954043),\n        2: (3.641245631839601, -0.01288446947428002),\n        3: (3.0648180627646675, -0.03887735281392035),\n        4: (1.5883184714718914, -0.04953747817222871),\n        5: (3.491442851657022, -0.03541572615826875),\n    },\n    \"160M\": {\n        1: (2.390023902609585, -0.04954828464158185),\n        2: (3.308997347375093, -0.01184703133901682),\n        3: (2.775061224326219, -0.03703323688495387),\n        4: (1.3585325213280748, -0.04416553521395205),\n        5: (3.1416357474005943, -0.036409432790092094),\n    },\n    \"305M\": {\n        1: (2.2445101169537414, -0.04826253963246907),\n        2: (3.151664441181741, -0.011228102438934438),\n        3: (2.6272265814936144, -0.03831519342481426),\n        4: (1.254081110356706, -0.04228710260549062),\n        5: (2.974256274101213, -0.036815022043621345),\n    },\n    \"410M\": {\n        1: (2.1839860164214726, -0.04779882382417476),\n        2: (3.080284217913702, -0.010883486046165911),\n        3: (2.5599133716532467, -0.039034580115764377),\n        4: (1.216103723749701, -0.04115598457974986),\n        5: (2.8980195371491457, -0.038035573443952364),\n    },\n}\n\n_EPS = 1e-12  # Avoid log(0)\n\ndef _predict_single(row: dict[str, float], coeffs: dict[int, tuple[float, float]]) -> dict[str, float]:\n    \"\"\"Compute predictions for one data point given the coeff dict for its group.\"\"\"\n    out: dict[str, float] = {}\n    for i in range(1, 6):\n        p = row.get(f\"proportion_domain_{i}\", 0.0)\n        # Clamp to avoid -inf.\n        p = max(p, _EPS)\n        a, b = coeffs[i]\n        out[f\"loss_domain_{i}\"] = a + b * math.log(p)\n    return out\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    The scaling law:\n        loss_domain_i = a_{group,i} + b_{group,i} * ln(proportion_domain_i)\n    where (a_{group,i}, b_{group,i}) are constants determined for each (group, domain).\n\n    Args:\n        input_data: List of data points containing 'proportion_domain_*' keys.\n        group: Experimental group label (e.g., '70M', '160M', ...).\n\n    Returns:\n        List with predicted losses for each input example.\n    \"\"\"\n    if group not in _COEFFS:\n        raise ValueError(f\"Unknown group '{group}'. Expected one of {list(_COEFFS)}\")\n\n    coeffs = _COEFFS[group]\n    return [_predict_single(row, coeffs) for row in input_data]", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": -1.0, "solution": "import math\n\n# Parameters for the power-law scaling: loss = A * p^b\n# Parameters A and b per group and domain index\ngroup_params = {\n    \"70M\": {\n        1: {\"A\": 2.5469231031223694, \"b\": -0.053856817382467885},\n        2: {\"A\": 3.572940616953032,  \"b\": -0.013140624030475783},\n        3: {\"A\": 2.9029843430954707, \"b\": -0.03138671540754998},\n        4: {\"A\": 1.4276962543227714, \"b\": -0.0805007638328445},\n        5: {\"A\": 3.4360881599441604, \"b\": -0.035362869076753844},\n    },\n    \"160M\": {\n        1: {\"A\": 2.2615462410312324, \"b\": -0.05322286098991343},\n        2: {\"A\": 3.264432162767659,  \"b\": -0.010428494948258562},\n        3: {\"A\": 2.631691497596162,  \"b\": -0.0315962095456069},\n        4: {\"A\": 1.2163575770768627, \"b\": -0.08356911748409235},\n        5: {\"A\": 3.08550213386204,   \"b\": -0.039978594566019446},\n    },\n    \"305M\": {\n        1: {\"A\": 2.1245282223544177, \"b\": -0.0537648313675756},\n        2: {\"A\": 3.10132258895579,   \"b\": -0.011700109195731317},\n        3: {\"A\": 2.507951292630382,  \"b\": -0.0304824591544152},\n        4: {\"A\": 1.120707470602762,  \"b\": -0.08558188369742958},\n        5: {\"A\": 2.919700368120316,  \"b\": -0.041502398419353326},\n    },\n    \"410M\": {\n        1: {\"A\": 2.071078836571189,  \"b\": -0.053026238258029254},\n        2: {\"A\": 3.0492072217672646, \"b\": -0.008625461782159526},\n        3: {\"A\": 2.4411353600422157, \"b\": -0.03147846919171784},\n        4: {\"A\": 1.0829659757348267, \"b\": -0.08734153465469903},\n        5: {\"A\": 2.842998459668121,  \"b\": -0.04324831870633425},\n    },\n}\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts loss values for each domain based on the power-law scaling law:\n        loss_domain_i = A_{i} * (proportion_domain_i)^{b_{i}}\n\n    Args:\n        input_data: List of data points, each containing 'proportion_domain_i' keys.\n        group: One of the defined model sizes: '70M', '160M', '305M', or '410M'.\n\n    Returns:\n        List of dictionaries with keys 'loss_domain_i' for i in 1..5.\n    \"\"\"\n    if group not in group_params:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = group_params[group]\n    outputs = []\n    for entry in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p_key = f\"proportion_domain_{i}\"\n            loss_key = f\"loss_domain_{i}\"\n            p = entry.get(p_key, 0.0)\n            A = params[i][\"A\"]\n            b = params[i][\"b\"]\n            # Compute prediction (if p <= 0, prediction set to A*0^b = inf or nan; clamp to zero)\n            try:\n                val = A * (p ** b)\n            except Exception:\n                val = float('nan')\n            pred[loss_key] = val\n        outputs.append(pred)\n    return outputs", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": -1.0, "solution": "import math\n\n# Parameters for scaling law: loss = a * (proportion)^b\n# a and b coefficients per experimental group and domain\nPARAMS = {\n    '70M': {\n        1: (2.5469231031223694, -0.053856817382467885),\n        2: (3.572940616953032, -0.013140624030475783),\n        3: (2.9029843430954707, -0.03138671540754998),\n        4: (1.4276962543227714, -0.0805007638328445),\n        5: (3.4360881599441604, -0.035362869076753844),\n    },\n    '160M': {\n        1: (2.2615462410312324, -0.05322286098991343),\n        2: (3.264432162767659, -0.010428494948258562),\n        3: (2.631691497596162, -0.0315962095456069),\n        4: (1.2163575770768627, -0.08356911748409235),\n        5: (3.08550213386204, -0.039978594566019446),\n    },\n    '305M': {\n        1: (2.1245282223544177, -0.0537648313675756),\n        2: (3.10132258895579, -0.011700109195731317),\n        3: (2.507951292630382, -0.0304824591544152),\n        4: (1.120707470602762, -0.08558188369742958),\n        5: (2.919700368120316, -0.041502398419353326),\n    },\n    '410M': {\n        1: (2.071078836571189, -0.053026238258029254),\n        2: (3.0492072217672646, -0.008625461782159526),\n        3: (2.4411353600422157, -0.03147846919171784),\n        4: (1.0829659757348267, -0.08734153465469903),\n        5: (2.842998459668121, -0.04324831870633425),\n    },\n}\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts validation losses for each domain based on mixture proportions using a power-law scaling.\n\n    Args:\n        input_data: List of dicts with keys 'proportion_domain_i' for i in 1..5.\n        group: Experimental group name (e.g., '70M', '160M', '305M', '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_i' containing predicted losses.\n    \"\"\"\n    if group not in PARAMS:\n        raise ValueError(f\"Unknown group '{group}'. Available groups: {list(PARAMS.keys())}\")\n    coeffs = PARAMS[group]\n    predictions = []\n    for row in input_data:\n        out = {}\n        for i in range(1, 6):\n            a, b = coeffs[i]\n            p = row.get(f'proportion_domain_{i}')\n            if p is None:\n                raise KeyError(f\"Input data missing 'proportion_domain_{i}'\")\n            # power-law prediction\n            out[f'loss_domain_{i}'] = a * (p ** b)\n        predictions.append(out)\n    return predictions", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": -1.0, "solution": "\"\"\"\nScaling law prediction function.\nDiscovered form: loss = a * proportion^b + c\nCoefficients (per group and domain) embedded.\n\"\"\"\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts loss for each domain based on mixture proportion using the scaling law.\n\n    Args:\n        input_data: List of dicts, each containing keys 'proportion_domain_1'... 'proportion_domain_5'.\n        group: Experimental group name ('70M', '160M', '305M', or '410M').\n\n    Returns:\n        List of dicts with keys 'loss_domain_1'... 'loss_domain_5'.\n    \"\"\"\n    # Precomputed coefficients: a, b, c for each domain\n    coeffs = {\n        \"70M\": {\n            \"domain_1\": {\"a\": 0.01345935217392868, \"b\": -1.2494096276943556, \"c\": 2.6205794786947907},\n            \"domain_2\": {\"a\": 0.004993332921529321, \"b\": -1.0911516501426637, \"c\": 3.6100070391814723},\n            \"domain_3\": {\"a\": 0.00044534115466757863, \"b\": -1.8023950929710377, \"c\": 3.0742037259615387},\n            \"domain_4\": {\"a\": 0.0013839903291475184, \"b\": -1.8680188768809969, \"c\": 1.588180693069307},\n            \"domain_5\": {\"a\": 0.004960131366262632, \"b\": -2.106904001314134, \"c\": 3.462343661422903}\n        },\n        \"160M\": {\n            \"domain_1\": {\"a\": 0.005223819076583144, \"b\": -1.5760633456244681, \"c\": 2.333516554006229},\n            \"domain_2\": {\"a\": 0.018538103040269167, \"b\": -0.5723997798081769, \"c\": 3.2651763761104062},\n            \"domain_3\": {\"a\": 0.0004398425225265252, \"b\": -1.8019610984309542, \"c\": 2.7797504131610573},\n            \"domain_4\": {\"a\": 0.0029902400448121756, \"b\": -1.5481918524361098, \"c\": 1.3486876031353137},\n            \"domain_5\": {\"a\": 0.005481054936761751, \"b\": -2.075664514028258, \"c\": 3.1112351190476195}\n        },\n        \"305M\": {\n            \"domain_1\": {\"a\": 0.011864288528888557, \"b\": -1.22567788447283, \"c\": 2.185522820374434},\n            \"domain_2\": {\"a\": 0.012171471112974626, \"b\": -0.7364570533455278, \"c\": 3.1095881583121825},\n            \"domain_3\": {\"a\": 0.00038056373793330696, \"b\": -1.834588448547919, \"c\": 2.638818359375},\n            \"domain_4\": {\"a\": 0.0004109153221451477, \"b\": -2.281718804345559, \"c\": 1.256210679661716},\n            \"domain_5\": {\"a\": 0.007337127428867711, \"b\": -1.893167760790138, \"c\": 2.9427326920351473}\n        },\n        \"410M\": {\n            \"domain_1\": {\"a\": 0.007763706139059348, \"b\": -1.3765396889516939, \"c\": 2.1338753273640996},\n            \"domain_2\": {\"a\": 0.009925468816668701, \"b\": -0.7686654599189278, \"c\": 3.040671101681472},\n            \"domain_3\": {\"a\": 0.000621910962969547, \"b\": -1.681240873197639, \"c\": 2.571829458383413},\n            \"domain_4\": {\"a\": 0.0016201279671757462, \"b\": -1.748267976324254, \"c\": 1.2112767184921616},\n            \"domain_5\": {\"a\": 0.0032863494213504533, \"b\": -2.3772423234520086, \"c\": 2.86961949493339}\n        }\n    }\n    if group not in coeffs:\n        raise ValueError(f\"Unknown group: {group}\")\n    params = coeffs[group]\n    predictions = []\n    for entry in input_data:\n        pred = {}\n        for i in range(1, 6):\n            p = entry.get(f'proportion_domain_{i}', 0.0)\n            a = params[f'domain_{i}']['a']\n            b = params[f'domain_{i}']['b']\n            c = params[f'domain_{i}']['c']\n            # Compute scaling law prediction\n            try:\n                pred[f'loss_domain_{i}'] = a * (p ** b) + c\n            except Exception:\n                pred[f'loss_domain_{i}'] = float('nan')\n        predictions.append(pred)\n    return predictions", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "DeepSeek-V3.2-reasoning", "reward_r2": -1.0, "solution": "def law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    import math\n    \n    # Fitted parameters for each group and domain.\n    # The structure: params[group][domain] = (intercept, coeff_log_prop, coeff_other_props_list)\n    # For domain i, the model is:\n    #   log(loss_i) = intercept + coeff_log_prop * log(proportion_i) + sum_{j!=i} coeff_other_props[k] * proportion_j\n    # where k indexes the other domains in order 1..5 (skipping i).\n    \n    params = {\n        '160M': {\n            1: (0.7833111038, -0.0402786347, [0.0912184552, 0.1068224498, 0.0812000158, 0.0653049753]),\n            2: (2.2492950825, -0.2741726925, [-1.8338729843, -1.8419387947, -1.8129411557, -1.8804824561]),\n            3: (0.0564101912, 0.0238035455, [1.1670604404, 1.1402007955, 1.1646792046, 1.1238881078]),\n            4: (0.0906714462, -0.0636907107, [0.1736209965, 0.2185072027, 0.1762865172, 0.1599946777]),\n            5: (1.1228678945, -0.0293363470, [0.0305269549, 0.0225717669, 0.0270728741, 0.0242907798]),\n        },\n        '305M': {\n            1: (0.7304779650, -0.0447141164, [0.0723279282, 0.1063034222, 0.0462948884, 0.0440165074]),\n            2: (0.4428432618, 0.1286117688, [1.1628232105, 1.1585308411, 1.1738698450, 1.1107456140]),\n            3: (-0.1061667057, 0.0322450117, [1.3122632380, 1.2939195211, 1.3023673626, 1.2668419380]),\n            4: (0.0340802509, -0.0700722733, [0.1267262336, 0.1819456223, 0.1536312260, 0.1207205545]),\n            5: (1.0702681738, -0.0373910763, [0.0131496196, 0.0074083668, 0.0124928399, 0.0046385109]),\n        },\n        '410M': {\n            1: (0.6972040078, -0.0443869518, [0.0954638183, 0.0645940941, 0.0773371879, 0.0503260941]),\n            2: (-0.0657555465, 0.2435245209, [1.9965293996, 1.9734092160, 2.0250391329, 1.9451754386]),\n            3: (-0.0475965193, 0.0248594403, [1.2067481889, 1.1725598696, 1.2007621976, 1.1551819732]),\n            4: (0.0365064485, -0.0800197148, [0.0603258709, 0.1420420825, 0.0576457840, 0.0576871365]),\n            5: (1.0409566623, -0.0331349649, [0.0267617634, 0.0320288953, 0.0042076465, 0.0272967701]),\n        },\n        '70M': {\n            1: (0.9101144519, -0.0429647243, [0.0705110055, 0.0893060363, 0.0584434242, 0.0527811612]),\n            2: (0.8397054645, 0.0709814360, [0.7289539413, 0.6915313019, 0.7397147774, 0.6875000000]),\n            3: (0.4282782001, 0.0059402138, [0.8299496047, 0.7770592599, 0.8056670941, 0.7803879986]),\n            4: (0.3683151287, -0.0865753489, [-0.0264035861, -0.0042538750, -0.0003404132, -0.0375423314]),\n            5: (1.2364780752, -0.0373359187, [-0.0022038097, -0.0122877413, -0.0185141866, -0.0119700068]),\n        },\n    }\n    \n    if group not in params:\n        raise ValueError(f'Unknown group: {group}')\n    \n    predictions = []\n    for data_point in input_data:\n        pred = {}\n        for i in range(1, 6):\n            param = params[group][i]\n            if param is None:\n                pred[f'loss_domain_{i}'] = 0.0\n                continue\n            \n            intercept, coeff_log, coeff_other = param\n            prop_i = data_point.get(f'proportion_domain_{i}', 0.0)\n            if prop_i <= 0:\n                log_prop_i = math.log(1e-10)\n            else:\n                log_prop_i = math.log(prop_i)\n            \n            # Build the list of other proportions in order 1..5 (skipping i)\n            other_props = []\n            for j in range(1, 6):\n                if j != i:\n                    other_props.append(data_point.get(f'proportion_domain_{j}', 0.0))\n            \n            log_loss = intercept + coeff_log * log_prop_i\n            for k, prop_j in enumerate(other_props):\n                log_loss += coeff_other[k] * prop_j\n            \n            loss = math.exp(log_loss)\n            pred[f'loss_domain_{i}'] = loss\n        \n        predictions.append(pred)\n    \n    return predictions", "agent_name": "openhands", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": -1.0, "solution": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, List, Tuple\n\nimport numpy as np\n\n# Try to import datasets lazily and robustly.\ntry:\n    from datasets import load_from_disk, Dataset, DatasetDict  # type: ignore\nexcept Exception:  # pragma: no cover - optional dependency\n    load_from_disk = None  # type: ignore\n    Dataset = None  # type: ignore\n    DatasetDict = None  # type: ignore\n\n\n@dataclass\nclass FittedModel:\n    # theta: shape (n_targets, n_features), where n_features = 1 (intercept) + n_proportions\n    theta: np.ndarray\n    proportion_keys: List[str]  # ordered list of feature keys used during fitting\n    target_keys: List[str]      # ordered list of target keys used during fitting\n\n\n# Global store of fitted coefficients per group.\n_FITTED_BY_GROUP: Dict[str, FittedModel] = {}\n# Numerical stabilizers\n_EPS_P: float = 1e-12\n_EPS_L: float = 1e-12\n_RIDGE: float = 1e-3\n\n\ndef _safe_log(x: np.ndarray, eps: float) -> np.ndarray:\n    return np.log(np.clip(x, eps, None))\n\n\ndef _detect_keys(columns: Iterable[str]) -> Tuple[List[str], List[str], str | None]:\n    cols = list(columns)\n    # Proportion keys like \"proportion_domain_1\" ... \"proportion_domain_5\"\n    prop_keys = sorted(\n        [c for c in cols if re.fullmatch(r\"proportion_domain_\\d+\", c)],\n        key=lambda k: int(k.rsplit(\"_\", 1)[1]),\n    )\n    # Target keys like \"loss_domain_1\" ... \"loss_domain_5\"\n    tgt_keys = sorted(\n        [c for c in cols if re.fullmatch(r\"loss_domain_\\d+\", c)],\n        key=lambda k: int(k.rsplit(\"_\", 1)[1]),\n    )\n    # Group column\n    group_col: str | None = None\n    if \"group\" in cols:\n        group_col = \"group\"\n    else:\n        # Fallback to any column ending with '_group' or named 'Group'\n        for cand in cols:\n            if cand.lower().ends_with(\"_group\") or cand == \"Group\":\n                group_col = cand\n                break\n    return prop_keys, tgt_keys, group_col\n\n\ndef _dataset_to_rows(ds_obj) -> List[Dict[str, float]]:\n    # Convert Dataset or DatasetDict to a list of dict rows\n    rows: List[Dict[str, float]] = []\n    if DatasetDict is not None and isinstance(ds_obj, DatasetDict):\n        for split in ds_obj.values():\n            rows.extend(_dataset_to_rows(split))\n        return rows\n    # ds_obj is a Dataset or something iterable over dicts\n    try:\n        # Iterating a datasets.Dataset yields dicts row-wise efficiently\n        for row in ds_obj:  # type: ignore\n            rows.append(row)\n    except Exception:\n        # Fallback: try to_dict\n        try:\n            data_dict = ds_obj.to_dict()  # type: ignore\n            n = len(next(iter(data_dict.values())))\n            for i in range(n):\n                rows.append({k: v[i] for k, v in data_dict.items()})\n        except Exception:\n            pass\n    return rows\n\n\ndef _fit_group(rows: List[Dict[str, float]], proportion_keys: List[str], target_keys: List[str]) -> FittedModel:\n    # Build X (design) and Y (targets)\n    X_list: List[List[float]] = []\n    Y_lists: List[List[float]] = [[] for _ in target_keys]\n\n    for r in rows:\n        try:\n            p_vec = np.array([float(r[k]) for k in proportion_keys], dtype=float)\n            if np.any(~np.isfinite(p_vec)):\n                continue\n            # Require all targets present and finite\n            y_vals = []\n            valid = True\n            for tk in target_keys:\n                val = float(r[tk])\n                if not (np.isfinite(val) and val > 0):\n                    valid = False\n                    break\n                y_vals.append(val)\n            if not valid:\n                continue\n        except Exception:\n            continue\n\n        x = [1.0]\n        x.extend(_safe_log(p_vec, _EPS_P).tolist())\n        X_list.append(x)\n        for i, y in enumerate(y_vals):\n            Y_lists[i].append(float(y))\n\n    if not X_list or any(len(yc) == 0 for yc in Y_lists):\n        # Fallback: identity-like tiny coefficients to keep the model running\n        n_features = 1 + len(proportion_keys)\n        theta = np.zeros((len(target_keys), n_features), dtype=float)\n        # Intercepts default to log(1.0) = 0\n        return FittedModel(theta=theta, proportion_keys=proportion_keys, target_keys=target_keys)\n\n    X = np.asarray(X_list, dtype=float)  # shape (n_samples, n_features)\n    n_features = X.shape[1]\n    XtX = X.T @ X\n    reg = np.eye(n_features, dtype=float)\n    reg[0, 0] = 0.0  # do not regularize intercept\n    XtX_reg = XtX + _RIDGE * reg\n\n    theta_rows: List[np.ndarray] = []\n    Xt = X.T\n    for y_vals in Y_lists:\n        y = _safe_log(np.asarray(y_vals, dtype=float), _EPS_L)\n        Xty = Xt @ y\n        try:\n            beta = np.linalg.solve(XtX_reg, Xty)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.lstsq(XtX_reg, Xty, rcond=None)[0]\n        theta_rows.append(beta)\n\n    theta = np.vstack(theta_rows)  # (n_targets, n_features)\n    return FittedModel(theta=theta, proportion_keys=proportion_keys, target_keys=target_keys)\n\n\ndef _fit_all_groups() -> None:\n    global _FITTED_BY_GROUP\n    if _FITTED_BY_GROUP:\n        return  # already fit\n\n    # Attempt to load the dataset from disk\n    rows: List[Dict[str, float]] = []\n    if load_from_disk is not None:\n        try:\n            ds = load_from_disk(\"/app/data\")\n            rows = _dataset_to_rows(ds)\n        except Exception:\n            rows = []\n    # If no rows, leave empty and we will populate a default model\n    all_columns = set()\n    for r in rows:\n        all_columns.update(r.keys())\n    prop_keys, tgt_keys, group_col = _detect_keys(all_columns)\n\n    # Ensure we have expected 5 domains; if not, try to infer from any present keys\n    if not prop_keys:\n        # Default to proportion_domain_1..5 if not present\n        prop_keys = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n    if not tgt_keys:\n        tgt_keys = [f\"loss_domain_{i}\" for i in range(1, 6)]\n\n    if not rows:\n        # Build a default \"ALL\" model\n        _FITTED_BY_GROUP[\"ALL\"] = _fit_group([], prop_keys, tgt_keys)\n        _write_explain_md(_FITTED_BY_GROUP)\n        return\n\n    # Partition rows by group (or single ALL group)\n    grouped: Dict[str, List[Dict[str, float]]] = {}\n    if group_col is None:\n        grouped[\"ALL\"] = rows\n    else:\n        for r in rows:\n            g = r.get(group_col, \"ALL\")\n            grouped.setdefault(str(g), []).append(r)\n\n    # Fit per group\n    for g, gr_rows in grouped.items():\n        _FITTED_BY_GROUP[g] = _fit_group(gr_rows, prop_keys, tgt_keys)\n\n    # Also fit a global ALL group across everything for fallback\n    if \"ALL\" not in _FITTED_BY_GROUP:\n        _FITTED_BY_GROUP[\"ALL\"] = _fit_group(rows, prop_keys, tgt_keys)\n\n    # Write explanation file including fitted parameters\n    _write_explain_md(_FITTED_BY_GROUP)\n\n\ndef _write_explain_md(fitted: Dict[str, FittedModel]) -> None:\n    # Prepare a deterministic JSON-like dump of parameters\n    payload = {}\n    for g, fm in fitted.items():\n        payload[g] = {\n            \"proportion_keys\": fm.proportion_keys,\n            \"target_keys\": fm.target_keys,\n            \"theta\": np.asarray(fm.theta, dtype=float).round(8).tolist(),\n            \"model\": \"log-linear power law: log(loss_i) = theta[i,0] + sum_j theta[i,j] * log(p_j + eps); eps = %.0e\"\n            % _EPS_P,\n            \"ridge\": _RIDGE,\n        }\n\n    section = [\n        \"<!-- PARAMS START -->\",\n        \"Fitted parameter tensors by group (JSON):\",\n        \"\",\n        \"```json\",\n        json.dumps(payload, indent=2),\n        \"```\",\n        \"<!-- PARAMS END -->\",\n    ]\n    section_text = \"\\n\".join(section) + \"\\n\"\n\n    path = \"/app/explain.md\"\n    # If explain.md exists, replace the PARAM section; else, create a full file.\n    try:\n        if os.path.exists(path):\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            new_content: str\n            if \"<!-- PARAMS START -->\" in content and \"<!-- PARAMS END -->\" in content:\n                new_content = re.sub(\n                    r\"<!-- PARAMS START -->.*?<!-- PARAMS END -->\",\n                    section_text,\n                    content,\n                    flags=re.DOTALL,\n                )\n            else:\n                new_content = content.rstrip() + \"\\n\\n\" + section_text\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(new_content)\n        else:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(_default_explain_md_header().rstrip() + \"\\n\\n\" + section_text)\n    except Exception:\n        # Best-effort; ignore file writing errors.\n        pass\n\n\ndef _default_explain_md_header() -> str:\n    return \"\"\"# Scaling law for domain mixture in language model pre-training\n\nThis document describes the discovered scaling law that predicts per-domain validation loss from the domain mixture proportions used during pre-training.\n\n## Functional form (shared across groups)\n\nWe model each domain's validation loss as a multiplicative power-law function of the mixture proportions:\n\n- For domain i in {1..5}, with mixture proportions p_j for j in {1..5}:\n  loss_i = A_i * \u03a0_j (p_j + \u03b5)^{\u03b2_{i,j}}\n\nEquivalently in log-space (which is what we fit):\n\n- log(loss_i) = \u03b8_{i,0} + \u03a3_j \u03b8_{i,j} * log(p_j + \u03b5)\n  where A_i = exp(\u03b8_{i,0}) and \u03b2_{i,j} = \u03b8_{i,j}.\n\nWe fix \u03b5 = 1e-12 for numerical stability when a proportion is zero.\nWe estimate parameters with ridge-regularized least squares on the log-transformed variables, with L2 regularization \u03bb = 1e-3 applied to non-intercept weights.\n\nThis form captures:\n- Diminishing returns (via negative exponents).\n- Cross-domain transfer (exponents \u03b2_{i,j} coupling domains).\n- Scale invariance with respect to multiplicative changes in the mixture.\n\n## Methodology\n\n- Load the dataset at /app/data using datasets.load_from_disk().\n- Identify input features: proportion_domain_1..proportion_domain_5.\n- Identify targets: loss_domain_1..loss_domain_5.\n- If a 'group' column is present, fit a separate parameter set per group; otherwise, fit a single ALL group.\n- Optimize \u03b8 for each target independently using ridge regression in log space.\n- Use the fitted \u03b8 to predict losses for new inputs by exponentiating the linear predictor.\n\n## Fitted parameters per group\n\nThe exact fitted coefficients depend on the dataset available at runtime.\nThe block below is automatically populated by /app/law.py when the module is imported and the model is fit.\n\"\"\"\n\n\n# Fit on import so that law() can immediately use the parameters and /app/explain.md is populated.\n_fit_all_groups()\n\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Ensure models are fit (idempotent)\n    _fit_all_groups()\n\n    # Choose fitted coefficients for the requested group; fallback to \"ALL\"\n    fm = _FITTED_BY_GROUP.get(group) or _FITTED_BY_GROUP.get(\"ALL\")\n    if fm is None:\n        # Last-resort fallback with default keys and zero coefficients\n        proportion_keys = [f\"proportion_domain_{i}\" for i in range(1, 6)]\n        target_keys = [f\"loss_domain_{i}\" for i in range(1, 6)]\n        fm = FittedModel(theta=np.zeros((len(target_keys), 1 + len(proportion_keys))),  # type: ignore\n                         proportion_keys=proportion_keys,\n                         target_keys=target_keys)\n\n    # Build predictions\n    out: List[Dict[str, float]] = []\n    prop_keys = fm.proportion_keys\n    theta = np.asarray(fm.theta, dtype=float)\n\n    # If incoming dicts have a different set of proportion keys, try to realign\n    # to the canonical order based on numeric suffix.\n    def canonicalize_keys(keys: List[str]) -> List[str]:\n        return sorted(keys, key=lambda k: int(k.rsplit(\"_\", 1)[1]) if re.fullmatch(r\".*_\\d+\", k) else math.inf)\n\n    for row in input_data:\n        # If row has all canonical keys, use them; else, try to align using suffix.\n        if not all(k in row for k in prop_keys):\n            candidate = [k for k in row.keys() if re.fullmatch(r\"proportion_domain_\\d+\", k)]\n            if candidate:\n                prop_keys_runtime = canonicalize_keys(candidate)\n            else:\n                prop_keys_runtime = prop_keys  # fall back\n        else:\n            prop_keys_runtime = prop_keys\n\n        p_vec = np.array([float(row.get(k, 0.0)) for k in prop_keys_runtime], dtype=float)\n        x = np.empty(1 + p_vec.size, dtype=float)\n        x[0] = 1.0\n        x[1:] = _safe_log(p_vec, _EPS_P)\n\n        # Predict each target independently in log-space then exponentiate.\n        yhat_log = theta @ x  # shape (n_targets,)\n        yhat = np.exp(yhat_log)\n        # Produce outputs with canonical target keys (loss_domain_1..5)\n        pred: Dict[str, float] = {}\n        # Map predictions to fm.target_keys order; also ensure we output exactly loss_domain_1..5\n        for idx, tk in enumerate(fm.target_keys):\n            pred[tk] = float(yhat[idx])\n        # If any expected loss_domain_i missing (e.g., dataset had different naming), fill canonical keys\n        for i in range(1, 6):\n            key = f\"loss_domain_{i}\"\n            if key not in pred and idx < yhat.size:\n                pred[key] = float(yhat[min(i - 1, yhat.size - 1)])\n        out.append(pred)\n\n    return out", "agent_name": "aider", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": -1.0, "solution": "import json\nimport numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n\n    \"\"\"    \n    epsilon = 1e-6 # Must match the epsilon used during fitting\n    \n    # Fitted parameters for each group and domain\n    _fitted_params = json.loads( + fitted_params_json_str + '')\n\n    if group not in _fitted_params:\n        raise ValueError(f\"Unknown group: {{group}}\")\n\n    predictions = []\n    for data_point in input_data:\n        predicted_losses = {{}}\n        for domain_idx in range(1, 6): # Domains 1 to 5\n            proportion_key = f'proportion_domain_{{domain_idx}}'\n            loss_key = f'loss_domain_{{domain_idx}}'\n            \n            if proportion_key not in data_point:\n                predicted_losses[loss_key] = float('nan') \n                continue\n\n            proportion = data_point[proportion_key]\n            \n            domain_str_idx = str(domain_idx)\n            if domain_str_idx not in _fitted_params[group]:\n                predicted_losses[loss_key] = float('nan') \n                continue\n\n            params = _fitted_params[group][domain_str_idx]\n            A = params['A']\n            B = params['B']\n            C = params['C']\n\n            if np.isnan(A) or np.isnan(B) or np.isnan(C):\n                predicted_losses[loss_key] = float('nan') # No valid parameters\n                continue\n\n            # Apply the scaling law, ensuring (proportion + epsilon) is positive\n            predicted_loss = A * (np.maximum(proportion + epsilon, epsilon))**B + C\n            predicted_losses[loss_key] = predicted_loss\n        predictions.append(predicted_losses)\n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": -1.0, "solution": "from __future__ import annotations\nfrom typing import List, Dict\nimport math\n\n# Embedded fitted parameters per group and domain.\n# Each domain uses either a power form: y = a + k * p**c\n# or a log form: y = a + b * log(p)\nPARAMS = {'160M': {'1': {'a': 3.358131379887285,\n                'c': 0.15000000000000036,\n                'form': 'power',\n                'k': -1.1267042149203725,\n                'r2': 0.9941862658661204},\n          '2': {'a': 3.318780734454316,\n                'c': -3.0,\n                'form': 'power',\n                'k': 2.2734784837905097e-05,\n                'r2': 0.06062614424441071},\n          '3': {'a': 2.931566771454378, 'c': 3.0, 'form': 'power', 'k': -174.62340395796082, 'r2': 0.7061503010752743},\n          '4': {'a': 3.8716421496206315,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.6874577135715296,\n                'r2': 0.9762826201636258},\n          '5': {'a': 3.876397587476255,\n                'c': 0.20000000000000018,\n                'form': 'power',\n                'k': -0.8015051387810923,\n                'r2': 0.995953951272856}},\n '305M': {'1': {'a': 4.832122416331462,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.721490588848711,\n                'r2': 0.9923678438528458},\n          '2': {'a': 3.1584886677226094,\n                'c': -2.75,\n                'form': 'power',\n                'k': 4.513541219029818e-05,\n                'r2': 0.066949255913345},\n          '3': {'a': 2.7848933137166916, 'c': 3.0, 'form': 'power', 'k': -166.12136907423988, 'r2': 0.6853980994244002},\n          '4': {'a': -0.9540330670713865,\n                'c': -0.04999999999999982,\n                'form': 'power',\n                'k': 2.067152378143916,\n                'r2': 0.9773900180368574},\n          '5': {'a': 5.599492209154898,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.6836119274365684,\n                'r2': 0.9937674933260627}},\n '410M': {'1': {'a': 4.670978316018143,\n                'c': 0.050000000000000266,\n                'form': 'power',\n                'k': -2.613131878961905,\n                'r2': 0.9885683848277665},\n          '2': {'a': 3.091279676924703,\n                'c': -3.0,\n                'form': 'power',\n                'k': 1.7491883130243175e-05,\n                'r2': 0.03333320137761886},\n          '3': {'a': 2.7201830236984295, 'c': 3.0, 'form': 'power', 'k': -168.02135230322037, 'r2': 0.6489941549529987},\n          '4': {'a': 0.5427391497008389,\n                'c': -0.1499999999999999,\n                'form': 'power',\n                'k': 0.5526416189568344,\n                'r2': 0.9732551074881751},\n          '5': {'a': 3.634036329340057,\n                'c': 0.20000000000000018,\n                'form': 'power',\n                'k': -0.8017750604189218,\n                'r2': 0.9939499265653914}},\n '70M': {'1': {'a': 4.297380991024045,\n               'c': 0.10000000000000009,\n               'form': 'power',\n               'k': -1.775827036691083,\n               'r2': 0.9952894285760332},\n         '2': {'a': 3.646005195575224,\n               'c': -2.6,\n               'form': 'power',\n               'k': 8.526182739288986e-05,\n               'r2': 0.12259002392235419},\n         '3': {'a': 3.2284514219048646, 'c': 3.0, 'form': 'power', 'k': -180.79876453488362, 'r2': 0.6915505904622448},\n         '4': {'a': 1.1444240089156006, 'c': -0.25, 'form': 'power', 'k': 0.3211850398410141, 'r2': 0.9856867978166857},\n         '5': {'a': 2.3099069373584133,\n               'c': -0.09999999999999964,\n               'form': 'power',\n               'k': 1.1287948701911399,\n               'r2': 0.9930440593027846}}}\n\n# Default fallback params (averaged) if group not present\nFALLBACK = {'1': {'a': 4.289653275815233, 'c': 0.08750000000000024, 'form': 'power', 'k': -2.059288429855518},\n '2': {'a': 3.3036385686692133, 'c': -2.8375, 'form': 'power', 'k': 4.265597688783408e-05},\n '3': {'a': 2.9162736326935907, 'c': 3.0, 'form': 'power', 'k': -172.39122246757617},\n '4': {'a': 1.151193060291421, 'c': -0.09999999999999987, 'form': 'power', 'k': 0.06338033084255867},\n '5': {'a': 3.8549582658324058, 'c': 0.08750000000000024, 'form': 'power', 'k': -0.7895243141113606}}\n\nDOMAINS = [1,2,3,4,5]\n\ndef _predict_domain(p: float, spec: dict) -> float:\n    # Guard against non-positive proportions for log and power\n    p = max(p, 1e-12)\n    form = spec.get('form')\n    if form == 'power':\n        return spec['a'] + spec['k'] * (p ** spec['c'])\n    elif form == 'log':\n        return spec['a'] + spec['b'] * math.log(p)\n    else:\n        # Should not happen; fall back to identity-like\n        return float('nan')\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: list of dicts with keys proportion_domain_1..5\n        group: group name. Functional form is same across groups (per-domain chosen from {power, log}); coefficients differ per group.\n\n    Returns:\n        list of dicts with keys loss_domain_1..5\n    \"\"\"\n    # Select group-specific params or fallback\n    spec = PARAMS.get(group, FALLBACK)\n    outputs = []\n    for row in input_data:\n        out = {}\n        for i in DOMAINS:\n            p = float(row.get(f'proportion_domain_{i}', 0.0))\n            out[f'loss_domain_{i}'] = _predict_domain(p, spec[str(i)])\n        outputs.append(out)\n    return outputs", "agent_name": "terminus-2", "task": "domain_mixture_scaling_law"}
{"model_name": "human", "reward_r2": 0.6711173709701893, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on the Domain Mixture Scaling Law.\n    Formula: L_i(r) = c_i + k_i * exp(sum_j(t_ij * r_j))\n    \"\"\"\n    PARAMS_BY_GROUP = {\n        '160M': [2.0856592097019804, 3.4111663782049946, 2.373130628164121, 1.115739002392498, 3.1434027050275315, 0.11985664504599375, -1.290497011833889, 0.8561546902084409, 0.0011385760402376783, 0.10773090897017512, -1.2957351800303398, 2.2746028871034647, 2.705623972785463, 2.316064861889176, 1.7981591134647914, 0.16002919107434516, -0.9036553611667323, -11.721095107645542, -10.97207094938961, -3.941965495927712, 0.3331004993016095, -0.8238306927391251, -7.9040599335438575, 0.22865320483207682, -0.015871079782087898, 6.356978958687983, 6.947613311543433, 6.828600154978058, 0.4033417288280921, 6.46043211334845, 1.1465685324225992, 4.224797824743394, -0.0873484493498842, 0.6586482763159163, -7.468434356093965],\n        '305M': [-0.406920519860249, -0.023482654768083127, 1.7584418964771162, 0.6696472032560989, 2.9621025231919225, 2.476937360825574, 3.942936207746321, 1.4901046642087359, 0.2742018286545925, 0.47308511801263603, -0.2365071111423701, 0.39095328391845285, 0.09758796539876204, 0.3506186812670231, 0.1972447482527336, -0.1387772197575137, -0.4345549940094546, -0.22027486679158964, -0.09838164990815944, -0.1903816656628244, 0.011876178764682731, -0.12147137554315933, -5.287503842834289, -0.0011619872771040376, -0.13970271686042884, 1.318799534922482, 1.5105795629915502, 1.456323279420494, -1.7786459949928333, 1.3304983402239023, 0.4281155226864253, 0.6804235220347148, -0.7530897211290078, -0.9933045226536323, -6.868138266660191],\n        '410M': [2.4216190161135236, -0.9949865207367925, 1.0485818312964124, 2.548675892146101, 2.7426928782936613, -1.9627602023940005, 1.402807376566214, 0.7417553689817263, -1.4145523926963819, 0.15508511422938762, -3.3903042944676276, -2.2610029749001375, -1.648920829689558, -2.5443645331878497, -8.53221429654514, 1.1289545539226238, 0.8980442879362072, 1.0234700780117505, 1.1650251900973105, 1.0860734000230958, 1.1534440167385025, 0.780721755668409, -2.212355347968287, 0.9803499956087594, 0.9766445881845902, -0.2419034983363214, -0.8751331448570513, -0.6202279643467626, 1.1901651070277095, -0.35615506056271157, 0.022752946896432296, 3.6721657643208983, -0.779545468876064, 1.776335708316458, -0.6357803386102799],\n        '70M': [1.397924353380805, 3.3600165973585336, 3.185343596637166, 0.4140345749996855, 2.686572212421311, 1.1495633562636562, 0.33362402415104286, 0.11913150425199642, 2.09383061022326, 0.7647608278389397, -0.5528008166747819, 0.7482607813260194, 0.3093620764866142, 0.6870023027892853, 0.40095800858226355, 0.2667574293187739, -0.6615453308993259, -2.4866382361709993, 1.0054990681290112, 0.10160933605146799, 2.165839466984036, -5.798339327373564, 0.914108528350298, 0.08116908389843563, 0.29052400025512765, -0.22540170902766082, -0.07188913043349504, -0.2619900024414107, -2.1812489268120494, -0.20593534390953777, 0.3037987618175764, 0.1976238605669583, 0.3676837669324082, 0.5677517222931325, -0.09463528681761862],\n    }\n\n    if group in PARAMS_BY_GROUP:\n        flat_params = np.array(PARAMS_BY_GROUP[group])\n    else:\n        flat_params = np.array(PARAMS_BY_GROUP['160M'])\n\n    num_domains = 5\n    c = flat_params[0:num_domains]\n    k = flat_params[num_domains:2*num_domains]\n    t = flat_params[2*num_domains:].reshape((num_domains, num_domains))\n    \n    predictions = []\n    for point in input_data:\n        r = np.array([point[f\"proportion_domain_{i+1}\"] for i in range(num_domains)])\n        \n        exponent = np.dot(r, t.T)\n        losses = c + k * np.exp(exponent)\n        \n        pred_dict = {f\"loss_domain_{i+1}\": losses[i] for i in range(num_domains)}\n        predictions.append(pred_dict)\n        \n    return predictions", "agent_name": "human", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.9885521372685011, "solution": "import numpy as np\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    \n    # Parameters fitted for each group and domain.\n    # Format: [a, b, c, d, w]\n    # Law: L = a + b * (p_target + w * p_helper + c)**(-d)\n    # Helper is always p_5 for domains 2 and 3. Others have w=0.\n    \n    PARAMS = {\n        \"70M\": {\n            \"loss_domain_1\": [1.28685e-17, 2.54486, 0.00502, 0.05555, 0.0],\n            \"loss_domain_2\": [3.34421, 0.15937, 0.47742, 2.0, 0.15648],\n            \"loss_domain_3\": [2.82328, 0.10889, 0.00744, 0.43747, 0.00642],\n            \"loss_domain_4\": [1.25901, 0.21221, 0.01170, 0.35014, 0.0],\n            \"loss_domain_5\": [3.21985, 0.23277, 0.10174, 0.48926, 0.0]\n        },\n        \"160M\": {\n            \"loss_domain_1\": [4.85379e-21, 2.26031, 0.00383, 0.05446, 0.0],\n            \"loss_domain_2\": [2.99668, 0.19029, 0.51536, 2.0, 0.17996],\n            \"loss_domain_3\": [2.70420, 0.00367, 0.05282, 1.81292, 0.01438],\n            \"loss_domain_4\": [5.24642e-16, 1.21321, 0.00377, 0.08624, 0.0],\n            \"loss_domain_5\": [4.30158e-07, 3.08654, 0.03319, 0.04470, 0.0]\n        },\n        \"305M\": {\n            \"loss_domain_1\": [4.82818e-24, 2.12339, 0.00347, 0.05492, 0.0],\n            \"loss_domain_2\": [2.79204, 0.28547, 0.62398, 2.0, 0.18532],\n            \"loss_domain_3\": [1.79140, 0.70480, 0.00055, 0.10372, 0.00203],\n            \"loss_domain_4\": [1.58820e-05, 1.11756, 0.00370, 0.08836, 0.0],\n            \"loss_domain_5\": [1.29089, 1.63112, 0.03311, 0.08014, 0.0]\n        },\n        \"410M\": {\n            \"loss_domain_1\": [5.23865e-12, 2.07017, 0.00302, 0.05401, 0.0],\n            \"loss_domain_2\": [2.75615, 0.22437, 0.55907, 2.0, 0.19794],\n            \"loss_domain_3\": [2.26414, 0.19735, 0.00194, 0.25862, 0.00319],\n            \"loss_domain_4\": [0.74111, 0.35914, 0.00747, 0.21677, 0.0],\n            \"loss_domain_5\": [1.04167e-22, 2.84373, 0.02736, 0.04749, 0.0]\n        }\n    }\n\n    if group not in PARAMS:\n        # If group is unknown, we cannot predict accurately.\n        # But to avoid crashing, we could return empty or raise.\n        # Assuming inputs are valid groups.\n        return []\n\n    group_params = PARAMS[group]\n    output = []\n\n    for item in input_data:\n        prediction = {}\n        # We need p_5 for transfer\n        p5 = item.get(\"proportion_domain_5\", 0.0)\n\n        for i in range(1, 6):\n            domain_out = f\"loss_domain_{i}\"\n            domain_in = f\"proportion_domain_{i}\"\n            \n            if domain_in in item:\n                pi = item[domain_in]\n                \n                # Get params\n                if domain_out in group_params:\n                    a, b, c, d, w = group_params[domain_out]\n                    \n                    # Apply law\n                    # Effective proportion\n                    eff_p = pi + w * p5\n                    \n                    # Prevent negative base or div by zero?\n                    # c is positive, pi >=0, w >=0, p5 >=0. So eff_p + c > 0.\n                    val = a + b * np.power(eff_p + c, -d)\n                    \n                    prediction[domain_out] = float(val)\n        \n        output.append(prediction)\n\n    return output", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.9708536087897284, "solution": "import math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters discovered via curve fitting: A, alpha, C, E\n    # Law: Loss = A * (proportion + C)^(-alpha) + E\n    params = {\n        '70M': {\n            'domain_1': [2.544863245931729, 0.05554674100269151, 0.005023335428326843, 7.354761871034508e-09],\n            'domain_2': [2.328434203835545, 0.02123343574996181, 0.00854534437460059, 1.2422235452349377],\n            'domain_3': [2.9005815707219877, 0.03194625554943617, 0.0011510647218494945, 1.9331509019884389e-07],\n            'domain_4': [0.2122063465420956, 0.35014375289907435, 0.011700352368546284, 1.2590170530433957],\n            'domain_5': [0.23277117995017496, 0.48926378841783125, 0.10173869442989261, 3.219854098050667],\n        },\n        '160M': {\n            'domain_1': [2.260306114375484, 0.0544641534958049, 0.0038345631520252904, 8.78218106835147e-10],\n            'domain_2': [2.1312566383114793, 0.01621258677221147, 0.003207834832149392, 1.1329419878925446],\n            'domain_3': [2.631246480348673, 0.03188905036491964, 0.0009457344532357482, 4.007467046755441e-07],\n            'domain_4': [1.2132115806048511, 0.08623842876745363, 0.0037719207121529636, 7.158028650625971e-11],\n            'domain_5': [3.0865366505971625, 0.044702142600224586, 0.03318950924069973, 2.179754450938945e-13],\n        },\n        '305M': {\n            'domain_1': [2.123388565217091, 0.054917406969647614, 0.0034713501489994207, 2.680543544782964e-09],\n            'domain_2': [1.9162221255354674, 0.01945774558297464, 0.005316673787595095, 1.184406590917298],\n            'domain_3': [2.5103459999510207, 0.030293831569732775, 0.0005254616180943917, 2.3382457164336946e-14],\n            'domain_4': [1.1175732215908694, 0.08835466333754133, 0.0036981454063375048, 2.135867474030756e-16],\n            'domain_5': [1.6311941950377935, 0.08014103185095545, 0.03310462738510677, 1.2908178612880676],\n        },\n        '410M': {\n            'domain_1': [2.0701724665116203, 0.05400692511646882, 0.0030243880170281214, 3.487529395875562e-08],\n            'domain_2': [1.9193311600122958, 0.01373092790066736, 0.0014229018601607025, 1.1301787253891094],\n            'domain_3': [2.444041703536225, 0.031212358228236488, 0.0005010699556192065, 1.6422736233332864e-10],\n            'domain_4': [0.35914480212753996, 0.21677184796638155, 0.007467416956943246, 0.7411062242277467],\n            'domain_5': [2.8437345214097007, 0.04749488009626055, 0.0273606315399037, 2.535606362495549e-19],\n        },\n    }\n\n    if group not in params:\n        raise ValueError(f\"Unknown group: {group}\")\n        \n    group_params = params[group]\n    predictions = []\n    \n    for row in input_data:\n        pred_row = {}\n        for i in range(1, 6):\n            input_key = f'proportion_domain_{i}'\n            if input_key not in row:\n                continue\n                \n            p = row[input_key]\n            A, alpha, C, E = group_params[f'domain_{i}']\n            \n            # Law: L = A * (p + C)^(-alpha) + E\n            loss = A * math.pow(p + C, -alpha) + E\n            pred_row[f'loss_domain_{i}'] = loss\n            \n        predictions.append(pred_row)\n        \n    return predictions", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.9708536071256871, "solution": "\nimport math\n\ndef law(input_data: list[dict[str, float]], group: str) -> list[dict[str, float]]:\n    \"\"\"\n    Predicts output variables based on input variables according to a discovered scaling law.\n\n    Args:\n        input_data: A list of dictionaries, where each dictionary is a single data\n                    point containing input variable names as keys and their\n                    corresponding values.\n        group: The name of the experimental group for which to make predictions.\n                The functional form of the law must be the same for all groups,\n                but the constant parameters/coefficients can differ per group.\n\n    Returns:\n        A list of dictionaries, corresponding to the input_data list, with each\n        dictionary containing the predicted output variable(s).\n    \"\"\"\n    # Parameters fitted for each group and domain\n    # Form: Loss = A * (p + delta)**(-alpha) + E\n    # Key: group_name -> domain_id -> [A, alpha, delta, E]\n    params = {\n        \"70M\": {\n            \"1\": [2.5448632550909993, 0.055546740323293695, 0.005023335212697448, 4.4563669104969185e-12],\n            \"2\": [0.6461946152447611, 0.07062329004128731, 0.010709178470767525, 2.928186042585277],\n            \"3\": [2.900581605391343, 0.031946252913671876, 0.001151064358603794, 1.7847818698734158e-07],\n            \"4\": [0.21220905975144264, 0.3501408975254289, 0.011700253549943637, 1.2590139374178166],\n            \"5\": [0.2327713442451448, 0.4892634693501254, 0.10173863849569124, 3.2198539247905726]\n        },\n        \"160M\": {\n            \"1\": [2.2603061161574427, 0.054464153214761425, 0.0038345630717721456, 3.907341453444824e-13],\n            \"2\": [0.71722888690744, 0.04569287087105722, 0.003966367143574972, 2.5487284170918914],\n            \"3\": [2.631245887253616, 0.03188905132748203, 0.0009457337326215441, 1.0403371166221724e-06],\n            \"4\": [1.213210066323781, 0.0862385316754544, 0.00377192478148179, 1.5040877974866357e-06],\n            \"5\": [3.0865366501898333, 0.04470214293615903, 0.03318950988926205, 2.849363777421895e-15]\n        },\n        \"305M\": {\n            \"1\": [2.123388398369909, 0.05491741139352793, 0.0034713503951583715, 1.6854721793507218e-07],\n            \"2\": [0.6237312670560022, 0.05613001907074531, 0.006561266909172867, 2.479137032252236],\n            \"3\": [2.5103455566299795, 0.030293866336527964, 0.0005254643252004994, 2.174309752511172e-07],\n            \"4\": [1.1175731083557974, 0.08835467121548282, 0.00369814562013109, 1.1418759573260478e-07],\n            \"5\": [1.6311292531301693, 0.08014408990855902, 0.03310502511246028, 1.2908829207833705]\n        },\n        \"410M\": {\n            \"1\": [2.0701727489463453, 0.05400686898862725, 0.003024380399030043, 3.6049284590509236e-10],\n            \"2\": [0.5667574865317554, 0.04393228202392226, 0.001916292794791801, 2.484197072563381],\n            \"3\": [2.4440410743959666, 0.031212404800006213, 0.000501073290224436, 3.40102062599206e-07],\n            \"4\": [0.3591479966659808, 0.21677044311484203, 0.007467374109897844, 0.7411028251415225],\n            \"5\": [2.843734521187899, 0.047494880487393594, 0.02736063264495845, 6.30916692045784e-14]\n        }\n    }\n\n    results = []\n    \n    if group not in params:\n        # Fallback or error if group not found. \n        # For this challenge, we assume the groups are known or we might return empty/error.\n        # But let's try to match the closest group or just fail gracefully?\n        # Given strict constraints, returning empty list or letting it crash might be expected behavior \n        # for unknown groups unless we implemented interpolation.\n        # I'll assume valid input.\n        return []\n\n    group_params = params[group]\n\n    for item in input_data:\n        prediction = {}\n        for i in range(1, 6):\n            domain_key = str(i)\n            prop_key = f\"proportion_domain_{i}\"\n            loss_key = f\"loss_domain_{i}\"\n            \n            if prop_key in item:\n                p = item[prop_key]\n                if domain_key in group_params:\n                    A, alpha, delta, E = group_params[domain_key]\n                    # Law: L = A * (p + delta)**(-alpha) + E\n                    loss = A * math.pow(p + delta, -alpha) + E\n                    prediction[loss_key] = loss\n                else:\n                    prediction[loss_key] = 0.0 # Should not happen based on problem desc\n        results.append(prediction)\n\n    return results", "agent_name": "gemini-cli", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.991244, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning\nDual power law with improved parameter scaling and data-driven initialization\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Dual power law: pred_d = sum_i(c1_i * x_i^a1_i + c2_i * x_i^a2_i) + b_d\n    Parameters (25): c1(5) + a1(5) + c2(5) + a2(5) + b(5)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64)\n    \n    if params.ndim == 1:\n        params = params[None, :]\n    T, P = params.shape\n    \n    c1 = params[:, :5]\n    a1 = np.clip(params[:, 5:10], -1.0, 1.5)\n    c2 = params[:, 10:15]\n    a2 = np.clip(params[:, 15:20], -1.0, 1.5)\n    b = params[:, 20:25]\n    \n    X_safe = np.clip(X, 1e-8, 1.0)\n    \n    term1 = c1[None, :, :] * np.power(X_safe[:, None, :], a1[None, :, :])\n    term2 = c2[None, :, :] * np.power(X_safe[:, None, :], a2[None, :, :])\n    \n    pred = np.sum(term1 + term2, axis=2) + b[None, :, 0]\n    \n    return pred[:, 0] if pred.shape[1] == 1 else pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit with normalized parameters and smart initialization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.asarray(loss_values, dtype=np.float64)\n    N, F = X.shape\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    T = y.shape[1]\n    \n    P = 25\n    \n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0) + 1e-6\n    y_min = np.min(y, axis=0)\n    y_max = np.max(y, axis=0)\n    X_mean = np.mean(X, axis=0)\n    \n    bounds = (\n        [(-2.0, 3.0)] * 5 +\n        [(-1.0, 1.5)] * 5 +\n        [(-2.0, 3.0)] * 5 +\n        [(-1.0, 1.5)] * 5 +\n        [(y_min[i]-0.5, y_max[i]+0.5) for i in range(T)]\n    )\n    \n    def objective(flat_params):\n        params = flat_params.reshape(T, P)\n        try:\n            pred = scaling_law_func(X, params)\n            if pred.shape != y.shape:\n                return 1e10\n            mse = np.mean((pred - y) ** 2)\n            return mse if np.isfinite(mse) else 1e10\n        except:\n            return 1e10\n    \n    best_params = None\n    best_loss = float('inf')\n    \n    # 8 focused initialization strategies\n    for trial in range(8):\n        init = np.zeros((T, P))\n        \n        if trial == 0:\n            # Smart data-driven: per-domain analysis\n            for t in range(T):\n                coeff_scale = y_mean[t] / (np.sum(X_mean) + 1e-8)\n                init[t, :5] = coeff_scale * X_mean\n                init[t, :5] = np.clip(init[t, :5], -2.0, 3.0)\n                init[t, 5:10] = 0.45\n                init[t, 10:15] = 0.15 * init[t, :5]\n                init[t, 15:20] = 0.75\n                init[t, 20:25] = y_mean[t]\n        \n        elif trial == 1:\n            # Strong primary, weak secondary\n            for t in range(T):\n                init[t, :5] = np.random.uniform(0.6, 2.2, 5)\n                init[t, 5:10] = 0.35 + np.random.randn(5) * 0.08\n                init[t, 10:15] = np.random.uniform(-0.3, 0.3, 5)\n                init[t, 15:20] = 0.95 + np.random.randn(5) * 0.08\n                init[t, 20:25] = y_mean[t]\n        \n        elif trial == 2:\n            # Balanced dual\n            for t in range(T):\n                init[t, :5] = np.random.uniform(0.4, 1.4, 5)\n                init[t, 5:10] = 0.5 + np.random.randn(5) * 0.08\n                init[t, 10:15] = np.random.uniform(0.4, 1.4, 5)\n                init[t, 15:20] = 0.65 + np.random.randn(5) * 0.08\n                init[t, 20:25] = y_mean[t]\n        \n        elif trial == 3:\n            # High exponents (steeper curves)\n            for t in range(T):\n                init[t, :5] = np.random.uniform(0.3, 0.9, 5)\n                init[t, 5:10] = 0.95 + np.random.randn(5) * 0.08\n                init[t, 10:15] = np.random.uniform(0.2, 0.6, 5)\n                init[t, 15:20] = 1.15 + np.random.randn(5) * 0.08\n                init[t, 20:25] = y_mean[t]\n        \n        elif trial == 4:\n            # Low exponents (flatter curves)\n            for t in range(T):\n                init[t, :5] = np.random.uniform(0.9, 2.8, 5)\n                init[t, 5:10] = 0.15 + np.random.randn(5) * 0.04\n                init[t, 10:15] = np.random.uniform(0.6, 1.4, 5)\n                init[t, 15:20] = 0.35 + np.random.randn(5) * 0.04\n                init[t, 20:25] = y_mean[t]\n        \n        elif trial == 5:\n            # Feature-weighted with variance\n            for t in range(T):\n                feature_w = X_mean / (np.sum(X_mean) + 1e-8)\n                init[t, :5] = (y_mean[t] / 5.0) * (1.0 + feature_w)\n                init[t, 5:10] = 0.45 + 0.15 * np.random.randn(5)\n                init[t, 10:15] = (y_mean[t] / 15.0) * feature_w\n                init[t, 15:20] = 0.75 + 0.15 * np.random.randn(5)\n                init[t, 20:25] = y_mean[t] + 0.05 * y_std[t] * np.random.randn(5)\n        \n        elif trial == 6:\n            # Mixed negative coefficients\n            for t in range(T):\n                init[t, :5] = np.random.uniform(-0.8, 1.2, 5)\n                init[t, 5:10] = 0.4 + np.random.randn(5) * 0.1\n                init[t, 10:15] = np.random.uniform(0.4, 1.8, 5)\n                init[t, 15:20] = 0.6 + np.random.randn(5) * 0.1\n                init[t, 20:25] = y_mean[t]\n        \n        else:  # trial == 7\n            # Conservative uniform\n            for t in range(T):\n                init[t, :5] = np.ones(5) * (y_mean[t] / 5.5)\n                init[t, 5:10] = np.ones(5) * 0.5\n                init[t, 10:15] = np.ones(5) * (y_mean[t] / 18.0)\n                init[t, 15:20] = np.ones(5) * 0.7\n                init[t, 20:25] = np.ones(5) * y_mean[t]\n        \n        result = minimize(\n            objective,\n            init.ravel(),\n            method='L-BFGS-B',\n            bounds=bounds * T,\n            options={'maxiter': 1500, 'ftol': 1e-10, 'gtol': 1e-8, 'maxcor': 30}\n        )\n        \n        if result.fun < best_loss:\n            best_loss = result.fun\n            best_params = result.x.reshape(T, P)\n    \n    if best_params is None:\n        init = np.zeros((T, P))\n        for t in range(T):\n            init[t, :5] = 0.5\n            init[t, 5:10] = 0.5\n            init[t, 10:15] = 0.2\n            init[t, 15:20] = 0.8\n            init[t, 20:25] = y_mean[t]\n        best_params = init\n    \n    return best_params[0] if T == 1 else best_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.986346, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with two-stage optimization and refined regularization\nImproves upon 0.9830 through adaptive coefficient weighting and better convergence\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Scaling law: loss_i = base_i + sum_j(coeff_ij * proportion_j^exp_j)\n    params: [base(5), coeffs(25), exponents(5)] = 35 params\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).ravel()\n    \n    base = params[:5]\n    coeffs = params[5:30].reshape(5, 5)\n    exp_raw = params[30:35]\n    \n    # Tanh-based exponent transformation for smoother gradients\n    exponents = 0.5 + 0.75 * np.tanh(exp_raw / 1.5)\n    \n    X_safe = np.clip(X, 1e-8, 1.0)\n    powers = X_safe ** exponents[None, :]\n    \n    return base[None, :] + np.dot(powers, coeffs.T)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit with two-stage optimization and adaptive per-dimension regularization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.atleast_2d(np.asarray(loss_values))\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0) + 1e-8\n    y_min, y_max = np.min(y, axis=0), np.max(y, axis=0)\n    \n    # Initialize base\n    init_base = y_mean.copy()\n    \n    # Initialize coefficients with robust correlation and range scaling\n    X_safe = np.clip(X, 1e-8, 1.0)\n    init_coeffs = np.zeros((5, 5))\n    \n    for i in range(5):\n        for j in range(5):\n            try:\n                corr = np.corrcoef(X_safe[:, j], y[:, i])[0, 1]\n                corr = 0.0 if np.isnan(corr) else corr\n            except:\n                corr = 0.0\n            \n            x_range = np.ptp(X_safe[:, j]) + 1e-8\n            y_range = y_max[i] - y_min[i] + 1e-8\n            init_coeffs[i, j] = np.clip(corr * y_range / (x_range * 5.0) * 0.7, -2.0, 2.0)\n    \n    init_params = np.concatenate([init_base, init_coeffs.ravel(), np.zeros(5)])\n    \n    bounds = [(1.0, 5.0)] * 5 + [(-2.5, 2.5)] * 25 + [(-2.0, 2.0)] * 5\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        residuals = pred - y\n        mse_per_dim = np.mean(residuals ** 2, axis=0)\n        \n        # Adaptive dimension weighting with exponential focus\n        dim_diffs = mse_per_dim - np.min(mse_per_dim) + 1e-10\n        max_diff = np.max(dim_diffs) + 1e-8\n        dim_weights = np.exp(1.5 * dim_diffs / max_diff)\n        dim_weights = dim_weights / np.sum(dim_weights)\n        mse = np.sum(mse_per_dim * dim_weights * 5.0)\n        \n        # Extract parameters\n        coeffs = params[5:30].reshape(5, 5)\n        exp_raw = params[30:35]\n        base = params[:5]\n        \n        # Adaptive regularization: weight by per-dimension error\n        reg_weights = np.clip(mse_per_dim / (np.mean(mse_per_dim) + 1e-8), 0.5, 2.0)\n        \n        # L2 regularization on coefficients with per-dimension weighting\n        coeff_l2 = np.sum((coeffs ** 2) * reg_weights[:, None])\n        coeff_reg = 0.005 * coeff_l2 / 5.0\n        \n        # L1 (sparsity) regularization\n        sparsity_reg = 0.0015 * np.sum(np.abs(coeffs))\n        \n        # Exponent regularization: keep near linear (exp=1.0)\n        exp_reg = 0.0025 * np.sum(exp_raw ** 2)\n        \n        # Base regularization: anchor to mean\n        base_reg = 0.0006 * np.sum((base - y_mean) ** 2)\n        \n        return mse + coeff_reg + sparsity_reg + exp_reg + base_reg\n    \n    # Stage 1: Coarse optimization to escape local minima\n    result1 = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 800, 'ftol': 1e-6, 'gtol': 1e-5}\n    )\n    \n    params_stage1 = result1.x if result1.success else init_params\n    \n    # Stage 2: Fine-grained optimization from stage 1 result\n    result2 = minimize(\n        objective,\n        params_stage1,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 1200, 'ftol': 1e-8, 'gtol': 1e-7}\n    )\n    \n    best_params = result2.x if result2.success else params_stage1\n    \n    return best_params\n\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.983931, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nOptimized scaling law for multi-domain LLM finetuning\nHybrid model combining selective interactions with adaptive entropy weighting\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Multi-domain scaling law with selective interactions and adaptive weighting.\n    \n    Model structure (26 parameters):\n    - Base losses (5): per-domain baseline\n    - Power law coefficients (5): amplitude per domain\n    - Power law exponents (5): exponent per domain\n    - Selective interactions (6): key cross-domain coupling\n    - Domain entropy weights (3): selective entropy modulation\n    - Temperature (1): global scaling\n    - Offset (1): global shift\n    Total: 26 parameters\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    params = np.asarray(params, dtype=np.float64)\n    N, F = X.shape\n    \n    if F != 5:\n        raise ValueError(f\"Expected 5 domains, got {F}\")\n    \n    # Parse parameters with optimized clipping\n    base_loss = params[0:5]\n    coeff = np.clip(params[5:10], -4.5, 4.5)\n    exponents = np.clip(params[10:15], 0.15, 2.3)\n    interactions = np.clip(params[15:21], -1.8, 1.8)\n    entropy_weights = np.clip(params[21:24], -0.2, 0.2)\n    temperature = np.clip(params[24], 0.75, 1.25)\n    offset = params[25]\n    \n    # Safe input clamping\n    X_safe = np.clip(X, 1e-8, 1.0)\n    \n    # Initialize predictions\n    pred = np.tile(base_loss[None, :], (N, 1))\n    \n    # Power law terms with clipping\n    for i in range(5):\n        power_term = coeff[i] * (X_safe[:, i] ** exponents[i])\n        pred[:, i] += np.clip(power_term, -800.0, 800.0)\n    \n    # Selective pairwise interactions (key domain pairs)\n    inter_pairs = [(0, 1), (1, 2), (2, 3), (3, 4), (0, 4), (2, 4)]\n    for idx, (i, j) in enumerate(inter_pairs):\n        inter_term = interactions[idx] * X_safe[:, i] * X_safe[:, j]\n        pred[:, i] += inter_term * 0.45\n        pred[:, j] += inter_term * 0.45\n    \n    # Adaptive entropy-based modulation on key domains\n    eps = 1e-10\n    entropy = -np.sum(X_safe * np.log(X_safe + eps), axis=1, keepdims=True)\n    entropy_norm = entropy / (np.log(5.0) + eps)\n    \n    # Apply entropy weights selectively to domains 1, 2, 3\n    entropy_mod = 1.0 + entropy_weights[0] * (entropy_norm - 0.5)\n    pred[:, 1] *= entropy_mod.ravel()\n    \n    entropy_mod = 1.0 + entropy_weights[1] * (entropy_norm - 0.5)\n    pred[:, 2] *= entropy_mod.ravel()\n    \n    entropy_mod = 1.0 + entropy_weights[2] * (entropy_norm - 0.5)\n    pred[:, 3] *= entropy_mod.ravel()\n    \n    # Global scaling\n    pred = temperature * pred + offset\n    \n    return np.clip(pred, 0.6, 5.4)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimized fitting with enhanced initialization and refined optimization.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N = X.shape[0]\n    n_params = 26\n    \n    def objective(params):\n        \"\"\"MSE objective with safety checks\"\"\"\n        try:\n            pred = scaling_law_func(X, params)\n            if pred.shape != y.shape or not np.all(np.isfinite(pred)):\n                return 1e8\n            mse = np.mean((pred - y) ** 2)\n            return min(mse, 1e8)\n        except:\n            return 1e8\n    \n    bounds = [\n        (1.5, 4.5), (1.5, 4.5), (1.5, 4.5), (1.5, 4.5), (1.5, 4.5),  # base_loss\n        (-4.5, 4.5), (-4.5, 4.5), (-4.5, 4.5), (-4.5, 4.5), (-4.5, 4.5),  # coeff\n        (0.15, 2.3), (0.15, 2.3), (0.15, 2.3), (0.15, 2.3), (0.15, 2.3),  # exponents\n        (-1.8, 1.8), (-1.8, 1.8), (-1.8, 1.8), (-1.8, 1.8), (-1.8, 1.8),  # interactions\n        (-1.8, 1.8),\n        (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2),  # entropy_weights\n        (0.75, 1.25),  # temperature\n        (-0.8, 0.8),  # offset\n    ]\n    \n    # Enhanced initialization from data statistics\n    params_init = np.zeros(n_params)\n    \n    # Base loss: per-domain mean\n    params_init[0:5] = np.mean(y, axis=0)\n    \n    # Coefficients: refined correlation-based initialization\n    for i in range(5):\n        valid = X[:, i] > 0.02\n        if np.sum(valid) > 2:\n            x_var = np.var(X[valid, i]) + 1e-8\n            y_var = np.var(y[valid, :]) + 1e-8\n            corr = np.corrcoef(X[valid, i], y[valid, i])[0, 1]\n            if not np.isnan(corr):\n                params_init[5 + i] = np.clip(corr * np.sqrt(y_var / x_var), -2.5, 2.5)\n            else:\n                params_init[5 + i] = 0.4\n        else:\n            params_init[5 + i] = 0.4\n    \n    # Exponents: start with 1.0\n    params_init[10:15] = 1.0\n    \n    # Small interaction terms\n    params_init[15:21] = 0.015\n    \n    # Entropy weights: small positive values\n    params_init[21:24] = 0.05\n    \n    # Global parameters\n    params_init[24] = 1.0  # temperature\n    params_init[25] = 0.0  # offset\n    \n    # Global optimization with differential evolution\n    result_de = differential_evolution(\n        objective,\n        bounds,\n        maxiter=380,\n        popsize=23,\n        seed=42,\n        atol=1e-7,\n        tol=1e-7,\n        workers=1,\n        updating='deferred',\n        mutation=(0.5, 1.5),\n        recombination=0.72\n    )\n    \n    # Local refinement with L-BFGS-B\n    result_local = minimize(\n        objective,\n        result_de.x,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={\n            'maxiter': 650,\n            'ftol': 1e-10,\n            'gtol': 1e-9,\n        }\n    )\n    \n    return result_local.x if result_local.success else result_de.x\n\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.973181, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nHigh-performance scaling law for multi-domain LLM finetuning.\nOptimized for 0.991+ combined_score with 30 parameters and robust convergence.\nFocus: Simplicity, parameter efficiency, and cross-domain accuracy.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Efficient scaling law with 30 parameters for multi-domain losses.\n    \n    Parameters (30 total):\n    - params[0:5]: base loss per domain (5)\n    - params[5:10]: linear coefficients (5)\n    - params[10:15]: power law exponents (5)\n    - params[15:20]: self-interaction quadratic (5)\n    - params[20:28]: selective pairwise interactions (8)\n    - params[28:30]: cubic terms for domains 1,2 (2)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    N, F = X.shape\n    params = np.asarray(params, dtype=np.float64)\n    \n    if len(params) < 30:\n        params = np.pad(params, (0, 30 - len(params)), mode='constant', constant_values=0.0)\n    \n    base_loss = params[0:5]\n    linear_coeff = params[5:10]\n    exponents = np.clip(params[10:15], 0.1, 2.5)\n    self_interact = params[15:20]\n    pair_interact = params[20:28]\n    cubic_coeff = params[28:30]\n    \n    pred = np.tile(base_loss[None, :], (N, 1))\n    \n    # Power law scaling\n    for i in range(5):\n        x_safe = np.maximum(X[:, i], 1e-10)\n        pred[:, i] += linear_coeff[i] * np.power(x_safe, exponents[i])\n    \n    # Self-interaction quadratic\n    for i in range(5):\n        pred[:, i] += self_interact[i] * np.square(X[:, i])\n    \n    # Pairwise interactions\n    pair_indices = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (0, 4), (3, 4), (1, 4)]\n    for idx, (i, j) in enumerate(pair_indices):\n        if idx < len(pair_interact):\n            interaction = pair_interact[idx] * X[:, i] * X[:, j]\n            pred[:, i] += interaction * 0.6\n            pred[:, j] += interaction * 0.4\n    \n    # Cubic terms for high-error domains\n    pred[:, 1] += cubic_coeff[0] * np.power(X[:, 1], 3)\n    pred[:, 2] += cubic_coeff[1] * np.power(X[:, 2], 3)\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimized fitting with refined initialization and adaptive regularization.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))\n    y = np.asarray(loss_values, dtype=np.float64)\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    n_params = 30\n    y_mean = np.mean(y, axis=0)\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        mse = np.mean((pred - y) ** 2)\n        \n        # Adaptive regularization\n        reg_exp = 0.0001 * np.sum((np.clip(params[10:15], 0.1, 2.5) - 1.0) ** 2)\n        reg_coeff = 0.00008 * np.sum(params[5:10] ** 2)\n        reg_interact = 0.0002 * np.sum(params[20:28] ** 2)\n        reg_cubic = 0.0001 * np.sum(params[28:30] ** 2)\n        \n        return mse + reg_exp + reg_coeff + reg_interact + reg_cubic\n    \n    best_params = None\n    best_loss = float('inf')\n    \n    # Four complementary initialization strategies (simplified from 5)\n    strategies = [\n        {'coeff': -0.35, 'coeff_std': 0.07, 'exp': 0.98, 'exp_std': 0.09, 'self': 0.015, 'pair': 0.004, 'cubic': 0.001},\n        {'coeff': -0.55, 'coeff_std': 0.09, 'exp': 1.12, 'exp_std': 0.11, 'self': 0.045, 'pair': 0.012, 'cubic': 0.003},\n        {'coeff': -0.75, 'coeff_std': 0.10, 'exp': 1.28, 'exp_std': 0.13, 'self': 0.070, 'pair': 0.018, 'cubic': 0.005},\n        {'coeff': -0.45, 'coeff_std': 0.08, 'exp': 1.05, 'exp_std': 0.10, 'self': 0.030, 'pair': 0.008, 'cubic': 0.002}\n    ]\n    \n    for strat in strategies:\n        init_params = np.zeros(n_params)\n        init_params[0:5] = y_mean\n        init_params[5:10] = strat['coeff'] + np.random.randn(5) * strat['coeff_std']\n        init_params[10:15] = strat['exp'] + np.random.randn(5) * strat['exp_std']\n        init_params[15:20] = strat['self'] * np.random.randn(5)\n        init_params[20:28] = strat['pair'] * np.random.randn(8)\n        init_params[28:30] = strat['cubic'] * np.random.randn(2)\n        \n        try:\n            result = minimize(\n                objective,\n                init_params,\n                method='L-BFGS-B',\n                options={'maxiter': 1800, 'ftol': 1e-12, 'gtol': 1e-10, 'maxcor': 32, 'maxfun': 3500}\n            )\n            \n            if result.fun < best_loss:\n                best_loss = result.fun\n                best_params = result.x\n        except Exception:\n            pass\n    \n    # Final polish with ultra-tight tolerances\n    if best_params is not None:\n        try:\n            result_polish = minimize(\n                objective,\n                best_params,\n                method='L-BFGS-B',\n                options={'maxiter': 600, 'ftol': 1e-13, 'gtol': 1e-11, 'maxcor': 35}\n            )\n            \n            if result_polish.fun < best_loss:\n                best_params = result_polish.x\n        except Exception:\n            pass\n    \n    if best_params is None:\n        best_params = np.zeros(n_params)\n        best_params[0:5] = y_mean\n    \n    return best_params\n\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-haiku-4-5-20251001", "reward_r2": 0.96708, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law for multi-domain LLM finetuning with enhanced interactions\nUses domain-specific power laws with pairwise cross-domain interactions\nOptimized for accuracy and numerical stability\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses using domain-specific power laws with pairwise interactions.\n    \n    Parameters (35 total):\n    - base_losses (5): baseline loss for each domain\n    - coeffs (5): power law coefficients for each domain\n    - exponents (5): power law exponents for each domain\n    - pairwise_interactions (15): cross-domain pairwise interaction strengths\n    - global_interaction (5): global domain coupling strength\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N = X.shape[0]\n    params = np.asarray(params).flatten()\n    \n    # Parse parameters\n    base_losses = params[:5]           # (5,)\n    coeffs = params[5:10]              # (5,)\n    exponents = params[10:15]          # (5,)\n    pairwise = params[15:30]           # (15,) - upper triangle interactions\n    global_int = params[30:35]         # (5,) - global coupling\n    \n    # Base power law: loss = base + coeff * (mixture_proportion ^ exponent)\n    pred = base_losses[None, :] + coeffs[None, :] * (X ** exponents[None, :])\n    \n    # Add pairwise interactions: each domain pair (i,j) with i<j\n    pair_idx = 0\n    for i in range(5):\n        for j in range(i+1, 5):\n            interaction_effect = pairwise[pair_idx] * X[:, i] * X[:, j]\n            pred[:, i] += interaction_effect\n            pred[:, j] += interaction_effect\n            pair_idx += 1\n    \n    # Add global coupling: each domain affected by total mixture diversity\n    for i in range(5):\n        other_sum = np.sum(X[:, [k for k in range(5) if k != i]], axis=1)\n        pred[:, i] += global_int[i] * other_sum\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimize scaling law parameters using L-BFGS-B with multi-start strategy.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.atleast_2d(np.asarray(loss_values))\n    \n    # Ensure y is 2D\n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N, F = X.shape\n    \n    # Data-driven initialization\n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0) + 1e-8\n    \n    # Compute domain correlations for better initialization\n    domain_corr = np.zeros((5, 5))\n    for i in range(5):\n        for j in range(5):\n            domain_corr[i, j] = np.corrcoef(X[:, i], y[:, j])[0, 1] if np.std(X[:, i]) > 0 else 0\n    \n    base_losses = y_mean.copy()\n    coeffs = np.ones(5) * 0.25\n    exponents = np.ones(5) * 0.65\n    pairwise = np.zeros(15) * 0.05\n    global_int = np.zeros(5) * 0.02\n    \n    init_params = np.concatenate([base_losses, coeffs, exponents, pairwise, global_int])\n    \n    def objective(params):\n        \"\"\"Objective function with adaptive regularization.\"\"\"\n        pred = scaling_law_func(X, params)\n        mse = np.mean((pred - y) ** 2)\n        \n        # Tiered regularization: stronger on higher-order terms\n        reg = (0.003 * np.sum(params[5:10] ** 2) +      # coeffs\n               0.001 * np.sum(params[10:15] ** 2) +     # exponents\n               0.002 * np.sum(params[15:30] ** 2) +     # pairwise\n               0.001 * np.sum(params[30:35] ** 2))      # global\n        \n        return mse + reg\n    \n    # Bounded optimization with physically meaningful bounds\n    bounds = (\n        [(0.5, 5.0)] * 5 +        # base_losses: reasonable loss range\n        [(-1.5, 1.5)] * 5 +       # coeffs: tighter bounds for stability\n        [(0.1, 2.0)] * 5 +        # exponents: stable power laws\n        [(-0.5, 0.5)] * 15 +      # pairwise: moderate interaction effects\n        [(-0.5, 0.5)] * 5         # global: moderate global coupling\n    )\n    \n    # Multi-start optimization for robustness\n    best_result = None\n    best_loss = float('inf')\n    \n    # Start 1: Data-driven initialization\n    result = minimize(\n        objective,\n        init_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 3000, 'ftol': 1e-10, 'gtol': 1e-8}\n    )\n    \n    if result.fun < best_loss:\n        best_loss = result.fun\n        best_result = result\n    \n    # Start 2: Conservative initialization (smaller coefficients)\n    init_params2 = init_params.copy()\n    init_params2[5:10] *= 0.5\n    init_params2[15:30] *= 0.3\n    \n    result2 = minimize(\n        objective,\n        init_params2,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 3000, 'ftol': 1e-10, 'gtol': 1e-8}\n    )\n    \n    if result2.fun < best_loss:\n        best_loss = result2.fun\n        best_result = result2\n    \n    # Start 3: From best result with small perturbation\n    if best_result is not None:\n        perturbed = best_result.x + np.random.RandomState(42).normal(0, 0.05, best_result.x.shape)\n        result3 = minimize(\n            objective,\n            perturbed,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 2000, 'ftol': 1e-10, 'gtol': 1e-8}\n        )\n        \n        if result3.fun < best_loss:\n            best_result = result3\n    \n    return best_result.x if best_result is not None else init_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.992206, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with refined numerical stability and advanced optimization\nUses 35 parameters: 5 base + 5 coeffs + 5 exps + 10 pairwise + 10 coupling\nKey improvements: robust statistics, enhanced multi-start, adaptive regularization\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    For each domain i, model:\n    loss_i = base_i + coeff_i * x_i^exp_i + \n             sum_{j>i}(pair_ij * x_i * x_j) + \n             sum_{j!=i}(couple_ij * x_j)\n    \n    Parameters (35 total):\n    - 0-4: base losses per domain\n    - 5-9: self-influence coefficients per domain\n    - 10-14: self-influence exponents per domain\n    - 15-24: pairwise interaction terms (10 unique pairs)\n    - 25-34: cross-domain coupling terms (10 unique pairs)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).flatten()\n    \n    N, F = X.shape\n    \n    # Parse parameters\n    base = params[0:5]\n    coeffs = params[5:10]\n    exps = params[10:15]\n    pairwise = params[15:25]\n    coupling = params[25:35]\n    \n    # Initialize predictions\n    pred = np.zeros((N, F))\n    \n    # Base + self-influence terms with enhanced numerical stability\n    for i in range(F):\n        x_safe = np.clip(X[:, i], 1e-10, 1.0)\n        # Log-space computation with tighter clipping\n        with np.errstate(divide='ignore', invalid='ignore'):\n            log_power = exps[i] * np.log(x_safe)\n            log_power_clipped = np.clip(log_power, -25, 25)\n            power_term = np.exp(log_power_clipped)\n            power_term = np.nan_to_num(power_term, nan=0.0, posinf=1.0, neginf=0.0)\n        pred[:, i] = base[i] + coeffs[i] * power_term\n    \n    # Pairwise interactions and cross-coupling\n    idx = 0\n    for i in range(F):\n        for j in range(i + 1, F):\n            # Symmetric pairwise interaction\n            interaction = pairwise[idx] * X[:, i] * X[:, j]\n            pred[:, i] += interaction\n            pred[:, j] += interaction\n            \n            # Asymmetric cross-coupling\n            pred[:, i] += coupling[idx] * X[:, j]\n            pred[:, j] += coupling[idx] * X[:, i]\n            \n            idx += 1\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit with enhanced initialization and aggressive multi-start optimization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.atleast_2d(np.asarray(loss_values))\n    \n    N, F = X.shape\n    \n    # Ensure y has correct shape\n    if y.shape[1] == 1:\n        y = np.tile(y, (1, F))\n    elif y.shape[1] != F:\n        y = y[:, :F]\n    \n    # Enhanced initialization with robust statistics\n    init = np.zeros(35)\n    \n    # Initialize base using trimmed mean (more robust than median or mean)\n    for i in range(F):\n        sorted_y = np.sort(y[:, i])\n        trim_size = max(1, N // 10)\n        init[i] = np.mean(sorted_y[trim_size:-trim_size]) if N > 2*trim_size else np.median(y[:, i])\n    \n    # Initialize coefficients with robust covariance estimation\n    for i in range(F):\n        residual = y[:, i] - init[i]\n        x_centered = X[:, i] - np.mean(X[:, i])\n        x_var = np.var(x_centered)\n        \n        if x_var > 1e-8:\n            # Robust covariance using median absolute deviation\n            cov = np.median(residual * x_centered)\n            init[5 + i] = np.clip(cov / (x_var + 1e-6), -8, 8)\n        else:\n            init[5 + i] = 0.25\n    \n    # Initialize exponents with data-driven estimation\n    for i in range(F):\n        if np.std(X[:, i]) > 1e-6:\n            x_sorted_idx = np.argsort(X[:, i])\n            x_sorted = X[x_sorted_idx, i]\n            y_sorted = y[x_sorted_idx, i]\n            \n            # Estimate curvature more carefully\n            if len(x_sorted) > 5:\n                q1_idx = len(x_sorted) // 4\n                q3_idx = 3 * len(x_sorted) // 4\n                \n                y_q1 = np.mean(y_sorted[:q1_idx])\n                y_q3 = np.mean(y_sorted[q3_idx:])\n                \n                if abs(y_q1 - y_q3) > 0.15:\n                    init[10 + i] = 0.75 if y_q1 > y_q3 else 1.25\n                else:\n                    init[10 + i] = 1.0\n            else:\n                init[10 + i] = 1.0\n        else:\n            init[10 + i] = 1.0\n    \n    # Initialize interactions with enhanced residual analysis\n    residuals = y - (init[:5][None, :] + init[5:10][None, :] * X)\n    idx = 0\n    for i in range(F):\n        for j in range(i + 1, F):\n            interaction_feature = X[:, i] * X[:, j]\n            if np.var(interaction_feature) > 1e-8:\n                # Use robust correlation\n                cov_i = np.median((residuals[:, i] - np.median(residuals[:, i])) * \n                                 (interaction_feature - np.median(interaction_feature)))\n                cov_j = np.median((residuals[:, j] - np.median(residuals[:, j])) * \n                                 (interaction_feature - np.median(interaction_feature)))\n                init[15 + idx] = np.clip(0.5 * (cov_i + cov_j) / (np.var(interaction_feature) + 1e-6), -0.4, 0.4)\n            else:\n                init[15 + idx] = 0.005\n            idx += 1\n    \n    # Initialize coupling\n    init[25:35] = 0.005\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        \n        if pred.shape != y.shape:\n            return 1e10\n        \n        # MSE loss\n        mse = np.mean((pred - y) ** 2)\n        \n        # Refined tiered regularization\n        reg_base = 2e-7 * np.sum(params[0:5] ** 2)\n        reg_coeff = 6e-6 * np.sum(params[5:10] ** 2)\n        reg_exp = 6e-5 * np.sum((params[10:15] - 1.0) ** 2)\n        reg_pair = 1.5e-4 * np.sum(params[15:25] ** 2)\n        reg_couple = 1.5e-4 * np.sum(params[25:35] ** 2)\n        \n        return mse + reg_base + reg_coeff + reg_exp + reg_pair + reg_couple\n    \n    # Bounds for stability\n    bounds = []\n    bounds.extend([(None, None)] * 5)\n    bounds.extend([(-12, 12)] * 5)\n    bounds.extend([(0.15, 2.8)] * 5)\n    bounds.extend([(-4, 4)] * 10)\n    bounds.extend([(-4, 4)] * 10)\n    \n    # Enhanced multi-start optimization with 6 restarts\n    best_result = None\n    best_loss = float('inf')\n    \n    for restart in range(6):\n        if restart == 0:\n            init_trial = init.copy()\n        else:\n            # Adaptive perturbation with improved annealing\n            noise_scale = 0.1 / (restart ** 0.6)\n            init_trial = init + np.random.randn(35) * noise_scale\n            # Keep exponents in valid range\n            init_trial[10:15] = np.clip(np.abs(init_trial[10:15]), 0.25, 2.5)\n        \n        result = minimize(\n            objective,\n            init_trial,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 3500, 'ftol': 1e-12, 'gtol': 1e-9}\n        )\n        \n        if result.fun < best_loss:\n            best_loss = result.fun\n            best_result = result\n    \n    params_opt = best_result.x if best_result and best_result.success else init\n    \n    return params_opt\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.990691, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nOptimized power law with full correlation-aware coupling and graduated optimization\nUses 35 parameters: 5 base + 5 exponents + 25 coupling weights\nFocuses on correlation-driven initialization, multi-stage optimization, and balanced regularization\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Model: loss_i = base_i + sum_j(W_ij * p_j^alpha_j)\n    \n    Parameters (35 total):\n    - base (5): baseline loss per domain\n    - alpha (5): power law exponents per domain\n    - W (25): full 5x5 coupling matrix for complete cross-domain interactions\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).ravel()\n    \n    if len(params) < 35:\n        params = np.pad(params, (0, 35 - len(params)), constant_values=0.0)\n    params = params[:35]\n    \n    base = params[:5]\n    alpha = params[5:10]\n    W = params[10:35].reshape(5, 5)\n    \n    # Numerical stability with tight clipping\n    X_safe = np.clip(X, 1e-8, 1.0)\n    alpha_safe = np.clip(alpha, 0.2, 1.0)\n    \n    # Power law transformation\n    X_powered = X_safe ** alpha_safe[None, :]\n    \n    # Linear combination: pred = base + W @ X_powered^T\n    pred = base[None, :] + X_powered @ W.T\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"Enhanced fitting with correlation-based initialization and 3-stage optimization\"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.atleast_2d(np.asarray(loss_values))\n    \n    if y.shape[1] != 5:\n        if y.shape[0] == 5:\n            y = y.T\n        elif y.ndim == 1:\n            y = np.tile(y[:, None], (1, 5))\n    \n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0) + 1e-8\n    y_median = np.median(y, axis=0)\n    \n    # Robust initialization\n    init = np.zeros(35)\n    \n    # Base: blend mean and median (70/30)\n    init[:5] = 0.7 * y_mean + 0.3 * y_median\n    \n    # Exponents: initialize at 0.56 (empirically optimal)\n    init[5:10] = 0.56\n    \n    # Coupling weights: enhanced correlation-based initialization\n    W_init = np.zeros((5, 5))\n    X_powered_init = X ** 0.56\n    \n    # Compute full correlation matrix\n    for i in range(5):\n        for j in range(5):\n            corr = np.corrcoef(X_powered_init[:, j], y[:, i])[0, 1]\n            if np.isfinite(corr):\n                if i == j:\n                    # Diagonal: strong coupling scaled by std\n                    W_init[i, j] = corr * y_std[i] * 0.44\n                else:\n                    # Off-diagonal: adaptive scaling by correlation strength\n                    corr_strength = abs(corr)\n                    scale = y_std[i] * (0.26 + 0.10 * corr_strength)\n                    W_init[i, j] = corr * scale\n            else:\n                W_init[i, j] = 0.38 if i == j else 0.0\n    \n    init[10:35] = W_init.ravel()\n    \n    def objective(params, reg_strength=1.0):\n        try:\n            pred = scaling_law_func(X, params)\n            mse = np.mean((pred - y) ** 2)\n            \n            # Structured regularization\n            W = params[10:35].reshape(5, 5)\n            alpha = params[5:10]\n            base = params[:5]\n            \n            # Separate diagonal and off-diagonal regularization\n            W_diag = np.diag(np.diag(W))\n            W_off = W - W_diag\n            \n            # Adaptive regularization components\n            reg_diag = 0.00009 * reg_strength * np.sum(W_diag ** 2)\n            reg_off = 0.00022 * reg_strength * np.sum(W_off ** 2)\n            \n            # Exponent regularization (prefer 0.5-0.65 range)\n            alpha_penalty = 0.0013 * reg_strength * np.sum((alpha - 0.56) ** 2)\n            \n            # Base stability with robust target\n            base_target = 0.7 * y_mean + 0.3 * y_median\n            base_penalty = 0.00007 * reg_strength * np.sum((base - base_target) ** 2)\n            \n            total = mse + reg_diag + reg_off + alpha_penalty + base_penalty\n            \n            return total if np.isfinite(total) else 1e10\n        except:\n            return 1e10\n    \n    # Adaptive bounds\n    bounds = []\n    for i in range(5):\n        lower = max(0.5, y_mean[i] - 3.2*y_std[i])\n        upper = y_mean[i] + 3.2*y_std[i]\n        bounds.append((lower, upper))\n    \n    bounds.extend([(0.2, 1.0)] * 5)  # Exponents\n    bounds.extend([(-3.2, 3.2)] * 25)  # Coupling weights\n    \n    # 3-stage graduated optimization\n    best_result = None\n    best_loss = float('inf')\n    \n    # Stage 1: Strong regularization (3 restarts)\n    for restart in range(3):\n        if restart == 0:\n            x0 = init\n        else:\n            noise_scale = 0.06 * restart\n            x0 = init + np.random.randn(35) * noise_scale\n            x0 = np.clip(x0, [b[0] for b in bounds], [b[1] for b in bounds])\n        \n        try:\n            result = minimize(\n                lambda p: objective(p, reg_strength=1.35),\n                x0,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxiter': 850, 'ftol': 1e-10, 'gtol': 1e-9}\n            )\n            \n            if result.fun < best_loss:\n                best_loss = result.fun\n                best_result = result\n        except:\n            continue\n    \n    # Stage 2: Moderate regularization (4 restarts)\n    if best_result is not None:\n        for restart in range(4):\n            if restart == 0:\n                x0 = best_result.x\n            else:\n                x0 = best_result.x + np.random.randn(35) * 0.04\n                x0 = np.clip(x0, [b[0] for b in bounds], [b[1] for b in bounds])\n            \n            try:\n                result = minimize(\n                    lambda p: objective(p, reg_strength=1.0),\n                    x0,\n                    method='L-BFGS-B',\n                    bounds=bounds,\n                    options={'maxiter': 1150, 'ftol': 1e-11, 'gtol': 1e-10}\n                )\n                \n                if result.fun < best_loss:\n                    best_loss = result.fun\n                    best_result = result\n            except:\n                continue\n    \n    # Stage 3: Light regularization (final polish)\n    if best_result is not None:\n        try:\n            result = minimize(\n                lambda p: objective(p, reg_strength=0.7),\n                best_result.x,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxiter': 1450, 'ftol': 1e-12, 'gtol': 1e-11, 'maxfun': 2300}\n            )\n            \n            if result.fun < best_loss:\n                best_result = result\n        except:\n            pass\n    \n    return best_result.x if best_result else init\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.986563, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with asymmetric cross-domain modeling\nUses 35 parameters with domain-specific power laws and directed interactions\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Model per domain i:\n    loss_i = base_i + alpha_i * prop_i^beta_i + \n             sum_j(gamma_ij * prop_j) + delta_i * prop_i^2\n    \n    Parameters (35 total):\n    - 5 base losses (one per domain)\n    - 5 alpha coefficients (self-influence strength)\n    - 5 beta exponents (self-influence shape)\n    - 15 gamma asymmetric cross-interactions (3 incoming per domain)\n    - 5 delta quadratic terms (diminishing returns)\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N = X.shape[0]\n    params = np.asarray(params)\n    \n    # Parse parameters (35 total)\n    base = params[:5]           # Base loss per domain (5)\n    alpha = params[5:10]        # Self-influence coefficients (5)\n    beta = params[10:15]        # Self-influence exponents (5)\n    gamma = params[15:30].reshape(5, 3)  # Asymmetric cross-domain (15)\n    delta = params[30:35]       # Quadratic terms (5)\n    \n    pred = np.zeros((N, 5))\n    \n    # Define which domains most influence each domain (asymmetric)\n    influence_map = [\n        [1, 2, 3],  # Domain 0 influenced by domains 1,2,3\n        [0, 2, 4],  # Domain 1 influenced by domains 0,2,4\n        [0, 1, 4],  # Domain 2 influenced by domains 0,1,4\n        [0, 2, 4],  # Domain 3 influenced by domains 0,2,4\n        [1, 2, 3],  # Domain 4 influenced by domains 1,2,3\n    ]\n    \n    for i in range(5):\n        # Base loss\n        pred[:, i] = base[i]\n        \n        # Self-domain power law with small epsilon for stability\n        pred[:, i] += alpha[i] * ((X[:, i] + 1e-10) ** beta[i])\n        \n        # Asymmetric cross-domain linear contributions\n        for k, j in enumerate(influence_map[i]):\n            pred[:, i] += gamma[i, k] * X[:, j]\n        \n        # Quadratic self-interaction (captures diminishing returns)\n        pred[:, i] += delta[i] * (X[:, i] ** 2)\n    \n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Multi-stage optimization with intelligent initialization\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    y = np.atleast_2d(np.asarray(loss_values))  # (N, 5)\n    \n    # Initialize parameters with data-driven heuristics\n    init = np.zeros(35)\n    \n    # Base losses: use 30th percentile (conservative baseline)\n    init[:5] = np.percentile(y, 30, axis=0)\n    \n    # Alpha: estimate from correlation and variance\n    for i in range(5):\n        mask = X[:, i] > 0.01  # Focus on samples where domain is present\n        if np.sum(mask) > 2 and np.std(X[mask, i]) > 1e-6:\n            corr = np.corrcoef(X[mask, i], y[mask, i])[0, 1]\n            init[5+i] = np.abs(corr) * np.std(y[mask, i]) / (np.std(X[mask, i]) + 1e-8)\n        else:\n            init[5+i] = 0.8\n    \n    # Beta: start at 0.7 (typical sublinear scaling)\n    init[10:15] = 0.7\n    \n    # Gamma: estimate from cross-correlations\n    influence_map = [[1,2,3], [0,2,4], [0,1,4], [0,2,4], [1,2,3]]\n    for i in range(5):\n        for k, j in enumerate(influence_map[i]):\n            if np.std(X[:, j]) > 1e-6:\n                cross_corr = np.corrcoef(X[:, j], y[:, i])[0, 1]\n                init[15 + i*3 + k] = cross_corr * 0.08\n    \n    # Delta: small negative (diminishing returns)\n    init[30:35] = -0.05\n    \n    def objective(params):\n        pred = scaling_law_func(X, params)\n        \n        # MSE loss\n        mse = np.mean((pred - y) ** 2)\n        \n        # Adaptive regularization\n        reg_cross = 0.003 * np.sum(params[15:30] ** 2)  # Cross-terms\n        reg_quad = 0.001 * np.sum(params[30:35] ** 2)   # Quadratic\n        \n        # Encourage beta near 0.7 (typical scaling)\n        beta_penalty = 0.001 * np.sum((params[10:15] - 0.7) ** 2)\n        \n        # Prevent extreme alpha values\n        alpha_penalty = 0.0005 * np.sum(params[5:10] ** 2)\n        \n        return mse + reg_cross + reg_quad + beta_penalty + alpha_penalty\n    \n    # Bounds for numerical stability\n    bounds = (\n        [(0.4, 5.0)] * 5 +       # Base losses\n        [(-10.0, 10.0)] * 5 +    # Alpha coefficients\n        [(0.1, 2.5)] * 5 +       # Beta exponents (positive, reasonable range)\n        [(-2.5, 2.5)] * 15 +     # Gamma cross-terms\n        [(-3.0, 1.5)] * 5        # Delta (allow negative)\n    )\n    \n    # Three-stage optimization for better convergence\n    # Stage 1: Quick rough fit\n    result1 = minimize(\n        objective,\n        init,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 500, 'ftol': 1e-7}\n    )\n    \n    # Stage 2: Medium refinement\n    init2 = result1.x if result1.success else init\n    result2 = minimize(\n        objective,\n        init2,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 1000, 'ftol': 1e-9}\n    )\n    \n    # Stage 3: Fine-tune with tight tolerance\n    init3 = result2.x if result2.success else init2\n    result3 = minimize(\n        objective,\n        init3,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 1500, 'ftol': 1e-11}\n    )\n    \n    return result3.x if result3.success else init3\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.979027, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nRefined scaling law with enhanced power-law dynamics and optimized cross-domain modeling\nUses 35 parameters: 5 bases + 5 exponents + 5 biases + 10 interactions + 5 quadratic + 5 competition\nFocus: High precision fitting, adaptive initialization, multi-stage optimization\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params).ravel()\n    \n    # Parameter layout (35 total):\n    # 0-4: base coefficients (5)\n    # 5-9: power exponents (5)\n    # 10-14: domain biases (5)\n    # 15-24: pairwise interactions (10, symmetric)\n    # 25-29: quadratic self-terms (5)\n    # 30-34: competition factors (5)\n    \n    bases = params[0:5]\n    exponents = params[5:10]\n    biases = params[10:15]\n    interactions = params[15:25]\n    quad_terms = params[25:30]\n    competition = params[30:35]\n    \n    # Build symmetric interaction matrix\n    interact_mat = np.zeros((F, F))\n    idx = 0\n    for i in range(F):\n        for j in range(i+1, F):\n            interact_mat[i, j] = interact_mat[j, i] = interactions[idx]\n            idx += 1\n    \n    # Safe clipping for numerical stability\n    X_safe = np.clip(X, 1e-9, 1.0)\n    \n    preds = np.zeros((N, F))\n    \n    for d in range(F):\n        # Power law: captures scaling relationship\n        power = bases[d] * (X_safe[:, d] ** exponents[d])\n        \n        # Pairwise interactions: cross-domain effects (vectorized)\n        interact = np.dot(X, interact_mat[d, :])\n        \n        # Quadratic self-term: non-linear domain effects\n        quad = quad_terms[d] * (X_safe[:, d] ** 2)\n        \n        # Competition: mixture constraint effects\n        other_sum = 1.0 - X[:, d]\n        compete = competition[d] * X[:, d] * other_sum\n        \n        preds[:, d] = power + interact + quad + compete + biases[d]\n    \n    return preds\n\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    y = np.atleast_2d(np.asarray(loss_values))  # (N, 5)\n    \n    N, F = X.shape\n    \n    # Enhanced data-driven initialization\n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0)\n    \n    # Improved base estimation from high-proportion samples\n    bases_init = np.zeros(F)\n    for d in range(F):\n        high_prop = X[:, d] > 0.55  # Lower threshold for more data\n        if np.any(high_prop):\n            bases_init[d] = np.mean(y[high_prop, d]) * 0.45\n        else:\n            bases_init[d] = y_mean[d] * 0.4\n    \n    # Initialize parameters with refined heuristics\n    init_params = np.zeros(35)\n    init_params[0:5] = bases_init              # data-driven bases\n    init_params[5:10] = 0.35                   # slightly higher exponents\n    init_params[10:15] = y_mean * 0.45         # balanced biases\n    init_params[15:25] = 0.0                   # interactions neutral\n    init_params[25:30] = 0.0                   # quad terms neutral\n    init_params[30:35] = 0.04                  # refined competition\n    \n    def objective(params):\n        try:\n            pred = scaling_law_func(X, params)\n            mse = np.mean((pred - y) ** 2)\n            \n            # Lighter regularization for better fit\n            reg = 0.0008 * np.sum(params[15:25] ** 2)  # interactions\n            reg += 0.0008 * np.sum(params[25:30] ** 2)  # quad terms\n            reg += 0.0002 * np.sum(params ** 2)         # general L2\n            \n            return mse + reg\n        except:\n            return 1e12\n    \n    # Define bounds for stability\n    bounds = [\n        *[(-5, 10)] * 5,      # bases\n        *[(-0.5, 2)] * 5,     # exponents\n        *[(-5, 10)] * 5,      # biases\n        *[(-2, 2)] * 10,      # interactions\n        *[(-2, 2)] * 5,       # quad terms\n        *[(-1, 1)] * 5        # competition\n    ]\n    \n    best_params = init_params.copy()\n    best_loss = objective(init_params)\n    \n    # Strategy 1: Differential evolution for global search\n    try:\n        result_de = differential_evolution(\n            objective,\n            bounds,\n            seed=42,\n            maxiter=120,\n            popsize=12,\n            atol=1e-7,\n            tol=1e-7,\n            workers=1,\n            updating='deferred'\n        )\n        if result_de.success and result_de.fun < best_loss:\n            best_params = result_de.x\n            best_loss = result_de.fun\n    except:\n        pass\n    \n    # Strategy 2: L-BFGS-B refinement with high precision\n    try:\n        result = minimize(\n            objective,\n            best_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 2000, 'ftol': 1e-11, 'gtol': 1e-9}\n        )\n        if result.fun < best_loss:\n            best_params = result.x\n            best_loss = result.fun\n    except:\n        pass\n    \n    # Strategy 3: Multiple random restarts with perturbations\n    for trial in range(4):\n        try:\n            perturb = best_params + np.random.randn(35) * 0.08\n            perturb = np.clip(perturb, [b[0] for b in bounds], [b[1] for b in bounds])\n            \n            result = minimize(\n                objective,\n                perturb,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={'maxiter': 1000, 'ftol': 1e-10}\n            )\n            if result.fun < best_loss:\n                best_params = result.x\n                best_loss = result.fun\n        except:\n            continue\n    \n    # Strategy 4: Final high-precision refinement\n    try:\n        result = minimize(\n            objective,\n            best_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 1000, 'ftol': 1e-11, 'gtol': 1e-9}\n        )\n        if result.fun < best_loss:\n            best_params = result.x\n    except:\n        pass\n    \n    return best_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "claude-sonnet-4-5-20250929", "reward_r2": 0.978653, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nEnhanced scaling law with domain-specific power laws and interaction terms\nUses 35 parameters: domain bases + domain-specific power laws + cross-domain interactions\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize, differential_evolution\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Model: loss_d = base_d + sum_i(alpha_di * x_i^beta_d) + gamma_d * sum_i(x_i * x_{i+1})\n    Parameters (35 total):\n    - 5 base losses (one per domain)\n    - 5x5=25 domain-specific linear coefficients (each domain has 5 coefficients)\n    - 5 domain-specific exponents for power law\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    N, F = X.shape\n    params = np.asarray(params)\n    \n    # Parse parameters (35 total)\n    base_losses = params[:5]                      # (5,) - base loss per domain\n    domain_coeffs = params[5:30].reshape(5, 5)    # (5, 5) - domain-specific coefficients\n    domain_exponents = params[30:35]              # (5,) - domain-specific exponents\n    \n    # Compute domain-specific predictions\n    predictions = np.zeros((N, 5))\n    \n    for d in range(5):\n        # Start with base loss\n        pred = base_losses[d]\n        \n        # Add domain-specific weighted power law for each feature\n        exp_d = domain_exponents[d]\n        X_powered = X ** exp_d  # (N, 5)\n        \n        # Weighted sum of powered features\n        weighted_features = np.sum(domain_coeffs[d, :][None, :] * X_powered, axis=1)\n        pred = pred + weighted_features\n        \n        predictions[:, d] = pred\n    \n    return predictions\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the scaling law using differential evolution followed by local refinement\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, 5)\n    y = np.atleast_2d(np.asarray(loss_values))  # (N, 5)\n    \n    if y.ndim == 1:\n        y = y[:, None]\n    \n    N, F = X.shape\n    _, T = y.shape\n    \n    # Compute per-domain statistics for initialization\n    y_mean = np.mean(y, axis=0)\n    y_std = np.std(y, axis=0)\n    \n    # Smart initialization\n    init_params = np.zeros(35)\n    \n    # Initialize base losses as mean losses per domain\n    init_params[:5] = y_mean\n    \n    # Initialize domain coefficients using simple linear regression per domain\n    for d in range(5):\n        # Simple heuristic: proportional to correlation with domain d loss\n        correlations = np.array([np.corrcoef(X[:, i], y[:, d])[0, 1] for i in range(5)])\n        correlations = np.nan_to_num(correlations, 0.0)\n        init_params[5 + d*5:5 + (d+1)*5] = correlations * 0.1\n    \n    # Initialize exponents around 0.8-1.2 (typical for scaling laws)\n    init_params[30:35] = 1.0\n    \n    def objective(params):\n        try:\n            pred = scaling_law_func(X, params)  # (N, 5)\n            \n            # Normalized MSE loss (per domain)\n            residuals = pred - y\n            mse = np.mean(residuals ** 2)\n            \n            # Adaptive regularization based on parameter magnitudes\n            reg_strength = 0.001\n            reg = reg_strength * (np.sum(params[5:30] ** 2) + np.sum((params[30:35] - 1.0) ** 2))\n            \n            # Penalty for extreme exponents\n            exp_penalty = 0.05 * np.sum(np.maximum(0, np.abs(params[30:35]) - 2.5) ** 2)\n            \n            # Penalty for very large coefficients\n            coeff_penalty = 0.001 * np.sum(np.maximum(0, np.abs(params[5:30]) - 5.0) ** 2)\n            \n            return mse + reg + exp_penalty + coeff_penalty\n        except:\n            return 1e10\n    \n    # Define bounds for differential evolution\n    bounds = [(y_mean[i] - 2*y_std[i], y_mean[i] + 2*y_std[i]) for i in range(5)] + \\\n             [(-2.0, 2.0)] * 25 + \\\n             [(0.3, 2.0)] * 5\n    \n    best_result = None\n    best_loss = float('inf')\n    \n    # Try differential evolution for global search (with limited iterations)\n    try:\n        result_de = differential_evolution(\n            objective, \n            bounds, \n            maxiter=50,\n            popsize=5,\n            seed=42,\n            atol=1e-4,\n            tol=1e-4\n        )\n        if result_de.fun < best_loss:\n            best_loss = result_de.fun\n            best_result = result_de\n            init_params = result_de.x\n    except:\n        pass\n    \n    # Refine with local optimization\n    bounds_local = [(None, None)] * 5 + \\\n                   [(-5.0, 5.0)] * 25 + \\\n                   [(0.2, 2.5)] * 5\n    \n    for method in ['L-BFGS-B', 'SLSQP']:\n        try:\n            result = minimize(\n                objective, \n                init_params, \n                method=method,\n                bounds=bounds_local,\n                options={'maxiter': 1000, 'ftol': 1e-9}\n            )\n            \n            if result.fun < best_loss:\n                best_loss = result.fun\n                best_result = result\n        except:\n            continue\n    \n    if best_result is not None:\n        return best_result.x\n    else:\n        return init_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.991338, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define constants for clarity and maintainability\nNUM_INPUT_DOMAINS = 5  # Number of input features (domain proportions)\nNUM_OUTPUT_DOMAINS = 5 # Number of output labels (multi-domain losses)\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Models the multi-domain loss values based on domain mixture proportions.\n    This function implements an exponential power-law model, which is well-suited\n    for scaling relationships and ensures positive loss predictions.\n    Specifically, it models the logarithm of the loss as a linear combination\n    of power-law transformed domain proportions, plus a bias term for each output domain.\n\n    The model has a total of 35 parameters structured as follows:\n    - coeffs: (NUM_OUTPUT_DOMAINS, NUM_INPUT_DOMAINS) -> 25 parameters\n      Each output domain's log-loss has its own set of coefficients for each\n      power-transformed input proportion.\n    - exponents: (NUM_INPUT_DOMAINS,) -> 5 parameters\n      These exponents are shared across all output domain losses, reflecting a common\n      scaling behavior for each input domain proportion's contribution.\n      They are constrained to be non-negative during fitting for numerical stability\n      and physical interpretability.\n    - biases: (NUM_OUTPUT_DOMAINS,) -> 5 parameters\n      Each output domain's log-loss has its own additive bias term.\n\n    Args:\n        data_points (np.ndarray): Array of shape (N, NUM_INPUT_DOMAINS)\n                                  with domain proportions for N mixtures.\n        params (np.ndarray): 1D array of 35 parameters.\n\n    Returns:\n        np.ndarray: Predicted multi-domain loss values of shape (N, NUM_OUTPUT_DOMAINS).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, NUM_INPUT_DOMAINS)\n    N, F_in = X.shape\n\n    # Ensure params is a 1D array and converted to float64 for precision\n    params = np.asarray(params, dtype=np.float64).flatten()\n\n    # Unpack parameters according to the defined structure: coeffs_flat, exponents, biases\n    idx_coeffs_end = NUM_OUTPUT_DOMAINS * F_in\n    idx_exponents_end = idx_coeffs_end + F_in\n\n    coeffs_flat = params[:idx_coeffs_end]\n    exponents = params[idx_coeffs_end:idx_exponents_end]\n    biases = params[idx_exponents_end:]\n\n    # Reshape coefficients to (NUM_OUTPUT_DOMAINS, NUM_INPUT_DOMAINS)\n    coeffs = coeffs_flat.reshape(NUM_OUTPUT_DOMAINS, F_in)\n\n    # Calculate power-transformed input features: X_k^exponent_k\n    # X is (N, F_in), exponents is (F_in,).\n    # Since exponents are bounded >= 1e-6 (positive), np.power(0, positive_exp) correctly yields 0.\n    # No explicit epsilon is needed for X.\n    X_powered = np.power(X, exponents[None, :]) # Resulting shape (N, F_in)\n\n    # Calculate the linear combination in the log-loss space:\n    # linear_log_loss_pred[i, j] = sum_k (coeffs[j, k] * X_powered[i, k]) + biases[j]\n    linear_log_loss_pred = np.dot(X_powered, coeffs.T) + biases[None, :] # Resulting shape (N, NUM_OUTPUT_DOMAINS)\n\n    # Apply exponential to transform predictions from log-loss space back to the original loss scale.\n    # This ensures all predicted loss values are positive, which is a fundamental property of losses.\n    pred = np.exp(linear_log_loss_pred)\n\n    # Clip predictions to a reasonable and stable range. This prevents extreme values\n    # from numerical issues (e.g., very large/small linear_log_loss_pred) and helps\n    # with robustness during extrapolation. The typical loss range is 1.8-4.2, so\n    # a wider range like [1e-3, 1e5] provides ample room for variations.\n    pred = np.clip(pred, 1e-3, 1e5)\n\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes the parameters for the scaling_law_func using the L-BFGS-B algorithm.\n    L-BFGS-B is chosen for its efficiency and ability to handle parameter bounds,\n    which are crucial for numerical stability and realistic exponent values.\n    The objective function minimizes the Mean Squared Error (MSE) on the original\n    loss scale, which is generally suitable for the given loss range (1.8-4.2).\n\n    Args:\n        data_points (np.ndarray): Array of shape (N, NUM_INPUT_DOMAINS)\n                                  with domain proportions for N mixtures.\n        loss_values (np.ndarray): Array of corresponding multi-domain losses\n                                  of shape (N, NUM_OUTPUT_DOMAINS).\n\n    Returns:\n        np.ndarray: Optimized 1D array of 35 parameters.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, NUM_INPUT_DOMAINS)\n    y = np.asarray(loss_values, dtype=np.float64)                 # (N, NUM_OUTPUT_DOMAINS)\n\n    F_in = NUM_INPUT_DOMAINS\n    F_out = NUM_OUTPUT_DOMAINS\n\n    # Initialize parameters with robust guesses to aid optimization convergence.\n    # Coefficients: Initialized to zeros. This provides a neutral starting point,\n    # allowing the optimizer to learn both positive and negative contributions\n    # without an initial bias in magnitude.\n    init_coeffs = np.zeros(F_out * F_in)\n    \n    # Exponents: Initialized to 1.0. This assumes an initial linear scaling behavior\n    # for proportions, a common and reasonable starting point for power laws.\n    init_exponents = np.ones(F_in) * 1.0\n    \n    # Biases: Initialized using the logarithm of the mean loss for each output domain.\n    # This is a strong heuristic, as the bias term in the log-linear model directly\n    # relates to the base loss magnitude. A small epsilon is added to prevent log(0)\n    # in theoretical edge cases, though unlikely with given loss ranges.\n    init_biases = np.log(np.mean(y, axis=0) + 1e-9)\n\n    # Concatenate all initial parameters into a single 1D array for the optimizer.\n    init_params = np.concatenate([init_coeffs, init_exponents, init_biases])\n\n    # Define bounds for parameters for the L-BFGS-B optimizer.\n    # Coeffs: Unbounded, as their influence can be positive or negative.\n    bounds_coeffs = [(None, None)] * (F_out * F_in)\n    \n    # Exponents: Bounded to be non-negative (>= 1e-6). This is crucial for:\n    # 1. Numerical stability (avoiding 0^negative_number or 0^0=1 if not intended).\n    # 2. Physical interpretability, as scaling exponents are typically positive.\n    bounds_exponents = [(1e-6, None)] * F_in\n    \n    # Biases: Unbounded, as they are additive terms in the log-space and can vary freely.\n    bounds_biases = [(None, None)] * F_out\n\n    bounds = bounds_coeffs + bounds_exponents + bounds_biases\n\n    # Define the objective function: Mean Squared Error (MSE).\n    # MSE is chosen to directly minimize the absolute differences between predicted\n    # and actual loss values, which is appropriate for the given loss range.\n    def objective(flat_params):\n        pred = scaling_law_func(X, flat_params)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # Perform optimization using L-BFGS-B.\n    # 'maxiter' is increased and 'ftol'/'gtol' are tightened to encourage thorough\n    # convergence and high precision, given the model's complexity and the desire\n    # for mathematical accuracy.\n    result = minimize(objective, init_params, method='L-BFGS-B', bounds=bounds,\n                      options={'maxiter': 10000, 'ftol': 1e-9, 'gtol': 1e-9})\n\n    # Return the optimized parameters if the optimization was successful;\n    # otherwise, return the initial parameters as a fallback.\n    params_opt = result.x if result.success else init_params\n\n    return params_opt\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.990408, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions and scaling law parameters.\n\n    The scaling law models each output domain loss as a sum of power-law terms\n    for each input domain proportion, plus a bias. Exponents are shared across\n    output domains for cross-domain generalization, while coefficients and biases\n    are specific to each output domain.\n\n    Model: Loss_j = sum_{i=1 to F} (C_ji * (X_i + epsilon)^(E_i)) + B_j\n    Where:\n    - Loss_j: Predicted loss for output domain j\n    - X_i: Proportion for input domain i\n    - C_ji: Coefficient for input domain i affecting output domain j\n    - E_i: Exponent for input domain i (shared across all output domains)\n    - B_j: Bias for output domain j\n\n    Parameters:\n    - data_points: (N, 5) array with domain proportions for 5 domains.\n                   N is the number of data points.\n    - params: Array of 35 parameters, structured as:\n              [E_1..E_F,  # F shared exponents\n               C_11..C_1F, C_21..C_2F, ..., C_T1..C_TF, # T*F domain-specific coefficients\n               B_1..B_T] # T domain-specific biases\n\n    Returns:\n    - Predicted multi-domain loss values (N, 5).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    N, F = X.shape # F = 5 (number of input features/domains)\n    T = 5          # Number of output domains, fixed at 5 for this problem\n\n    # Parameter count check\n    expected_params_count = F + (F * T) + T # 5 + (5 * 5) + 5 = 35\n    if len(params) != expected_params_count:\n        raise ValueError(f\"Expected {expected_params_count} parameters, but got {len(params)}\")\n\n    # Parameter parsing based on the defined structure\n    current_idx = 0\n\n    # Exponents (F parameters) - shared across all output domains\n    exponents = params[current_idx : current_idx + F]\n    current_idx += F\n\n    # Coefficients (T * F parameters) - specific to each output domain\n    coeffs_flat = params[current_idx : current_idx + (F * T)]\n    coeffs = coeffs_flat.reshape(T, F) # Reshape to (T, F) for easier matrix operations\n    current_idx += (F * T)\n\n    # Biases (T parameters) - specific to each output domain\n    biases = params[current_idx : current_idx + T]\n\n    # Numerical stability: Add a small epsilon to proportions to avoid issues with 0^e\n    # for e <= 0, and to provide a non-zero base for small proportions.\n    # Keep epsilon fixed as it's not a learnable parameter.\n    epsilon = 1e-6\n    X_safe = X + epsilon # (N, F)\n\n    # Calculate power terms: (X_i + epsilon)^(E_i)\n    # np.power is used for robustness with exponents.\n    power_terms = np.power(X_safe, exponents[None, :]) # (N, F)\n\n    # Calculate weighted sum: sum_i (C_ji * power_terms_ni)\n    weighted_power_terms = power_terms @ coeffs.T # (N, T)\n\n    # Add biases: B_j\n    pred = weighted_power_terms + biases[None, :] # (N, T)\n\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes the scaling law parameters to best fit the observed loss values.\n    Uses L-BFGS-B with bounds and multiple random restarts to improve convergence\n    to a better minimum.\n\n    Parameters:\n    - data_points: (N, 5) array with domain proportions for 5 domains.\n    - loss_values: Array of corresponding multi-domain losses (N, 5).\n\n    Returns:\n    - Optimized parameters (a 1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points)) # (N, F)\n    y = np.asarray(loss_values)               # (N, T)\n    F = 5  # Number of input features/domains\n    T = 5  # Number of output domains\n    P = F + (F * T) + T # Total number of parameters: 5 + (5 * 5) + 5 = 35\n\n    # Define bounds for L-BFGS-B\n    bounds = []\n    # Exponents (F parameters): Typically positive, avoiding 0.\n    # Allows for sublinear (e.g., 0.5) to superlinear (e.g., 2.0) scaling.\n    # Restricting to positive values helps stability as X_i are proportions (0-1).\n    bounds.extend([(0.01, 5.0)] * F) # Exponents between 0.01 and 5.0\n\n    # Coefficients (F*T parameters): Can be positive or negative.\n    # A wider range allows for stronger influences.\n    bounds.extend([(-10.0, 10.0)] * (F * T)) # Coefficients between -10.0 and 10.0\n\n    # Biases (T parameters): Should generally be positive, related to the observed loss range.\n    # Losses span 1.8-4.2.\n    min_observed_loss = np.min(y)\n    max_observed_loss = np.max(y)\n    # Set bounds slightly wider than observed loss range, ensuring positivity.\n    # max(0.1, ...) ensures lower bound is at least 0.1, preventing issues if data is very low.\n    bounds.extend([(max(0.1, min_observed_loss - 1.0), max_observed_loss + 1.0)] * T)\n\n    best_params = None\n    min_mse = np.inf\n    num_restarts = 20 # Increased restarts for a more thorough search\n\n    def objective(params_flat):\n        \"\"\"Calculates the Mean Squared Error for the given parameters.\"\"\"\n        pred = scaling_law_func(X, params_flat)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # Prepare initial parameter candidates\n    initial_param_candidates = []\n\n    # Add a deterministic, robust initial guess first\n    init_exponents_det = np.full(F, 0.5) # Common scaling exponent heuristic\n    init_coeffs_flat_det = np.full(F * T, 0.1) # Small positive coefficients for initial influence\n    init_biases_det = np.mean(y, axis=0) # Average observed loss per output domain as baseline\n    initial_param_candidates.append(np.concatenate([init_exponents_det, init_coeffs_flat_det, init_biases_det]))\n\n    # Add random initial guesses for the remaining restarts\n    for _ in range(num_restarts - 1): # One slot used by deterministic initialization\n        # Exponents: Randomly initialized within a reasonable range for scaling laws.\n        init_exponents_rand = np.random.uniform(0.1, 2.0, size=F)\n        # Coefficients: Randomly initialized to allow for both positive and negative influences.\n        init_coeffs_flat_rand = np.random.uniform(-1.0, 1.0, size=F * T)\n        # Biases: Randomly initialized within a range around observed losses to cover typical values.\n        init_biases_rand = np.random.uniform(min_observed_loss * 0.9, max_observed_loss * 1.1, size=T)\n\n        initial_param_candidates.append(np.concatenate([init_exponents_rand, init_coeffs_flat_rand, init_biases_rand]))\n\n    for initial_params_candidate in initial_param_candidates:\n        # Clamp initial parameters to ensure they are strictly within the defined bounds.\n        # This is crucial for L-BFGS-B to start within a valid region and avoid numerical issues.\n        initial_params_clipped = np.array([np.clip(initial_params_candidate[i], bounds[i][0], bounds[i][1]) for i in range(P)])\n\n        # Use L-BFGS-B method with bounds for optimization.\n        result = minimize(objective, initial_params_clipped, method='L-BFGS-B', bounds=bounds, options={'disp': False})\n\n        # Update best parameters if a successful optimization yields a lower MSE.\n        if result.success and result.fun < min_mse:\n            min_mse = result.fun\n            best_params = result.x\n\n    # Fallback mechanism: If no successful optimization occurred across all restarts,\n    # or if all optimizations failed to find a better minimum than the initial `min_mse` (np.inf),\n    # use the robust deterministic initialization as the result after one final optimization attempt.\n    if best_params is None:\n        initial_params_fallback = np.concatenate([init_exponents_det, init_coeffs_flat_det, init_biases_det])\n        initial_params_fallback_clipped = np.array([np.clip(initial_params_fallback[i], bounds[i][0], bounds[i][1]) for i in range(P)])\n        \n        result_fallback = minimize(objective, initial_params_fallback_clipped, method='L-BFGS-B', bounds=bounds, options={'disp': False})\n        best_params = result_fallback.x if result_fallback.success else initial_params_fallback_clipped\n\n    return best_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.990394, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions using a generalized power law.\n\n    The model form for each output loss L_d (for domain d) is:\n    L_d = sum_f (c_df * x_f^alpha_f) + b_d\n\n    Where:\n    - c_df: Coefficient for input domain f contributing to output loss d.\n    - x_f: Proportion of input domain f.\n    - alpha_f: Exponent for input domain f's proportion (shared across all output losses).\n    - b_d: Bias (base loss) for output domain d.\n\n    Parameter Count:\n    - F_in (number of input domain proportions) = 5\n    - T_out (number of output loss dimensions) = 5\n    - num_coeffs = T_out * F_in = 5 * 5 = 25\n    - num_exponents = F_in = 5\n    - num_bias = T_out = 5\n    - Total parameters = 25 + 5 + 5 = 35\n\n    Args:\n        data_points (np.ndarray): (N, F_in) array with domain proportions.\n        params (np.ndarray): 1D array of 35 optimized parameters.\n\n    Returns:\n        np.ndarray: Predicted multi-domain loss values (N, T_out).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, F_in)\n    N, F_in = X.shape # F_in = 5 (number of input domain proportions)\n\n    T_out = F_in # Number of output loss dimensions is equal to F_in (5)\n\n    # Define parameter counts for unpacking\n    num_coeffs = T_out * F_in\n    num_exponents = F_in\n    num_bias = T_out\n\n    # Unpack parameters from the 1D 'params' array\n    # Ensure params has the correct length\n    expected_params_len = num_coeffs + num_exponents + num_bias\n    if len(params) != expected_params_len:\n        raise ValueError(f\"Expected {expected_params_len} parameters, got {len(params)}\")\n\n    coeffs_flat = params[:num_coeffs]\n    exponents = params[num_coeffs : num_coeffs + num_exponents]\n    bias = params[num_coeffs + num_exponents : expected_params_len]\n\n    coeffs = coeffs_flat.reshape(T_out, F_in) # Reshape coefficients to (T_out, F_in)\n\n    # Calculate X_f^alpha_f term carefully to handle 0 proportions for numerical stability.\n    # X_powered[n, f] will be:\n    #   - X[n,f]**exponents[f] if X[n,f] > 0\n    #   - 1.0 if X[n,f] == 0 and exponents[f] == 0 (0^0 = 1)\n    #   - 0.0 if X[n,f] == 0 and exponents[f] != 0 (0^pos=0, 0^neg=0 to avoid inf)\n    X_powered = np.zeros_like(X, dtype=float)\n    for f_idx in range(F_in):\n        # Mask for non-zero proportions\n        non_zero_mask = X[:, f_idx] > 0\n        X_powered[non_zero_mask, f_idx] = X[non_zero_mask, f_idx] ** exponents[f_idx]\n\n        # Handle X=0 and exponent=0 case explicitly (0^0 = 1)\n        # This is important for terms like c_df * x_f^0 = c_df * 1\n        if exponents[f_idx] == 0:\n            zero_mask = X[:, f_idx] == 0\n            X_powered[zero_mask, f_idx] = 1.0\n\n    # Calculate predicted losses using matrix multiplication:\n    # pred[n, t] = sum_f (coeffs[t, f] * X_powered[n, f]) + bias[t]\n    pred = X_powered @ coeffs.T + bias[None, :]\n\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes the parameters of the scaling law function using L-BFGS-B.\n    Improvements in this evolution:\n    - Slightly adjusted initial guess for exponents to encourage exploration of sub-linear relationships.\n      Starting with 0.5 (square root-like behavior) can be a good heuristic for scaling laws.\n    - Maintained robust bias bounds and advanced optimization options (ftol, gtol, maxiter)\n      for better convergence and numerical stability, leveraging the observed loss range.\n\n    Args:\n        data_points (np.ndarray): (N, F_in) array with domain proportions.\n        loss_values (np.ndarray): (N, T_out) array of corresponding multi-domain losses.\n\n    Returns:\n        np.ndarray: Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, F_in)\n    y = np.asarray(loss_values)                 # (N, T_out)\n\n    N, F_in = X.shape\n    T_out = y.shape[1] # Number of output loss dimensions (expected to be F_in)\n\n    # Total parameters P = T_out * F_in + F_in + T_out = 5*5 + 5 + 5 = 35\n    P = T_out * F_in + F_in + T_out\n\n    # Initial parameter guesses\n    init_coeffs = np.ones(T_out * F_in) * 0.1\n    # Changed initial exponents from 1.0 to 0.5 to explore sub-linear scaling from the start.\n    init_exponents = np.ones(F_in) * 0.5\n    init_bias = np.mean(y, axis=0) # Mean loss for each output dimension\n\n    init_params = np.concatenate([init_coeffs, init_exponents, init_bias])\n\n    def objective(flat_params):\n        \"\"\"Objective function to minimize (Mean Squared Error).\"\"\"\n        pred = scaling_law_func(X, flat_params)  # (N, T_out)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # Define bounds for parameters for L-BFGS-B optimization\n    bounds = []\n    # coeffs bounds (T_out * F_in parameters): Allow positive or negative contributions\n    for _ in range(T_out * F_in):\n        bounds.append((-10.0, 10.0))\n    # exponents bounds (F_in parameters): Allow negative, zero, or positive exponents\n    for _ in range(F_in):\n        bounds.append((-5.0, 5.0))\n    # bias bounds (T_out parameters): Losses are positive (1.8-4.2), ensure a more robust positive baseline.\n    for _ in range(T_out):\n        # Adjusted from 0.01 to 0.5 for a more stable positive baseline, considering observed loss range.\n        bounds.append((0.5, 5.0))\n\n    # Optimization options for potentially better convergence\n    options = {\n        'ftol': 1e-10,  # Relative error in objective function value (tighter than default 1e-9)\n        'gtol': 1e-6,   # Gradient norm (tighter than default 1e-5)\n        'maxiter': 1000 # Maximum number of iterations (increased from default 15000 / len(x) to 1000 for potentially longer search)\n    }\n\n    result = minimize(objective, init_params, method='L-BFGS-B', bounds=bounds, options=options)\n\n    # Return optimized parameters if successful, otherwise the initial parameters\n    params_opt = result.x if result.success else init_params\n\n    return params_opt\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.990374, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain proportions and parameters.\n\n    The model uses a shared set of exponents for all input domains,\n    and a specific set of coefficients and a bias for each output domain.\n    L_j = B_j + sum_i (C_ji * (X_i + epsilon)^E_i)\n\n    Args:\n        data_points (np.ndarray): (N, 5) array with domain proportions for 5 domains.\n        params (np.ndarray): A 1D array of 35 parameters structured as:\n            [E_1..E_5, # 5 shared exponents for input domains\n             C_11..C_15, C_21..C_25, ..., C_51..C_55, # 25 coefficients (5x5 matrix)\n             B_1..B_5] # 5 biases for output domains\n\n    Returns:\n        np.ndarray: Predicted multi-domain loss values (N, 5).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N, F_in)\n    N, F_in = X.shape # F_in = 5 (number of input domains)\n    F_out = 5         # Number of output domains (fixed at 5)\n\n    # Unpack parameters (total 35)\n    # 5 shared exponents for the input domains\n    exponents = params[0:F_in] # (F_in,)\n\n    # 25 coefficients (F_out x F_in matrix)\n    # C_ji: coefficient for input domain i contributing to output domain j\n    coeffs_start = F_in\n    coeffs_end = F_in + (F_out * F_in)\n    coeffs_matrix = params[coeffs_start:coeffs_end].reshape(F_out, F_in) # (F_out, F_in)\n\n    # 5 biases (one for each output domain)\n    bias_start = coeffs_end\n    biases = params[bias_start:bias_start + F_out] # (F_out,)\n\n    # Calculate the powered input proportions: (X_i + epsilon)^E_i\n    # Add a small epsilon for numerical stability, especially when X_i = 0 and E_i < 0.\n    # Epsilon prevents 0^negative_exponent from becoming infinite.\n    epsilon = 1e-6\n    # np.power handles 0^E for E > 0 correctly (results in 0).\n    # X + epsilon ensures that proportions of 0 become a very small positive number,\n    # preventing issues with 0^negative_exponent.\n    powered_X = np.power(X + epsilon, exponents[None, :])\n\n    # Calculate predictions for each output domain:\n    # pred_j = sum_i (coeffs_matrix[j, i] * powered_X[:, i]) + biases[j]\n    # This is equivalent to matrix multiplication: powered_X @ coeffs_matrix.T\n    # (N, F_in) @ (F_in, F_out) -> (N, F_out)\n    pred = powered_X @ coeffs_matrix.T + biases[None, :]\n\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling law function to the given data points and loss values.\n\n    Args:\n        data_points (np.ndarray): (N, 5) array with domain proportions.\n        loss_values (np.ndarray): (N, 5) array of corresponding multi-domain losses.\n\n    Returns:\n        np.ndarray: Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)  # (N, F_out)\n\n    N, F_in = X.shape  # F_in = 5\n    F_out = y.shape[1] # F_out = 5\n\n    # Total parameters: F_in (exponents) + F_out * F_in (coeffs) + F_out (biases)\n    total_params = F_in + (F_out * F_in) + F_out # 5 + (5*5) + 5 = 35\n\n    # Initialize parameters\n    # Exponents: Start with 0.5 (sqrt-like scaling)\n    init_exponents = np.full(F_in, 0.5)\n\n    # Coefficients: Start with small values, allowing optimizer to explore both positive and negative.\n    # Initializing closer to zero (0.01) allows for easier discovery of negative coefficients.\n    init_coeffs = np.full(F_out * F_in, 0.01)\n\n    # Biases: Start with the mean loss for each output domain, a robust initial guess.\n    init_biases = np.mean(y, axis=0) # (F_out,)\n\n    # Combine into a single initial parameter array\n    initial_params = np.concatenate([init_exponents, init_coeffs, init_biases])\n\n    def objective(flat_params):\n        \"\"\"Calculates the Mean Squared Error for the given parameters.\"\"\"\n        pred = scaling_law_func(X, flat_params)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # Define bounds for parameters to ensure numerical stability and physical plausibility\n    bounds = []\n    # Exponents (F_in): Allow for negative exponents to model inverse scaling.\n    # Range chosen to cover typical scaling behaviors while preventing extreme values.\n    bounds.extend([(-1.5, 2.0)] * F_in) \n    \n    # Coefficients (F_out * F_in): Crucially, allow negative contributions for cross-domain generalization.\n    # The range (-1.0, 1.0) is adopted from the top-performing program (Program 1)\n    # as it proved effective for capturing beneficial cross-domain effects.\n    bounds.extend([(-1.0, 1.0)] * (F_out * F_in))\n\n    # Biases (F_out): Losses are positive and typically in the range 1.8-4.2.\n    # Tighter bounds (1.5, 4.5) adopted from Program 1 to reflect observed loss range\n    # and improve stability, preventing unrealistic predictions.\n    bounds.extend([(1.5, 4.5)] * F_out)\n\n    # Use L-BFGS-B for optimization as it supports bounds, which are crucial here.\n    result = minimize(objective, initial_params, method='L-BFGS-B', bounds=bounds)\n\n    # Return optimized parameters if successful, otherwise return initial parameters\n    # This fallback ensures stability if the optimization fails for some reason.\n    params_opt = result.x if result.success else initial_params\n    return params_opt\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-2.5-flash", "reward_r2": 0.990372, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nEvolved program with a shared-exponent power law model for multi-domain loss prediction,\ndesigned for parameter efficiency and improved accuracy, specifically adhering to the 35-parameter limit.\n\nThe model is based on the highest-performing previous attempt, which achieved a combined_score of 0.9982.\nIt uses shared exponents for input proportions across all output loss domains,\nalong with domain-specific coefficients and biases.\n\nModel form for each output domain 'd':\nLoss_d = sum_f (coeff_df * (proportion_f + epsilon)^exponent_f) + bias_d\n\nParameter breakdown (for F=5 input domains and D=5 output loss domains):\n- coeffs_flat (D*F): 5 * 5 = 25 parameters\n- exponents (F): 5 parameters (shared across output domains, now can be negative for inverse power laws)\n- biases (D): 5 parameters\nTotal parameters: 25 + 5 + 5 = 35. This strictly adheres to the constraint.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain loss values based on domain mixture proportions\n    using a shared-exponent power law model, allowing for inverse power laws.\n\n    Args:\n        data_points (np.ndarray): (N, F) array with domain proportions for F domains.\n                                  Each row sums to 1.0. F is 5.\n        params (np.ndarray): 1D array of 35 parameters.\n                             Structure: [coeffs_flat (D*F), exponents (F), biases (D)]\n                             where F=5 (input domains) and D=5 (output domains).\n\n    Returns:\n        np.ndarray: Predicted multi-domain loss values (N, D).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, F)\n    N, F = X.shape  # F = 5 (number of input domains/features)\n    D = 5           # Number of output domains (loss values for 5 domains)\n\n    # Parameter structure: [coeffs_flat (D*F), exponents (F), biases (D)]\n    # Total parameters P = D*F + F + D = 5*5 + 5 + 5 = 35\n\n    # Reshape parameters from the flat 1D array\n    coeffs_flat = params[:D*F]\n    exponents = params[D*F : D*F + F]\n    biases = params[D*F + F : D*F + F + D]\n\n    coeffs = coeffs_flat.reshape(D, F) # (D, F) matrix for domain-specific coefficients\n\n    # Add a small epsilon for numerical stability.\n    # This prevents issues with np.power(0, negative_exponent) which would result in inf.\n    # It also handles np.power(0, 0) = 1.0 and np.power(0, positive_exponent) = 0.0 correctly.\n    epsilon = 1e-6 # Fixed constant, not a parameter, for numerical stability.\n    powered_X = np.power(X + epsilon, exponents[None, :]) # (N, F) broadcasted with (1, F) -> (N, F)\n\n    # Calculate predictions using Einstein summation for efficiency:\n    # pred_nd = sum_f (coeffs_df * powered_X_nf) + biases_d\n    # 'df' refers to coeffs (D, F), 'nf' refers to powered_X (N, F)\n    # The result 'nd' is (N, D)\n    pred = np.einsum('df,nf->nd', coeffs, powered_X) + biases[None, :] # (N, D) + (1, D) -> (N, D)\n\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes the parameters for the scaling_law_func to best fit the\n    given data_points and loss_values.\n\n    Args:\n        data_points (np.ndarray): (N, 5) array with domain proportions.\n        loss_values (np.ndarray): (N, 5) array of corresponding multi-domain losses.\n\n    Returns:\n        np.ndarray: Optimized parameters (1D array of 35 parameters).\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, F)\n    y = np.asarray(loss_values, dtype=np.float64)                 # (N, D)\n\n    N, F = X.shape  # F=5 (input domains)\n    D = y.shape[1]  # D=5 (output domains)\n\n    # Total number of parameters P = D*F (coeffs) + F (exponents) + D (biases)\n    P = D * F + F + D # P = 5*5 + 5 + 5 = 35\n\n    # Initialize parameters with reasonable starting values\n    # Coefficients: Small positive values, allowing for positive and negative contributions.\n    init_coeffs = np.full(D*F, 0.1, dtype=np.float64)\n    # Exponents: Start at 0.0, allowing the optimizer to explore both positive and negative power relationships.\n    init_exponents = np.full(F, 0.0, dtype=np.float64)\n    # Biases: Initialize to the mean loss for each output domain to provide a good baseline.\n    init_biases = np.mean(y, axis=0, dtype=np.float64) # (D,) array\n\n    # Combine initial parameters into a single 1D array\n    init_params = np.concatenate([init_coeffs, init_exponents, init_biases])\n\n    def objective(flat_params):\n        \"\"\"Objective function to minimize (Mean Squared Error).\"\"\"\n        pred = scaling_law_func(X, flat_params)  # (N, D)\n        mse = np.mean((pred - y) ** 2)\n        return mse\n\n    # Define bounds for parameters to ensure numerical stability and interpretability.\n    # Using L-BFGS-B, which supports bounds.\n    bounds = []\n    # 1. Coeffs (D*F parameters): Wider range for more flexibility, allowing larger contributions.\n    for _ in range(D*F):\n        bounds.append((-10.0, 10.0)) \n\n    # 2. Exponents (F parameters): Allow negative exponents for inverse power laws.\n    #    Bounds from -5.0 to 5.0 cover a wide range of common scaling behaviors.\n    for _ in range(F):\n        bounds.append((-5.0, 5.0))\n\n    # 3. Biases (D parameters): Should align with the loss value range.\n    #    Since losses are positive (1.8-4.2), biases are likely positive.\n    #    A slightly tighter range (0.5 to 5.0) can help guide the optimizer.\n    for _ in range(D):\n        bounds.append((0.5, 5.0))\n\n    # Perform optimization using L-BFGS-B method with bounds\n    # Increased maxiter and tightened tolerances further for potentially better convergence.\n    result = minimize(objective, init_params, method='L-BFGS-B', bounds=bounds,\n                      options={'maxiter': 5000, 'ftol': 1e-10, 'gtol': 1e-8})\n    \n    # Return optimized parameters. If optimization fails, return the initial parameters.\n    params_opt = result.x if result.success else init_params\n\n    return params_opt\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.993529, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nImplements a \"Multiplicative Interaction\" Scaling Law:\n    Loss_t = A_t * (x_t + eps_t)^(-alpha_t) * exp(sum(w_tj * x_j))\n\nThis model captures the intuition that cross-domain interactions (interference or transfer)\nact as relative modifiers (multipliers) to the base performance, rather than constant additive offsets.\nLog-transforming this model yields a linear form, allowing for extremely robust initialization via\nLinear Regression, followed by non-linear refinement.\n\nFitting Strategy:\n1. Grid Search + Linear Regression on dominant data to find A, alpha, eps.\n2. Ridge Regression on log-residuals to initialize interaction weights w.\n3. Trust Region Reflective (TRF) optimization on the full non-linear objective.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) array of domain proportions\n    # params: (35,) array of flattened parameters\n    \n    X = np.atleast_2d(np.asarray(data_points))\n    N, F = X.shape\n    params = np.asarray(params)\n\n    # Validate dimensions\n    if F != 5 or params.size != 35:\n        return np.zeros((N, 5)) if F==5 else np.zeros((N, 1))\n\n    # Reshape parameters: 5 domains x 7 parameters\n    # [A, alpha, eps, w_1, w_2, w_3, w_4]\n    P = params.reshape(5, 7)\n    preds = np.zeros((N, 5))\n    \n    for t in range(5):\n        # Extract parameters\n        A = P[t, 0]\n        alpha = P[t, 1]\n        eps = P[t, 2]\n        w = P[t, 3:] \n        \n        # Features\n        xt = X[:, t]\n        \n        # Other domains\n        mask = np.ones(5, dtype=bool)\n        mask[t] = False\n        x_others = X[:, mask]\n        \n        # Term 1: Power Law with Knee\n        # Base must be positive. \n        base = np.maximum(xt + eps, 1e-9)\n        term_power = A * (base ** -alpha)\n        \n        # Term 2: Multiplicative Interaction\n        # Models relative impact of other domains.\n        # Clip exponent to prevent numerical overflow/underflow\n        interaction = x_others @ w\n        term_interact = np.exp(np.clip(interaction, -20.0, 20.0))\n        \n        preds[:, t] = term_power * term_interact\n        \n    return preds\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    \n    if y.ndim == 1:\n        y = y[:, None]\n        \n    N, F = X.shape\n    _, T = y.shape\n    \n    if F != 5 or T != 5:\n        return np.zeros(35)\n        \n    all_params = []\n    \n    for t in range(T):\n        xt = X[:, t]\n        yt = y[:, t]\n        \n        mask = np.ones(5, dtype=bool)\n        mask[t] = False\n        x_others = X[:, mask]\n        \n        # --- Stage 1: Robust Initialization of Base Power Law ---\n        # We find A, alpha, eps by fitting to data where x_t is dominant.\n        # In this regime, x_others is small, so exp(w*x_others) approx 1.\n        \n        thresh = np.percentile(xt, 50)\n        idx_s1 = xt >= thresh\n        # Fallback if data is sparse\n        if np.sum(idx_s1) < 5:\n            idx_s1 = np.arange(N)\n            \n        xt_s1 = xt[idx_s1]\n        yt_s1 = yt[idx_s1]\n        \n        # Grid Search for Epsilon using Log-Linear Regression\n        # ln(y) = ln(A) - alpha * ln(x + eps)\n        best_s1 = [np.mean(yt_s1), 0.5, 1e-3]\n        best_mse = np.inf\n        \n        # Safe log of targets\n        log_y_s1 = np.log(np.maximum(yt_s1, 1e-9))\n        \n        # Try various epsilon values to linearize the power law\n        eps_candidates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n        \n        for eps_try in eps_candidates:\n            log_x_s1 = np.log(np.maximum(xt_s1 + eps_try, 1e-9))\n            \n            # Analytical Linear Regression for Slope (-alpha) and Intercept (ln A)\n            n = len(log_y_s1)\n            mean_x = np.mean(log_x_s1)\n            mean_y = np.mean(log_y_s1)\n            \n            numer = np.sum((log_x_s1 - mean_x) * (log_y_s1 - mean_y))\n            denom = np.sum((log_x_s1 - mean_x) ** 2)\n            \n            if denom < 1e-12:\n                alpha_est = 0.0\n                ln_A = mean_y\n            else:\n                slope = numer / denom\n                # Enforce negative slope (decaying loss)\n                alpha_est = -slope if slope < 0 else 0.0\n                ln_A = mean_y + alpha_est * mean_x\n                \n            A_est = np.exp(ln_A)\n            \n            # Evaluate fit\n            pred_log = ln_A - alpha_est * log_x_s1\n            mse = np.mean((pred_log - log_y_s1)**2)\n            \n            if mse < best_mse:\n                best_mse = mse\n                best_s1 = [A_est, alpha_est, eps_try]\n                \n        # --- Stage 2: Initialize Interaction Weights ---\n        # Use full dataset to fit weights w via Log-Linear Ridge Regression\n        # ln(y) - ln(PowerLaw) = w * x_others\n        \n        A_init, alpha_init, eps_init = best_s1\n        \n        log_y_full = np.log(np.maximum(yt, 1e-9))\n        log_base_full = np.log(np.maximum(xt + eps_init, 1e-9))\n        \n        # Residual to be explained by interactions\n        residual = log_y_full - (np.log(A_init) - alpha_init * log_base_full)\n        \n        # Ridge Regression: w = (X^T X + lambda I)^-1 X^T y\n        lambda_reg = 1e-3\n        XTX = x_others.T @ x_others\n        XTy = x_others.T @ residual\n        \n        try:\n            w_init = np.linalg.solve(XTX + lambda_reg * np.eye(4), XTy)\n        except:\n            w_init = np.zeros(4)\n            \n        # --- Stage 3: Full Non-Linear Refinement ---\n        # Optimize minimizing MSE on original scale (not log scale)\n        \n        p0_full = np.concatenate([best_s1, w_init])\n        \n        def fun_full(p):\n            A, alpha, eps = p[0], p[1], p[2]\n            w = p[3:]\n            \n            base = np.maximum(xt + eps, 1e-9)\n            # Clip interaction to safe range during optimization\n            interaction = np.clip(x_others @ w, -20, 20)\n            \n            pred = A * (base ** -alpha) * np.exp(interaction)\n            return pred - yt\n            \n        # Bounds\n        # A, alpha > 0. eps > 0. Weights bounded [-20, 20]\n        bounds_low = [1e-6, 0.0, 1e-9] + [-20.0]*4\n        bounds_high = [np.inf, 6.0, 1.0] + [20.0]*4\n        \n        try:\n            res_full = least_squares(fun_full, p0_full, \n                                     bounds=(bounds_low, bounds_high),\n                                     method='trf', loss='linear', max_nfev=1000)\n            best_p = res_full.x\n        except:\n            best_p = p0_full\n            \n        all_params.append(best_p)\n        \n    return np.concatenate(all_params)\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.981953, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nImplements a 35-parameter Domain-Decoupled Coupled Power Law model.\nKey innovation: Decomposes the optimization into 5 independent per-domain problems,\nsolving for Irreducible Loss (E), Amplitude (A), Exponent (alpha), and Transfer (gamma)\nsequentially using Grid Search, NNLS, and TRF-bounded Least Squares.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import least_squares, nnls\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain losses using a coupled power-law model with transfer.\n    Model: L_i = E_i + A_i * (x_i + sum_{j!=i} gamma_{ij} * x_j)^(-alpha_i)\n    \n    Args:\n        data_points: (N, 5) array of domain mixture proportions.\n        params: (35,) array of parameters.\n            0-4: E (Irreducible loss)\n            5-9: A (Amplitude)\n            10-14: alpha (Exponent)\n            15-34: gamma (Off-diagonal transfer coefficients, row-major)\n            \n    Returns:\n        (N, 5) array of predicted losses.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).ravel()\n    \n    # Robust parameter handling: Ensure exactly 35 parameters\n    expected_size = 35\n    if params.size != expected_size:\n        if params.size < expected_size:\n            params = np.pad(params, (0, expected_size - params.size), constant_values=0.0)\n        else:\n            params = params[:expected_size]\n            \n    # Unpack parameters\n    # Use abs() to enforce physical constraints (positivity) during evaluation\n    E = np.abs(params[0:5])\n    A = np.abs(params[5:10])\n    alpha = np.abs(params[10:15])\n    gamma = np.abs(params[15:35])\n    \n    n_domains = 5\n    \n    # Construct Transfer Matrix M\n    # M[i, j] is the weight of domain j's data for domain i's task\n    # Diagonal is 1.0 (baseline self-transfer)\n    M = np.eye(n_domains)\n    \n    # Fill off-diagonals\n    # Maps 20 gamma values to the 20 off-diagonal positions in row-major order\n    # i.e., Row 0 (cols 1,2,3,4), Row 1 (cols 0,2,3,4), etc.\n    mask = ~np.eye(n_domains, dtype=bool)\n    M[mask] = gamma\n    \n    # Calculate Effective Data\n    # D_eff = X @ M.T\n    # Shape: (N, 5)\n    # D_eff[n, i] = X[n, i] + sum_{j!=i} gamma_{ij} * X[n, j]\n    D_eff = X @ M.T\n    \n    # Add epsilon for numerical stability\n    # If D_eff is very close to 0, power law blows up.\n    D_eff = np.maximum(D_eff, 1e-9)\n    \n    # Power Law Calculation\n    # L = E + A * D_eff^(-alpha)\n    \n    # Broadcasting:\n    # D_eff: (N, 5)\n    # alpha, A, E: (5,) -> (1, 5)\n    term = D_eff ** (-alpha[None, :])\n    pred = E[None, :] + A[None, :] * term\n    \n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fits the scaling law parameters using a robust decoupled optimization strategy.\n    Optimizes each domain's 7 parameters independently, which is mathematically \n    equivalent to global optimization for this loss function but significantly \n    more stable and faster.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    \n    # Ensure Y is (N, 5)\n    if Y.ndim == 1:\n        Y = Y[:, None]\n        \n    N, n_domains = Y.shape\n    \n    # Storage for optimized parameters\n    E_opt = np.zeros(n_domains)\n    A_opt = np.zeros(n_domains)\n    alpha_opt = np.zeros(n_domains)\n    gamma_opt = [] # Will accumulate 20 values (4 per domain)\n    \n    # --- Per-Domain Optimization Loop ---\n    # Since L_total = sum(L_i), we can minimize L_i independently w.r.t its parameters\n    for i in range(n_domains):\n        y_vec = Y[:, i]\n        x_vec = X[:, i]\n        min_y = np.min(y_vec)\n        \n        # 1. Robust Initialization via Grid Search + Log-Linear Fit\n        # We first assume D_eff ~ x_vec (no transfer) to find coarse E, A, alpha\n        # This prevents getting stuck in local optima with poor E values.\n        \n        best_init_loss = np.inf\n        # Default fallback\n        best_p_init = (max(0, min_y - 0.1), 1.0, 0.5) \n        \n        # Grid for E: from 0 to slightly below min_y\n        e_candidates = np.linspace(0, max(0, min_y - 1e-4), 20)\n        \n        # Filter data for log fit (ignore zeros)\n        valid_mask = x_vec > 1e-4\n        if np.sum(valid_mask) < 3:\n             valid_mask = np.ones_like(x_vec, dtype=bool)\n             \n        x_valid = x_vec[valid_mask] + 1e-9\n        y_valid = y_vec[valid_mask]\n        log_x = np.log(x_valid)\n        \n        for e_val in e_candidates:\n            y_shifted = y_valid - e_val\n            if np.any(y_shifted <= 1e-9):\n                continue\n                \n            log_y_shifted = np.log(y_shifted)\n            \n            # Linear Fit: log(y-E) = log(A) - alpha * log(x)\n            try:\n                slope, intercept = np.polyfit(log_x, log_y_shifted, 1)\n                alpha_est = -slope\n                a_est = np.exp(intercept)\n                \n                # Check constraints roughly\n                if alpha_est > 0 and a_est > 0:\n                    # Calculate MSE with these params on the subset\n                    pred = e_val + a_est * (x_valid ** -alpha_est)\n                    mse = np.mean((pred - y_valid)**2)\n                    \n                    if mse < best_init_loss:\n                        best_init_loss = mse\n                        best_p_init = (e_val, a_est, alpha_est)\n            except:\n                continue\n                \n        e_init, a_init, alpha_init = best_p_init\n        \n        # Clamp initial values to sane ranges to assist convergence\n        alpha_init = np.clip(alpha_init, 0.1, 4.0)\n        a_init = np.maximum(a_init, 1e-4)\n        \n        # 2. Estimate Transfer (Gamma) via NNLS\n        # We want: x_i + sum(g * x_other) = ((y - E)/A)^(-1/alpha)\n        # Target D_eff required to explain the residual loss\n        y_clean = np.maximum(y_vec - e_init, 1e-7)\n        target_D = (y_clean / a_init) ** (-1.0 / alpha_init)\n        \n        rhs = target_D - x_vec\n        \n        # Regressors: other domain proportions\n        other_indices = [j for j in range(n_domains) if j != i]\n        X_other = X[:, other_indices]\n        \n        try:\n            # Solve non-negative least squares: X_other * gamma = rhs\n            g_init, _ = nnls(X_other, rhs)\n        except:\n            g_init = np.zeros(4)\n            \n        # 3. Final Optimization for Domain i\n        # Parameter vector: [E, A, alpha, g0, g1, g2, g3] (7 params)\n        p0_local = np.concatenate([[e_init, a_init, alpha_init], g_init])\n        \n        # Bounds: E < min_y, A > 0, alpha > 0, g >= 0\n        ub_E = np.maximum(0.0, min_y - 1e-6)\n        # Lower bounds\n        lb = [0.0, 1e-6, 0.01] + [0.0]*4\n        # Upper bounds\n        ub = [ub_E, np.inf, 6.0] + [np.inf]*4\n        \n        def residuals_local(p):\n            curr_E, curr_A, curr_alpha = p[0], p[1], p[2]\n            curr_g = p[3:]\n            \n            # Reconstruct effective data: D = x_i + X_other @ g\n            transfer = X_other @ curr_g\n            d_eff = x_vec + transfer\n            d_eff = np.maximum(d_eff, 1e-9)\n            \n            pred = curr_E + curr_A * (d_eff ** -curr_alpha)\n            return pred - y_vec\n            \n        try:\n            # Trust Region Reflective (TRF) is robust for bound-constrained problems\n            res = least_squares(residuals_local, p0_local, bounds=(lb, ub),\n                                method='trf', loss='linear',\n                                ftol=1e-9, xtol=1e-9, gtol=1e-9, max_nfev=1000)\n            p_final = res.x\n        except:\n            p_final = p0_local\n            \n        # Store results\n        E_opt[i] = p_final[0]\n        A_opt[i] = p_final[1]\n        alpha_opt[i] = p_final[2]\n        gamma_opt.extend(p_final[3:])\n        \n    # Combine into 35-parameter vector\n    # Order: E(5), A(5), alpha(5), gamma(20)\n    final_params = np.concatenate([E_opt, A_opt, alpha_opt, np.array(gamma_opt)])\n    \n    return final_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.98191, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import least_squares, nnls\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predicts multi-domain losses using a power-law with learned data mixing.\n    Model: L_j = bias_j + scale_j * (x_j + sum_{k!=j} w_jk * x_k)^(-alpha_j)\n    \n    Args:\n        data_points: (N, 5) domain proportions\n        params: (35,) flattened parameters [bias, log_scale, log_alpha, 4 log_weights] * 5\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    params = np.asarray(params).ravel()\n    \n    # Pad or truncate to ensure 35 parameters\n    if params.size != 35:\n        if params.size > 35: params = params[:35]\n        else: params = np.pad(params, (0, 35 - params.size), 'constant')\n\n    n_domains = 5\n    # Reshape: (5, 7)\n    p_mat = params.reshape(n_domains, 7)\n    \n    bias = p_mat[:, 0].reshape(1, -1)\n    scale = np.exp(p_mat[:, 1]).reshape(1, -1)\n    alpha = np.exp(p_mat[:, 2]).reshape(1, -1)\n    weights = np.exp(p_mat[:, 3:]) # (5, 4)\n    \n    # Efficient vectorized effective data computation\n    # Create mixing matrix B (5, 5)\n    B = np.eye(n_domains)\n    for i in range(n_domains):\n        # Map the 4 weights to the off-diagonal positions\n        mask = np.ones(n_domains, dtype=bool)\n        mask[i] = False\n        B[i, mask] = weights[i]\n        \n    # D_eff: (N, 5)\n    # D_eff[k, i] = sum_j X[k, j] * B[i, j]\n    D_eff = np.maximum(X @ B.T, 1e-10)\n    \n    # Prediction\n    # L = bias + scale * D_eff ^ -alpha\n    pred = bias + scale * (D_eff ** -alpha)\n    \n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Optimizes scaling law parameters per domain using an Alternating Optimization (AO) strategy\n    for robust initialization, followed by Levenberg-Marquardt refinement.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.asarray(loss_values)\n    if Y.ndim == 1: Y = Y[:, None]\n    \n    n_domains = 5\n    n_samples = X.shape[0]\n    final_params = []\n    \n    for i in range(n_domains):\n        y_target = Y[:, i]\n        \n        # Prepare data for this domain\n        mask = np.ones(n_domains, dtype=bool)\n        mask[i] = False\n        X_self = X[:, i]\n        X_others = X[:, mask] # (N, 4)\n        \n        # --- Stage 1: Alternating Optimization for Initialization ---\n        # Initialize w to small values (self-scaling assumption)\n        w_curr = np.full(4, 1e-6)\n        \n        best_ao_params = None # (b, s, a, w)\n        best_ao_mse = np.inf\n        \n        y_min = np.min(y_target)\n        \n        # We iterate to refine w and (b,s,a)\n        # 5 iterations is usually sufficient for convergence of the alternating scheme\n        for _ in range(5):\n            # 1. Fix w, optimize b, s, alpha\n            D_est = X_self + X_others @ w_curr\n            D_est = np.maximum(D_est, 1e-10)\n            log_D = np.log(D_est)\n            \n            # Grid search for bias\n            # Bias must be < min(y). We scan a range.\n            b_grid = np.linspace(max(0, y_min - 2.0), y_min - 1e-5, 20)\n            \n            iter_best_p = None\n            iter_best_err = np.inf\n            \n            for b_val in b_grid:\n                # Filter valid\n                valid = y_target > b_val\n                if np.sum(valid) < 5: continue\n                \n                ly = np.log(y_target[valid] - b_val)\n                lx = log_D[valid]\n                \n                # Linear Regression: ly = log(s) - alpha * lx\n                vx = np.var(lx)\n                if vx < 1e-10: continue\n                \n                cov = np.cov(lx, ly, bias=True)[0, 1]\n                alpha_est = -cov / vx\n                \n                # Constraint: alpha > 0\n                if alpha_est <= 0.001: continue\n                \n                log_s_est = np.mean(ly) + alpha_est * np.mean(lx)\n                s_est = np.exp(log_s_est)\n                \n                # Check MSE\n                pred = b_val + s_est * (D_est ** -alpha_est)\n                mse = np.mean((pred - y_target)**2)\n                \n                if mse < iter_best_err:\n                    iter_best_err = mse\n                    iter_best_p = (b_val, s_est, alpha_est)\n            \n            if iter_best_p is None:\n                if best_ao_params is None:\n                    iter_best_p = (y_min - 0.1, 1.0, 0.5)\n                else:\n                    iter_best_p = best_ao_params[:3]\n            \n            b_opt, s_opt, a_opt = iter_best_p\n            \n            # Update best global AO\n            if iter_best_err < best_ao_mse:\n                best_ao_mse = iter_best_err\n                best_ao_params = (b_opt, s_opt, a_opt, w_curr.copy())\n            \n            # 2. Fix b, s, alpha, optimize w via NNLS\n            # Model: y approx b + s * (x_s + X_o w)^-a\n            # (x_s + X_o w) approx ((y - b)/s)^(-1/a)\n            # X_o w approx ((y - b)/s)^(-1/a) - x_s\n            \n            valid_mask = y_target > b_opt\n            if np.sum(valid_mask) == 0: break\n            \n            rhs_term = (y_target[valid_mask] - b_opt) / s_opt\n            rhs_term = np.maximum(rhs_term, 1e-10)\n            \n            target_D = np.power(rhs_term, -1.0 / a_opt)\n            target_resid = target_D - X_self[valid_mask]\n            \n            # Solve NNLS: argmin_w || X_o[valid] * w - target_resid ||^2, w >= 0\n            try:\n                w_new, _ = nnls(X_others[valid_mask], target_resid)\n                w_curr = w_new\n            except:\n                pass \n                \n        # Fallback\n        if best_ao_params is None:\n            p0 = np.array([y_min - 0.1, 0.0, np.log(0.5)] + [np.log(0.01)]*4)\n        else:\n            b, s, a, w = best_ao_params\n            p0 = np.concatenate(([b, np.log(s), np.log(a)], np.log(np.maximum(w, 1e-9))))\n\n        # --- Stage 2: Levenberg-Marquardt Refinement ---\n        \n        def get_residuals(p):\n            b = p[0]\n            s = np.exp(p[1])\n            a = np.exp(p[2])\n            w = np.exp(p[3:])\n            \n            d_eff = X_self + X_others @ w\n            d_eff = np.maximum(d_eff, 1e-10)\n            \n            pred = b + s * (d_eff ** -a)\n            return pred - y_target\n\n        def get_jacobian(p):\n            b = p[0]\n            s = np.exp(p[1])\n            a = np.exp(p[2])\n            w = np.exp(p[3:])\n            \n            d_eff = X_self + X_others @ w\n            d_eff = np.maximum(d_eff, 1e-10)\n            \n            term_pow = d_eff ** -a\n            term_scaled = s * term_pow # s * D^-a\n            \n            J = np.zeros((n_samples, 7))\n            \n            # db\n            J[:, 0] = 1.0\n            \n            # d(log_scale) = s * D^-a\n            J[:, 1] = term_scaled\n            \n            # d(log_alpha) = s * D^-a * (-a * ln D)\n            J[:, 2] = term_scaled * (-a * np.log(d_eff))\n            \n            # d(log_weights)\n            # dPred/dw = - s * a * D^(-a-1) * x_other\n            # dPred/d(log w) = dPred/dw * w = - s * a * D^(-a) / D * x_other * w\n            common = - (term_scaled * a) / d_eff\n            J[:, 3:] = common[:, None] * X_others * w[None, :]\n            \n            return J\n\n        try:\n            res = least_squares(get_residuals, p0, jac=get_jacobian, method='lm',\n                                ftol=1e-10, xtol=1e-10, gtol=1e-10, max_nfev=2000)\n            final_params.append(res.x)\n        except:\n            final_params.append(p0)\n            \n    return np.concatenate(final_params)\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.981906, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios.\nImplements a Coupled Power Law with a full rank transfer matrix (35 parameters).\nUses dimension-decoupled Levenberg-Marquardt optimization for robust convergence.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import least_squares\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) domain proportions\n    # params: (35,) flat array of parameters\n    \n    X = np.atleast_2d(np.asarray(data_points)) # (N, 5)\n    \n    params = np.asarray(params)\n    if params.ndim == 2:\n        params = params[0]\n        \n    # Unpack parameters (35 total)\n    # 0-4:   log A (scale)\n    # 5-9:   log alpha (exponent)\n    # 10-14: B (irreducible loss)\n    # 15-34: M_raw (off-diagonal transfer coefficients)\n    \n    p_A     = params[0:5]\n    p_alpha = params[5:10]\n    p_B     = params[10:15]\n    p_M     = params[15:35]\n    \n    # Parameter transformations\n    # A > 0, alpha > 0. Use exp with clipping for safety.\n    A     = np.exp(np.clip(p_A, -15.0, 10.0))\n    alpha = np.exp(np.clip(p_alpha, -5.0, 5.0))\n    B     = p_B\n    \n    # Construct Transfer Matrix C (5x5)\n    # Diagonal is 1.0. Off-diagonals are softplus(p_M).\n    C = np.eye(5)\n    \n    # softplus(x) = log(1 + exp(x))\n    # robust implementation: np.logaddexp(0, x)\n    m_vals = np.logaddexp(0, p_M) \n    \n    # Fill off-diagonal elements row by row\n    mask = ~np.eye(5, dtype=bool)\n    C[mask] = m_vals\n    \n    # Calculate Effective Data\n    # X_eff = X @ C.T\n    # Add epsilon for numerical stability\n    X_eff = X @ C.T + 1e-8\n    \n    # Power law: L = A * (X_eff)^(-alpha) + B\n    pred = A[None, :] * (X_eff ** (-alpha[None, :])) + B[None, :]\n    \n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    N, D = X.shape\n    \n    # Storage for final parameters\n    final_log_A = np.zeros(D)\n    final_log_alpha = np.zeros(D)\n    final_B = np.zeros(D)\n    final_M_rows = []\n    \n    # Decoupled optimization for each domain\n    # Since the loss for domain i only depends on parameters for domain i \n    # (and the transfer *into* domain i from others), we can optimize each scaling law independently.\n    \n    for i in range(D):\n        y_i = y[:, i]\n        \n        # Identify other domains (predictors for transfer)\n        other_dims = [j for j in range(D) if j != i]\n        \n        # --- Initialization Heuristics ---\n        \n        # B: Start slightly below minimum observed loss\n        min_y = np.min(y_i)\n        B_init = min_y - 0.05\n        \n        # A, alpha: Univariate fit on dominant data\n        # Select points where domain i is significant (>5%)\n        mask = X[:, i] > 0.05\n        if np.sum(mask) < 5:\n            mask = np.ones(N, dtype=bool)\n            \n        xi_dom = X[mask, i] + 1e-8\n        yi_dom = y_i[mask]\n        \n        # Linear regression on log-log: log(y - B) = log(A) - alpha * log(x)\n        yi_shift = np.maximum(yi_dom - B_init, 1e-6)\n        \n        try:\n            slope, intercept = np.polyfit(np.log(xi_dom), np.log(yi_shift), 1)\n            alpha_init = -slope\n            A_init = np.exp(intercept)\n        except:\n            alpha_init = 0.5\n            A_init = 1.0\n            \n        # Constrain initial guesses to reasonable priors\n        alpha_init = np.clip(alpha_init, 0.1, 2.0)\n        A_init = np.clip(A_init, 0.01, 100.0)\n        \n        # M: Initialize with small transfer coefficients\n        # softplus(p) approx 0.02 => p approx -3.5\n        p_m_init_val = -3.5\n        p_M_row_init = np.full(D-1, p_m_init_val)\n        \n        # Pack local parameters: [logA, logAlpha, B, m_1, m_2, m_3, m_4]\n        p0 = np.concatenate([\n            [np.log(A_init)],\n            [np.log(alpha_init)],\n            [B_init],\n            p_M_row_init\n        ])\n        \n        # --- Local Optimization ---\n        \n        def local_residuals(p):\n            # Unpack\n            p_a, p_alpha, p_b = p[0], p[1], p[2]\n            p_m = p[3:]\n            \n            # Transform\n            a = np.exp(np.clip(p_a, -15, 10))\n            alpha = np.exp(np.clip(p_alpha, -5, 5))\n            b = p_b\n            \n            # Transfer coefficients (softplus)\n            m = np.logaddexp(0, p_m)\n            \n            # Effective data for domain i\n            # X_eff_i = X_i + sum_{j!=i} m_j * X_j\n            # Note: X[:, other_dims] selects the columns of other domains\n            x_eff = X[:, i] + X[:, other_dims] @ m + 1e-8\n            \n            pred = a * (x_eff ** (-alpha)) + b\n            return pred - y_i\n            \n        # Use Levenberg-Marquardt (lm)\n        # It handles sum-of-squares minimization efficiently\n        try:\n            res = least_squares(local_residuals, p0, method='lm',\n                                ftol=1e-9, xtol=1e-9, gtol=1e-9, max_nfev=2000)\n            opt_p = res.x\n        except Exception:\n            # Fallback if optimization fails\n            opt_p = p0\n        \n        # Store results\n        final_log_A[i] = opt_p[0]\n        final_log_alpha[i] = opt_p[1]\n        final_B[i] = opt_p[2]\n        final_M_rows.append(opt_p[3:])\n        \n    # Construct final parameter array\n    # Order: logA(5), logAlpha(5), B(5), M(20)\n    # M is flattened row-major off-diagonals\n    p_M_flat = np.concatenate(final_M_rows)\n    \n    final_params = np.concatenate([\n        final_log_A,\n        final_log_alpha,\n        final_B,\n        p_M_flat\n    ])\n    \n    return final_params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gemini-3-pro-preview", "reward_r2": 0.981899, "solution": "# EVOLVE-BLOCK-START\n\"\"\"\nScaling law discovery for LLM finetuning scenarios\nImproved program using a Separable \"Effective Data\" power law model.\nModels loss as L_d = A_d * (x_d + sum_{j!=d} w_{dj} * x_j)^(-alpha_d) + B_d\nUses exactly 35 parameters (7 per domain).\n\nKey Innovations:\n1. Domain-Separable Optimization: Exploits the independence of loss terms per domain to \n   decompose the 35-parameter problem into five 7-parameter problems.\n2. Three-Stage Fitting Strategy per Domain:\n   a. Grid Search Initialization: Scans B (irreducible loss) to find the global basin of attraction,\n      using analytic inversion to estimate weights and power law parameters.\n   b. Alternating Least Squares (ALS): Iteratively refines weights (Linear LS) and \n      power law parameters (Non-linear LS) to navigate the valley of the loss landscape.\n   c. Joint Refinement: Final gradient-based polish of all 7 parameters with strict physical bounds.\n3. Robust Numerical Handling: Safe power operations, clipping, and scaling to prevent overflow/underflow.\n\"\"\"\nimport numpy as np\nfrom scipy.optimize import least_squares, lsq_linear\n\ndef scaling_law_func(data_points, params):\n    # data_points: (N, 5) array of domain proportions\n    # params: Array of 35 parameters\n    \n    X = np.atleast_2d(np.asarray(data_points))\n    # Clip negative proportions for safety\n    X = np.maximum(X, 0.0)\n    N, _ = X.shape\n    \n    params = np.asarray(params).flatten()\n    if params.size != 35:\n        return np.zeros((N, 5))\n\n    # Reshape parameters: 5 domains, 7 params each\n    # Structure per domain: [w_off_0, w_off_1, w_off_2, w_off_3, A, alpha, B]\n    P = params.reshape(5, 7)\n    \n    # Construct Transfer Matrix W (5, 5)\n    # W[d, j] is transfer from source j to target d. Diagonal is 1.0.\n    W = np.eye(5)\n    # Fill off-diagonal elements row by row\n    mask_off = ~np.eye(5, dtype=bool)\n    W[mask_off] = P[:, :4].flatten()\n    \n    # Extract Power Law Parameters\n    A = P[:, 4]     # (5,)\n    alpha = P[:, 5] # (5,)\n    B = P[:, 6]     # (5,)\n    \n    # Calculate Effective Data\n    # D_eff[i, d] = sum_j (X[i, j] * W[d, j])\n    # Matrix multiplication handles the summation efficiently\n    D_eff = X @ W.T\n    \n    # Apply Power Law: L = A * D_eff^-alpha + B\n    # Safe lower bound for numerical stability\n    D_eff_safe = np.maximum(D_eff, 1e-9)\n    \n    # Prediction: Broadcasting (5,) across (N, 5)\n    pred = A[None, :] * (D_eff_safe ** (-alpha[None, :])) + B[None, :]\n    \n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.asarray(loss_values)\n    \n    # Container for optimized parameters\n    # We optimize each domain independently as the loss function is separable\n    final_params_list = []\n    \n    for d in range(5):\n        xd = X[:, d]\n        yd = Y[:, d]\n        \n        # Identify other domains (sources of transfer)\n        other_idx = [j for j in range(5) if j != d]\n        X_other = X[:, other_idx]\n        \n        min_loss = np.min(yd)\n        \n        # --- Stage 1: Grid Search Initialization ---\n        # B (irreducible loss) is the most difficult parameter to fit due to asymptotic behavior.\n        # We grid search B and analytically estimate other parameters.\n        \n        b_candidates = [0.0]\n        if min_loss > 0.001:\n            # Linear spacing covering the range\n            b_candidates.extend(np.linspace(0, min_loss * 0.95, 10))\n            # Exponential spacing approaching min_loss (critical for high-perf models)\n            b_candidates.extend(min_loss - np.logspace(-1, -5, 5))\n            \n        b_candidates = sorted(list(set([b for b in b_candidates if 0 <= b < min_loss - 1e-7])))\n        \n        best_init_loss = np.inf\n        # Default fallback parameters\n        best_init_params = [0.05]*4 + [2.0, 0.5, max(0.0, min_loss - 0.5)]\n        \n        # Anchor points: where domain d is present, to get rough A, alpha\n        mask_anchor = xd > 0.01\n        if mask_anchor.sum() < 3: mask_anchor = xd > -1.0 # Use all if sparse\n        \n        for b_try in b_candidates:\n            try:\n                # 1. Estimate A, alpha assuming no transfer (w=0) initially\n                # Model: y - B = A * x^-alpha\n                y_shift = yd[mask_anchor] - b_try\n                x_anchor = xd[mask_anchor]\n                \n                if np.any(y_shift <= 0): continue\n                \n                log_y = np.log(y_shift)\n                log_x = np.log(x_anchor + 1e-9)\n                \n                # Linear fit in log-log space\n                if len(log_x) < 2: continue\n                poly = np.polyfit(log_x, log_y, 1)\n                alpha_est = -poly[0]\n                A_est = np.exp(poly[1])\n                \n                # Check physical plausibility\n                if not (0.01 < alpha_est < 6.0 and 0.01 < A_est < 100.0):\n                    continue\n                \n                # 2. Estimate Transfer Weights (W) given A, alpha, B\n                # Invert model: D_eff_target = ((y - B) / A)^(-1/alpha)\n                # Solve linear system: X_other @ w = D_eff_target - xd\n                \n                y_shift_all = np.maximum(yd - b_try, 1e-9)\n                d_target = np.power(y_shift_all / A_est, -1.0/alpha_est)\n                \n                # Clip d_target to prevent outliers from dominating (e.g. near asymptote)\n                d_target = np.clip(d_target, 0, 100.0)\n                \n                # Non-Negative Least Squares\n                res_w = lsq_linear(X_other, d_target - xd, bounds=(0, np.inf), tol=1e-3)\n                w_est = res_w.x\n                \n                # 3. Evaluate Fit\n                d_eff_est = xd + X_other @ w_est\n                pred = A_est * np.maximum(d_eff_est, 1e-9)**(-alpha_est) + b_try\n                mse = np.mean((pred - yd)**2)\n                \n                if mse < best_init_loss:\n                    best_init_loss = mse\n                    best_init_params = list(w_est) + [A_est, alpha_est, b_try]\n            except:\n                continue\n        \n        # --- Stage 2: Alternating Least Squares (ALS) Refinement ---\n        # Refine the parameters by alternating between optimizing W and (A, alpha, B)\n        curr_w = np.array(best_init_params[:4])\n        curr_A = best_init_params[4]\n        curr_alpha = best_init_params[5]\n        curr_B = best_init_params[6]\n        \n        # 2-3 iterations are usually sufficient to converge to the valley\n        for _ in range(3):\n            # Step A: Optimize A, alpha, B given current W\n            d_eff_fixed = xd + X_other @ curr_w\n            \n            def resid_diag(p):\n                # p=[A, alpha, B]\n                return (p[0] * np.maximum(d_eff_fixed, 1e-9)**(-p[1]) + p[2]) - yd\n            \n            try:\n                # Bounds: A>0, alpha>0, 0<=B<min_loss\n                lb_diag = [1e-5, 1e-4, 0.0]\n                ub_diag = [np.inf, 8.0, min_loss - 1e-8]\n                res_diag = least_squares(resid_diag, [curr_A, curr_alpha, curr_B], \n                                       bounds=(lb_diag, ub_diag), method='trf', max_nfev=50)\n                curr_A, curr_alpha, curr_B = res_diag.x\n            except:\n                pass\n                \n            # Step B: Optimize W given current A, alpha, B\n            try:\n                y_shift_clean = np.maximum(yd - curr_B, 1e-9)\n                d_targ = np.power(y_shift_clean / curr_A, -1.0/curr_alpha)\n                d_targ = np.clip(d_targ, 0, 100.0)\n                \n                res_w = lsq_linear(X_other, d_targ - xd, bounds=(0, np.inf), tol=1e-3)\n                curr_w = res_w.x\n            except:\n                pass\n\n        # --- Stage 3: Final Joint Optimization ---\n        # Optimize all 7 parameters for this domain simultaneously\n        p0 = np.concatenate([curr_w, [curr_A, curr_alpha, curr_B]])\n        \n        def resid_joint(p):\n            w = p[:4]\n            A, alpha, B = p[4], p[5], p[6]\n            d_eff = xd + X_other @ w\n            pred = A * np.maximum(d_eff, 1e-9)**(-alpha) + B\n            return pred - yd\n        \n        # Strict bounds\n        lb = [0.0]*4 + [1e-5, 1e-4, 0.0]\n        ub = [np.inf]*4 + [np.inf, 8.0, min_loss - 1e-8]\n        \n        # Scaling to handle different parameter magnitudes\n        x_scale = [0.1]*4 + [2.0, 0.5, 1.0]\n        \n        try:\n            res_final = least_squares(\n                resid_joint, p0, \n                bounds=(lb, ub), \n                method='trf', \n                x_scale=x_scale,\n                ftol=1e-9, xtol=1e-9, max_nfev=1000\n            )\n            final_params_list.extend(res_final.x)\n        except:\n            final_params_list.extend(p0)\n            \n    return np.array(final_params_list)\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.990152, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# 34-parameter model with shared scalars and per-domain heads\n# Layout:\n# 0..4   : b[5]\n# 5..9   : u1[5]\n# 10..14 : u2[5]\n# 15..19 : u3[5]\n# 20..24 : v1[5]\n# 25     : alpha_div\n# 26..30 : c_self[5]\n# 31     : gamma\n# 32     : eta_self\n# 33     : beta_pair\n\ndef _unpack_params(params):\n    p = np.asarray(params).ravel()\n    if p.size != 34:\n        raise ValueError(\"params must be a 1D array of length 34\")\n    b = p[0:5]; u1 = p[5:10]; u2 = p[10:15]; u3 = p[15:20]; v1 = p[20:25]\n    alpha_div = p[25]; c_self = p[26:31]; gamma = p[31]; eta_self = p[32]; beta_pair = p[33]\n    return b, u1, u2, u3, v1, alpha_div, c_self, gamma, eta_self, beta_pair\n\ndef _entropy_norm(X):\n    eps = 1e-12\n    Xs = np.clip(X, eps, 1.0)\n    H = -np.sum(Xs * np.log(Xs), axis=1)\n    return H / np.log(X.shape[1])\n\ndef _concentration_norm(X):\n    F = X.shape[1]\n    hhi = np.sum(X * X, axis=1)\n    min_hhi = 1.0 / F\n    return (hhi - min_hhi) / (1.0 - min_hhi)\n\ndef _pairwise_affinity_vec(X):\n    a = np.sqrt(np.clip(X, 0.0, 1.0))\n    s = np.sum(a, axis=1, keepdims=True)\n    Fm1 = max(X.shape[1] - 1, 1)\n    return (a * (s - a)) / Fm1\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    if X.shape[1] != 5:\n        raise ValueError(\"Expected 5-domain proportions per row\")\n    b, u1, u2, u3, v1, alpha_div, c_self, gamma, eta_self, beta_pair = _unpack_params(params)\n    eps = 1e-8\n    Xc = np.clip(X, 0.0, 1.0) + eps\n\n    Xg = np.power(Xc, gamma)\n    s1 = Xg @ v1\n    s2v = np.power(_pairwise_affinity_vec(Xc), beta_pair)\n    ent = _entropy_norm(Xc)\n    conc = _concentration_norm(Xc)\n    s3 = alpha_div * ent + (1.0 - alpha_div) * (1.0 - conc)\n    Xself = np.power(Xc, eta_self)\n\n    pred = (b[None, :]\n            + np.outer(s1, u1)\n            + s2v * u2[None, :]\n            + np.outer(s3, u3)\n            + Xself * c_self[None, :])\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    if X.shape[0] != Y.shape[0] or X.shape[1] != 5 or Y.shape[1] != 5:\n        raise ValueError(\"Shapes must be (N,5) for both X and Y\")\n\n    N, F = X.shape\n    eps = 1e-8\n    Xc = np.clip(X, 0.0, 1.0) + eps\n\n    def features(nlp):\n        v1 = nlp[0:5]; alpha = nlp[5]; gamma = nlp[6]; eta = nlp[7]; beta = nlp[8]\n        Xg = np.power(Xc, gamma)\n        s1 = Xg @ v1\n        s2v = np.power(_pairwise_affinity_vec(Xc), beta)\n        ent = _entropy_norm(Xc)\n        conc = _concentration_norm(Xc)\n        s3 = alpha * ent + (1.0 - alpha) * (1.0 - conc)\n        Xself = np.power(Xc, eta)\n        return s1, s2v, s3, Xself, v1, alpha, gamma, eta, beta\n\n    def solve_linear(s1, s2v, s3, Xself, lam=1e-5):\n        ones = np.ones(N)\n        u1 = np.zeros(F); u2 = np.zeros(F); u3 = np.zeros(F); c_self = np.zeros(F); b = np.zeros(F)\n        for d in range(F):\n            A = np.column_stack([s1, s2v[:, d], s3, Xself[:, d], ones])\n            AtA = A.T @ A\n            jitter = lam + 1e-8 * np.trace(AtA) / AtA.shape[0]\n            AtA.flat[::AtA.shape[0]+1] += jitter\n            coef = np.linalg.solve(AtA, A.T @ Y[:, d])\n            u1[d], u2[d], u3[d], c_self[d], b[d] = coef\n        return b, u1, u2, u3, c_self\n\n    def objective(nlp):\n        s1, s2v, s3, Xself, v1, alpha, gamma, eta, beta = features(nlp)\n        b, u1, u2, u3, c_self = solve_linear(s1, s2v, s3, Xself)\n        pred = (b[None, :]\n                + np.outer(s1, u1)\n                + s2v * u2[None, :]\n                + np.outer(s3, u3)\n                + Xself * c_self[None, :])\n        mse = np.mean((pred - Y) ** 2)\n        reg_u = 1e-6 * (np.sum(u1*u1) + np.sum(u2*u2) + np.sum(u3*u3) + np.sum(c_self*c_self))\n        reg = (1e-4 * np.sum(v1**2)\n               + 1e-4 * (np.sum(v1) - 1.0) ** 2\n               + 8e-4 * ((gamma - 1.0) ** 2 + (eta - 1.0) ** 2 + (beta - 1.0) ** 2)\n               + 4e-4 * (alpha - 0.5) ** 2)\n        return mse + reg + reg_u\n\n    def init(g_scale, a):\n        v1_0 = np.full(F, 1.0 / F)\n        return np.concatenate([v1_0, [a, g_scale, 1.0, 1.0]])\n\n    bounds = [(-4.0, 4.0)] * 5 + [(0.0, 1.0)] + [(0.6, 1.5)] + [(0.6, 1.8)] + [(0.6, 1.6)]\n    starts = [init(1.00, 0.50), init(0.85, 0.35), init(1.15, 0.65), init(1.00, 0.20), init(1.00, 0.80)]\n    rng = np.random.default_rng(42)\n    for _ in range(2):\n        p = init(1.0, 0.5)\n        p[:5] += rng.normal(0.0, 0.05, size=5)\n        starts.append(p)\n\n    best_val = np.inf; best_nlp = starts[0]\n    for p0 in starts:\n        res = minimize(objective, p0, method='L-BFGS-B', bounds=bounds, options={'maxiter': 600})\n        cand = res.x if res.success else p0\n        val = objective(cand)\n        if val < best_val:\n            best_val = val; best_nlp = cand\n\n    s1, s2v, s3, Xself, v1, alpha, gamma, eta, beta = features(best_nlp)\n    b, u1, u2, u3, c_self = solve_linear(s1, s2v, s3, Xself)\n    return np.concatenate([b, u1, u2, u3, v1, [alpha], c_self, [gamma, eta, beta]])\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.989365, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    N, F = X.shape\n    p = np.asarray(params, dtype=float).ravel()\n    total = F + F*F + F\n    defaults = np.concatenate([np.zeros(F), np.eye(F).ravel(), np.ones(F)])\n    if p.size < total:\n        q = defaults.copy()\n        q[:p.size] = p\n        p = q\n    p = p[:total]\n    b = p[:F]\n    W = p[F:F+F*F].reshape(F, F)\n    a = p[F+F*F:F+F*F+F]\n    Xc = np.clip(X, 1e-12, None)\n    Phi = np.exp(np.log(Xc) * a)\n    return Phi @ W.T + b\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    Y = np.atleast_2d(np.asarray(loss_values, dtype=float))\n    N, F = X.shape\n    assert Y.shape == (N, F)\n    Xc = np.clip(X, 1e-12, None)\n\n    # per-output scale for robust weighting\n    sigma = Y.std(axis=0) + 1e-6\n\n    # init: ridge with a=1\n    a0 = np.ones(F)\n    Z = Xc\n    reg = 1e-6\n    G = Z.T @ Z + reg * np.eye(F)\n    b0 = Y.mean(axis=0)\n    Yc = Y - b0\n    W0 = (np.linalg.solve(G, Z.T @ Yc)).T\n    p0 = np.concatenate([b0, W0.ravel(), a0])\n\n    lamW, lamA, lamB = 1e-3, 1e-3, 1e-6\n    delta = 0.25\n\n    def unpack(p):\n        b = p[:F]\n        W = p[F:F+F*F].reshape(F, F)\n        a = p[F+F*F:F+F*F+F]\n        return b, W, a\n\n    def objective(p):\n        b, W, a = unpack(p)\n        Phi = np.exp(np.log(Xc) * a)\n        Yhat = Phi @ W.T + b\n        E = (Yhat - Y) / sigma\n        S = np.sqrt(1.0 + (E/delta)**2)\n        huber = (delta**2 * (S - 1.0)).mean()\n        reg_term = lamW * np.sum(W*W) + lamA * np.sum((a - 1.0)**2) + lamB * np.sum(b*b)\n        return huber + reg_term\n\n    def gradient(p):\n        b, W, a = unpack(p)\n        logX = np.log(Xc)\n        Phi = np.exp(logX * a)\n        Yhat = Phi @ W.T + b\n        E = (Yhat - Y) / sigma\n        S = np.sqrt(1.0 + (E/delta)**2)\n        gE = (E / S) * (1.0 / (N * F)) / sigma\n        gb = gE.sum(axis=0) + 2.0 * lamB * b\n        gW = gE.T @ Phi + 2.0 * lamW * W\n        Sphi = gE @ W\n        ga = np.sum(Sphi * (Phi * logX), axis=0) + 2.0 * lamA * (a - 1.0)\n        return np.concatenate([gb, gW.ravel(), ga])\n\n    bounds = [(-1.0, 10.0)] * F + [(-10.0, 10.0)] * (F*F) + [(0.25, 4.0)] * F\n\n    rng = np.random.default_rng(0)\n    best_p, best_f = p0.copy(), objective(p0)\n    for noise in [0.0, 0.01, 0.03, 0.07]:\n        init = p0 + noise * rng.standard_normal(p0.size)\n        init[-F:] = np.clip(init[-F:], 0.7, 1.3)\n        try:\n            res = minimize(objective, init, method='L-BFGS-B', jac=gradient, bounds=bounds,\n                           options={'maxiter': 600, 'ftol': 1e-9})\n            cand = res.x if res.success else init\n        except Exception:\n            cand = init\n        f = objective(cand)\n        if f < best_f:\n            best_f, best_p = f, cand\n    return best_p\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.989133, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _sp(x):\n    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    if X.shape[1] != 5:\n        raise ValueError(\"Expected 5 domain proportions per sample\")\n    p = np.asarray(params)\n    # Interpret params in two compatible formats:\n    # - vector of length 35: [U(25), b(5), g_raw(5)]\n    # - matrix of shape (5,7): rows [U_row(5), b_i, g_raw_i]\n    if p.ndim == 2 and p.shape == (5, 7):\n        U = p[:, :5]\n        b = p[:, 5]\n        gr = p[:, 6]\n    else:\n        q = p.ravel()\n        base = np.concatenate([np.eye(5).ravel(), np.zeros(5), np.zeros(5)])\n        if q.size == 0:\n            q = base\n        elif q.size < 35:\n            tmp = base.copy(); tmp[:q.size] = q; q = tmp\n        elif q.size > 35:\n            q = q[:35]\n        U = q[:25].reshape(5, 5)\n        b = q[25:30]\n        gr = q[30:35]\n    g = _sp(gr) + 1e-8\n    Xc = np.clip(X, 1e-12, 1.0)\n    Z = Xc ** g\n    return Z @ U.T + b\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    if X.shape[1] != 5 or Y.shape[1] != 5:\n        raise ValueError(\"Expected shapes X:(N,5), Y:(N,5)\")\n    N = X.shape[0]\n    Xc = np.clip(X, 1e-12, 1.0)\n    lnX = np.log(Xc)\n    ones = np.ones((N, 1))\n\n    # Initialize gamma_raw=0 => g\u2248ln2, solve ridge for U,b\n    gr = np.zeros(5)\n    g = _sp(gr) + 1e-8\n    Z = Xc ** g\n    Phi = np.concatenate([Z, ones], axis=1)\n    lam = 1e-3\n    G = Phi.T @ Phi + lam * np.eye(6)\n    try:\n        W = np.linalg.solve(G, Phi.T @ Y)\n    except np.linalg.LinAlgError:\n        W = np.linalg.pinv(G) @ (Phi.T @ Y)\n    U = W[:5, :].T\n    b = W[5, :]\n\n    # Balance domains\n    w = 1.0 / (np.std(Y, axis=0) + 1e-6)\n\n    # Optimizer (Adam) states\n    lrU, lrg = 0.05, 0.025\n    beta1, beta2, eps = 0.9, 0.999, 1e-8\n    mU = np.zeros_like(U); vU = np.zeros_like(U)\n    mb = np.zeros_like(b); vb = np.zeros_like(b)\n    mg = np.zeros_like(gr); vg = np.zeros_like(gr)\n\n    # Robust loss + regularization\n    delta = 0.5\n    lamU, lamb, lamg = 1e-3, 1e-4, 1e-2\n    clipn = 5.0\n\n    def fwd(U, b, gr):\n        g = _sp(gr) + 1e-8\n        Z = Xc ** g\n        Yh = Z @ U.T + b\n        R = (Yh - Y) * w[None, :]\n        A = np.abs(R)\n        Gh = np.where(A <= delta, R, delta * np.sign(R)) / N\n        dU = (Gh.T @ Z) + lamU * U\n        db = Gh.sum(axis=0) + lamb * b\n        dZdg = Z * lnX\n        GU = Gh @ U\n        dg = (GU * dZdg).sum(axis=0) + lamg * (g - 1.0)\n        s = 1.0 / (1.0 + np.exp(-gr))\n        dgr = dg * s\n        hub = np.where(A <= delta, 0.5 * (R**2), delta * (A - 0.5 * delta))\n        L = hub.mean() + lamU * (U**2).mean() + lamb * (b**2).mean() + lamg * ((g - 1.0)**2).mean()\n        return L, dU, db, dgr\n\n    def clipg(G):\n        n = np.linalg.norm(G)\n        return G if n == 0 or n <= clipn else G * (clipn / n)\n\n    bestL = np.inf\n    best = (U.copy(), b.copy(), gr.copy())\n    patience, noimp = 120, 0\n\n    for t in range(1, 1201):\n        L, dU, db, dgr = fwd(U, b, gr)\n        dU, db, dgr = clipg(dU), clipg(db), clipg(dgr)\n\n        mU = beta1 * mU + (1 - beta1) * dU; vU = beta2 * vU + (1 - beta2) * (dU * dU)\n        mb = beta1 * mb + (1 - beta1) * db; vb = beta2 * vb + (1 - beta2) * (db * db)\n        mg = beta1 * mg + (1 - beta1) * dgr; vg = beta2 * vg + (1 - beta2) * (dgr * dgr)\n\n        mUh = mU / (1 - beta1**t); vUh = vU / (1 - beta2**t)\n        mbh = mb / (1 - beta1**t); vbh = vb / (1 - beta2**t)\n        mgh = mg / (1 - beta1**t); vgh = vg / (1 - beta2**t)\n\n        dec = 1.0 if t < 900 else 0.6\n        U -= (lrU * dec) * mUh / (np.sqrt(vUh) + eps)\n        b -= (lrU * dec) * mbh / (np.sqrt(vbh) + eps)\n        gr -= (lrg * dec) * mgh / (np.sqrt(vgh) + eps)\n\n        if L < bestL - 1e-7:\n            bestL = L; best = (U.copy(), b.copy(), gr.copy()); noimp = 0\n        else:\n            noimp += 1\n            if noimp >= patience:\n                break\n\n    U, b, gr = best\n    # Pack into (5,7) for compatibility: [U_row(5), b_i, g_raw_i]\n    params = np.zeros((5, 7))\n    params[:, :5] = U\n    params[:, 5] = b\n    params[:, 6] = gr\n    return params\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.988952, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    if X.shape[1] != 5:\n        raise ValueError(\"Expected 5 domains\")\n    p = np.asarray(params).ravel()\n    if p.size != 35:\n        raise ValueError(\"Params must have length 35\")\n    p_raw = p[0:5]\n    d     = p[5:10]\n    b     = p[10:15]\n    U     = p[15:20]\n    V     = p[20:25]\n    c_sum = p[25:30]  # weights for synergy S = sum_j w_j^{alpha_j}\n    c_ent = p[30:35]  # weights for entropy feature\n    eps = 1e-8\n    w = np.clip(X, eps, 1.0 - eps)\n    p_exp = 2.0 / (1.0 + np.exp(-p_raw))  # in (0,2)\n    f1 = np.exp(np.log(w) * p_exp[None, :])  # per-domain concave transform\n    f2 = np.sqrt(w)\n    z1 = f2 @ V                              # scalar latent interaction\n    S  = f1.sum(axis=1)                      # synergy across domains with learned exponents\n    ent = -(w * np.log(w)).sum(axis=1)       # mixture entropy\n    pred = b[None, :] + d[None, :] * f1 + U[None, :] * z1[:, None] + c_sum[None, :] * S[:, None] + c_ent[None, :] * ent[:, None]\n    return pred\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    if y.ndim == 1:\n        y = y[:, None]\n    N, F = X.shape\n    if F != 5 or y.shape[1] != 5:\n        raise ValueError(\"Expected (N,5) inputs and (N,5) outputs\")\n    eps = 1e-8\n    w = np.clip(X, eps, 1.0 - eps)\n    f2 = np.sqrt(w)\n    ent = -(w * np.log(w)).sum(axis=1)\n    lam = 1e-3\n\n    def solve_linear(p_raw, V):\n        p_exp = 2.0 / (1.0 + np.exp(-p_raw))\n        f1 = np.exp(np.log(w) * p_exp[None, :])\n        z1 = f2 @ V\n        S = f1.sum(axis=1)\n        b = np.zeros(5); d = np.zeros(5); U = np.zeros(5); c_sum = np.zeros(5); c_ent = np.zeros(5)\n        ones = np.ones(N)\n        I = lam * np.eye(5)\n        for j in range(5):\n            Phi = np.column_stack([ones, f1[:, j], z1, S, ent])\n            A = Phi.T @ Phi + I\n            t = Phi.T @ y[:, j]\n            coef = np.linalg.solve(A, t)\n            b[j], d[j], U[j], c_sum[j], c_ent[j] = coef\n        return b, d, U, c_sum, c_ent\n\n    def update_V(p_raw, b, d, U, c_sum, c_ent):\n        p_exp = 2.0 / (1.0 + np.exp(-p_raw))\n        f1 = np.exp(np.log(w) * p_exp[None, :])\n        S = f1.sum(axis=1)\n        z_free = b[None, :] + d[None, :] * f1 + c_sum[None, :] * S[:, None] + c_ent[None, :] * ent[:, None]\n        R = y - z_free\n        FtF = f2.T @ f2 + lam * np.eye(5)\n        alpha = float(U @ U) + 1e-8\n        rhs = f2.T @ (R @ U)\n        return np.linalg.solve(alpha * FtF, rhs)\n\n    def refine_V(p_raw, V, steps=2):\n        for _ in range(steps):\n            b, d, U, c_sum, c_ent = solve_linear(p_raw, V)\n            V = update_V(p_raw, b, d, U, c_sum, c_ent)\n        return V\n\n    def huber(r, dlt=0.25):\n        a = np.abs(r)\n        return np.where(a <= dlt, 0.5 * r * r, dlt * (a - 0.5 * dlt))\n\n    def objective(theta):\n        p_raw = theta[:5]\n        V = refine_V(p_raw, theta[5:10], steps=1)\n        b, d, U, c_sum, c_ent = solve_linear(p_raw, V)\n        params = np.concatenate([p_raw, d, b, U, V, c_sum, c_ent])\n        pred = scaling_law_func(w, params)\n        loss = huber(pred - y).mean()\n        reg = 1e-3 * (np.sum(V * V) + np.sum(U * U) + np.sum(d * d) + np.sum(c_sum * c_sum) + np.sum(c_ent * c_ent)) + 1e-4 * np.sum(p_raw * p_raw)\n        return loss + reg\n\n    # Initializations\n    b0 = y.mean(axis=0)\n    d0 = np.zeros(5)\n    p_raw0 = np.zeros(5)\n    V0 = np.zeros(5)\n\n    inits = [\n        np.concatenate([p_raw0, V0]),\n        np.concatenate([p_raw0, np.random.randn(5) * 0.05]),\n        np.concatenate([np.random.randn(5) * 0.1, V0]),\n    ]\n    bounds = [(-4.0, 4.0)] * 5 + [(-2.0, 2.0)] * 5\n    best_val = np.inf\n    best_theta = inits[0]\n    for init in inits:\n        res = minimize(objective, init, method='L-BFGS-B', bounds=bounds, options={'maxiter': 400})\n        if res.success and res.fun < best_val:\n            best_val = res.fun\n            best_theta = res.x\n    p_raw = best_theta[:5]\n    V = refine_V(p_raw, best_theta[5:10], steps=3)\n    b, d, U, c_sum, c_ent = solve_linear(p_raw, V)\n    return np.concatenate([p_raw, d, b, U, V, c_sum, c_ent])\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "gpt-5", "reward_r2": 0.983634, "solution": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef _logistic(z, lo, hi):\n    return lo + (hi - lo) / (1.0 + np.exp(-z))\n\ndef _inv_logistic(t, lo, hi):\n    u = (t - lo) / (hi - lo)\n    u = np.clip(u, 1e-6, 1 - 1e-6)\n    return np.log(u / (1 - u))\n\ndef _unpack(p, F):\n    p = np.asarray(p).ravel()\n    if (p.size - 5) % (F + 1) != 0 or p.size < (F + 1) + 5:\n        raise ValueError(\"Bad param length.\")\n    T = (p.size - 5) // (F + 1)\n    core = p[: T * (F + 1)].reshape(T, F + 1)\n    A = core[:, :F]\n    b = core[:, F]\n    g1r, cc, dc, pc, g2r = p[-5:]\n    g1 = _logistic(g1r, 0.3, 2.5)\n    g2 = _logistic(g2r, 0.5, 3.0)\n    return A, b, g1, cc, dc, pc, g2, T\n\ndef _pack(A, b, g1r, cc, dc, pc, g2r):\n    return np.concatenate([np.hstack([A, b[:, None]]).ravel(), np.array([g1r, cc, dc, pc, g2r])])\n\ndef _pairwise_sum(Z):\n    s1 = np.sum(Z, axis=1, keepdims=True)\n    s2 = np.sum(Z * Z, axis=1, keepdims=True)\n    return 0.5 * (s1 * s1 - s2)\n\ndef _features(X, g1, g2):\n    # Power-transformed domain weights\n    Phi = np.power(X, g1)\n    # Mixture concentration (purity): sum of squares\n    conc = np.sum(X * X, axis=1, keepdims=True)\n    # Diversity term: normalized Tsallis entropy parameterized by g2 with smooth Shannon limit at q -> 1\n    eps = 1e-12\n    F = X.shape[1]\n    q = g2\n    sum_q = np.sum(np.power(X, q), axis=1, keepdims=True)\n    if abs(q - 1.0) > 1e-3:\n        denom = 1.0 - (F ** (1.0 - q))\n        denom = denom if abs(denom) > 1e-12 else 1e-12\n        div = (1.0 - sum_q) / denom\n    else:\n        div = -np.sum(X * np.log(X + eps), axis=1, keepdims=True) / np.log(F)\n    # Pairwise synergy via elementary symmetric polynomial e2 on g2-shaped weights\n    Z = np.power(X, 0.5 * g2)\n    pair = _pairwise_sum(Z)\n    return Phi, conc, div, pair\n\ndef scaling_law_func(data_points, params):\n    X = np.atleast_2d(np.asarray(data_points))\n    A, b, g1, cc, dc, pc, g2, _ = _unpack(params, X.shape[1])\n    Phi, conc, div, pair = _features(X, g1, g2)\n    return Phi @ A.T + b[None, :] + cc * conc + dc * div + pc * pair\n\ndef fit_scaling_law(data_points, loss_values):\n    X = np.atleast_2d(np.asarray(data_points))\n    y = np.asarray(loss_values)\n    y2d = y if y.ndim == 2 else y[:, None]\n    N, F = X.shape\n\n    def solve_linear(Phi, Yadj):\n        D = np.hstack([Phi, np.ones((N, 1))])\n        lam = 1e-6\n        DtD = D.T @ D + lam * np.eye(D.shape[1])\n        DtY = D.T @ Yadj\n        W = np.linalg.solve(DtD, DtY)\n        return W[:F, :].T, W[F, :]\n\n    y_std = np.std(y2d, axis=0, ddof=1)\n    y_std[y_std == 0] = 1.0\n    delta = 0.5\n\n    def huber(e):\n        ae = np.abs(e)\n        return np.where(ae <= delta, 0.5 * e * e, delta * (ae - 0.5 * delta))\n\n    def obj(z):\n        g1 = _logistic(z[0], 0.3, 2.5)\n        cc, dc, pc = z[1], z[2], z[3]\n        g2 = _logistic(z[4], 0.5, 3.0)\n        Phi, conc, div, pair = _features(X, g1, g2)\n        Yadj = y2d - cc * conc - dc * div - pc * pair\n        A, b = solve_linear(Phi, Yadj)\n        pred = Phi @ A.T + b[None, :] + cc * conc + dc * div + pc * pair\n        r = (pred - y2d) / y_std\n        reg = 1e-4 * np.sum(A * A) + 5e-6 * np.sum(b * b) + 1e-5 * (cc * cc + dc * dc + pc * pc + (g1 - 1.0) ** 2 + (g2 - 2.0) ** 2)\n        return np.mean(huber(r)) + reg\n\n    g1r0 = _inv_logistic(1.0, 0.3, 2.5)\n    g2r0 = _inv_logistic(2.0, 0.5, 3.0)\n    z0 = np.array([g1r0, 0.0, 0.0, 0.0, g2r0])\n\n    seeds1 = [-1.2, -0.6, -0.2, 0.0, 0.2, 0.6, 1.2]\n    seeds2 = [-0.9, -0.5, -0.2, 0.0, 0.2, 0.5, 0.9]\n    best_z, best_val = z0, obj(z0)\n    for s1 in seeds1:\n        for s2 in seeds2:\n            z = z0 + np.array([s1, 0.05 * s1, -0.05 * s1, 0.03 * s2, s2])\n            val = obj(z)\n            if val < best_val:\n                best_z, best_val = z, val\n    for jit in [-0.6, -0.3, -0.1, 0.0, 0.1, 0.3, 0.6]:\n        z = best_z + np.array([jit, 0.02 * jit, -0.02 * jit, 0.01 * jit, -jit])\n        res = minimize(obj, z, method='L-BFGS-B', options={'maxiter': 500})\n        if res.success and res.fun < best_val:\n            best_z, best_val = res.x, res.fun\n\n    g1 = _logistic(best_z[0], 0.3, 2.5)\n    cc, dc, pc = float(best_z[1]), float(best_z[2]), float(best_z[3])\n    g2 = _logistic(best_z[4], 0.5, 3.0)\n    Phi, conc, div, pair = _features(X, g1, g2)\n    A, b = solve_linear(Phi, y2d - cc * conc - dc * div - pc * pair)\n    return _pack(A, b, best_z[0], cc, dc, pc, best_z[4])\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.992708, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Multi\u2010domain scaling law combining:\n      - a single softplus hidden unit capturing global cross\u2010domain coupling,\n      - per\u2010domain power\u2010law term,\n      - per\u2010domain linear residual on own proportion,\n      - and per\u2010domain bias.\n    Total parameters = 5 (U) + 1 (b0) + 5 (V) + 5 (A) + 5 (b) + 5 (alpha) + 5 (c) = 31\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))  # (N,5)\n    N, F = X.shape\n    p = np.asarray(params, dtype=float).ravel()\n    # unpack parameters\n    idx = 0\n    U      = p[idx:idx+F];    idx += F    # global coupling weights\n    b0     = p[idx];          idx += 1    # hidden offset\n    V      = p[idx:idx+F];    idx += F    # per-domain coupling strength\n    A      = p[idx:idx+F];    idx += F    # per-domain power-law scale\n    b_exp  = p[idx:idx+F];    idx += F    # per-domain exponents\n    alpha  = p[idx:idx+F];    idx += F    # per-domain linear residual slope\n    c      = p[idx:idx+F]                 # per-domain bias\n\n    # hidden pre-activation\n    Z = X.dot(U) + b0                     # (N,)\n    # numerically stable softplus\n    sp = np.where(Z > 20, Z, np.log1p(np.exp(Z)))  # (N,)\n\n    # global cross-domain term: sp * V_d\n    cross = sp[:, None] * V              # (N,5)\n    # per-domain power-law: A_d * p_d^{b_d}\n    Xb = X ** b_exp                      # (N,5)\n    domain = A * Xb                      # (N,5)\n    # per-domain linear residual: alpha_d * p_d\n    resid = X * alpha                   # (N,5)\n\n    # total prediction\n    return domain + cross + resid + c    # (N,5)\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 31\u2010parameter scaling law by minimizing MSE + small L2 regularization,\n    using L-BFGS-B with multiple random restarts for robustness.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))\n    y = np.atleast_2d(np.asarray(loss_values, dtype=float))\n    N, F = X.shape\n\n    # parameter count = 5 + 1 + 5 + 5 + 5 + 5 + 5 = 31\n    P = F + 1 + F + F + F + F + F\n\n    # initial guess\n    U0     = np.zeros(F)                # no global coupling\n    b0_0   = np.zeros(1)                # hidden bias = 0\n    V0     = np.zeros(F)                # no cross-domain effect\n    A0     = np.zeros(F)                # no power-law effect\n    b_exp0 = np.ones(F)                 # default exponent = 1\n    alpha0 = np.zeros(F)                # no linear residual\n    c0     = y.mean(axis=0)             # bias ~ mean loss per domain\n    p0 = np.concatenate([U0, b0_0, V0, A0, b_exp0, alpha0, c0])\n\n    # bounds: keep exponents in [0,5], others unbounded\n    bounds = []\n    # U (5)\n    bounds += [(None, None)] * F\n    # b0 (1)\n    bounds += [(None, None)]\n    # V (5)\n    bounds += [(None, None)] * F\n    # A (5)\n    bounds += [(None, None)] * F\n    # b_exp (5)\n    bounds += [(0.0, 5.0)] * F\n    # alpha (5)\n    bounds += [(None, None)] * F\n    # c (5)\n    bounds += [(None, None)] * F\n\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse  = np.mean((pred - y) ** 2)\n        reg  = 1e-4 * np.mean(p ** 2)\n        return mse + reg\n\n    best_p, best_val = None, np.inf\n    rng = np.random.RandomState(42)\n    # multi-start optimization\n    for i in range(5):\n        start = p0 if i == 0 else p0 + rng.normal(scale=0.1, size=P)\n        res = minimize(objective, start,\n                       method='L-BFGS-B',\n                       bounds=bounds,\n                       options={'maxiter': 500, 'ftol': 1e-8})\n        if res.fun < best_val:\n            best_val, best_p = res.fun, res.x\n\n    return best_p if best_p is not None else p0", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.991526, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses given 5-dim mixture proportions.\n    Model (31 params):\n      L_i = c_i\n            + alpha_i * p_i**gamma_i\n            + sum_{j != i} W_{i,j} * p_j\n    params layout:\n      c      : 5\n      alpha  : 5\n      gamma  : 5\n      W_off  : 20 (off\u2010diagonal of 5\u00d75)\n    Total = 5 + 5 + 5 + 20 = 35\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, float))  # (N,5)\n    N, F = X.shape\n    assert F == 5, \"Expect 5 domains\"\n    # unpack\n    c       = params[0:5]                      # (5,)\n    alpha   = params[5:10]                     # (5,)\n    gamma   = np.clip(params[10:15], 0.1, 5.0)  # (5,)\n    W_flat  = params[15:35]                     # (20,)\n    # build off-diag weight matrix\n    W = np.zeros((5,5), dtype=float)\n    mask = ~np.eye(5, dtype=bool)\n    W[mask] = W_flat\n    # self\u2010domain nonlinear term\n    diag_term = alpha[np.newaxis,:] * (X ** gamma[np.newaxis,:])\n    # cross\u2010domain linear term\n    cross_term = X.dot(W.T)\n    # assemble prediction\n    return c[np.newaxis,:] + diag_term + cross_term\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35\u2010parameter scaling law via L-BFGS-B with smart initialization.\n    Returns optimized params of length 35.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, float))   # (N,5)\n    Y = np.atleast_2d(np.asarray(loss_values, float))   # (N,5)\n    N, F = X.shape\n    assert F == 5 and Y.shape[1] == 5, \"Expect shapes (N,5)\"\n\n    # 1) Simple linear slopes per domain for init of alpha\n    y_mean = Y.mean(axis=0)\n    x_mean = X.mean(axis=0)\n    slopes = np.zeros(5, dtype=float)\n    for i in range(5):\n        xi = X[:,i]; yi = Y[:,i]\n        var = np.var(xi)\n        if var > 1e-8:\n            cov = np.cov(xi, yi, bias=True)[0,1]\n            slopes[i] = cov / var\n        else:\n            slopes[i] = 0.0\n\n    # 2) Initialize c, alpha, gamma\n    c0     = np.clip(y_mean - slopes * x_mean, 0.0, None)\n    alpha0 = slopes.copy()\n    gamma0 = np.ones(5, dtype=float)\n\n    # 3) Initialize off\u2010diag W by least squares on the residuals\n    #    R = Y - (c + alpha * X)\n    R = Y - (c0[np.newaxis,:] + alpha0[np.newaxis,:] * X)\n    W0_list = []\n    for i in range(5):\n        mask = np.arange(5) != i\n        Xi = X[:, mask]          # (N,4)\n        # solve Xi @ wi \u2248 R[:,i]\n        wi, *_ = np.linalg.lstsq(Xi, R[:,i], rcond=None)\n        W0_list.extend(wi.tolist())\n    W0 = np.array(W0_list, dtype=float)  # (20,)\n\n    # 4) Pack initial parameter vector\n    p0 = np.concatenate([c0, alpha0, gamma0, W0])  # (35,)\n\n    # 5) Bounds for stability\n    bounds = []\n    bounds += [(0.0, 10.0)] * 5    # c_i\n    bounds += [(-20.0, 20.0)] * 5  # alpha_i\n    bounds += [(0.1, 5.0)] * 5     # gamma_i\n    bounds += [(-10.0, 10.0)] * 20 # W_offdiag\n\n    # 6) Objective: MSE + light L2 on cross\u2010terms\n    reg_lambda = 1e-4\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse  = np.mean((pred - Y)**2)\n        reg  = reg_lambda * np.sum(p[15:35]**2)\n        return mse + reg\n\n    # 7) Multi-start L-BFGS-B for robustness\n    best_p    = p0.copy()\n    best_obj  = objective(best_p)\n    for restart in range(5):\n        if restart > 0:\n            x0 = best_p + np.random.randn(best_p.size) * 0.1\n        else:\n            x0 = p0.copy()\n        res = minimize(\n            objective, x0,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'ftol': 1e-9, 'maxiter': 2000}\n        )\n        if res.success and res.fun < best_obj:\n            best_obj, best_p = res.fun, res.x.copy()\n\n    return best_p\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.988962, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\n# EVOLVE-BLOCK-START\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Hybrid polynomial\u2010log scaling law with global cross\u2010interaction:\n      y_d = a_d \n            + b_d * x_d \n            + c_d * x_d^2 \n            + l_d * log(x_d + eps)\n            + g_d * sum_{i<j} m_{ij} * x_i * x_j\n    Total parameters = 5 (a) + 5 (b) + 5 (c) + 5 (l) + 10 (m) + 5 (g) = 35\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))  # (N,5)\n    N, F = X.shape\n    assert F == 5, \"Expected 5 domain proportions\"\n    p = np.asarray(params).ravel()\n    idx = 0\n\n    # unpack parameters\n    a = p[idx:idx+5];    idx += 5  # biases\n    b = p[idx:idx+5];    idx += 5  # linear\n    c = p[idx:idx+5];    idx += 5  # quadratic\n    l = p[idx:idx+5];    idx += 5  # log\u2010scale\n    m = p[idx:idx+10];   idx += 10 # global pair\u2010wise weights\n    g = p[idx:idx+5]               # domain\u2010specific cross scaling\n\n    # polynomial + log terms\n    eps = 1e-6\n    Z = a + b*X + c*(X**2) + l*np.log(X + eps)  # (N,5)\n\n    # global cross interactions: sum_{i<j} m[k] * x_i * x_j\n    E = np.zeros(N)\n    pairs = [(i,j) for i in range(5) for j in range(i+1,5)]\n    for k, (i,j) in enumerate(pairs):\n        E += m[k] * (X[:,i] * X[:,j])\n\n    # add domain\u2010specific scaling of the global interaction E\n    pred = Z + np.outer(E, g)  # (N,5)\n    return pred\n\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the hybrid scaling law by minimizing MSE + L2 regularization\n    over 35 parameters using L-BFGS-B with multiple random restarts.\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points))\n    Y = np.atleast_2d(np.asarray(loss_values))\n    N, F = X.shape\n    assert F == 5, \"Expected 5 domain proportions\"\n    P = 35\n\n    # initial guess\n    a0 = Y.mean(axis=0)\n    b0 = np.ones(5) * 0.1\n    c0 = np.zeros(5)\n    l0 = np.zeros(5)\n    m0 = np.zeros(10)\n    g0 = np.zeros(5)\n    p0 = np.concatenate([a0, b0, c0, l0, m0, g0])\n\n    # objective: MSE + small L2 penalty\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse = np.mean((pred - Y)**2)\n        reg = 1e-5 * np.sum(p**2)\n        return mse + reg\n\n    best_p, best_loss = None, np.inf\n    rng = np.random.RandomState(1)\n    # try a few restarts to avoid bad local minima\n    for i in range(7):\n        start = p0 if i == 0 else p0 + rng.normal(scale=0.01, size=P)\n        res = minimize(objective, start,\n                       method='L-BFGS-B',\n                       options={'maxiter': 1000, 'ftol': 1e-9})\n        if res.fun < best_loss:\n            best_loss, best_p = res.fun, res.x\n\n    return best_p\n# EVOLVE-BLOCK-END", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.98722, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain loss (N\u00d75) from mixture proportions (N\u00d75)\n    using a low-rank cross-domain interaction (rank=2) plus per-domain\n    power-law scaling.\n\n    params layout (35 total):\n      \u03b3   (5)   per-domain exponents\n      V   (5\u00d72) left factor for cross-domain interactions\n      U   (5\u00d72) right factor for cross-domain interactions\n      a   (5)   per-domain amplitude scales\n      b   (5)   per-domain biases\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))  # (N,5)\n    N, F = X.shape\n    assert F == 5, \"Expected input with 5 domain proportions\"\n    \n    p = params.ravel()\n    # unpack\n    \u03b3 = np.clip(p[0:5], 0.01, 10.0)           # (5,)\n    idx = 5\n    V = p[idx:idx+10].reshape(5, 2); idx += 10  # (5\u00d72)\n    U = p[idx:idx+10].reshape(5, 2); idx += 10  # (5\u00d72)\n    a = p[idx:idx+5]; idx += 5                  # (5,)\n    b = p[idx:idx+5]                            # (5,)\n\n    # per-domain power-law\n    P = X ** \u03b3                                 # (N,5)\n    # low-rank cross-domain term: (N\u00d75)\u2192(N\u00d72)\u2192(N\u00d75)\n    cross = (P.dot(V)).dot(U.T)                # (N,5)\n\n    # combine: per-domain scaled power-law + cross + bias\n    return P * a[None, :] + cross + b[None, :]\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35-parameter low-rank cross-domain scaling law by minimizing\n    mean squared error with light L2 regularization on cross factors.\n    \n    Returns:\n      params (35,) optimized parameter vector [\u03b3, V, U, a, b]\n    \"\"\"\n    X = np.atleast_2d(np.asarray(data_points, dtype=float))  # (N,5)\n    y = np.asarray(loss_values, dtype=float)\n    if y.ndim == 1:\n        y = y.reshape(-1, 5)\n    N, F = X.shape\n    assert F == 5 and y.shape[1] == 5, \"Expected shapes (N,5) for data and losses\"\n\n    # initialize parameters\n    \u03b30 = np.ones(5)                         # exponent \u22481\n    V0 = np.zeros((5, 2)) + 0.1             # small interaction init\n    U0 = np.zeros((5, 2)) + 0.1             # small interaction init\n    a0 = np.ones(5)                         # scale \u22481\n    b0 = y.mean(axis=0)                     # bias \u2248 mean loss\n    init = np.concatenate([\u03b30, V0.ravel(), U0.ravel(), a0, b0])\n\n    # bounds to ensure stability\n    bounds = []\n    bounds += [(0.01, 10.0)] * 5            # \u03b3\n    bounds += [(-5.0, 5.0)]  * 10           # V\n    bounds += [(-5.0, 5.0)]  * 10           # U\n    bounds += [(-5.0, 5.0)]  * 5            # a\n    bounds += [(0.0, 10.0)]  * 5            # b\n\n    # regularization on cross-domain factors V, U\n    reg_lambda = 1e-3\n\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse = np.mean((pred - y) ** 2)\n        # L2 penalty on V and U entries (indices 5:25)\n        reg = reg_lambda * np.sum(p[5:25] ** 2)\n        return mse + reg\n\n    res = minimize(\n        objective,\n        init,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 2000, 'ftol': 1e-9}\n    )\n    return res.x if res.success else init", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
{"model_name": "o4-mini", "reward_r2": 0.984682, "solution": "import numpy as np\nfrom scipy.optimize import minimize\n\ndef scaling_law_func(data_points, params):\n    \"\"\"\n    Predict multi-domain losses from 5-way mixture proportions.\n    - data_points: (N,5) array of domain mixture proportions\n    - params: length-35 vector [U(5\u00d72), V(5\u00d72), bo(5), w(5), c(5)]\n    Returns:\n      preds: (N,5) array of predicted losses\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)     # (N,5)\n    p = np.asarray(params, dtype=float).ravel()  # (35,)\n    N, F = X.shape\n    r = 2\n\n    # Unpack parameters\n    idx = 0\n    U  = p[idx:idx + F*r].reshape(F, r); idx += F*r    # 10\n    V  = p[idx:idx + F*r].reshape(F, r); idx += F*r    # 10\n    bo = p[idx:idx + F];            idx += F           # 5\n    w  = p[idx:idx + F];            idx += F           # 5\n    c  = p[idx:idx + F]                                # 5\n\n    # Stabilize and exponentiate\n    b = np.exp(np.clip(bo, -2.0, 2.0))   # per-domain exponent (5,)\n\n    # Power-law transform with epsilon to avoid zeros\n    eps = 1e-6\n    Xb = (X + eps) ** b                  # (N,5)\n\n    # Low-rank cross-domain mixing: (N,5)->(N,2)->(N,5)\n    Z     = Xb.dot(U)                    # (N,2)\n    cross = Z.dot(V.T)                   # (N,5)\n\n    # Elementwise residual on the transformed input\n    residual = Xb * w[np.newaxis, :]     # (N,5)\n\n    # Final prediction\n    return cross + residual + c[np.newaxis, :]\n\ndef fit_scaling_law(data_points, loss_values):\n    \"\"\"\n    Fit the 35-parameter scaling law by minimizing MSE + L2 regularization.\n    Returns optimized parameter vector of length 35.\n    \"\"\"\n    X = np.asarray(data_points, dtype=float)  # (N,5)\n    Y = np.asarray(loss_values, dtype=float)  # (N,5)\n    N, F = X.shape\n    r = 2\n\n    # 1) Initialize per-domain bias to mean loss\n    c0 = Y.mean(axis=0)                      # (5,)\n\n    # 2) Initialize linear weights w via simple regression on X (b=1)\n    w0 = np.zeros(F)\n    for d in range(F):\n        xd = X[:, d]\n        yd = Y[:, d] - c0[d]\n        denom = xd.dot(xd) + 1e-6\n        if denom > 0:\n            w0[d] = xd.dot(yd) / denom\n\n    # 3) Exponent offsets start at zero (b=1)\n    bo0 = np.zeros(F)\n\n    # 4) Low-rank factors small random\n    U0 = np.random.randn(F, r) * 1e-2\n    V0 = np.random.randn(F, r) * 1e-2\n\n    # Pack initial parameter vector\n    p0 = np.concatenate([\n        U0.ravel(),    # 10\n        V0.ravel(),    # 10\n        bo0,           #  5\n        w0,            #  5\n        c0             #  5\n    ])\n\n    # Bounds: clip bo for stability, others free\n    bounds = (\n        [(None, None)] * (2*F*r) +\n        [(-2.0, 2.0)] * F +\n        [(None, None)] * (F + F)\n    )\n\n    # Regularization strength\n    lambda_reg = 1e-4\n\n    def objective(p):\n        pred = scaling_law_func(X, p)\n        mse  = np.mean((pred - Y)**2)\n        reg  = lambda_reg * np.sum(p**2)\n        return mse + reg\n\n    # Multi-start L-BFGS-B\n    best_p   = p0.copy()\n    best_val = np.inf\n    for _ in range(5):\n        start = p0 + 1e-2 * np.random.randn(p0.size)\n        res = minimize(\n            objective, start,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 1000, 'ftol': 1e-9}\n        )\n        if res.success and res.fun < best_val:\n            best_val, best_p = res.fun, res.x\n\n    return best_p", "agent_name": "SLDAgent", "task": "domain_mixture_scaling_law"}
