<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - LR-BSZ Scaling Law - SLDAgent + Gemini 3 Pro Preview</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>LR-BSZ Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Gemini 3 Pro Preview</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.940806 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.940713</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.940533</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940806 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Implements a refined scaling law with terms for Model Size, Dataset Size, 
Learning Rate (quadratic penalty in log-space), and Batch Size.
Uses physics-informed bounds and multi-start L-BFGS-B optimization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LM loss based on scaling law parameters.
    
    Model Form:
    L = E + A*N^(-alpha) + B*D^(-beta) + C*(log(lr) - log_lr_opt)^2 + F*bsz^G
    
    Where:
    log_lr_opt = d0 + d1*log(N) + d2*log(bsz)
    
    Inputs are normalized:
    - N: Parameters / 1e9
    - D: Tokens / 1e10
    - lr: Learning Rate / 1e-3
    - bsz: Batch Size / 2048
    
    Parameters (11 total):
    0: E (Irreducible loss)
    1: A (Model size coeff)
    2: alpha (Model size exponent)
    3: B (Data size coeff)
    4: beta (Data size exponent)
    5: C (LR penalty coeff)
    6: d0 (Opt LR intercept)
    7: d1 (Opt LR slope w.r.t N)
    8: d2 (Opt LR slope w.r.t bsz)
    9: F (Batch size coeff)
    10: G (Batch size exponent)
    &quot;&quot;&quot;
    # Normalization constants (based on dataset statistics)
    # Feature order: [lr, bsz, data_size, non_embedding_param_size]
    # Using 2048 for bsz as it matches the max value in the dataset
    scales = np.array([1e-3, 2048.0, 1e10, 1e9])
    
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    X_norm = X / scales[None, :]
    
    lr = X_norm[:, 0]
    bsz = X_norm[:, 1]
    D = X_norm[:, 2]
    N_param = X_norm[:, 3]
    
    # Handle params shape
    params = np.asarray(params, dtype=np.float64)
    original_ndim = params.ndim
    if original_ndim == 1:
        params = params[None, :]
    
    # Unpack parameters
    E     = params[:, 0:1]
    A     = params[:, 1:2]
    alpha = params[:, 2:3]
    B     = params[:, 3:4]
    beta  = params[:, 4:5]
    C     = params[:, 5:6]
    d0    = params[:, 6:7]
    d1    = params[:, 7:8]
    d2    = params[:, 8:9]
    F     = params[:, 9:10]
    G     = params[:, 10:11]
    
    eps = 1e-9
    
    # Broadcasting preparation
    N_p = N_param[None, :]
    D_p = D[None, :]
    lr_p = lr[None, :]
    bsz_p = bsz[None, :]
    
    # 1. Power Laws for N and D
    # Using abs(alpha/beta) to ensure decay behavior
    term_N = A * ((N_p + eps) ** (-np.abs(alpha)))
    term_D = B * ((D_p + eps) ** (-np.abs(beta)))
    
    # 2. Learning Rate Penalty
    # Optimal LR depends on N and bsz
    log_N = np.log(N_p + eps)
    log_bsz = np.log(bsz_p + eps)
    log_lr = np.log(lr_p + eps)
    
    # Linear relationship in log-log space
    opt_log_lr = d0 + d1 * log_N + d2 * log_bsz
    term_LR = C * ((log_lr - opt_log_lr) ** 2)
    
    # 3. Batch Size Independent Effect
    term_BSZ = F * ((bsz_p + eps) ** G)
    
    # Total Loss
    pred = E + term_N + term_D + term_LR + term_BSZ
    
    # Return shape handling
    pred = pred.T
    if original_ndim == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law parameters using multi-start L-BFGS-B.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    def objective(p):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y)**2)
    
    # Parameter Bounds
    # E: [1.0, 2.2] - Irreducible loss must be &lt; min(loss) ~ 2.1
    # A, B: [0, inf] - Coefficients
    # alpha, beta: [0.01, 1.0] - Exponents typically &lt; 1.0
    # C: [0, inf] - Penalty curvature
    # d1: [-2.0, 0.5] - LR usually decreases with Model Size
    # d2: [-0.5, 2.0] - LR usually increases with Batch Size
    bounds = [
        (1.0, 2.2),   # E
        (0.0, None),  # A
        (0.01, 1.0),  # alpha
        (0.0, None),  # B
        (0.01, 1.0),  # beta
        (0.0, None),  # C
        (None, None), # d0
        (-2.0, 0.5),  # d1
        (-0.5, 2.0),  # d2
        (None, None), # F
        (None, None)  # G
    ]
    
    # Multiple initializations to avoid local minima
    guesses = [
        # 1. Standard Scaling: alpha, beta ~ 0.1-0.3
        [1.8, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.2, 0.2, 0.01, 0.0],
        # 2. Chinchilla-like: alpha, beta ~ 0.5
        [1.6, 2.0, 0.5, 2.0, 0.5, 0.5, -0.5, -0.5, 0.5, 0.05, -0.1],
        # 3. High Variance: small E, large coeffs
        [1.2, 5.0, 0.2, 5.0, 0.2, 0.1, 0.0, -0.1, 0.1, 0.0, 0.0]
    ]
    
    best_loss = np.inf
    best_params = np.array(guesses[0])
    
    for p0 in guesses:
        try:
            res = minimize(objective, p0, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                          options={&#x27;maxiter&#x27;: 3000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10})
            if res.fun &lt; best_loss:
                best_loss = res.fun
                best_params = res.x
        except Exception:
            continue
            
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940792 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Implements a refined scaling law with terms for Model Size, Dataset Size, 
Learning Rate (quadratic penalty with interaction), and Batch Size.
Uses a two-stage optimization strategy to ensure robust fitting of core parameters.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LM loss based on scaling law parameters.
    
    Model Form:
    L = e + a*N^(-alpha) + b*D^(-beta) + c*(log(lr) - log_opt_lr)^2 + f*bsz^g
    
    Where:
    log_opt_lr = d0 + d1*log(N) + d2*log(bsz)
    
    Inputs are normalized:
    - N: Parameters / 1e9
    - D: Tokens / 1e10
    - lr: Learning Rate / 1e-3
    - bsz: Batch Size / 2048
    
    Parameters (11 total):
    e: Irreducible loss
    a, alpha: Model size scaling coeff and exponent
    b, beta: Data size scaling coeff and exponent
    c: Learning rate penalty coefficient
    d0: Optimal log-lr intercept
    d1: Optimal log-lr slope w.r.t log(N)
    d2: Optimal log-lr slope w.r.t log(bsz)
    f, g: Batch size residual scaling coeff and exponent
    &quot;&quot;&quot;
    # Constants for normalization
    # Feature order: [lr, bsz, data_size, non_embedding_param_size]
    scales = np.array([1e-3, 2048.0, 1e10, 1e9])
    
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    X_norm = X / scales[None, :]
    
    lr = X_norm[:, 0]
    bsz = X_norm[:, 1]
    D = X_norm[:, 2]
    N_param = X_norm[:, 3]
    
    params = np.asarray(params, dtype=np.float64)
    if params.ndim == 1:
        params = params[None, :]
    
    # Unpack parameters (T, 1)
    e     = params[:, 0:1]
    a     = params[:, 1:2]
    alpha = params[:, 2:3]
    b     = params[:, 3:4]
    beta  = params[:, 4:5]
    c     = params[:, 5:6]
    d0    = params[:, 6:7]
    d1    = params[:, 7:8]
    d2    = params[:, 8:9]
    f     = params[:, 9:10]
    g     = params[:, 10:11]
    
    eps = 1e-9
    
    # Reshape data for broadcasting: (1, N_samples)
    N_p = N_param[None, :]
    D_p = D[None, :]
    lr_p = lr[None, :]
    bsz_p = bsz[None, :]
    
    # 1. Model Size Scaling
    term_N = a * ((N_p + eps) ** -np.abs(alpha))
    
    # 2. Data Size Scaling
    term_D = b * ((D_p + eps) ** -np.abs(beta))
    
    # 3. Learning Rate Penalty with Interaction
    log_lr = np.log(lr_p + eps)
    log_N = np.log(N_p + eps)
    log_B = np.log(bsz_p + eps)
    
    # Optimal LR depends on Model Size and Batch Size
    opt_log_lr = d0 + d1 * log_N + d2 * log_B
    term_LR = c * ((log_lr - opt_log_lr) ** 2)
    
    # 4. Batch Size Residual
    term_BSZ = f * ((bsz_p + eps) ** g)
    
    pred = e + term_N + term_D + term_LR + term_BSZ
    
    if params.shape[0] == 1:
        return pred.flatten()
    return pred.T

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law parameters using a two-stage L-BFGS-B optimization.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    # Parameters: e, a, alpha, b, beta, c, d0, d1, d2, f, g
    # Bounds designed to guide optimization towards physically meaningful regions
    bounds = [
        (1.0, 4.0),   # e: irreducible loss
        (0.0, 10.0),  # a
        (0.0, 2.0),   # alpha
        (0.0, 10.0),  # b
        (0.0, 2.0),   # beta
        (0.0, 10.0),  # c
        (None, None), # d0
        (None, 0.5),  # d1: usually negative
        (-1.0, 2.0),  # d2: usually positive
        (None, None), # f
        (None, None)  # g
    ]
    
    def objective(p):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y)**2)

    # Step 1: Fit core parameters (N, D, basic LR). Fix d2 (BSZ-LR) and f,g (BSZ residual) to 0.
    # This establishes the baseline Chinchilla + LR scaling.
    p0_step1 = np.array([2.5, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.2, 0.0, 0.0, 0.0])
    
    bounds_step1 = bounds[:]
    # Fix d2, f, g to 0 by setting equal bounds
    bounds_step1[8] = (0.0, 0.0)
    bounds_step1[9] = (0.0, 0.0)
    bounds_step1[10] = (0.0, 0.0)
    
    res1 = minimize(objective, p0_step1, method=&#x27;L-BFGS-B&#x27;, bounds=bounds_step1,
                    options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-9})
    
    p_step1 = res1.x
    
    # Step 2: Unfreeze all parameters. Use p_step1 as initialization.
    # We add a few restarts with perturbations to d2 to check if BSZ interaction helps.
    
    candidates = []
    
    # Candidate A: Start from Step 1 result (d2=0, f=0)
    candidates.append(p_step1)
    
    # Candidate B: Start with theoretical BSZ scaling for LR (approx sqrt scaling -&gt; d2=0.5)
    p_init_B = p_step1.copy()
    p_init_B[8] = 0.5 
    candidates.append(p_init_B)
    
    best_loss = np.inf
    best_params = p_step1
    
    for p0 in candidates:
        try:
            res = minimize(objective, p0, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                           options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-10})
            if res.fun &lt; best_loss:
                best_loss = res.fun
                best_params = res.x
        except:
            continue
            
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940739 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Refined model with 11 parameters including Model Size (N), Dataset Size (D), 
Batch Size (B), and Learning Rate (LR) interactions.
Includes B-dependent optimal LR and residual B scaling.
Optimization uses multiple restarts with L-BFGS-B.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LM loss based on scaling law parameters.
    
    Model Form:
    L = e + a*N^(-alpha) + b*D^(-beta) + c*(log(lr) - log_lr_opt)^2 + f*B^g
    
    Where:
    log_lr_opt = d0 + d1*log(N) + d2*log(B)
    
    Inputs are normalized:
    - N: Parameters / 1e9
    - D: Tokens / 1e10
    - lr: Learning Rate / 1e-3
    - bsz: Batch Size / 2048
    &quot;&quot;&quot;
    # Normalization constants [lr, bsz, data_size, non_embedding_param_size]
    scales = np.array([1e-3, 2048.0, 1e10, 1e9])
    
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    X_norm = X / scales[None, :]
    
    lr = X_norm[:, 0]
    bsz = X_norm[:, 1]
    D = X_norm[:, 2]
    N_param = X_norm[:, 3]
    
    params = np.asarray(params, dtype=np.float64)
    original_ndim = params.ndim
    if original_ndim == 1:
        params = params[None, :]
    
    # Unpack 11 parameters
    e     = params[:, 0:1]
    a     = params[:, 1:2]
    alpha = params[:, 2:3]
    b     = params[:, 3:4]
    beta  = params[:, 4:5]
    c     = params[:, 5:6]
    d0    = params[:, 6:7]
    d1    = params[:, 7:8]
    d2    = params[:, 8:9]
    f     = params[:, 9:10]
    g     = params[:, 10:11]
    
    eps = 1e-9
    
    # Reshape for broadcasting
    N_p = N_param[None, :]
    D_p = D[None, :]
    lr_p = lr[None, :]
    bsz_p = bsz[None, :]
    
    # 1. Model Size Power Law
    term_N = a * ((N_p + eps) ** -np.abs(alpha))
    
    # 2. Data Size Power Law
    term_D = b * ((D_p + eps) ** -np.abs(beta))
    
    # 3. Learning Rate Penalty with Interactions
    log_lr = np.log(lr_p + eps)
    log_N = np.log(N_p + eps)
    log_B = np.log(bsz_p + eps)
    
    # Optimal log LR shifts with Model Size (d1) and Batch Size (d2)
    opt_log_lr = d0 + d1 * log_N + d2 * log_B
    term_LR = c * ((log_lr - opt_log_lr) ** 2)
    
    # 4. Batch Size Residual Power Law
    term_BSZ = f * ((bsz_p + eps) ** g)
    
    pred = e + term_N + term_D + term_LR + term_BSZ
    
    pred = pred.T
    if original_ndim == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the 11-parameter scaling law using L-BFGS-B with multiple initializations.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    # Bounds for parameters
    # e: Irreducible loss (1.0 to 2.5)
    # a, b, c, f: Coefficients (non-negative)
    # alpha, beta: Exponents (0 to 2.0)
    # d0, d1, d2: LR shift parameters (unbounded)
    # g: BSZ exponent (unbounded, allows various scaling behaviors)
    bounds = [
        (1.0, 2.5),   # e
        (0.0, None),  # a
        (0.0, 2.0),   # alpha
        (0.0, None),  # b
        (0.0, 2.0),   # beta
        (0.0, None),  # c
        (None, None), # d0
        (None, None), # d1
        (None, None), # d2
        (None, None), # f
        (None, None)  # g
    ]
    
    # Define a set of diverse initial guesses
    # [e, a, alpha, b, beta, c, d0, d1, d2, f, g]
    guesses = [
        # 1. Standard initialization
        [1.8, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.1, 0.1, 0.0, 0.0],
        # 2. High scaling exponents, lower LR penalty
        [1.6, 1.0, 0.3, 1.0, 0.3, 0.1, 0.0, -0.2, 0.2, 0.0, 0.1],
        # 3. Emphasis on Batch Size effect
        [1.8, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.1, 0.1, 0.1, 1.0],
        # 4. Low baseline, high LR penalty
        [1.4, 0.8, 0.1, 0.8, 0.1, 0.5, 0.5, -0.1, 0.0, 0.0, 0.0]
    ]
    
    best_loss = np.inf
    best_params = np.array(guesses[0])
    
    def objective(p):
        preds = scaling_law_func(X, p)
        # Using MSE
        return np.mean((preds - y)**2)
    
    for p0 in guesses:
        try:
            res = minimize(objective, p0, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                           options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-10})
            if res.fun &lt; best_loss:
                best_loss = res.fun
                best_params = res.x
        except:
            continue
            
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940693 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Refined scaling law with additive terms for Model, Data, LR penalty, and Batch Size.
Uses geometric mean normalization for stability and physics-informed bounds.
Includes a diverse set of initial guesses including known scaling regimes.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LM loss based on scaling law parameters.
    
    Model Form:
    L = E + A*N^(-alpha) + B*D^(-beta) + C*(log(lr) - log_lr_opt)^2 + F*bsz^G
    
    Where:
    log_lr_opt = d0 + d1*log(N) + d2*log(bsz)
    
    Inputs are normalized:
    - N: Parameters / 1e9
    - D: Tokens / 1e10
    - lr: Learning Rate / 1e-3
    - bsz: Batch Size / 2048
    
    Parameters (11 total):
    0: E (Irreducible loss)
    1: A (Model size coeff)
    2: alpha (Model size exponent)
    3: B (Data size coeff)
    4: beta (Data size exponent)
    5: C (LR penalty coeff)
    6: d0 (Opt LR intercept)
    7: d1 (Opt LR slope w.r.t N)
    8: d2 (Opt LR slope w.r.t bsz)
    9: F (Batch size coeff)
    10: G (Batch size exponent)
    &quot;&quot;&quot;
    # Normalization constants (Program 1 settings proved effective)
    scales = np.array([1e-3, 2048.0, 1e10, 1e9])
    
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    X_norm = X / scales[None, :]
    
    lr = X_norm[:, 0]
    bsz = X_norm[:, 1]
    D = X_norm[:, 2]
    N_param = X_norm[:, 3]
    
    # Handle params shape
    params = np.asarray(params, dtype=np.float64)
    original_ndim = params.ndim
    if original_ndim == 1:
        params = params[None, :]
    
    # Unpack parameters
    E     = params[:, 0:1]
    A     = params[:, 1:2]
    alpha = params[:, 2:3]
    B     = params[:, 3:4]
    beta  = params[:, 4:5]
    C     = params[:, 5:6]
    d0    = params[:, 6:7]
    d1    = params[:, 7:8]
    d2    = params[:, 8:9]
    F     = params[:, 9:10]
    G     = params[:, 10:11]
    
    eps = 1e-9
    
    # Broadcasting preparation
    N_p = N_param[None, :]
    D_p = D[None, :]
    lr_p = lr[None, :]
    bsz_p = bsz[None, :]
    
    # 1. Power Laws for N and D
    # Use abs(alpha/beta) to ensure decay
    term_N = A * ((N_p + eps) ** (-np.abs(alpha)))
    term_D = B * ((D_p + eps) ** (-np.abs(beta)))
    
    # 2. Learning Rate Penalty
    # Optimal LR depends on N and bsz
    log_N = np.log(N_p + eps)
    log_bsz = np.log(bsz_p + eps)
    log_lr = np.log(lr_p + eps)
    
    opt_log_lr = d0 + d1 * log_N + d2 * log_bsz
    term_LR = C * ((log_lr - opt_log_lr) ** 2)
    
    # 3. Batch Size Effect
    # Power law scaling. If G &lt; 0, larger batch size -&gt; smaller loss (efficiency/stability)
    # If G &gt; 0, larger batch size -&gt; larger loss (noise reduction saturation?)
    term_BSZ = F * ((bsz_p + eps) ** G)
    
    # Total Loss
    pred = E + term_N + term_D + term_LR + term_BSZ
    
    # Return shape handling
    pred = pred.T
    if original_ndim == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law parameters using multi-start L-BFGS-B.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    min_loss = np.min(y)
    
    def objective(p):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y)**2)
    
    # Parameter Bounds
    # E: [1.0, min_loss] - Irreducible loss must be below achieved loss
    # A, B: [0, inf]
    # alpha, beta: [0, 3]
    # C: [0, inf]
    # d0, d1, d2: unconstrained
    # F: unconstrained (though usually positive if G is chosen well)
    # G: unconstrained
    bounds = [
        (1.0, min_loss - 0.01), # E
        (0.0, None),            # A
        (0.0, 3.0),             # alpha
        (0.0, None),            # B
        (0.0, 3.0),             # beta
        (0.0, None),            # C
        (None, None),           # d0
        (None, None),           # d1
        (None, None),           # d2
        (None, None),           # F
        (None, None)            # G
    ]
    
    # Initial Guesses
    # p = [E, A, alpha, B, beta, C, d0, d1, d2, F, G]
    guesses = [
        # 1. Kaplan-like (low exponents)
        [1.8, 0.5, 0.07, 0.5, 0.07, 0.2, 0.0, -0.1, 0.1, 0.01, -0.1],
        # 2. Chinchilla-like (higher exponents)
        [1.6, 1.0, 0.33, 1.0, 0.33, 0.2, 0.0, -0.2, 0.1, 0.01, -0.1],
        # 3. High data dependence
        [1.5, 0.5, 0.05, 2.0, 0.5, 0.1, 0.5, -0.1, 0.0, 0.05, -0.2],
        # 4. Previous Best (Program 1 params approx)
        [1.8, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.1, 0.0, 0.01, -0.01],
        # 5. Flat batch size effect
        [2.0, 1.0, 0.1, 1.0, 0.1, 0.5, 0.0, -0.2, 0.2, 0.0, 0.0],
        # 6. Negative BSZ exponent (efficiency gain)
        [1.7, 0.5, 0.1, 0.5, 0.1, 0.2, 0.0, -0.1, 0.1, 0.1, -0.2]
    ]
    
    best_loss = np.inf
    best_params = np.array(guesses[0])
    
    for p0 in guesses:
        try:
            res = minimize(objective, p0, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                          options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10})
            if res.fun &lt; best_loss:
                best_loss = res.fun
                best_params = res.x
        except Exception:
            continue
            
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940533 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning.
Models loss as a sum of power laws (Model, Data) and a quadratic log-LR penalty.
The optimal LR is modeled as a power-law function of Model Size and Batch Size.
Includes a residual power-law term for Batch Size effects.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts LM loss.
    
    L = e + a*N^-alpha + b*D^-beta + c*(log(lr) - log_opt_lr)^2 + f*B^g
    log_opt_lr = d0 + d1*log(N) + d2*log(B)
    
    Inputs (normalized):
    - lr: Learning Rate / 1e-3
    - B:  Batch Size / 256
    - D:  Tokens / 1e10
    - N:  Parameters / 5e8
    &quot;&quot;&quot;
    # Fixed normalization constants
    # [lr, bsz, data_size, n_params]
    scales = np.array([1e-3, 256.0, 1e10, 5e8])
    
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    X_norm = X / scales[None, :]
    
    # Unpack features
    lr = X_norm[:, 0]
    bsz = X_norm[:, 1]
    D = X_norm[:, 2]
    N_param = X_norm[:, 3]
    
    # Parse parameters
    params = np.asarray(params, dtype=np.float64)
    one_dim = (params.ndim == 1)
    if one_dim:
        params = params[None, :]
        
    # 11 Params: e, a, alpha, b, beta, c, d0, d1, d2, f, g
    e     = params[:, 0:1]
    a     = params[:, 1:2]
    alpha = params[:, 2:3]
    b     = params[:, 3:4]
    beta  = params[:, 4:5]
    c     = params[:, 5:6]
    d0    = params[:, 6:7]
    d1    = params[:, 7:8]
    d2    = params[:, 8:9]
    f     = params[:, 9:10]
    g     = params[:, 10:11]
    
    eps = 1e-9
    
    # Term 1: Model Size
    term_model = a * ((N_param[None, :] + eps) ** -alpha)
    
    # Term 2: Data Size
    term_data = b * ((D[None, :] + eps) ** -beta)
    
    # Term 3: Learning Rate Penalty
    # Optimal log LR shifts with N and B
    log_N = np.log(N_param[None, :] + eps)
    log_B = np.log(bsz[None, :] + eps)
    opt_log_lr = d0 + d1 * log_N + d2 * log_B
    
    log_lr = np.log(lr[None, :] + eps)
    term_lr = c * ((log_lr - opt_log_lr) ** 2)
    
    # Term 4: Batch Size explicit scaling
    term_bsz = f * ((bsz[None, :] + eps) ** g)
    
    pred = e + term_model + term_data + term_lr + term_bsz
    
    pred = pred.T
    if one_dim:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    # Bounds for parameters
    # e, a, alpha, b, beta, c, d0, d1, d2, f, g
    bounds = [
        (1.0, 4.0),   # e: Irreducible loss
        (0.0, 20.0),  # a: Model coeff
        (0.0, 1.5),   # alpha: Model exp
        (0.0, 20.0),  # b: Data coeff
        (0.0, 1.5),   # beta: Data exp
        (0.0, 10.0),  # c: LR penalty
        (-5.0, 5.0),  # d0: Opt LR intercept
        (-2.0, 2.0),  # d1: Opt LR slope (N)
        (-2.0, 2.0),  # d2: Opt LR slope (B)
        (-2.0, 2.0),  # f: Bsz coeff
        (-2.0, 2.0)   # g: Bsz exp
    ]
    
    # Diverse initialization seeds
    seeds = [
        # 1. Theoretical (Chinchilla + Linear Scaling)
        [1.8, 0.5, 0.3, 0.5, 0.3, 0.2, 0.0, -0.2, 0.5, 0.0, 0.0],
        # 2. High irreducible, low exponents
        [2.2, 0.2, 0.1, 0.2, 0.1, 0.5, -0.5, -0.1, 0.8, 0.01, 0.1],
        # 3. Low irreducible, high exponents
        [1.5, 1.0, 0.5, 1.0, 0.5, 0.1, 0.5, -0.3, 0.3, -0.01, 0.1],
        # 4. Strong LR sensitivity
        [1.9, 0.4, 0.2, 0.4, 0.2, 2.0, 0.0, -0.2, 0.5, 0.0, 0.0]
    ]
    
    best_loss = np.inf
    best_params = np.array(seeds[0])
    
    def objective(p):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y)**2)
    
    for p0 in seeds:
        try:
            res = minimize(
                objective, 
                p0, 
                method=&#x27;L-BFGS-B&#x27;, 
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-9}
            )
            if res.fun &lt; best_loss:
                best_loss = res.fun
                best_params = res.x
        except Exception:
            continue
            
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
