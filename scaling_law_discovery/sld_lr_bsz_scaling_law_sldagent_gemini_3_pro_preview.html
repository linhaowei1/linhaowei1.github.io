<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - LR & Batch Size Scaling Law - SLDAgent + Gemini 3 Pro Preview</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>LR & Batch Size Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 3 Pro Preview</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        0.803199
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.639067</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.404715</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.803199
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning.
Implements a Generalized Multiplicative-Additive scaling law.
Model Form:
  Loss = (Bias + Term1 + Term2) * (1 + LR_Penalty)

Key Features:
- Multiplicative interactions model the relative degradation from suboptimal hyperparameters.
- Two flexible power-law terms (linear in log-space) capture:
  - Model Scaling (N)
  - Data Scaling (D)
  - Batch Size effects (B) and critical batch size interactions.
- Input normalization using approximate geometric means improves numerical conditioning.
- Robust optimization with multiple initialization heuristics and bound constraints.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 4) array [lr, bsz, data_size, non_embedding_param_size]
    # params: (T, P) array or (P,) array
    
    # Ensure inputs are float64 for precision
    X = np.asarray(data_points, dtype=np.float64)
    params = np.asarray(params, dtype=np.float64)
    
    # Handle single parameter vector vs batch of parameters
    if params.ndim == 1:
        params = params[None, :]
    
    # 1. Normalization Constants
    # Chosen based on the geometric means of the typical input ranges.
    # Centers the log-transformed features around 0.
    LR_C    = 3.0e-3
    BSZ_C   = 128.0
    DATA_C  = 2.0e10
    MODEL_C = 3.0e8
    
    # 2. Log-space Feature Extraction
    # Adding epsilon inside max to ensure safety, though inputs are positive
    ln_lr    = np.log(np.maximum(X[:, 0], 1e-20) / LR_C)
    ln_bsz   = np.log(np.maximum(X[:, 1], 1e-20) / BSZ_C)
    ln_data  = np.log(np.maximum(X[:, 2], 1e-20) / DATA_C)
    ln_model = np.log(np.maximum(X[:, 3], 1e-20) / MODEL_C)
    
    # Reshape to (1, N) for broadcasting against params (T, 1)
    ln_lr    = ln_lr[None, :]
    ln_bsz   = ln_bsz[None, :]
    ln_data  = ln_data[None, :]
    ln_model = ln_model[None, :]
    
    # 3. Parameter Unpacking (14 Parameters)
    # 0: log_Bias (Irreducible Loss)
    # Term 1 (Primary Scaling): 1:log_A1, 2:n1 (Model), 3:d1 (Data), 4:b1 (Batch)
    # Term 2 (Secondary/Interaction): 5:log_A2, 6:n2, 7:d2, 8:b2
    # LR Penalty: 9:log_S (Scale), 10:mu_0, 11:mu_n (Opt slope N), 12:mu_b (Opt slope B), 13:mu_d (Opt slope D)
    
    p = [params[:, i][:, None] for i in range(14)]
    
    # 4. Compute Base Loss (Bias + T1 + T2)
    bias = np.exp(p[0])
    
    # Term 1: Generalized Power Law A * N^n * D^d * B^b
    # Calculated as exp(log_A + n*lnN + d*lnD + b*lnB)
    log_t1 = p[1] + p[2]*ln_model + p[3]*ln_data + p[4]*ln_bsz
    term1  = np.exp(log_t1)
    
    # Term 2: Additional Power Law for corrections or interactions
    log_t2 = p[5] + p[6]*ln_model + p[7]*ln_data + p[8]*ln_bsz
    term2  = np.exp(log_t2)
    
    base_loss = bias + term1 + term2
    
    # 5. Compute Learning Rate Penalty (Multiplicative)
    # Optimal Log LR modeled as plane in Log-Feature space
    ln_lr_opt = p[10] + p[11]*ln_model + p[12]*ln_bsz + p[13]*ln_data
    
    # Quadratic penalty in log-space
    # Penalty factor = S * (ln_lr - ln_lr_opt)^2
    penalty_scale = np.exp(p[9])
    penalty_factor = penalty_scale * ((ln_lr - ln_lr_opt) ** 2)
    
    # 6. Final Prediction
    # Loss = Base * (1 + Penalty)
    pred = base_loss * (1.0 + penalty_factor)
    
    # Handle output shape: (T, N) -&gt; (N, T) or (N,)
    pred = pred.T
    return pred[:, 0] if pred.shape[1] == 1 else pred

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points, dtype=np.float64)
    y = np.asarray(loss_values, dtype=np.float64)
    
    single_target = False
    if y.ndim == 1:
        y = y[:, None]
        single_target = True
        
    T = y.shape[1]
    P = 14
    final_params = np.zeros((T, P))
    
    def objective(p, y_true):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y_true)**2)
    
    # Parameter Bounds (Box Constraints)
    # Constrain search to physically plausible regions to avoid overfitting and instability.
    # Exponents typically between -4 (steep decay) and +4 (steep growth, rare).
    # Slopes for LR Opt typically small (-1 to 1).
    bounds = [
        (-10.0, 3.0),         # 0: Bias (approx 4e-5 to 20)
        (-20.0, 5.0),         # 1: log_A1
        (-4.0, 4.0), (-4.0, 4.0), (-4.0, 4.0), # 2-4: T1 Exponents (N, D, B)
        (-20.0, 5.0),         # 5: log_A2
        (-4.0, 4.0), (-4.0, 4.0), (-4.0, 4.0), # 6-8: T2 Exponents (N, D, B)
        (-10.0, 10.0),        # 9: LR Penalty Scale
        (-5.0, 5.0),          # 10: LR Opt Intercept
        (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0)  # 11-13: LR Opt Slopes (N, B, D)
    ]
    
    for t in range(T):
        y_curr = y[:, t]
        min_loss = np.min(y_curr)
        
        # Heuristic Initialization for Bias
        init_bias = np.log(max(min_loss - 0.2, 1e-4))
        
        guesses = []
        
        # Guess 1: Canonical Chinchilla-like
        # Term 1: Model dominant (N^-0.4)
        # Term 2: Data dominant (D^-0.4)
        # LR: Scales with N^-0.5 and B^0.5
        g1 = np.zeros(P)
        g1[0] = init_bias
        g1[1] = -1.0; g1[2] = -0.4; g1[3] = 0.0; g1[4] = 0.0 # T1
        g1[5] = -1.0; g1[6] = 0.0; g1[7] = -0.4; g1[8] = 0.0 # T2
        g1[9] = 0.0 # Penalty Scale ~ 1.0
        g1[10] = 0.0; g1[11] = -0.5; g1[12] = 0.5; g1[13] = 0.0 # LR Opt
        guesses.append(g1)
        
        # Guess 2: Coupled N-D Scaling + Batch Efficiency
        # T1: (ND)^-0.3
        # T2: Batch efficiency term (loss increases with B? or decreases?)
        # Let&#x27;s assume T2 captures small batch penalty (B^-0.5)
        g2 = np.zeros(P)
        g2[0] = init_bias
        g2[1] = -1.0; g2[2] = -0.3; g2[3] = -0.3; g2[4] = 0.0
        g2[5] = -2.0; g2[6] = 0.0; g2[7] = 0.0; g2[8] = -0.5
        g2[9] = 0.0
        g2[10] = 0.0; g2[11] = -0.5; g2[12] = 0.5; g2[13] = 0.0
        guesses.append(g2)
        
        # Guess 3: Steep Scaling (Focus on N)
        g3 = g1.copy()
        g3[2] = -0.7; g3[7] = -0.7
        g3[0] = init_bias - 0.5 # Lower bias
        guesses.append(g3)
        
        best_fun = np.inf
        best_p = guesses[0]
        
        # Optimization Loop
        for p0 in guesses:
            try:
                # L-BFGS-B handles bounds effectively
                res = minimize(objective, p0, args=(y_curr,), 
                               method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                               options={&#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-8, &#x27;maxiter&#x27;: 5000})
                if res.fun &lt; best_fun:
                    best_fun = res.fun
                    best_p = res.x
            except Exception:
                continue
                
        final_params[t] = best_p
        
    return final_params[0] if single_target else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.789465
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning.
Implements a Generalized Additive Power Law with Adaptive LR Dynamics.
Model: L = E + A_n N^-a_n + A_d D^-a_d + A_b B^a_b + S(B)(log lr - log lr*)^2.
Key features:
- Unconstrained Batch scaling exponent (allows discovering if B helps or hurts).
- Batch-dependent LR penalty sensitivity (S(B) = S_base * B^gamma).
- Coupled optimal LR dynamics.
- Weighted objective function to prioritize fitting the optimal loss frontier.
- Robust L-BFGS-B optimization with physically motivated initialization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 4) [lr, bsz, data_size, non_embedding_param_size]
    # params: (T, 13) or (13,) array of parameters
    
    X = np.asarray(data_points, dtype=np.float64)
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
        return_1d = True
    else:
        return_1d = False
    
    # Normalization Constants (Geometric means of typical ranges)
    # Centers inputs in log-space for numerical stability.
    NORM_LR    = 2.0e-3
    NORM_BSZ   = 128.0
    NORM_DATA  = 1.0e10
    NORM_MODEL = 2.0e8
    
    lr    = X[:, 0] / NORM_LR
    bsz   = X[:, 1] / NORM_BSZ
    data  = X[:, 2] / NORM_DATA
    model = X[:, 3] / NORM_MODEL
    
    # Log-space inputs
    # Clamp to avoid log(0) - use a safe floor
    log_lr    = np.log(np.maximum(lr, 1e-10))
    log_bsz   = np.log(np.maximum(bsz, 1e-10))
    log_data  = np.log(np.maximum(data, 1e-10))
    log_model = np.log(np.maximum(model, 1e-10))
    
    # Parameters Unpacking (13 parameters)
    # 0: E          - Irreducible loss (Bias)
    # 1: log_An     - Model term coefficient
    # 2: pre_an     - Model term exponent (softplus -&gt; positive)
    # 3: log_Ad     - Data term coefficient
    # 4: pre_ad     - Data term exponent (softplus -&gt; positive)
    # 5: log_Ab     - Batch term coefficient
    # 6: alpha_b    - Batch term exponent (Real: allows decay or growth)
    # 7: log_Slr    - Base LR penalty sensitivity
    # 8: gamma_s    - Scaling of LR penalty with Batch (S ~ B^gamma)
    # 9: mu_0       - Base optimal log_lr
    # 10: mu_n      - Shift in opt_lr due to model size
    # 11: mu_b      - Shift in opt_lr due to batch size
    # 12: mu_d      - Shift in opt_lr due to data size

    E       = params[:, 0]
    A_n     = np.exp(params[:, 1])
    alpha_n = np.logaddexp(0, params[:, 2]) + 1e-4
    
    A_d     = np.exp(params[:, 3])
    alpha_d = np.logaddexp(0, params[:, 4]) + 1e-4
    
    A_b     = np.exp(params[:, 5])
    alpha_b = params[:, 6]
    
    S_base  = np.exp(params[:, 7])
    gamma_s = params[:, 8]
    
    mu_0    = params[:, 9]
    mu_n    = params[:, 10]
    mu_b    = params[:, 11]
    mu_d    = params[:, 12]
    
    # Power Law Terms
    # Broadcasting: (T, 1) params vs (1, N) data
    term_n = A_n[:, None] * np.exp(-alpha_n[:, None] * log_model[None, :])
    term_d = A_d[:, None] * np.exp(-alpha_d[:, None] * log_data[None, :])
    
    # Batch term: B^alpha. If alpha &lt; 0, larger batch reduces loss (better gradients?).
    # If alpha &gt; 0, larger batch increases loss (inefficiency?).
    term_b = A_b[:, None] * np.exp(alpha_b[:, None] * log_bsz[None, :])
    
    # LR Dynamics
    # Optimal log_lr
    opt_log_lr = mu_0[:, None] + \
                 mu_n[:, None] * log_model[None, :] + \
                 mu_b[:, None] * log_bsz[None, :] + \
                 mu_d[:, None] * log_data[None, :]
    
    # Quadratic penalty with variable sensitivity
    # S(B) = S_base * B^gamma
    S_lr = S_base[:, None] * np.exp(gamma_s[:, None] * log_bsz[None, :])
    lr_penalty = S_lr * ((log_lr[None, :] - opt_log_lr) ** 2)
    
    # Total Prediction
    pred = E[:, None] + term_n + term_d + term_b + lr_penalty
    
    if return_1d:
        return pred[0]
    return pred.T

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points, dtype=np.float64)
    y = np.asarray(loss_values, dtype=np.float64)
    
    if y.ndim == 1:
        y2d = y[:, None]
    else:
        y2d = y
        
    T = y2d.shape[1]
    P = 13
    final_params = np.zeros((T, P))
    
    # Precompute logs
    NORM_LR    = 2.0e-3
    NORM_BSZ   = 128.0
    NORM_DATA  = 1.0e10
    NORM_MODEL = 2.0e8
    
    log_lr    = np.log(np.maximum(X[:, 0] / NORM_LR, 1e-10))
    log_bsz   = np.log(np.maximum(X[:, 1] / NORM_BSZ, 1e-10))
    log_data  = np.log(np.maximum(X[:, 2] / NORM_DATA, 1e-10))
    log_model = np.log(np.maximum(X[:, 3] / NORM_MODEL, 1e-10))
    
    # Helper to inverse softplus
    def inv_softplus(val):
        return np.log(np.exp(val) - 1.0 + 1e-6)

    for t in range(T):
        y_curr = y2d[:, t]
        y_min = np.min(y_curr)
        
        # Weighted Objective Function
        # Assigns higher weight to points near the optimal loss frontier.
        # Weight drops off as Lorentzian: 1 / (1 + (delta/scale)^2)
        # Using scale=0.05 for log-loss seems robust.
        scale = 0.05
        weights = 1.0 / (1.0 + ((y_curr - y_min) / scale)**2)
        weights = weights / np.mean(weights) # Normalize to mean 1
        
        def objective(p):
            # Param unpacking matching scaling_law_func
            E       = p[0]
            A_n     = np.exp(p[1])
            alpha_n = np.logaddexp(0, p[2]) + 1e-4
            A_d     = np.exp(p[3])
            alpha_d = np.logaddexp(0, p[4]) + 1e-4
            A_b     = np.exp(p[5])
            alpha_b = p[6]
            S_base  = np.exp(p[7])
            gamma_s = p[8]
            mu_0, mu_n, mu_b, mu_d = p[9], p[10], p[11], p[12]
            
            term_n = A_n * np.exp(-alpha_n * log_model)
            term_d = A_d * np.exp(-alpha_d * log_data)
            term_b = A_b * np.exp(alpha_b * log_bsz)
            
            opt_lr = mu_0 + mu_n * log_model + mu_b * log_bsz + mu_d * log_data
            S_lr = S_base * np.exp(gamma_s * log_bsz)
            penalty = S_lr * ((log_lr - opt_lr)**2)
            
            pred = E + term_n + term_d + term_b + penalty
            
            # Weighted MSE
            mse = np.mean(weights * (pred - y_curr)**2)
            
            # L2 Regularization (small)
            reg = 1e-7 * np.sum(p**2)
            return mse + reg

        # Parameter Bounds for L-BFGS-B
        bounds = [
            (0.0, y_min - 0.01),   # E (Bias)
            (-20, 5), (-5, 5),     # Model term (log_A, pre_alpha)
            (-20, 5), (-5, 5),     # Data term
            (-20, 5), (-3, 3),     # Batch term (alpha_b unconstrained)
            (-10, 10), (-2, 2),    # S_lr term (log_S, gamma)
            (-5, 5), (-2, 2), (-2, 2), (-2, 2) # mu terms
        ]
        
        guesses = []
        
        # Guess 1: Standard Scaling (B reduces loss, alpha_b &lt; 0)
        g1 = np.array([
            y_min - 0.2,
            np.log(0.5), inv_softplus(0.3),
            np.log(0.5), inv_softplus(0.3),
            np.log(0.1), -0.5,
            0.0, 0.0,
            0.0, -0.5, 0.5, 0.0
        ])
        guesses.append(g1)
        
        # Guess 2: Batch Penalty (B increases loss, alpha_b &gt; 0)
        g2 = g1.copy()
        g2[6] = 0.5
        guesses.append(g2)
        
        # Guess 3: Strong Scaling (High alphas)
        g3 = g1.copy()
        g3[2] = inv_softplus(0.6)
        g3[4] = inv_softplus(0.6)
        guesses.append(g3)
        
        # Guess 4: Flat Scaling (Low alphas)
        g4 = g1.copy()
        g4[2] = inv_softplus(0.1)
        g4[4] = inv_softplus(0.1)
        guesses.append(g4)

        # Guess 5: Variable S_lr (High sensitivity at large batch)
        g5 = g1.copy()
        g5[8] = 0.5
        guesses.append(g5)

        best_loss = np.inf
        best_p = guesses[0]
        
        for init_p in guesses:
            try:
                res = minimize(objective, init_p, method=&#x27;L-BFGS-B&#x27;, bounds=bounds, 
                             options={&#x27;gtol&#x27;: 1e-8, &#x27;maxiter&#x27;: 500})
                
                if res.fun &lt; best_loss:
                    best_loss = res.fun
                    best_p = res.x
            except:
                pass
                
        final_params[t] = best_p

    return final_params[0] if T == 1 else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.600677
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning.
Implements a Comprehensive Additive Power Law with Interaction Terms.
Key features:
1. Centered normalization using dataset geometric means for optimization stability.
2. Distinct power law terms for Model, Data, and Batch size.
3. Batch size term efficiency scales with Model and Data size (captures regime changes).
4. Learning Rate penalty with optimal LR surface depending on N, D, B.
5. Stiffness of LR penalty depends on N and B (stability boundaries).
6. Uses softplus parameterization for strictly positive exponents (physically motivated).
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 4) [lr, bsz, data_size, non_embedding_param_size]
    # params: (T, P) array or (P,) array
    
    X = np.asarray(data_points, dtype=np.float64)
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    
    # Normalization Constants (Geometric means of typical ranges)
    # LR: ~2e-3, Bsz: ~180, Data: ~1e10, Model: ~3e8
    # Centering inputs in log-space improves the conditioning of the optimization problem.
    norm_lr    = X[:, 0] / 2.0e-3
    norm_bsz   = X[:, 1] / 180.0
    norm_data  = X[:, 2] / 1.0e10
    norm_model = X[:, 3] / 3.0e8
    
    # Log-space inputs with safety floor
    eps = 1e-12
    log_lr    = np.log(np.maximum(norm_lr, eps))
    log_bsz   = np.log(np.maximum(norm_bsz, eps))
    log_data  = np.log(np.maximum(norm_data, eps))
    log_model = np.log(np.maximum(norm_model, eps))
    
    # Parameters Unpacking (16 parameters)
    # 0: E_param    -&gt; Irreducible loss = softplus(p)
    # 1: log_An     -&gt; Model term coefficient
    # 2: alpha_n_p  -&gt; Model term exponent (softplus)
    # 3: log_Ad     -&gt; Data term coefficient
    # 4: alpha_d_p  -&gt; Data term exponent (softplus)
    # 5: log_Ab     -&gt; Batch term coefficient
    # 6: alpha_b_p  -&gt; Batch term exponent (softplus)
    # 7: phi_b_n    -&gt; Batch coeff scaling with Model
    # 8: phi_b_d    -&gt; Batch coeff scaling with Data
    # 9: log_S_base -&gt; LR Penalty stiffness base
    # 10: sigma_n   -&gt; Stiffness scaling with Model
    # 11: sigma_b   -&gt; Stiffness scaling with Batch
    # 12: mu_0      -&gt; Opt LR base intercept
    # 13: mu_n      -&gt; Opt LR slope w.r.t Model
    # 14: mu_b      -&gt; Opt LR slope w.r.t Batch
    # 15: mu_d      -&gt; Opt LR slope w.r.t Data

    E       = np.logaddexp(0, params[:, 0]) # Ensure E &gt; 0
    
    A_n     = np.exp(params[:, 1])
    alpha_n = np.logaddexp(0, params[:, 2]) + 1e-4
    
    A_d     = np.exp(params[:, 3])
    alpha_d = np.logaddexp(0, params[:, 4]) + 1e-4
    
    A_b     = np.exp(params[:, 5])
    alpha_b = np.logaddexp(0, params[:, 6]) + 1e-4
    
    phi_b_n = params[:, 7]
    phi_b_d = params[:, 8]
    
    S_base  = np.exp(params[:, 9])
    sigma_n = params[:, 10]
    sigma_b = params[:, 11]
    
    mu_0    = params[:, 12]
    mu_n    = params[:, 13]
    mu_b    = params[:, 14]
    mu_d    = params[:, 15]
    
    # --- Term Calculation ---
    
    # 1. Model Power Law: A_n * N^-alpha_n
    term_n = A_n[:, None] * np.exp(-alpha_n[:, None] * log_model[None, :])
    
    # 2. Data Power Law: A_d * D^-alpha_d
    term_d = A_d[:, None] * np.exp(-alpha_d[:, None] * log_data[None, :])
    
    # 3. Batch Term: A_b * N^phi_n * D^phi_d * B^-alpha_b
    # Scaling the amplitude of the batch term allows the model to learn 
    # that larger models/datasets might be more/less sensitive to batch size.
    log_Ab_eff = (np.log(A_b[:, None]) + 
                  phi_b_n[:, None] * log_model[None, :] + 
                  phi_b_d[:, None] * log_data[None, :])
    term_b = np.exp(log_Ab_eff - alpha_b[:, None] * log_bsz[None, :])
    
    # 4. Learning Rate Penalty
    # Optimal LR Surface: Plane in log-log space
    log_opt_lr = (mu_0[:, None] + 
                  mu_n[:, None] * log_model[None, :] + 
                  mu_b[:, None] * log_bsz[None, :] + 
                  mu_d[:, None] * log_data[None, :])
    
    # Stiffness (Curvature) of the penalty
    # Can depend on Model size and Batch size (noise level)
    log_S_eff = (np.log(S_base[:, None]) + 
                 sigma_n[:, None] * log_model[None, :] + 
                 sigma_b[:, None] * log_bsz[None, :])
    S_eff = np.exp(log_S_eff)
    
    lr_penalty = S_eff * ((log_lr[None, :] - log_opt_lr) ** 2)
    
    # Total Prediction
    pred = E[:, None] + term_n + term_d + term_b + lr_penalty
    
    pred = pred.T
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points, dtype=np.float64)
    y = np.asarray(loss_values, dtype=np.float64)
    
    if y.ndim == 1:
        y2d = y[:, None]
    else:
        y2d = y
        
    T = y2d.shape[1]
    P = 16
    final_params = np.zeros((T, P))
    
    # Inverse softplus helper for initialization
    def inv_softplus(val):
        return np.log(np.exp(np.maximum(val, 1e-6)) - 1.0 + 1e-8)
    
    # Objective Function (MSE + Regularization)
    def objective(p, y_target):
        preds = scaling_law_func(X, p)
        mse = np.mean((preds - y_target) ** 2)
        # Regularization:
        # L2 on all parameters to prevent divergence
        # Slightly stronger on interaction terms to favor simpler power laws initially
        reg = 1e-6 * np.sum(p**2)
        return mse + reg

    for t in range(T):
        y_curr = y2d[:, t]
        y_min = np.min(y_curr)
        
        guesses = []
        
        # Base guess for E (Bias)
        init_E_param = inv_softplus(np.maximum(y_min - 0.1, 0.1))
        
        # Guess 1: Standard Scaling Law Defaults
        # alpha ~ 1/3, LR scales with sqrt(B)/sqrt(N)
        g1 = np.zeros(P)
        g1[0] = init_E_param
        g1[1], g1[2] = -1.0, inv_softplus(0.33)  # Model
        g1[3], g1[4] = -1.0, inv_softplus(0.33)  # Data
        g1[5], g1[6] = -2.0, inv_softplus(0.5)   # Batch
        # Interactions = 0
        g1[9] = 0.0                              # log_S
        g1[12], g1[13], g1[14] = 0.0, -0.3, 0.5  # LR Opt
        guesses.append(g1)
        
        # Guess 2: Data Limited (Data term dominates)
        g2 = g1.copy()
        g2[3] = 0.0              # Larger A_d
        g2[4] = inv_softplus(0.6)
        guesses.append(g2)
        
        # Guess 3: High Interaction (Batch &amp; LR sensitivity depend on Model)
        g3 = g1.copy()
        g3[7] = 0.2              # phi_b_n
        g3[10] = 0.2             # sigma_n
        guesses.append(g3)
        
        # Guess 4: Conservative / Flat
        g4 = np.zeros(P)
        g4[0] = inv_softplus(y_min)
        g4[1:] = -3.0
        guesses.append(g4)

        best_loss = np.inf
        best_p = guesses[0]
        
        for init_p in guesses:
            try:
                # Two-phase optimization for robustness
                # Phase 1: Coarse fit
                res1 = minimize(objective, init_p, args=(y_curr,),
                              method=&#x27;BFGS&#x27;,
                              options={&#x27;gtol&#x27;: 1e-4, &#x27;maxiter&#x27;: 200})
                
                # Phase 2: Fine tuning
                res2 = minimize(objective, res1.x, args=(y_curr,),
                              method=&#x27;BFGS&#x27;,
                              options={&#x27;gtol&#x27;: 1e-7, &#x27;maxiter&#x27;: 500})
                
                if res2.fun &lt; best_loss:
                    best_loss = res2.fun
                    best_p = res2.x
            except:
                continue
                
        final_params[t] = best_p

    return final_params[0] if T == 1 else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.597279
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning.
Implements an Additive Power Law with refined interaction terms for Batch Size, Data Size, and Learning Rate.
Features:
1. Model &amp; Data Power Laws: Standard A*N^-alpha + B*D^-beta structure.
2. Batch Size Penalty: A * B^-alpha * N^delta * D^lambda.
   - Models the efficiency loss from finite batch size.
   - Allows critical batch size to scale with Model Size (N) and Data Size (D).
   - Enforces physical constraints (alpha &gt; 0) so penalty vanishes as B -&gt; inf.
3. Learning Rate Penalty: Quadratic log-penalty.
   - Optimal LR depends on Model, Batch, and Data size.
   - Curvature (sensitivity) depends on Model and Batch size.
4. Optimization:
   - Uses L-BFGS-B to minimize MSE (optimal for NMSE metric).
   - Enforces strict physical bounds (positive scaling exponents, E &lt; min_loss).
   - Uses Log-space inputs/parameters for numerical stability.
   - Multiple heuristic restarts to avoid local minima.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    # data_points: (N, 4) array [lr, bsz, data_size, non_embedding_param_size]
    # params: (T, P) array or (P,) array
    # Returns: Predicted lm loss values (N,) or (N, T)
    
    X = np.asarray(data_points, dtype=np.float64)
    params = np.asarray(params, dtype=np.float64)
    
    return_1d = False
    if params.ndim == 1:
        params = params[None, :]
        return_1d = True
        
    # Input Normalization (Centering in log-space)
    # Constants approximate geometric means of the provided ranges
    LR_C    = 1.0e-3
    BSZ_C   = 256.0
    DATA_C  = 2.0e10
    MODEL_C = 2.0e8
    
    eps = 1e-12
    # Log-transform inputs
    ln_lr    = np.log(np.maximum(X[:, 0] / LR_C, eps))
    ln_bsz   = np.log(np.maximum(X[:, 1] / BSZ_C, eps))
    ln_data  = np.log(np.maximum(X[:, 2] / DATA_C, eps))
    ln_model = np.log(np.maximum(X[:, 3] / MODEL_C, eps))
    
    # Parameter Unpacking (16 Parameters)
    # 0: E (Bias)
    # 1: a_n, 2: alpha_n (Model Term)
    # 3: a_d, 4: alpha_d (Data Term)
    # 5: a_b, 6: alpha_b, 7: delta_b, 8: lambda_b (Batch Term)
    # 9: a_lr, 10: gamma_n, 11: gamma_b (LR Stiffness)
    # 12: mu_0, 13: mu_n, 14: mu_b, 15: mu_d (LR Optimum)
    
    # Reshape for broadcasting: (T, 1)
    E = params[:, 0][:, None]
    
    # 1. Model Power Law: exp(a_n) * N^-alpha_n
    a_n     = params[:, 1][:, None]
    alpha_n = params[:, 2][:, None]
    term_model = np.exp(a_n - alpha_n * ln_model[None, :])
    
    # 2. Data Power Law: exp(a_d) * D^-alpha_d
    a_d     = params[:, 3][:, None]
    alpha_d = params[:, 4][:, None]
    term_data = np.exp(a_d - alpha_d * ln_data[None, :])
    
    # 3. Batch Penalty: exp(a_b) * B^-alpha_b * N^delta_b * D^lambda_b
    # alpha_b &gt; 0 ensures penalty decreases with Batch Size
    a_b      = params[:, 5][:, None]
    alpha_b  = params[:, 6][:, None]
    delta_b  = params[:, 7][:, None]
    lambda_b = params[:, 8][:, None]
    term_batch = np.exp(a_b - alpha_b * ln_bsz[None, :] + 
                        delta_b * ln_model[None, :] + 
                        lambda_b * ln_data[None, :])
    
    # 4. Learning Rate Penalty
    # Stiffness: exp(a_lr) * N^gamma_n * B^gamma_b
    a_lr    = params[:, 9][:, None]
    gamma_n = params[:, 10][:, None]
    gamma_b = params[:, 11][:, None]
    
    # Optimal LR: exp(mu_0) * N^mu_n * B^mu_b * D^mu_d
    mu_0    = params[:, 12][:, None]
    mu_n    = params[:, 13][:, None]
    mu_b    = params[:, 14][:, None]
    mu_d    = params[:, 15][:, None]
    
    ln_opt   = mu_0 + mu_n * ln_model[None, :] + mu_b * ln_bsz[None, :] + mu_d * ln_data[None, :]
    stiffness = np.exp(a_lr + gamma_n * ln_model[None, :] + gamma_b * ln_bsz[None, :])
    
    term_lr  = stiffness * ((ln_lr[None, :] - ln_opt) ** 2)
    
    # Total Loss
    pred = E + term_model + term_data + term_batch + term_lr
    
    if return_1d:
        return pred[0]
    return pred.T

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points, dtype=np.float64)
    y = np.asarray(loss_values, dtype=np.float64)
    
    single_target = False
    if y.ndim == 1:
        y = y[:, None]
        single_target = True
        
    T_targets = y.shape[1]
    P = 16 # Number of parameters
    
    final_params = np.zeros((T_targets, P))
    
    def objective(p, y_true):
        preds = scaling_law_func(X, p)
        return np.mean((preds - y_true)**2)

    for t in range(T_targets):
        y_curr = y[:, t]
        min_y = np.min(y_curr)
        
        # Parameter Bounds for L-BFGS-B
        # 0: E [0, min_y - epsilon] (Must leave room for scaling terms)
        # 2, 4, 6: Decay exponents (Model, Data, Batch) must be positive
        
        bounds = [
            (0.0, max(1e-6, min_y - 0.05)), # E
            (None, None),                   # a_n
            (0.0, 5.0),                     # alpha_n
            (None, None),                   # a_d
            (0.0, 5.0),                     # alpha_d
            (None, None),                   # a_b
            (0.0, 5.0),                     # alpha_b (Batch penalty decay)
            (None, None),                   # delta_b
            (None, None),                   # lambda_b
            (None, None),                   # a_lr
            (None, None),                   # gamma_n
            (None, None),                   # gamma_b
            (None, None),                   # mu_0
            (None, None),                   # mu_n
            (None, None),                   # mu_b
            (None, None)                    # mu_d
        ]
        
        # Heuristic Initial Guesses to explore solution space
        guesses = []
        
        # Guess 1: Standard Scaling Theory
        # E ~ small
        # Model/Data Alpha ~ 0.3 - 0.5
        # Batch Penalty ~ 1/B (alpha_b = 1.0)
        # LR Opt ~ 1/sqrt(N) (mu_n = -0.5), sqrt(B) (mu_b = 0.5)
        g1 = np.zeros(P)
        g1[0] = max(0, min_y - 0.2)
        g1[1] = 0.0; g1[2] = 0.3 # Model
        g1[3] = 0.0; g1[4] = 0.3 # Data
        g1[5] = -2.0; g1[6] = 1.0; g1[7] = 0.2; g1[8] = 0.0 # Batch
        g1[9] = 0.0; g1[10] = 0.0; g1[11] = 0.0 # LR Stiff
        g1[12] = 0.0; g1[13] = -0.5; g1[14] = 0.5; g1[15] = 0.0 # LR Opt
        guesses.append(g1)
        
        # Guess 2: Steep Scaling (Chinchilla-like)
        g2 = g1.copy()
        g2[0] = max(0, min_y - 0.1)
        g2[2] = 0.5; g2[4] = 0.5
        g2[6] = 0.8
        guesses.append(g2)
        
        # Guess 3: High Batch Sensitivity
        g3 = g1.copy()
        g3[5] = -1.0; g3[6] = 1.2
        g3[7] = 0.5 # delta_b (Larger models need larger batches)
        guesses.append(g3)
        
        # Guess 4: Flat Scaling (High Bias dominant)
        g4 = g1.copy()
        g4[0] = max(0, min_y - 0.02)
        g4[2] = 0.1; g4[4] = 0.1
        guesses.append(g4)
        
        best_fun = np.inf
        best_p = guesses[0]
        
        for p0 in guesses:
            try:
                # L-BFGS-B handles bounds effectively and is efficient for smooth problems
                res = minimize(objective, p0, args=(y_curr,), method=&#x27;L-BFGS-B&#x27;,
                               bounds=bounds, options={&#x27;gtol&#x27;: 1e-8, &#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 2000})
                if res.fun &lt; best_fun:
                    best_fun = res.fun
                    best_p = res.x
            except Exception:
                continue
                
        final_params[t] = best_p
        
    return final_params[0] if single_target else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.404715
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Refines the additive power law with Levenberg-Marquardt optimization.
Features 14 parameters to capture complex interactions:
1.  **Model &amp; Data Terms**: Standard power laws.
2.  **Batch Size Interaction**: Penalty $B^{-\alpha}$ scales with Model Size $N^\delta$, modeling that larger models have larger critical batch sizes.
3.  **Advanced LR Penalty**:
    - Optimal Log-LR depends on Model and Batch Size ($N^{\mu_n}, B^{\mu_b}$).
    - **New**: The curvature (stiffness) of the LR penalty scales with both Model Size ($N^{\gamma_n}$) and Batch Size ($B^{\gamma_b}$), allowing the tolerance for LR deviations to vary across regimes.
4.  **Robust Optimization**: Uses `least_squares` (LM method) with geometric mean centering for numerical stability.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    # data_points: (N, 4) array [lr, bsz, data_size, non_embedding_param_size]
    # params: (T, P) array or (P,) array
    
    X = np.asarray(data_points, dtype=np.float64)
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    
    # 1. Normalization Constants (Geometric means of typical ranges)
    # Centers the log-features, improving condition number of the Hessian
    LR_C    = 1.0e-3
    BSZ_C   = 256.0
    DATA_C  = 2.0e10
    MODEL_C = 2.0e8
    
    # 2. Feature Extraction (Log Space)
    # Clipping ensures stability
    ln_lr    = np.log(np.maximum(X[:, 0] / LR_C, 1e-10))
    ln_bsz   = np.log(np.maximum(X[:, 1] / BSZ_C, 1e-10))
    ln_data  = np.log(np.maximum(X[:, 2] / DATA_C, 1e-10))
    ln_model = np.log(np.maximum(X[:, 3] / MODEL_C, 1e-10))
    
    # 3. Parameter Unpacking (14 Parameters)
    # E: Irreducible Loss
    E       = params[:, 0][:, None]
    
    # Model Term: A_n * N^-alpha_n
    A_n     = np.exp(params[:, 1][:, None])
    alpha_n = np.abs(params[:, 2][:, None])
    
    # Data Term: A_d * D^-beta_d
    A_d     = np.exp(params[:, 3][:, None])
    beta_d  = np.abs(params[:, 4][:, None])
    
    # Batch Term: A_b * N^delta_b * B^-alpha_b
    # Captures that critical batch size scales with model size (if delta_b &gt; 0)
    A_b     = np.exp(params[:, 5][:, None])
    delta_b = params[:, 6][:, None]
    alpha_b = np.abs(params[:, 7][:, None])
    
    # LR Term: A_lr * N^gamma_n * B^gamma_b * (ln_lr - ln_opt)^2
    # Allows the &quot;sharpness&quot; of the LR basin to depend on scale and batch size
    A_lr    = np.exp(params[:, 8][:, None])
    gamma_n = params[:, 9][:, None]
    gamma_b = params[:, 10][:, None]
    
    # Optimal LR dependencies
    mu_0    = params[:, 11][:, None]
    mu_n    = params[:, 12][:, None]
    mu_b    = params[:, 13][:, None]
    
    # 4. Computation (Broadcasting over T scenarios)
    
    # Power Laws
    term_model = A_n * np.exp(-alpha_n * ln_model[None, :])
    term_data  = A_d * np.exp(-beta_d * ln_data[None, :])
    
    # Batch Penalty
    # Form: A * (N^delta / B^alpha)
    term_bsz   = A_b * np.exp(delta_b * ln_model[None, :] - alpha_b * ln_bsz[None, :])
    
    # LR Penalty
    # Optimal Log LR
    ln_lr_opt  = mu_0 + mu_n * ln_model[None, :] + mu_b * ln_bsz[None, :]
    
    # Curvature Scaling
    # Larger models often have sharper minima (gamma_n &gt; 0) or different stability
    curvature  = A_lr * np.exp(gamma_n * ln_model[None, :] + gamma_b * ln_bsz[None, :])
    
    term_lr    = curvature * ((ln_lr[None, :] - ln_lr_opt) ** 2)
    
    pred = E + term_model + term_data + term_bsz + term_lr
    
    pred = pred.T
    return pred[:, 0] if pred.shape[1] == 1 else pred

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points, dtype=np.float64)
    y = np.asarray(loss_values, dtype=np.float64)
    
    single_target = False
    if y.ndim == 1:
        y = y[:, None]
        single_target = True
        
    T = y.shape[1]
    P = 14 # 14 parameters
    
    final_params = np.zeros((T, P))
    
    # Constants matching scaling_law_func
    LR_C, BSZ_C, DATA_C, MODEL_C = 1.0e-3, 256.0, 2.0e10, 2.0e8

    # Pre-calculate log features
    ln_lr    = np.log(np.maximum(X[:, 0] / LR_C, 1e-10))
    ln_bsz   = np.log(np.maximum(X[:, 1] / BSZ_C, 1e-10))
    ln_data  = np.log(np.maximum(X[:, 2] / DATA_C, 1e-10))
    ln_model = np.log(np.maximum(X[:, 3] / MODEL_C, 1e-10))
    
    def residual_func(p, y_target):
        # Unpack parameters
        E = p[0]
        
        # Model &amp; Data
        t_model = np.exp(p[1]) * np.exp(-np.abs(p[2]) * ln_model)
        t_data  = np.exp(p[3]) * np.exp(-np.abs(p[4]) * ln_data)
        
        # Batch
        t_bsz   = np.exp(p[5]) * np.exp(p[6] * ln_model - np.abs(p[7]) * ln_bsz)
        
        # LR
        curv    = np.exp(p[8]) * np.exp(p[9] * ln_model + p[10] * ln_bsz)
        ln_opt  = p[11] + p[12] * ln_model + p[13] * ln_bsz
        t_lr    = curv * (ln_lr - ln_opt)**2
        
        return (E + t_model + t_data + t_bsz + t_lr) - y_target

    for t in range(T):
        y_curr = y[:, t]
        min_y = np.min(y_curr)
        
        # Robust Initializations
        # E is lower bound
        init_E = max(0.0, min_y - 0.2)
        
        guesses = []
        
        # Guess 1: Canonical Scaling
        # Model/Data slope ~ 0.3-0.5
        # Batch slope ~ 1.0 (1/B scaling)
        # LR opt ~ sqrt(B)/sqrt(N)
        g1 = np.array([
            init_E,
            -1.0, 0.4,       # Model
            -1.0, 0.4,       # Data
            -2.0, 0.2, 1.0,  # Batch
            -1.0, 0.0, 0.0,  # LR Curvature
            0.0, -0.5, 0.5   # LR Opt
        ])
        guesses.append(g1)
        
        # Guess 2: Steeper scaling, significant curvature interactions
        g2 = np.array([
            init_E - 0.1,
            0.0, 0.6,
            0.0, 0.6,
            -2.0, 0.3, 1.0,
            -1.0, -0.2, 0.0,
            0.0, -0.5, 0.5
        ])
        guesses.append(g2)
        
        # Guess 3: Flatter, high bias
        g3 = np.array([
            min_y,
            -2.0, 0.2,
            -2.0, 0.2,
            -3.0, 0.1, 0.8,
            -1.0, 0.0, 0.0,
            0.0, -0.3, 0.3
        ])
        guesses.append(g3)
        
        best_cost = np.inf
        best_p = guesses[0]
        
        for p0 in guesses:
            try:
                # Levenberg-Marquardt is robust for sum-of-squares
                res = least_squares(residual_func, p0, args=(y_curr,), 
                                    method=&#x27;lm&#x27;, max_nfev=5000, 
                                    ftol=1e-8, xtol=1e-8, gtol=1e-8)
                
                if res.cost &lt; best_cost:
                    best_cost = res.cost
                    best_p = res.x
            except Exception:
                continue
                
        final_params[t] = best_p
        
    return final_params[0] if single_target else final_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>