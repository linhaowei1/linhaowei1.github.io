<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - SFT Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>SFT Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 2.5 Flash</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.982485
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.981118</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.980766</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.982485
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Models the loss based on data size using a four-parameter scaling law.
    The functional form is: L(D) = A * (D + D0)^B + C.
    Parameters A and D0 are internally treated as exp(param_val) for numerical stability during optimization.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes (D).
    - params (numpy.ndarray): 1D array of 4 parameters [log_A, log_D0, B, C].
        log_A: Logarithm of the coefficient A. A = exp(log_A).
        log_D0: Logarithm of the data offset D0. D0 = exp(log_D0).
        B: Exponent, typically negative for loss reduction.
        C: Irreducible loss, the asymptotic minimum loss as data size approaches infinity.

    Returns:
    - numpy.ndarray: Predicted loss values (N,).
    &quot;&quot;&quot;
    data_size = np.atleast_1d(data_points).flatten()

    # Interpret log-transformed parameters
    coeff_A = np.exp(params[0])
    data_offset_D0 = np.exp(params[1])
    exponent_B = params[2]
    irreducible_loss_C = params[3]

    effective_data_size = data_size + data_offset_D0
    # Ensure effective_data_size is strictly positive for exponentiation,
    # handling cases where D + D0 might be extremely small.
    effective_data_size = np.maximum(effective_data_size, 1e-12) 

    predicted_loss = coeff_A * (effective_data_size ** exponent_B) + irreducible_loss_C
    
    # Ensure predicted loss values are physically meaningful:
    # 1. Loss should not fall below the irreducible loss C.
    # 2. Loss must be positive.
    predicted_loss = np.maximum(predicted_loss, irreducible_loss_C)
    predicted_loss = np.maximum(predicted_loss, 1e-12) # Absolute floor for loss values

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the four-parameter scaling law (L(D) = A * (D + D0)^B + C) to given data.
    Uses log-transformation for A and D0 during optimization for improved numerical stability.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes.
    - loss_values (numpy.ndarray): (N,) or (N,1) array of corresponding loss values.

    Returns:
    - numpy.ndarray: Optimized parameters [log_A, log_D0, B, C] (4 parameters).
    &quot;&quot;&quot;
    X = np.atleast_1d(data_points).flatten()
    y = np.atleast_1d(loss_values).flatten()

    min_loss = np.min(y)
    max_loss = np.max(y)
    min_data = np.min(X)
    max_data = np.max(X)

    # --- Initial parameter guesses ---
    # C_init: Irreducible loss, typically slightly below the minimum observed loss.
    # A lower bound for C to ensure positivity.
    lower_bound_C_val = 1e-9
    C_init = max(lower_bound_C_val, min_loss * 0.8) 

    # B_init: Common exponent for power laws, typically negative. -0.7 is often seen.
    B_init = -0.7                   
    
    # D0_init: Data offset. Adaptive to the scale of input data, typically smaller than min_data.
    D0_init_linear = max(1.0, min_data / 2.0) 
    
    # A_init: Coefficient, calculated to roughly match max_loss at min_data.
    denom_base = np.maximum(min_data + D0_init_linear, 1e-12) 
    A_init_linear = (max_loss - C_init) / (denom_base ** B_init)
    A_init_linear = max(1e-6, A_init_linear) # Ensure A_init_linear is positive

    # Convert A_init and D0_init to log-space for optimization
    initial_params = np.array([
        np.log(A_init_linear), 
        np.log(D0_init_linear), 
        B_init, 
        C_init
    ])

    # --- Parameter bounds (log-transformed for A, D0) ---
    # C bounds:
    upper_bound_C_val = min_loss 
    # Ensure C&#x27;s upper bound is at least its minimum possible value.
    if upper_bound_C_val &lt; lower_bound_C_val:
        upper_bound_C_val = lower_bound_C_val
    
    bounds = [
        # log_A: Wide range from very small A to very large A relative to max_loss.
        (np.log(1e-9), np.log(max_loss * 1e4)), 
        # log_D0: From very small D0 to a reasonable fraction of max_data.
        # This prevents D0 from becoming excessively large and making the model degenerate.
        (np.log(1e-9), np.log(max_data * 0.5)),     
        # exponent_B: Must be negative. Allows for steeper decays than -3.0.
        (-5.0, -1e-9),                             
        # irreducible_loss_C: Must be positive and less than or equal to the minimum observed loss.
        (lower_bound_C_val, upper_bound_C_val)                   
    ]

    # Ensure initial C_init is strictly within its computed bounds before starting optimization.
    initial_params[3] = max(lower_bound_C_val, min(initial_params[3], upper_bound_C_val * 0.99))


    def objective(params):
        &quot;&quot;&quot;Objective function to minimize (Mean Squared Error).&quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)
        mse = np.mean((predicted_loss - y) ** 2)
        return mse

    # Use L-BFGS-B with refined initial guesses and bounds.
    # Increased maxiter and maxfun, and tightened tolerances for a more thorough search.
    result = minimize(objective, initial_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                      options={&#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 2000, &#x27;maxfun&#x27;: 5000})

    # Return optimized parameters if successful, otherwise the refined initial parameters as fallback
    optimized_params = result.x if result.success else initial_params
    return optimized_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.980792
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Models the loss based on data size using a four-parameter scaling law.
    The functional form is: L(D) = A * (D + D0)^B + C

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes (D).
    - params (numpy.ndarray): 1D array of 4 parameters [A, D0, B, C].
        A: Coefficient, typically positive.
        D0: Data offset, shifts the effective data size.
        B: Exponent, typically negative for loss reduction.
        C: Irreducible loss, the asymptotic minimum loss as data size approaches infinity.

    Returns:
    - numpy.ndarray: Predicted loss values (N,).
    &quot;&quot;&quot;
    # Ensure data_points is a 1D array of data sizes (N,)
    data_size = np.atleast_1d(data_points).flatten()

    coeff_A = params[0]
    data_offset_D0 = params[1]
    exponent_B = params[2]
    irreducible_loss_C = params[3]

    effective_data_size = data_size + data_offset_D0
    # Ensure effective_data_size is positive to avoid issues with exponentiation (e.g., log(0) or x**neg_frac for x&lt;0)
    effective_data_size = np.maximum(effective_data_size, 1e-9) 

    predicted_loss = coeff_A * (effective_data_size ** exponent_B) + irreducible_loss_C
    
    # The predicted loss should not fall below the irreducible_loss_C for empirical stability and physical meaning.
    predicted_loss = np.maximum(predicted_loss, irreducible_loss_C)

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the four-parameter scaling law (L(D) = A * (D + D0)^B + C) to given data.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes.
    - loss_values (numpy.ndarray): (N,) or (N,1) array of corresponding loss values.

    Returns:
    - numpy.ndarray: Optimized parameters [A, D0, B, C] (4 parameters).
    &quot;&quot;&quot;
    X = np.atleast_1d(data_points).flatten()
    y = np.atleast_1d(loss_values).flatten()

    min_loss = np.min(y)
    max_loss = np.max(y)
    min_data = np.min(X)
    max_data = np.max(X)

    # --- Parameter bounds ---
    # Bounds for [coeff_A, data_offset_D0, exponent_B, irreducible_loss_C]
    bounds = [
        (1e-9, None),                           # A: Must be positive.
        (0.0, max_data * 5),                    # D0: Non-negative, allowing more flexibility beyond max_data.
        (-5.0, -1e-9),                          # B: Must be negative (loss decreases with data), allowing steeper slopes.
        (1e-9, min_loss * 0.95)                 # C: Must be positive and strictly less than min observed loss.
    ]
    # Adjust upper bound for C if min_loss is very small, ensuring it doesn&#x27;t become negative or zero.
    if bounds[3][1] &lt; 1e-9: 
        bounds[3] = (1e-9, max(1e-9, min_loss * 0.5))
    
    # --- Robust Base Initial Parameter Guess ---
    # C_base_init: Estimate irreducible loss as a fraction of the minimum observed loss.
    C_base_init = min_loss * 0.95
    # For very flat loss curves, allow C to be potentially lower to capture the asymptote better.
    if np.std(y) &lt; 0.05 * np.mean(y) and len(y) &gt; 1: 
        C_base_init = min_loss * 0.8
    C_base_init = np.clip(C_base_init, bounds[3][0], bounds[3][1]) # Ensure C_base_init is within its bounds

    # D0_base_init: Estimate data offset, preventing it from being too small or too large.
    D0_base_init = max(1.0, min_data * 0.1) # Small positive offset or a fraction of min_data
    D0_base_init = np.clip(D0_base_init, bounds[1][0], bounds[1][1]) # Ensure D0_base_init is within its bounds

    # Use log-log linear regression for B_base_init and A_base_init after adjusting for C and D0.
    y_adjusted_base = y - C_base_init
    x_adjusted_base = X + D0_base_init
    
    valid_indices_base = (y_adjusted_base &gt; 1e-9) &amp; (x_adjusted_base &gt; 1e-9)

    B_base_init_candidate = -0.15 # Default B if polyfit fails
    A_base_init_from_polyfit = None 

    if np.sum(valid_indices_base) &gt;= 2: 
        try:
            log_x = np.log(x_adjusted_base[valid_indices_base])
            log_y = np.log(y_adjusted_base[valid_indices_base])
            slope, intercept = np.polyfit(log_x, log_y, 1)
            B_base_init_candidate = slope
            A_base_init_from_polyfit = np.exp(intercept)
        except np.linalg.LinAlgError:
            pass # If polyfit fails (e.g., collinear points), use default B_base_init_candidate
    
    B_base_init = np.clip(B_base_init_candidate, bounds[2][0], bounds[2][1])
    
    if A_base_init_from_polyfit is not None and np.isfinite(A_base_init_from_polyfit):
        A_base_init = A_base_init_from_polyfit
    else:
        # Heuristic A calculation using the first data point if polyfit is not available or fails
        numerator = y[0] - C_base_init
        numerator = max(1e-9, numerator) # Ensure numerator is positive
        denom_base_A = X[0] + D0_base_init
        denom_base_A = np.maximum(denom_base_A, 1e-9) # Ensure denominator is positive
        A_base_init = numerator / (denom_base_A ** B_base_init)
    A_base_init = np.clip(A_base_init, bounds[0][0], None) # Ensure A respects its lower bound

    base_initial_params = np.array([A_base_init, D0_base_init, B_base_init, C_base_init])

    def objective(params):
        &quot;&quot;&quot;Objective function to minimize (Mean Squared Error).&quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)
        mse = np.mean((predicted_loss - y) ** 2)
        return mse

    best_mse = np.inf
    best_params = base_initial_params.copy() # Initialize with the robust base guess as fallback

    # --- Multiple Initializations Strategy with perturbations for B, C, and D0 ---
    # This helps explore the parameter space more thoroughly and avoid local minima.
    num_perturb_b = 4 # Increased from 3 to 4 for better exploration
    num_perturb_c = 4 # Increased from 3 to 4 for better exploration
    num_perturb_d0 = 4 # Increased from 3 to 4 for better exploration

    b_perturb_factors = np.linspace(0.7, 1.3, num_perturb_b) 
    c_perturb_factors = np.linspace(0.7, 1.0, num_perturb_c) # Adjusted lower bound from 0.8 to 0.7 for more flexibility
    d0_perturb_factors = np.linspace(0.5, 1.5, num_perturb_d0)

    initial_guess_sets_to_test = []
    initial_guess_sets_to_test.append(base_initial_params) # Always include the original base guess

    for b_factor in b_perturb_factors:
        for c_factor in c_perturb_factors:
            for d0_factor in d0_perturb_factors: 
                current_params = base_initial_params.copy()
                
                # Apply perturbations to B, C, and D0, clipping to respect their bounds
                current_params[1] = np.clip(base_initial_params[1] * d0_factor, bounds[1][0], bounds[1][1]) # Perturb D0
                current_params[2] = np.clip(base_initial_params[2] * b_factor, bounds[2][0], bounds[2][1])
                current_params[3] = np.clip(base_initial_params[3] * c_factor, bounds[3][0], bounds[3][1])

                # Recalculate A based on the perturbed B, C, and D0 for consistency
                numerator_recalc_A = y[0] - current_params[3]
                numerator_recalc_A = max(1e-9, numerator_recalc_A)
                denom_for_A = X[0] + current_params[1] 
                denom_for_A = np.maximum(denom_for_A, 1e-9)
                A_recalculated = numerator_recalc_A / (denom_for_A ** current_params[2])
                current_params[0] = np.clip(A_recalculated, bounds[0][0], None) # Ensure A respects its lower bound

                initial_guess_sets_to_test.append(current_params)

    # Use a set to store unique parameter tuples to avoid redundant optimization runs
    unique_initial_guess_sets = []
    seen_params = set()
    for params_arr in initial_guess_sets_to_test:
        # Convert array to a tuple for hashability, rounding to manage floating-point precision
        param_tuple = tuple(np.round(params_arr, decimals=6))
        if param_tuple not in seen_params:
            seen_params.add(param_tuple)
            unique_initial_guess_sets.append(params_arr)

    # Run optimization for each unique set of initial parameters
    for current_init_params in unique_initial_guess_sets:
        try:
            result = minimize(objective, current_init_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                              options={&#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 2000})
            
            # Only consider successful optimizations for selecting the best parameters
            if result.success:
                current_mse = objective(result.x)
                if current_mse &lt; best_mse:
                    best_mse = current_mse
                    best_params = result.x
        except ValueError:
            # Catch potential optimization errors (e.g., initial point invalid for L-BFGS-B bounds).
            # This allows the fitting process to continue with other initializations.
            pass 
        except Exception:
            # Catch other unexpected exceptions during optimization, just in case.
            pass


    # Final clamp to bounds for safety, in case of slight floating point deviations from the optimizer
    for i in range(len(best_params)):
        lower_bound = bounds[i][0]
        upper_bound = bounds[i][1]
        if lower_bound is not None:
            best_params[i] = np.maximum(best_params[i], lower_bound)
        if upper_bound is not None:
            best_params[i] = np.minimum(best_params[i], upper_bound)

    return best_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.980774
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import curve_fit

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Models loss L(D) = A * (D + D0)^B + C based on data size D.
    The functional form is: L(D) = A * (D + D0)^B + C

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes (D).
    - params (numpy.ndarray): 1D array of 4 parameters [A, D0, B, C].
        A: Coefficient, typically positive.
        D0: Data offset, shifts the effective data size.
        B: Exponent, typically negative for loss reduction.
        C: Irreducible loss, the asymptotic minimum loss as data size approaches infinity.

    Returns:
    - numpy.ndarray: Predicted loss values (N,).
    &quot;&quot;&quot;
    # Ensure data_points is a 1D array of data sizes (N,)
    data_size = np.atleast_1d(data_points).flatten()

    coeff_A, data_offset_D0, exponent_B, irreducible_loss_C = params

    # Calculate effective data size, ensuring it&#x27;s positive for exponentiation.
    # Clipping to a small positive value prevents issues with 0^B or negative bases.
    effective_data_size = data_size + data_offset_D0
    effective_data_size = np.maximum(effective_data_size, 1e-9) 

    predicted_loss = coeff_A * (effective_data_size ** exponent_B) + irreducible_loss_C
    
    # Ensure predicted loss is never below the irreducible loss.
    # Since irreducible_loss_C is bounded to be &gt;= 1e-9 (from fit_scaling_law),
    # this also implicitly ensures predicted_loss is positive and at least 1e-9.
    predicted_loss = np.maximum(predicted_loss, irreducible_loss_C)

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the four-parameter scaling law (L(D) = A * (D + D0)^B + C) to data
    using scipy.optimize.curve_fit for robust non-linear least squares.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes.
    - loss_values (numpy.ndarray): (N,) or (N,1) array of corresponding loss values.

    Returns:
    - numpy.ndarray: Optimized parameters [A, D0, B, C] (4 parameters).
    &quot;&quot;&quot;
    # Ensure data_points and loss_values are 1D arrays for consistent processing
    X = np.atleast_1d(data_points).flatten()
    y = np.atleast_1d(loss_values).flatten()

    # Calculate basic statistics from the data to inform initial guesses and bounds
    min_loss, max_loss = np.min(y), np.max(y)
    min_data, max_data = np.min(X), np.max(X)

    # --- Refined Initial parameter guesses ---
    
    # C_init: Irreducible loss, estimated slightly below the minimum observed loss.
    # Using 0.9 factor (as seen in top-performing attempts) for robustness in log-linear regression.
    C_init = np.maximum(1e-6, min_loss * 0.9) 
    
    # D0_init: Data offset. Using a fixed small positive constant (1.0) as in top-performing attempts.
    # This ensures a positive base for log-linear regression and a stable starting point.
    D0_init = 1.0 
    
    # Default exponent B_init and coefficient A_init as fallbacks
    B_init = -0.1 
    
    # Calculate A_init based on max_loss, C_init, min_data, D0_init, and the fallback B_init.
    # Ensure numerator and effective data size are positive for numerical stability.
    numerator_A_init = np.maximum(1e-9, max_loss - C_init)
    effective_min_data_plus_D0 = np.maximum(min_data + D0_init, 1e-9)
    A_init = np.maximum(1e-6, numerator_A_init / (effective_min_data_plus_D0 ** B_init))

    # --- Log-linear regression for improved B_init and A_init ---
    # This approach gives a more robust initial guess for power-law parameters.
    # Transform data for linear regression: log(L - C_init) = log(A) + B * log(D + D0_init)
    y_adjusted = y - C_init
    X_adjusted = X + D0_init

    # Filter out points where y_adjusted is non-positive, as log is undefined
    valid_indices = y_adjusted &gt; 1e-9 
    
    # Perform regression if enough valid points exist and X values have variance
    if np.sum(valid_indices) &gt;= 2: # Need at least 2 points for np.polyfit
        log_X_adj_valid = np.log(X_adjusted[valid_indices])
        log_y_adj_valid = np.log(y_adjusted[valid_indices])

        # Ensure there&#x27;s sufficient variance in log(X) to perform polyfit
        if np.std(log_X_adj_valid) &gt; 1e-9: 
            try:
                # Perform linear regression to get initial B and log(A)
                coeffs = np.polyfit(log_X_adj_valid, log_y_adj_valid, 1)
                B_init_regress = coeffs[0]
                log_A_init_regress = coeffs[1]
                
                # Apply clipping to regression results to stay within reasonable bounds
                A_init = np.maximum(1e-6, np.exp(log_A_init_regress))
                B_init = np.clip(B_init_regress, -2.0, -1e-9) # Ensure negative exponent, not excessively steep
                
            except np.linalg.LinAlgError:
                # If polyfit fails (e.g., singular matrix), fall back to default A_init, B_init.
                pass # Initial A_init and B_init values (from fallback or prior calculation) are retained.
    
    initial_params = np.array([A_init, D0_init, B_init, C_init])

    # --- Parameter bounds for robust optimization with curve_fit ---
    # Bounds for [coeff_A, data_offset_D0, exponent_B, irreducible_loss_C]
    bounds_lower = [1e-9, 0.0, -2.0, 1e-9]        # Lower bounds
    bounds_upper = [np.inf, max_data * 2, -1e-9, min_loss - 1e-6] # Upper bounds

    # Adjust the upper bound for C if min_loss is very small or results in an invalid range.
    # C must be positive and strictly less than the minimum observed loss.
    if bounds_upper[3] &lt;= bounds_lower[3]: 
        bounds_upper[3] = np.maximum(bounds_lower[3], min_loss * 0.9) # Fallback to 90% of min_loss, ensuring positivity.

    # --- Wrapper function for curve_fit ---
    # curve_fit expects the function signature func(x, *params).
    # This wrapper unpacks the parameters from curve_fit&#x27;s arguments into a numpy array
    # before passing them to our scaling_law_func.
    def func_for_curve_fit(D, A, D0, B, C):
        return scaling_law_func(D, np.array([A, D0, B, C]))

    try:
        # Use scipy.optimize.curve_fit for non-linear least squares regression.
        # It is well-suited for fitting functions to data and often more robust than general minimizers.
        popt, pcov = curve_fit(
            func_for_curve_fit,
            X,
            y,
            p0=initial_params, # Initial guess for parameters
            bounds=(bounds_lower, bounds_upper), # Parameter bounds
            maxfev=5000, # Increased max function evaluations for thorough search
            ftol=1e-10,  # Relative error in the fitting parameters
            xtol=1e-10,  # Relative error in the solution
            gtol=1e-10   # Orthogonality of the residual vector to the columns of the Jacobian
        )
        optimized_params = popt
    except (RuntimeError, ValueError):
        # If curve_fit fails to converge (RuntimeError) or encounters an invalid parameter range (ValueError),
        # return the robustly calculated initial parameters as a fallback to ensure stability.
        optimized_params = initial_params
        
    return optimized_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.980774
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import least_squares # Changed from minimize to least_squares

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Models the loss based on data size using a four-parameter scaling law.
    The functional form is: L(D) = A * (D + D0)^B + C

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes (D).
    - params (numpy.ndarray): 1D array of 4 parameters [A, D0, B, C].
        A: Coefficient, typically positive.
        D0: Data offset, shifts the effective data size.
        B: Exponent, typically negative for loss reduction.
        C: Irreducible loss, the asymptotic minimum loss as data size approaches infinity.

    Returns:
    - numpy.ndarray: Predicted loss values (N,).
    &quot;&quot;&quot;
    # Ensure data_points is a 1D array of data sizes (N,)
    data_size = np.atleast_1d(data_points).flatten()

    coeff_A = params[0]
    data_offset_D0 = params[1]
    exponent_B = params[2]
    irreducible_loss_C = params[3]

    # Calculate effective data size.
    effective_data_size = data_size + data_offset_D0
    # Ensure effective_data_size is strictly positive to prevent issues with exponentiation
    # (e.g., 0^B or negative base raised to a non-integer power).
    effective_data_size = np.maximum(effective_data_size, 1e-9)

    predicted_loss = coeff_A * (effective_data_size ** exponent_B) + irreducible_loss_C
    
    # Ensure predicted loss values are physically meaningful:
    # 1. They should not fall below the irreducible_loss_C.
    # 2. Losses are generally positive.
    predicted_loss = np.maximum(predicted_loss, irreducible_loss_C)
    predicted_loss = np.maximum(predicted_loss, 1e-9) # Absolute floor for loss values, ensuring positivity

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the four-parameter scaling law (L(D) = A * (D + D0)^B + C) to given data
    using scipy.optimize.least_squares for robust non-linear regression.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes.
    - loss_values (numpy.ndarray): (N,) or (N,1) array of corresponding loss values.

    Returns:
    - numpy.ndarray: Optimized parameters [A, D0, B, C] (4 parameters).
    &quot;&quot;&quot;
    # Ensure data_points and loss_values are 1D arrays for consistent processing
    X = np.atleast_1d(data_points).flatten()
    y = np.atleast_1d(loss_values).flatten()

    # Calculate basic statistics from the data to inform initial guesses and bounds
    min_loss = np.min(y)
    max_loss = np.max(y)
    min_data = np.min(X)
    max_data = np.max(X)

    # --- Parameter Bounds ---
    # `least_squares` expects bounds as a tuple: (lower_bounds_array, upper_bounds_array)
    # Bounds for [coeff_A, data_offset_D0, exponent_B, irreducible_loss_C]
    lower_bounds = np.array([1e-9, 0.0, -2.0, 1e-9])
    # Upper bound for C must be strictly less than min_loss observed
    upper_bounds = np.array([np.inf, max_data * 2, -1e-9, min_loss * 0.999])

    # Adjust upper bound for C if min_loss is extremely small or negative
    if upper_bounds[3] &lt; 1e-9: 
        upper_bounds[3] = max(1e-9, min_loss * 0.5) # Fallback to a safer positive value

    # --- Initial Parameter Guesses ---
    # C_init: Irreducible loss is typically slightly below the minimum observed loss.
    C_init = max(1e-9, min_loss * 0.9) # Slightly more relaxed initial guess for C

    # D0_init: Data offset. Helps avoid log(0) issues and shifts the curve.
    D0_init = max(1.0, min_data * 0.01)

    # Fallback values for A_init and B_init if log-linear regression fails
    B_init_fallback = -0.1
    # Ensure denominator for A_init calculation is positive and not extremely small
    A_init_fallback_denom_base = np.maximum(min_data + D0_init, 1e-9)
    # Ensure numerator for A_init calculation is positive
    A_init_fallback_numerator = np.maximum(max_loss - C_init, 1e-9) 
    A_init_fallback = A_init_fallback_numerator / (A_init_fallback_denom_base ** B_init_fallback)
    A_init_fallback = max(1e-6, A_init_fallback) # Ensure A_init is positive

    B_init = B_init_fallback
    A_init = A_init_fallback

    # Attempt log-linear regression for better initial A and B
    effective_y = y - C_init
    # Filter for values strictly greater than a small epsilon to avoid log(0) or log(negative)
    valid_indices = effective_y &gt; 1e-9 
    
    if np.sum(valid_indices) &gt;= 2: # Need at least two points for linear regression
        log_X_eff = np.log(X[valid_indices] + D0_init)
        log_y_eff = np.log(effective_y[valid_indices])

        # Check for non-finite values (NaNs, Infs) before polyfit to prevent potential errors
        if np.all(np.isfinite(log_X_eff)) and np.all(np.isfinite(log_y_eff)):
            try:
                # Perform linear regression to estimate B (slope) and log(A) (intercept)
                slope_B, intercept_log_A = np.polyfit(log_X_eff, log_y_eff, 1)
                A_init_reg = np.exp(intercept_log_A)
                B_init_reg = slope_B
                
                # Only use regression results if they are physically plausible and within initial bounds
                if A_init_reg &gt; lower_bounds[0] and B_init_reg &lt; upper_bounds[2]: 
                    A_init = A_init_reg
                    B_init = B_init_reg
            except (np.linalg.LinAlgError, ValueError):
                # Regression failed (e.g., singular matrix, constant input), use fallback values
                pass
    
    initial_params = np.array([A_init, D0_init, B_init, C_init])
    
    # Clip initial parameters to ensure they are strictly within the specified bounds.
    # This is crucial for `least_squares` to start from a valid region.
    initial_params = np.clip(initial_params, lower_bounds, upper_bounds)
    # Ensure B_init is strictly within its bounds, giving a small margin for numerical stability
    initial_params[2] = np.clip(initial_params[2], lower_bounds[2] + 1e-9, upper_bounds[2] - 1e-9)

    def residuals(params, x_data, y_true):
        &quot;&quot;&quot;Residuals function for least_squares optimization.&quot;&quot;&quot;
        return scaling_law_func(x_data, params) - y_true

    # Use least_squares. The &#x27;trf&#x27; (Trust Region Reflective) algorithm is generally
    # recommended for bounded problems.
    result = least_squares(residuals, initial_params, bounds=(lower_bounds, upper_bounds),
                           args=(X, y), verbose=0, max_nfev=3000, ftol=1e-9, xtol=1e-9)

    optimized_params = result.x
    
    # Final clipping to ensure optimized parameters strictly respect bounds,
    # addressing any floating point inaccuracies and maintaining physical constraints.
    optimized_params = np.clip(optimized_params, lower_bounds, upper_bounds)
    # Ensure B remains strictly negative after optimization
    optimized_params[2] = np.clip(optimized_params[2], lower_bounds[2] + 1e-9, upper_bounds[2] - 1e-9) 

    return optimized_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.980766
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Models the loss based on data size using a four-parameter scaling law.
    The functional form is: L(D) = A * (D + D0)^B + C

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes (D).
    - params (numpy.ndarray): 1D array of 4 parameters [A, D0, B, C].
        A: Coefficient, typically positive.
        D0: Data offset, shifts the effective data size.
        B: Exponent, typically negative for loss reduction.
        C: Irreducible loss, the asymptotic minimum loss as data size approaches infinity.

    Returns:
    - numpy.ndarray: Predicted loss values (N,).
    &quot;&quot;&quot;
    # Ensure data_points is a 1D array of data sizes (N,)
    data_size = np.atleast_1d(data_points).flatten()

    coeff_A = params[0]
    data_offset_D0 = params[1]
    exponent_B = params[2]
    irreducible_loss_C = params[3]

    # Calculate effective data size.
    # Ensure it&#x27;s non-negative before exponentiation. D0 can be 0.
    effective_data_size = data_size + data_offset_D0
    # Clip to a small positive value to prevent issues with exponentiation
    # (e.g., negative base raised to a non-integer power, or 0^B leading to NaN/inf).
    effective_data_size = np.maximum(effective_data_size, 1e-9) 

    power_term = coeff_A * (effective_data_size ** exponent_B)
    
    predicted_loss = power_term + irreducible_loss_C
    
    # Ensure predicted loss values are physically meaningful:
    # They should not fall below the irreducible_loss_C.
    # Since irreducible_loss_C is bounded to be &gt;= 1e-9, this also ensures loss is positive.
    predicted_loss = np.maximum(predicted_loss, irreducible_loss_C)
    predicted_loss = np.maximum(predicted_loss, 1e-9) # Absolute floor for loss values to ensure positive

    return predicted_loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the four-parameter scaling law (L(D) = A * (D + D0)^B + C) to given data.

    Parameters:
    - data_points (numpy.ndarray): (N,) or (N,1) array of data sizes.
    - loss_values (numpy.ndarray): (N,) or (N,1) array of corresponding loss values.

    Returns:
    - numpy.ndarray: Optimized parameters [A, D0, B, C] (4 parameters).
    &quot;&quot;&quot;
    X = np.atleast_1d(data_points).flatten()
    y = np.atleast_1d(loss_values).flatten()

    min_loss = np.min(y)
    max_loss = np.max(y)
    min_data = np.min(X)
    max_data = np.max(X)
    mean_data = np.mean(X) # Added for more diverse D0 guesses

    # --- Initial parameter guesses (base calculation) ---
    
    # C_base_init: Estimate irreducible loss from the tail of the data (largest data points).
    # This is where the curve should flatten towards C.
    num_tail_points = max(2, int(len(y) * 0.2)) # Use at least last 2 points, or 20% of data.
    C_tail_estimate = np.mean(y[-num_tail_points:]) if len(y) &gt;= num_tail_points else y[-1]
    
    # Initialize C_base_init: positive, and strictly less than min_loss.
    # Start with a value based on the tail estimate, or a safe fraction of min_loss.
    C_base_init = np.clip(C_tail_estimate * 0.9, 1e-9, min_loss * 0.99)
    
    # Ensure there&#x27;s sufficient &quot;room&quot; for the power-law component (max_loss - C_base_init &gt; small_value).
    loss_range = max_loss - min_loss
    min_power_law_contribution = max(1e-6, loss_range * 0.05) # At least 5% of range or a small const
    if (max_loss - C_base_init) &lt; min_power_law_contribution:
        C_base_init = np.clip(max_loss - min_power_law_contribution, 1e-9, min_loss * 0.99)
        # Re-clip against the upper bound of min_loss * 0.99 for safety
        C_base_init = min(C_base_init, min_loss * 0.99) 


    # D0_base_init: Data offset. Small positive value, often a fraction of min_data.
    D0_base_init = max(1.0, min_data * 0.05) 

    # B_base_init: The exponent. Dynamically estimate via log-linear regression.
    effective_X_for_B_est = X + D0_base_init
    effective_X_for_B_est = np.maximum(effective_X_for_B_est, 1e-9) # Safeguard against non-positive base

    y_minus_C = y - C_base_init
    # Filter for points where y - C is sufficiently positive for log transform.
    positive_y_minus_C_indices = y_minus_C &gt; 1e-9 
    
    B_base_init = -0.15 # Default if regression fails or is not applicable
    if np.sum(positive_y_minus_C_indices) &gt;= 2:
        log_y_minus_C = np.log(y_minus_C[positive_y_minus_C_indices])
        log_effective_X = np.log(effective_X_for_B_est[positive_y_minus_C_indices])
        
        # Check for sufficient variance in log_effective_X to avoid division by zero in polyfit.
        if np.std(log_effective_X) &gt; 1e-9:
            B_base_init_est, _ = np.polyfit(log_effective_X, log_y_minus_C, 1)
            # Clip B to a typical scaling law range; bounds will enforce final limits.
            B_base_init = np.clip(B_base_init_est, -1.0, -0.05)
    
    # A_base_init: Coefficient, calculated to match max_loss at min_data given other parameters.
    denom_base = min_data + D0_base_init
    denom_base = np.maximum(denom_base, 1e-9)

    loss_diff_for_A = max_loss - C_base_init
    loss_diff_for_A = np.maximum(loss_diff_for_A, min_power_law_contribution) # Ensure positive for A calculation

    A_base_init = loss_diff_for_A / (denom_base ** B_base_init)
    A_base_init = max(1e-6, A_base_init) # Ensure A_base_init is positive and not extremely small

    base_initial_params = np.array([A_base_init, D0_base_init, B_base_init, C_base_init])

    # --- Parameter bounds for robust optimization ---
    bounds = [
        (1e-9, None),                           # coeff_A: Must be positive. Can be very large.
        (0.0, max_data * 2.0),                  # data_offset_D0: Non-negative, allowing flexibility up to 2x max observed data.
        (-3.0, -1e-9),                          # exponent_B: Must be negative. Wider range than previous [-2.0, -1e-9].
        (1e-9, min_loss * 0.999)                # irreducible_loss_C: Must be positive and strictly less than min observed loss.
    ]
    # Re-check and adjust upper bound for C if min_loss is extremely small.
    if bounds[3][1] &lt; 1e-9: 
        bounds[3] = (1e-9, max(1e-9, min_loss * 0.5)) 

    def objective(params):
        &quot;&quot;&quot;Objective function to minimize (Mean Squared Error).&quot;&quot;&quot;
        predicted_loss = scaling_law_func(X, params)
        mse = np.mean((predicted_loss - y) ** 2)
        return mse

    # --- Multiple Initializations Strategy for enhanced robustness ---
    best_mse = np.inf
    best_params = base_initial_params.copy()

    # Perturbation factors for B (exponent_B) - vary magnitude
    b_factors = np.linspace(0.2, 2.0, 7) # More steps, wider range around 1.0 (base init)

    # Perturbation factors for C (irreducible_loss_C) - vary closeness to min_loss/tail
    c_factors = np.linspace(0.5, 1.1, 7) # Explore values slightly above initial estimate (but still below min_loss)

    # Perturbation values for D0 (data_offset_D0) - cover small to large offsets
    d0_perturb_values = np.array([
        0.0,                                    # D0=0
        1.0,                                    # Smallest non-zero
        min_data * 0.005,                       # 0.5% of min data
        min_data * 0.05,                        # 5% of min data
        min_data * 0.5,                         # 50% of min data
        mean_data * 0.01,                       # 1% of mean data
        mean_data * 0.1,                        # 10% of mean data
        mean_data * 0.5,                        # 50% of mean data
        max_data * 0.005,                       # 0.5% of max data
        max_data * 0.05,                        # 5% of max data
        max_data * 0.25,                        # 25% of max data
        base_initial_params[1]                  # The original calculated base D0
    ])
    d0_perturb_values = np.unique(d0_perturb_values).round(decimals=6)
    d0_perturb_values = np.clip(d0_perturb_values, bounds[1][0], bounds[1][1])

    initial_guess_sets = []
    initial_guess_sets.append(base_initial_params)

    for b_factor in b_factors:
        for c_factor in c_factors:
            for d0_val in d0_perturb_values:
                current_params = base_initial_params.copy()
                
                # Apply perturbations, clipping to respect their bounds
                current_params[2] = np.clip(base_initial_params[2] * b_factor, bounds[2][0], bounds[2][1])
                current_params[3] = np.clip(C_base_init * c_factor, bounds[3][0], bounds[3][1])
                current_params[1] = d0_val # D0 is directly from perturb_values and already clipped.

                # Recalculate A based on the perturbed B, C, and D0
                denom_for_A = min_data + current_params[1] 
                denom_for_A = np.maximum(denom_for_A, 1e-9)
                
                loss_diff = max_loss - current_params[3]
                loss_diff = np.maximum(loss_diff, 1e-9) # Ensure positive for A calculation

                A_recalculated = loss_diff / (denom_for_A ** current_params[2])
                current_params[0] = np.clip(A_recalculated, bounds[0][0], bounds[0][1])

                initial_guess_sets.append(current_params)

    unique_initial_guess_sets = []
    seen_params = set()
    for params_arr in initial_guess_sets:
        param_tuple = tuple(np.round(params_arr, decimals=8)) 
        if param_tuple not in seen_params:
            seen_params.add(param_tuple)
            unique_initial_guess_sets.append(params_arr)

    # Run optimization for each unique set of initial parameters
    for current_init_params in unique_initial_guess_sets:
        try: # Add try-except for robustness against initial params leading to NaNs/infs early in optimization
            result = minimize(objective, current_init_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                              options={&#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 3000}) # Slightly tighter tolerance, more iterations
            
            if result.success:
                current_mse = objective(result.x)
                if current_mse &lt; best_mse:
                    best_mse = current_mse
                    best_params = result.x
        except (ValueError, RuntimeError): # Catch cases where initial params might lead to numerical issues
            continue
        except Exception: # Catch any other unexpected errors
            continue
            
    if best_mse == np.inf: # If no optimization was successful, return the initial base guess
        # Fallback: if all optimizations failed, return the base initial params, clipped to bounds
        return np.clip(base_initial_params, [b[0] for b in bounds], [b[1] if b[1] is not None else np.inf for b in bounds])
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>