<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Domain Mixture Scaling Law - SLDAgent + Claude Haiku 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Domain Mixture Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Haiku 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.986123 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.959494</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.930786</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.986123 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for multi-domain LLM finetuning
Uses shared exponent with per-domain adjustments, additive entropy regularization,
and learned cross-domain interactions. 32 parameters with improved per-domain control.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with shared exponent, per-domain refinements, and entropy regularization.
    
    Parameters (32 total):
    - [0:5]: base coefficients (per-domain)
    - [5:10]: exponent offsets (per-domain adjustment to shared base)
    - [10:15]: interaction strengths (cross-domain coupling)
    - [15:20]: domain bias terms
    - [20:25]: per-domain quadratic regularization (smoothness)
    - [25]: shared exponent base
    - [26]: global scale factor
    - [27]: entropy weight (additive contribution)
    - [28]: entropy strength coefficient
    - [29]: interaction regularization strength
    - [30]: entropy modulation type (blend between additive and multiplicative)
    - [31]: per-domain bias scaling
    
    Total: 32 parameters (well under 35 limit)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)
    N, F = X.shape
    params = np.asarray(params, dtype=np.float64).ravel()
    
    eps = 1e-10
    
    # Parse parameters
    base_coeffs = params[0:5]        # (5,) - per-domain base coefficients
    exp_offsets = params[5:10]       # (5,) - per-domain exponent adjustments
    interactions = params[10:15]     # (5,) - cross-domain coupling strength
    domain_bias = params[15:20]      # (5,) - per-domain bias
    quad_regs = params[20:25]        # (5,) - per-domain quadratic regularization
    shared_exp_base = params[25]     # scalar - base exponent (shared)
    global_scale = params[26]        # scalar - global scaling factor
    entropy_weight = params[27]      # scalar - entropy contribution weight
    entropy_strength = params[28]    # scalar - entropy effect magnitude
    interact_reg = params[29]        # scalar - interaction regularization
    entropy_blend = params[30]       # scalar - blend between additive/multiplicative
    bias_scale = params[31]          # scalar - per-domain bias scaling factor
    
    # Constrain shared exponent to [0.4, 1.6] via sigmoid
    shared_exp = 0.4 + 1.2 / (1.0 + np.exp(-np.clip(shared_exp_base, -5, 5)))
    
    # Constrain per-domain exponent offsets to [-0.35, 0.35]
    exp_adjust = 0.35 * np.tanh(np.clip(exp_offsets, -3, 3))
    
    # Per-domain exponents with controlled variation
    exponents = np.clip(shared_exp + exp_adjust, 0.3, 2.0)  # (5,)
    
    # Compute entropy of mixture: Shannon entropy normalized to [0, 1]
    X_safe = np.maximum(X, eps)
    entropy = -np.sum(X_safe * np.log(X_safe), axis=1)  # (N,)
    max_entropy = np.log(F)
    entropy_norm = entropy / max_entropy  # (N,) in [0, 1]
    
    # Entropy modulation: blend between additive and multiplicative
    # When blend=0: pure additive, when blend=1: pure multiplicative
    blend = 0.5 * (1.0 + np.tanh(np.clip(entropy_blend, -2, 2)))
    entropy_mod = blend * np.exp(entropy_strength * (entropy_norm - 0.5)) + (1.0 - blend) * 1.0
    
    pred = np.zeros((N, F))
    
    for d in range(F):
        # Base power law term: base_coeffs[d] * X[:, d] ^ exponents[d]
        X_safe_d = np.maximum(X[:, d], eps)
        base_term = base_coeffs[d] * (X_safe_d ** exponents[d])
        
        # Cross-domain interaction: weighted pairwise interactions
        interaction_term = np.zeros(N)
        for other_d in range(F):
            if other_d != d:
                # Pairwise interaction strength scaled by both proportions
                interaction_term += interactions[d] * X[:, other_d] * X[:, d]
        
        # Apply interaction regularization
        interaction_term = interact_reg * interaction_term
        
        # Per-domain quadratic regularization term for smoothness
        quad_term = quad_regs[d] * (X[:, d] ** 2)
        
        # Entropy contribution: additive entropy effect
        entropy_term = entropy_weight * entropy_strength * entropy_norm
        
        # Combine all components with entropy modulation
        pred[:, d] = global_scale * entropy_mod * (base_term + interaction_term + quad_term + 
                                                     entropy_term) + bias_scale * domain_bias[d]
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law using robust initialization and multi-start L-BFGS-B optimization.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)
    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))  # (N, 5)
    
    # Ensure y is (N, 5)
    if y.ndim == 1:
        y = y[:, None]
    if y.shape[0] != X.shape[0]:
        y = y.T
    
    N, F = X.shape
    n_domains = y.shape[1]
    n_params = 32
    
    # ===== Robust Initialization =====
    init_params = np.zeros(n_params)
    
    # Statistics from data
    y_mean = np.mean(y, axis=0)           # (5,)
    y_std = np.std(y, axis=0) + 1e-8      # (5,)
    
    # Initialize base coefficients from log-scale regression
    for d in range(min(n_domains, 5)):
        y_d = y[:, d]
        x_d_safe = np.maximum(X[:, d], 1e-8)
        try:
            # Log-scale fitting for better initialization
            log_y_d = np.log(np.maximum(y_d, 1e-8))
            log_x_d = np.log(x_d_safe)
            A = np.column_stack([log_x_d, np.ones(N)])
            coeffs = np.linalg.lstsq(A, log_y_d, rcond=None)[0]
            init_params[d] = np.exp(coeffs[1])
            init_params[5 + d] = 0.08 * (coeffs[0] - 0.8)  # Offset from shared exp
        except:
            init_params[d] = y_mean[d] / (np.mean(x_d_safe ** 0.7) + 1e-8)
            init_params[5 + d] = 0.0
    
    # Initialize exponent offsets to small values
    init_params[5:10] = np.clip(init_params[5:10], -0.8, 0.8)
    
    # Initialize interactions: small values for coupling
    init_params[10:15] = 0.004 * np.ones(5)
    
    # Initialize domain biases
    init_params[15:20] = y_mean if len(y_mean) &gt;= 5 else np.mean(y)
    
    # Initialize per-domain quadratic regularization: small positive values
    init_params[20:25] = 0.0008 * np.ones(5)
    
    # Initialize global parameters
    init_params[25] = 0.3  # shared_exp_base (maps to ~0.8 via sigmoid)
    init_params[26] = 0.2  # global_scale
    init_params[27] = 0.05  # entropy_weight
    init_params[28] = 0.15  # entropy_strength
    init_params[29] = 0.4  # interact_reg (moderate coupling)
    init_params[30] = 0.1  # entropy_blend (mostly additive initially)
    init_params[31] = 1.0  # bias_scale
    
    # ===== Define Objective Function =====
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            
            # Ensure shapes match
            if pred.ndim == 1:
                pred = pred[:, None]
            
            # MSE loss
            mse = np.mean((pred - y) ** 2)
            
            # Structured L2 regularization
            reg_coeff = 0.0007 * np.sum(params[0:5] ** 2)
            reg_interaction = 0.0002 * np.sum(params[10:15] ** 2)
            reg_quad = 0.0001 * np.sum(params[20:25] ** 2)
            
            total_loss = mse + reg_coeff + reg_interaction + reg_quad
            
            if not np.isfinite(total_loss):
                return 1e10
            
            return total_loss
        except (ValueError, RuntimeWarning, FloatingPointError, OverflowError):
            return 1e10
    
    # ===== Define Bounds =====
    bounds = [
        # base_coeffs[0:5]
        (0.05, 15.0), (0.05, 15.0), (0.05, 15.0), (0.05, 15.0), (0.05, 15.0),
        # exp_offsets[5:10]
        (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0), (-2.0, 2.0),
        # interactions[10:15]
        (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0),
        # domain_bias[15:20]
        (0.2, 5.5), (0.2, 5.5), (0.2, 5.5), (0.2, 5.5), (0.2, 5.5),
        # quad_regs[20:25]
        (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2), (-0.2, 0.2),
        # shared_exp_base[25]
        (-5.0, 5.0),
        # global_scale[26]
        (-5.0, 5.0),
        # entropy_weight[27]
        (-2.0, 2.0),
        # entropy_strength[28]
        (-2.0, 2.0),
        # interact_reg[29]
        (-2.0, 2.0),
        # entropy_blend[30]
        (-2.0, 2.0),
        # bias_scale[31]
        (0.1, 2.0),
    ]
    
    # ===== Multi-Start Optimization =====
    best_result = None
    best_loss = float(&#x27;inf&#x27;)
    
    # Primary optimization with careful initialization
    result = minimize(
        objective,
        init_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-8}
    )
    
    if result.fun &lt; best_loss:
        best_loss = result.fun
        best_result = result.x
    
    # Secondary optimization with small perturbation
    perturbed = init_params + 0.12 * np.random.randn(n_params)
    for i, (lower, upper) in enumerate(bounds):
        perturbed[i] = np.clip(perturbed[i], lower, upper)
    
    result2 = minimize(
        objective,
        perturbed,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-8}
    )
    
    if result2.fun &lt; best_loss:
        best_loss = result2.fun
        best_result = result2.x
    
    # Tertiary optimization with larger perturbation
    perturbed2 = init_params + 0.28 * np.random.randn(n_params)
    for i, (lower, upper) in enumerate(bounds):
        perturbed2[i] = np.clip(perturbed2[i], lower, upper)
    
    result3 = minimize(
        objective,
        perturbed2,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1800, &#x27;ftol&#x27;: 1e-9}
    )
    
    if result3.fun &lt; best_loss:
        best_loss = result3.fun
        best_result = result3.x
    
    return best_result if best_result is not None else init_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.982148 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for multi-domain LLM finetuning
Refined hierarchical mixture model with improved entropy regularization and adaptive bounds
Focus: Better per-domain accuracy, improved numerical stability, simplified code
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predict multi-domain losses using refined hierarchical mixture model
    
    Model: loss_t = base_t + sum_d(w_td * x_d^exp_d) + interact_t * entropy
    Uses shared exponents with adaptive domain selection
    
    Args:
        data_points: (N, 5) array of domain proportions
        params: Flattened array of 35 parameters
    
    Returns:
        (N, 5) predictions for N mixtures across 5 domains
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N, F = X.shape
    params = np.atleast_1d(np.asarray(params, dtype=np.float64))
    
    n_exp, n_base, n_weights = 5, 5, 20
    
    shared_exp = np.clip(params[:n_exp], 0.3, 3.0)
    base_losses = np.clip(params[n_exp:n_exp+n_base], 0.9, 4.9)
    weights = np.clip(params[n_exp+n_base:n_exp+n_base+n_weights].reshape(5, 4), -2.0, 2.0)
    interact_coeff = np.clip(params[n_exp+n_base+n_weights:], -1.2, 1.2)
    
    predictions = np.zeros((N, F))
    X_safe = np.clip(X, 1e-7, 1.0 - 1e-7)
    
    # Precompute entropy-based diversity (improved from concentration)
    # Use Shannon entropy proxy: -sum(x*log(x)) but simplified
    entropy = -np.sum(X * np.log(X_safe + 1e-10), axis=1)  # (N,)
    entropy = np.clip((entropy - 0.6) / 1.2, -1.0, 1.0)  # Normalize to ~[-1, 1]
    
    for t in range(F):
        pred = base_losses[t] * np.ones(N)
        
        # Adaptive domain selection: prioritize self, then neighbors
        main_domains = [t, (t+1) % 5, (t-1) % 5, (t+2) % 5]
        
        for idx, d in enumerate(main_domains):
            w = weights[t, idx]
            exp_val = shared_exp[d]
            pred += w * (X_safe[:, d] ** exp_val)
        
        # Entropy-based interaction (more principled than diversity)
        pred += interact_coeff[t] * entropy
        
        predictions[:, t] = np.clip(pred, 0.7, 5.3)
    
    return predictions


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law parameters with improved initialization and adaptive regularization
    
    Args:
        data_points: (N, 5) domain proportions
        loss_values: (N, 5) observed losses
    
    Returns:
        Optimized parameter vector (35 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    if y.ndim == 1:
        y = y[:, None]
    
    N, F = X.shape
    T = y.shape[1]
    n_exp, n_base, n_weights, n_interact = 5, 5, 20, 5
    total = n_exp + n_base + n_weights + n_interact
    
    init_params = np.zeros(total)
    
    # Better exponent initialization: use percentile-based log-log regression
    for d in range(F):
        x_d = X[:, d]
        y_d = np.mean(y, axis=1)
        
        # Use robust percentile filtering
        p10, p90 = np.percentile(x_d, 10), np.percentile(x_d, 90)
        valid_mask = (x_d &gt; p10) &amp; (x_d &lt; p90) &amp; (y_d &gt; 1.5)
        
        if np.sum(valid_mask) &gt; 4:
            x_log = np.log(np.clip(x_d[valid_mask], 1e-6, 1.0))
            y_log = np.log(np.clip(y_d[valid_mask], 1.0, 5.2))
            coeff = np.polyfit(x_log, y_log, 1)[0]
            init_params[d] = np.clip(coeff, 0.3, 3.0)
        else:
            init_params[d] = 1.2
    
    # Initialize base losses with per-domain refinement
    for t in range(T):
        y_t = y[:, t]
        # Use robust median + small correction based on variance
        init_params[n_exp + t] = np.median(y_t) + 0.05 * np.std(y_t)
        init_params[n_exp + t] = np.clip(init_params[n_exp + t], 0.9, 4.9)
    
    # Initialize weights with adaptive scaling
    w_idx = n_exp + n_base
    for t in range(F):
        main_domains = [t, (t+1) % 5, (t-1) % 5, (t+2) % 5]
        for idx, d in enumerate(main_domains):
            if idx == 0:
                init_params[w_idx + t*4 + idx] = 0.4
            elif idx == 1:
                init_params[w_idx + t*4 + idx] = 0.15
            else:
                init_params[w_idx + t*4 + idx] = 0.06
    
    # Initialize interaction coefficients based on entropy range
    interact_idx = n_exp + n_base + n_weights
    init_params[interact_idx:interact_idx+5] = 0.12
    
    def objective(flat_params):
        &quot;&quot;&quot;MSE with domain-aware adaptive regularization&quot;&quot;&quot;
        pred = scaling_law_func(X, flat_params)
        mse = np.mean((pred - y) ** 2)
        
        # Compute per-domain MSE to guide regularization
        mse_per_dim = np.mean((pred - y) ** 2, axis=0)
        
        # Higher regularization on high-error domains
        error_weights = np.clip(mse_per_dim / np.mean(mse_per_dim), 0.5, 2.0)
        
        shared_exp = flat_params[:n_exp]
        weights = flat_params[n_exp+n_base:n_exp+n_base+n_weights]
        interact = flat_params[n_exp+n_base+n_weights:]
        base = flat_params[n_exp:n_exp+n_base]
        
        # Domain-weighted regularization
        reg_base = 0.00015 * np.sum((base - init_params[n_exp:n_exp+n_base]) ** 2 * error_weights)
        reg_exp = 0.00008 * np.sum((shared_exp - 1.0) ** 2)
        reg_weights = 0.00012 * np.sum(weights ** 2)
        reg_interact = 0.0002 * np.sum(interact ** 2)
        
        return mse + reg_base + reg_exp + reg_weights + reg_interact
    
    bounds = []
    bounds.extend([(0.3, 3.0)] * n_exp)
    bounds.extend([(0.9, 4.9)] * n_base)
    bounds.extend([(-2.0, 2.0)] * n_weights)
    bounds.extend([(-1.2, 1.2)] * n_interact)
    
    result = minimize(
        objective,
        init_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={
            &#x27;maxiter&#x27;: 2500,
            &#x27;ftol&#x27;: 1e-11,
            &#x27;gtol&#x27;: 1e-10,
            &#x27;maxcor&#x27;: 30
        }
    )
    
    return result.x if result.success else init_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.967271 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for multi-domain LLM training with robust optimization
Combines power laws, pairwise interactions, and entropy regularization (35 parameters)
Uses differential evolution + multi-phase L-BFGS-B for reliable convergence
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Multi-domain scaling law predicting 5 domain losses from mixture proportions.
    
    Parameters (35 total):
    - [0:5]: Base losses per domain (1.5-4.5)
    - [5:10]: Domain weight coefficients (0.01-10.0)
    - [10:20]: Pairwise interaction coefficients (10 pairs, -5.0-5.0)
    - [20:30]: Polynomial correction terms - quadratic + linear (2 per domain)
    - [30:35]: Entropy regularization coefficients (-2.0-2.0)
    
    Returns: (N, 5) array of predicted multi-domain losses
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N, F = X.shape
    params = np.asarray(params, dtype=np.float64)
    
    # Extract and clip parameter groups for stability
    base_loss = np.asarray(params[0:5], dtype=np.float64)
    weight_coef = np.clip(params[5:10], 0.01, 10.0)
    interactions = np.asarray(params[10:20], dtype=np.float64)
    poly_coefs = np.asarray(params[20:30], dtype=np.float64)
    entropy_coefs = np.clip(params[30:35], -2.0, 2.0)
    
    # Safely clip input proportions
    X_safe = np.clip(X, 1e-8, 1.0)
    
    # Initialize predictions (N, 5)
    pred = np.zeros((N, 5), dtype=np.float64)
    
    for d in range(5):
        # Base component: constant per domain
        pred[:, d] = base_loss[d]
        
        # Domain-specific weighted term: coef * x_d
        pred[:, d] += weight_coef[d] * X_safe[:, d]
        
        # Pairwise interaction terms from upper triangle of 5x5 matrix
        # This creates symmetric interactions between domains
        idx = 0
        for i in range(5):
            for j in range(i + 1, 5):
                # Interaction between domains i and j affects domain d
                interaction_strength = interactions[idx]
                pred[:, d] += interaction_strength * X_safe[:, i] * X_safe[:, j]
                idx += 1
        
        # Polynomial correction terms: quadratic + linear adjustments
        # Quadratic saturation term
        pred[:, d] += poly_coefs[d] * (X_safe[:, d] ** 2)
        # Linear correction term (second derivative-like behavior)
        pred[:, d] += poly_coefs[d + 5] * X_safe[:, d]
        
        # Entropy regularization: penalize extreme domain concentrations
        # Shannon entropy approximation: -x*ln(x)
        # This regularizes the model to not overfit to concentrated mixtures
        safe_x = np.clip(X_safe[:, d], 1e-8, 1.0)
        entropy_term = -safe_x * np.log(safe_x + 1e-8)
        pred[:, d] += entropy_coefs[d] * entropy_term
    
    # Ensure predictions stay in valid loss range
    return np.clip(pred, 0.5, 5.5)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law using hybrid optimization strategy:
    1. Global search with differential_evolution to escape local minima
    2. Local refinement with L-BFGS-B for fine-tuning
    3. Multiple targeted restarts to ensure robustness
    
    Args:
        data_points: (N, 5) array of domain mixture proportions
        loss_values: (N, 5) array of multi-domain losses
    
    Returns:
        params: (35,) array of optimized parameters
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))
    
    # Handle shape variations in input
    if y.shape[0] == 1 and y.shape[1] &gt; 5:
        y = y.T
    if y.ndim == 1:
        y = y[:, None]
    
    N, F = X.shape
    
    # Data normalization for improved numerical conditioning
    y_mean = np.mean(y, axis=0, keepdims=True)
    y_std = np.std(y, axis=0, keepdims=True)
    y_std = np.where(y_std &lt; 1e-8, 1.0, y_std)
    y_normalized = (y - y_mean) / y_std
    
    def objective(params_flat):
        &quot;&quot;&quot;Objective function: normalized MSE with multi-component regularization.&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params_flat)
            pred_normalized = (pred - y_mean) / y_std
            
            # Main error term: MSE on normalized data
            mse = np.mean((pred_normalized - y_normalized) ** 2)
            
            # Multi-component regularization for stability
            # 1. Penalize extreme interaction coefficients
            reg_interactions = 0.002 * np.sum(params_flat[10:20] ** 2)
            
            # 2. Penalize divergence of base_loss from reasonable range [1.5, 4.5]
            reg_base = 0.001 * np.sum((np.clip(params_flat[0:5], 1.5, 4.5) - params_flat[0:5]) ** 2)
            
            # Total objective with regularization
            total_reg = reg_interactions + reg_base
            
            return mse + total_reg
        except:
            return 1e10
    
    # Define tight parameter bounds for stability
    bounds = [
        (1.5, 4.5),      # base_loss[0:5] - reasonable CE loss range
        (1.5, 4.5),
        (1.5, 4.5),
        (1.5, 4.5),
        (1.5, 4.5),
        (0.01, 10.0),    # weight_coef[5:10] - positive domain sensitivity
        (0.01, 10.0),
        (0.01, 10.0),
        (0.01, 10.0),
        (0.01, 10.0),
        (-5.0, 5.0),     # interactions[10:20] - upper triangle pairs
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-5.0, 5.0),
        (-2.0, 2.0),     # poly_coefs[20:30] - polynomial corrections
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),     # entropy_coefs[30:35] - entropy regularization
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
        (-2.0, 2.0),
    ]
    
    # ===== PHASE 1: Global Search with Differential Evolution =====
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=300,
        popsize=15,
        atol=1e-10,
        tol=1e-10,
        workers=1,
        polish=True,
        updating=&#x27;deferred&#x27;,
        mutation=(0.5, 1.5),
        recombination=0.7
    )
    
    params_best = result_de.x
    best_loss = result_de.fun
    
    # ===== PHASE 2: Local Refinement from DE Solution =====
    result_local1 = minimize(
        objective,
        params_best,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 250, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-8, &#x27;maxcor&#x27;: 20}
    )
    
    if result_local1.fun &lt; best_loss:
        params_best = result_local1.x
        best_loss = result_local1.fun
    
    # ===== PHASE 3: Multiple Targeted Restarts =====
    for restart_idx in range(4):
        # Generate small perturbation with adaptive magnitude
        perturbation_scale = 0.01 * (1.0 - restart_idx / 5.0)
        x0_pert = params_best + np.random.randn(35) * perturbation_scale * (
            np.abs(params_best) + 0.1
        )
        
        # Clip perturbed parameters to bounds
        x0_pert = np.array([
            np.clip(x0_pert[i], bounds[i][0], bounds[i][1])
            for i in range(35)
        ])
        
        result_pert = minimize(
            objective,
            x0_pert,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 200, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-8, &#x27;maxcor&#x27;: 20}
        )
        
        if result_pert.fun &lt; best_loss:
            params_best = result_pert.x
            best_loss = result_pert.fun
    
    return params_best
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.931141 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for multi-domain LLM finetuning
Refined with domain-specific modulation and improved interaction terms
Uses 34 parameters with enhanced domain 3 modeling and numerical stability
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))  # (N, 5)
    N, F = X.shape
    params = np.asarray(params, dtype=np.float64)
    
    # Parameter organization (34 params total):
    # [0:5] base losses for each domain
    # [5:10] linear mixture coefficients (self-domain sensitivity)
    # [10:15] cross-domain coefficients (sum of other domains)
    # [15:20] quadratic mixture terms (non-linear self-sensitivity)
    # [20:25] entropy modulation per domain
    # [25:30] concentration modulation per domain
    # [30:33] pairwise interaction coefficients (3 shared params)
    # [33] domain 3 boost factor (special handling for high-variance domain)
    
    base_losses = params[0:5]
    linear_coeff = params[5:10]
    cross_coeff = params[10:15]
    quad_coeff = params[15:20]
    entropy_coeff = params[20:25]
    conc_coeff = params[25:30]
    pair_coeff = params[30:33]
    domain3_boost = params[33]
    
    # Compute global features (numerically stable)
    eps = 1e-8
    X_safe = np.clip(X, eps, 1.0 - eps)
    
    # Entropy: measure of mixture diversity
    entropy = -np.sum(X_safe * np.log(X_safe + eps), axis=1, keepdims=True)  # (N, 1)
    max_entropy = np.log(5.0)
    entropy_norm = entropy / max_entropy  # (N, 1) in [0, 1]
    
    # Concentration: L2 norm squared (measure of dominance)
    concentration = np.sum(X ** 2, axis=1, keepdims=True)  # (N, 1)
    
    # Max proportion: identifies dominant domain
    max_prop = np.max(X, axis=1, keepdims=True)  # (N, 1)
    
    # Compute pairwise interaction strength for each sample
    pair_strength = np.zeros((N, 1))
    for i in range(5):
        for j in range(i+1, 5):
            pair_strength += X[:, i:i+1] * X[:, j:j+1]
    pair_strength *= 2  # account for symmetry
    
    # Compute domain 3 specific feature (sensitivity to its own proportion changes)
    domain3_prop = X[:, 3:4]
    
    # Build predictions for each domain
    preds = np.zeros((N, 5))
    
    for d in range(5):
        # Start with base loss
        pred_d = base_losses[d]
        
        # Linear term: own domain proportion effect
        pred_d = pred_d + linear_coeff[d] * X[:, d:d+1]
        
        # Cross-domain term: sum of all other domain proportions
        other_domains = np.sum(X[:, [i for i in range(5) if i != d]], axis=1, keepdims=True)
        pred_d = pred_d + cross_coeff[d] * other_domains
        
        # Quadratic term: non-linear self-sensitivity
        pred_d = pred_d + quad_coeff[d] * (X[:, d:d+1] ** 2)
        
        # Entropy modulation: how diversity affects this domain
        pred_d = pred_d + entropy_coeff[d] * entropy_norm
        
        # Concentration modulation: how dominance affects this domain
        pred_d = pred_d + conc_coeff[d] * concentration
        
        # Pairwise interaction effects: domain-specific response to mixed compositions
        pred_d = pred_d + pair_coeff[0] * pair_strength * X[:, d:d+1]
        pred_d = pred_d + pair_coeff[1] * (max_prop - X[:, d:d+1]) * X[:, d:d+1]
        
        # Domain 3 special modulation: interaction with its own proportion
        if d == 3:
            pred_d = pred_d + domain3_boost * domain3_prop * (1.0 - domain3_prop)
        
        preds[:, d] = pred_d.squeeze()
    
    return preds


def fit_scaling_law(data_points, loss_values):
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))
    
    N, F = X.shape
    if y.ndim == 1:
        y = y[:, None]
    
    n_params = 34
    
    # Parameter bounds based on expected ranges
    bounds = [
        (1.8, 4.2),   # base losses (5): observed loss range
        (1.8, 4.2),
        (1.8, 4.2),
        (1.8, 4.2),
        (1.8, 4.2),
        (-1.8, 1.8),  # linear coefficients (5): sensitivity to own proportion
        (-1.8, 1.8),
        (-1.8, 1.8),
        (-1.8, 1.8),
        (-1.8, 1.8),
        (-1.2, 1.2),  # cross-domain coefficients (5): interaction strength
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-1.2, 1.2),  # quadratic coefficients (5): non-linearity
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-1.2, 1.2),
        (-0.6, 0.6),  # entropy modulation (5): diversity effect
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.6, 0.6),  # concentration modulation (5): dominance effect
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.6, 0.6),
        (-0.4, 0.4),  # pairwise coefficients (3): interaction terms
        (-0.4, 0.4),
        (-0.4, 0.4),
        (-0.8, 0.8),  # domain 3 boost factor: special modulation for domain 3
    ]
    
    def objective(params):
        pred = scaling_law_func(X, params)
        mse = np.mean((pred - y) ** 2)
        # Light regularization to prevent overfitting
        reg = 0.0002 * np.sum(params ** 2)
        return mse + reg
    
    # Two-phase optimization: global + local refinement
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=350,
        popsize=30,
        atol=1e-8,
        tol=1e-8,
        workers=1,
        updating=&#x27;deferred&#x27;,
        polish=True
    )
    
    # Local refinement with L-BFGS-B for fine-tuning
    result_final = minimize(
        objective,
        result_de.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 250, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-8}
    )
    
    # Return best result
    best_fun = min(result_final.fun, result_de.fun)
    return result_final.x if result_final.fun &lt;= result_de.fun else result_de.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.930786 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced per-domain scaling law with refined domain-specific modeling.
Uses 35 parameters (7 per domain) for enhanced multi-domain loss prediction.
Improved handling of difficult domains through dedicated modulation and interaction terms.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Per-domain scaling law with polynomial, cross-domain interactions, and domain-specific refinement.
    
    Parameters (35 total, 7 per domain):
    - [0:5]: Base losses L0_i
    - [5:10]: Linear coefficients a_i
    - [10:15]: Quadratic coefficients b_i
    - [15:20]: Cross-domain interaction weights c_i
    - [20:25]: Secondary linear modulation d_i
    - [25:30]: Domain-specific coupling e_i
    - [30:35]: Per-domain modulation factors m_i
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N, F = X.shape
    params = np.asarray(params, dtype=np.float64).ravel()
    
    # Pad if necessary
    if len(params) &lt; 35:
        params = np.pad(params, (0, 35 - len(params)), mode=&#x27;constant&#x27;)
    
    # Parse and clip parameters for numerical stability
    L0 = np.clip(params[0:5], 0.5, 4.5)
    a = np.clip(params[5:10], -2.0, 2.0)
    b = np.clip(params[10:15], -1.5, 1.5)
    c = np.clip(params[15:20], -0.8, 0.8)
    d = np.clip(params[20:25], -1.2, 1.2)
    e = np.clip(params[25:30], -0.4, 0.4)
    mod = np.clip(params[30:35], 0.8, 1.2)
    
    pred = np.zeros((N, F))
    
    for i in range(F):
        x_i = X[:, i]
        
        # Primary: polynomial in own domain
        pred[:, i] = L0[i] + a[i] * x_i + b[i] * (x_i ** 2)
        
        # Secondary: interactions with other domains
        other_sum = np.sum(X[:, [j for j in range(F) if j != i]], axis=1)
        pred[:, i] += c[i] * other_sum + d[i] * other_sum * x_i
        
        # Tertiary: weighted coupling from specific domains
        for j in range(F):
            if j != i:
                # Adaptive coupling strength based on domain interaction
                coupling_strength = e[i] * np.sign(a[j]) * np.abs(a[j]) / (1.0 + np.abs(a[j]))
                pred[:, i] += coupling_strength * X[:, j]
        
        # Apply per-domain modulation factor
        pred[:, i] *= mod[i]
    
    return np.clip(pred, 1.2, 4.3)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law with enhanced multi-start optimization and domain-specific refinement.
    Uses 35 parameters with adaptive regularization targeting weak dimensions.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.atleast_2d(np.asarray(loss_values, dtype=np.float64))
    
    if y.shape[0] == 1:
        y = y.T
    if y.ndim == 1 or y.shape[1] == 1:
        y = y.reshape(-1, 1)
    
    N, F = X.shape
    n_domains = y.shape[1]
    P = 35
    
    # Compute per-domain error statistics for adaptive regularization
    per_domain_loss = np.std(y, axis=0)
    domain_importance = per_domain_loss / (np.mean(per_domain_loss) + 1e-6)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if pred.shape != y.shape:
                return 1e10
            
            # MSE with domain-weighted importance
            domain_errors = np.mean((pred - y) ** 2, axis=0)
            mse = np.mean(domain_errors * domain_importance)
            
            # Adaptive regularization: stronger on modulation factors and coupling
            reg_l0 = 0.00008 * np.sum(params[0:5] ** 2)
            reg_linear = 0.0001 * np.sum(params[5:10] ** 2)
            reg_quad = 0.00015 * np.sum(params[10:15] ** 2)
            reg_interact = 0.0002 * np.sum(params[15:25] ** 2)
            reg_coupling = 0.00025 * np.sum(params[25:30] ** 2)
            reg_mod = 0.00015 * np.sum((params[30:35] - 1.0) ** 2)
            
            reg = reg_l0 + reg_linear + reg_quad + reg_interact + reg_coupling + reg_mod
            return mse + reg
        except:
            return 1e10
    
    def smart_init():
        &quot;&quot;&quot;Enhanced data-driven initialization with domain-specific tuning&quot;&quot;&quot;
        init = np.zeros(P)
        
        for i in range(n_domains):
            y_i = y[:, i]
            x_i = X[:, i]
            
            # L0: mean loss
            init[i] = np.clip(np.mean(y_i), 0.5, 4.5)
            
            # a: linear slope with domain-specific scaling
            x_range = np.max(x_i) - np.min(x_i)
            if x_range &gt; 1e-6:
                y_range = np.max(y_i) - np.min(y_i)
                slope = y_range / x_range
                # Strengthen slope estimation for high-variance domains
                slope_scale = 0.7 + 0.15 * (domain_importance[i] - 1.0)
                init[5 + i] = np.clip(slope * slope_scale, -2.0, 2.0)
            
            # b: quadratic term capturing curvature
            y_sorted_idx = np.argsort(x_i)
            y_sorted = y_i[y_sorted_idx]
            if len(y_sorted) &gt;= 3:
                curvature = (y_sorted[-1] - 2 * y_sorted[len(y_sorted)//2] + y_sorted[0]) / (x_range ** 2 + 1e-6)
                init[10 + i] = np.clip(curvature * 0.1, -1.5, 1.5)
            else:
                init[10 + i] = -0.02
            
            # c: cross-domain interaction (scaled by domain importance)
            init[15 + i] = 0.06 * (1.0 + 0.3 * (domain_importance[i] - 1.0))
            
            # d: interaction modifier
            init[20 + i] = -0.05 * (1.0 + 0.2 * (domain_importance[i] - 1.0))
            
            # e: coupling (very small, domain-aware)
            init[25 + i] = 0.01 * (1.0 + 0.25 * (domain_importance[i] - 1.0))
            
            # m: per-domain modulation (near 1.0)
            init[30 + i] = 1.0
        
        return init
    
    bounds = (
        [(0.5, 4.5)] * 5 +
        [(-2.0, 2.0)] * 5 +
        [(-1.5, 1.5)] * 5 +
        [(-0.8, 0.8)] * 5 +
        [(-1.2, 1.2)] * 5 +
        [(-0.4, 0.4)] * 5 +
        [(0.8, 1.2)] * 5
    )
    
    best_params = None
    best_loss = np.inf
    
    # Start 1: Smart data-driven initialization
    init1 = smart_init()
    r1 = minimize(objective, init1, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r1.fun &lt; best_loss:
        best_loss = r1.fun
        best_params = r1.x
    
    # Start 2: Amplified linear terms for high-variance domains
    init2 = smart_init()
    init2[5:10] = np.clip(init2[5:10] * (0.9 + 0.4 * (domain_importance - 1.0)), -2.0, 2.0)
    r2 = minimize(objective, init2, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r2.fun &lt; best_loss:
        best_loss = r2.fun
        best_params = r2.x
    
    # Start 3: Reduced quadratic terms
    init3 = smart_init()
    init3[10:15] = np.clip(init3[10:15] * 0.3, -1.5, 1.5)
    r3 = minimize(objective, init3, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r3.fun &lt; best_loss:
        best_loss = r3.fun
        best_params = r3.x
    
    # Start 4: Enhanced cross-domain interactions
    init4 = smart_init()
    init4[15:20] = np.clip(init4[15:20] * 2.2, -0.8, 0.8)
    init4[20:25] = np.clip(init4[20:25] * 1.3, -1.2, 1.2)
    r4 = minimize(objective, init4, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r4.fun &lt; best_loss:
        best_loss = r4.fun
        best_params = r4.x
    
    # Start 5: Domain-specific modulation emphasis
    init5 = smart_init()
    init5[30:35] = np.clip(1.0 + 0.1 * np.random.randn(5), 0.8, 1.2)
    r5 = minimize(objective, init5, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r5.fun &lt; best_loss:
        best_loss = r5.fun
        best_params = r5.x
    
    # Start 6: Focused on coupling strength for problematic domains
    init6 = smart_init()
    init6[25:30] = np.clip(init6[25:30] * 1.8, -0.4, 0.4)
    r6 = minimize(objective, init6, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                  options={&#x27;ftol&#x27;: 1e-8, &#x27;maxiter&#x27;: 1400, &#x27;gtol&#x27;: 1e-6})
    if r6.fun &lt; best_loss:
        best_loss = r6.fun
        best_params = r6.x
    
    # Final polish with tighter tolerance
    if best_params is not None:
        try:
            final = minimize(objective, best_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                           options={&#x27;ftol&#x27;: 1e-9, &#x27;maxiter&#x27;: 1000, &#x27;gtol&#x27;: 1e-7})
            if final.fun &lt; best_loss:
                best_params = final.x
        except:
            pass
    
    return best_params if best_params is not None else smart_init()

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
