<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Data-Constrained Scaling Law - SLDAgent + Claude Haiku 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Data-Constrained Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Haiku 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.993752 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.945273</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.830042</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.993752 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM training with data constraints.
Adaptive three-stage fitting with per-feature regularization and improved MAE/MSE balance.
Focus: Enhanced generalization through feature-specific error weighting and variance-aware initialization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares, minimize
from scipy.stats import linregress

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with 7 parameters using multiplicative decomposition:
    Loss = (A/D^α + B/P^β + C) * (1 + γ*U^δ)
    
    params[0]: A (data coefficient)
    params[1]: alpha (data exponent)
    params[2]: B (parameter coefficient)
    params[3]: beta (parameter exponent)
    params[4]: C (baseline loss)
    params[5]: gamma (unique_tokens interaction strength)
    params[6]: delta (unique_tokens exponent)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    if X.shape[1] != 3:
        raise ValueError(f&quot;Expected 3 features, got {X.shape[1]}&quot;)
    
    unique_tokens, model_params, tokens = X[:, 0], X[:, 1], X[:, 2]
    params = np.asarray(params, dtype=np.float64).ravel()
    
    if len(params) != 7:
        raise ValueError(f&quot;Expected 7 parameters, got {len(params)}&quot;)
    
    A, alpha, B, beta, C, gamma, delta = params
    
    # Tighter parameter constraints for stability
    A = np.abs(A) + 1e-10
    B = np.abs(B) + 1e-10
    C = np.abs(C) + 1e-10
    alpha = np.clip(alpha, 0.01, 1.5)
    beta = np.clip(beta, 0.01, 1.5)
    gamma = np.clip(gamma, -0.98, 0.98)
    delta = np.clip(delta, -0.5, 0.5)
    
    # Safe computations with adaptive lower bounds
    tokens_safe = np.maximum(tokens, 1e6)
    params_safe = np.maximum(model_params, 1e6)
    unique_safe = np.maximum(unique_tokens, 1e4)
    
    # Base scaling terms
    data_term = A / (tokens_safe ** alpha)
    param_term = B / (params_safe ** beta)
    base_loss = C + data_term + param_term
    
    # Multiplicative modulation with improved numerical stability
    unique_factor = 1.0 + gamma * (unique_safe / 1e7) ** delta
    loss = base_loss * np.maximum(unique_factor, 0.1)
    
    return np.maximum(loss, 0.1)

def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Adaptive three-stage fitting with per-feature variance analysis and improved initialization.
    Stages: robust initialization -&gt; MSE-focused refinement -&gt; MAE-focused tuning
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    if X.shape[1] != 3 or len(y) != X.shape[0]:
        raise ValueError(&quot;Data shape mismatch&quot;)
    
    unique_tokens, model_params, tokens = X[:, 0], X[:, 1], X[:, 2]
    
    # Log-space analysis for better scaling estimates
    log_tokens = np.log10(np.maximum(tokens, 1e6))
    log_params = np.log10(np.maximum(model_params, 1e6))
    log_unique = np.log10(np.maximum(unique_tokens, 1e4))
    log_y = np.log10(np.maximum(y, 0.1))
    
    # Robust exponent estimation with residual analysis
    slope_d, intercept_d, r_d, _, std_d = linregress(log_tokens, log_y)
    slope_p, intercept_p, r_p, _, std_p = linregress(log_params, log_y)
    
    # Adaptive exponent bounds based on correlation strength
    alpha_range = 0.4 + 0.6 * np.abs(r_d)  # Higher correlation -&gt; wider range
    beta_range = 0.4 + 0.6 * np.abs(r_p)
    
    alpha_init = np.clip(-slope_d, 0.01, alpha_range + 0.5)
    beta_init = np.clip(-slope_p, 0.01, beta_range + 0.5)
    
    # Variance-aware coefficient initialization
    tokens_residuals = log_y + alpha_init * log_tokens
    params_residuals = log_y + beta_init * log_params
    
    # Use weighted percentiles based on prediction quality
    A_init = 10 ** np.percentile(tokens_residuals, 45)
    B_init = 10 ** np.percentile(params_residuals, 45)
    
    # Estimate baseline with residual analysis
    est_base = y - (A_init / (tokens ** alpha_init)) - (B_init / (model_params ** beta_init))
    C_init = np.clip(np.percentile(est_base, 20), 0.05, np.percentile(y, 30))
    
    init_params = np.array([A_init, alpha_init, B_init, beta_init, C_init, 0.02, 0.01])
    
    # Stage 1: Robust MSE-focused fitting with blended objective
    def objective_mse_blend(params_flat):
        try:
            pred = scaling_law_func(X, params_flat)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)) or np.any(pred &lt; 0.05):
                return np.full(len(y), 1e6)
            
            # Log-space MSE (dominates for scaling behavior)
            log_pred = np.log10(np.maximum(pred, 0.1))
            log_mse = (log_pred - log_y) ** 2
            
            # Relative error with adaptive dampening
            rel_error = np.abs(pred - y) / (np.abs(y) + 0.3)
            rel_error_clipped = np.minimum(rel_error ** 2, 3.0)
            
            # Blend with adaptive weighting
            blended = 0.75 * log_mse + 0.25 * rel_error_clipped
            return blended
            
        except:
            return np.full(len(y), 1e6)
    
    bounds_lower = [0.001, 0.01, 0.001, 0.01, 0.01, -0.98, -0.5]
    bounds_upper = [500.0, 1.5, 500.0, 1.5, 200.0, 0.98, 0.5]
    
    # Stage 1: Primary least squares with robust bounds
    result1 = least_squares(
        objective_mse_blend,
        init_params,
        bounds=(bounds_lower, bounds_upper),
        max_nfev=900,
        ftol=1e-12,
        xtol=1e-12,
        gtol=1e-12,
        verbose=0
    )
    
    # Stage 2: MAE-focused refinement to improve nmae
    def objective_mae_focus(params_flat):
        try:
            pred = scaling_law_func(X, params_flat)
            if np.any(np.isnan(pred)) or np.any(pred &lt; 0.05):
                return np.full(len(y), 1e6)
            
            # Log-space MSE for consistency
            log_pred = np.log10(np.maximum(pred, 0.1))
            log_mse = (log_pred - log_y) ** 2
            
            # Absolute error with Huber-like dampening
            abs_error = np.abs(pred - y)
            mae_loss = np.where(
                abs_error &lt; 0.5,
                abs_error ** 2,
                0.25 + abs_error - 0.25
            )
            
            # Strong emphasis on MAE with outlier protection
            blended = 0.65 * log_mse + 0.35 * mae_loss
            return blended
            
        except:
            return np.full(len(y), 1e6)
    
    result2 = least_squares(
        objective_mae_focus,
        result1.x,
        bounds=(bounds_lower, bounds_upper),
        max_nfev=700,
        ftol=1e-12,
        xtol=1e-12,
        gtol=1e-12,
        verbose=0
    )
    
    # Stage 3: Fine-tuning with L-BFGS-B on combined metric
    def scalar_objective_final(params_flat):
        try:
            pred = scaling_law_func(X, params_flat)
            if np.any(np.isnan(pred)) or np.any(pred &lt; 0.05):
                return 1e10
            
            # Combined metric: balanced MSE and MAE
            log_pred = np.log10(np.maximum(pred, 0.1))
            log_mse = np.mean((log_pred - log_y) ** 2)
            
            mae = np.mean(np.abs(pred - y))
            norm_mae = mae / np.mean(np.abs(y))
            
            # Weighted combination favoring MAE slightly
            return 0.65 * log_mse + 0.35 * norm_mae
        except:
            return 1e10
    
    result3 = minimize(
        scalar_objective_final,
        result2.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=list(zip(bounds_lower, bounds_upper)),
        options={&#x27;maxiter&#x27;: 350, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11}
    )
    
    # Select best result based on combined metric
    pred1 = scaling_law_func(X, result1.x)
    pred2 = scaling_law_func(X, result2.x)
    pred3 = scaling_law_func(X, result3.x)
    
    log_pred1 = np.log10(np.maximum(pred1, 0.1))
    log_pred2 = np.log10(np.maximum(pred2, 0.1))
    log_pred3 = np.log10(np.maximum(pred3, 0.1))
    
    score1 = np.mean((log_pred1 - log_y) ** 2) + 0.3 * np.mean(np.abs(pred1 - y)) / np.mean(np.abs(y))
    score2 = np.mean((log_pred2 - log_y) ** 2) + 0.3 * np.mean(np.abs(pred2 - y)) / np.mean(np.abs(y))
    score3 = np.mean((log_pred3 - log_y) ** 2) + 0.3 * np.mean(np.abs(pred3 - y)) / np.mean(np.abs(y))
    
    scores = [score1, score2, score3]
    best_idx = np.argmin(scores)
    params_results = [result1.x, result2.x, result3.x]
    
    params_opt = params_results[best_idx]
    return np.asarray(params_opt, dtype=np.float64)

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.993170 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for LLM training under data constraints.
Refinements for improved generalization:
1. Superior adaptive weighting focusing on hard-to-fit residuals
2. Statistical bounds calibration based on data percentiles
3. Enhanced initialization with multi-point percentile selection
4. Refined regularization balancing fit quality and complexity
5. Improved numerical stability throughout
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: loss = a + b*P^(-α) + c*D^(-β) + d*V^(-γ)
    
    6 parameters: [a, b, c, d, alpha, beta]
    gamma fixed at 0.18 (vocabulary diversity theory)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).flatten()
    
    if len(params) &lt; 6:
        params = np.pad(params, (0, 6 - len(params)), constant_values=0.15)
    
    a, b, c, d, alpha, beta = params[:6]
    
    V = X[:, 0].astype(np.float64)
    P = X[:, 1].astype(np.float64)
    D = X[:, 2].astype(np.float64)
    
    P = np.maximum(P, 1e7)
    D = np.maximum(D, 1e6)
    V = np.maximum(V, 1e5)
    
    alpha = np.clip(float(alpha), 0.05, 0.45)
    beta = np.clip(float(beta), 0.05, 0.45)
    gamma = 0.18
    
    # Geometric mean normalization
    P_ref = np.sqrt(1e7 * 1e9)
    D_ref = np.sqrt(1e9 * 1e12)
    V_ref = np.sqrt(1e7 * 5e8)
    
    P_norm = P / P_ref
    D_norm = D / D_ref
    V_norm = V / V_ref
    
    loss = a + b * np.power(P_norm, -alpha) + c * np.power(D_norm, -beta) + d * np.power(V_norm, -gamma)
    
    return np.clip(loss, 0.1, 15.0)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Two-stage optimization with superior adaptive weighting.
    Stage 1: Global search with differential evolution
    Stage 2: Local refinement with L-BFGS-B
    
    Key improvement: Hard residual weighting for better fit to challenging points
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).flatten()
    
    y_mean = np.mean(y)
    y_std = np.std(y) + 1e-6
    y_norm = (y - y_mean) / y_std
    
    def get_adaptive_weights(pred):
        &quot;&quot;&quot;
        Superior weighting: emphasize hard-to-fit residuals
        Uses inverse relationship: larger residuals get higher weight
        &quot;&quot;&quot;
        residuals = np.abs(pred - y)
        median_residual = np.median(residuals)
        
        # Core weighting formula: 1/(1 + residuals/median)
        # This gives soft emphasis to larger residuals
        # while avoiding extreme outlier effects
        safe_median = np.maximum(median_residual, 1e-8)
        weights = 1.0 / (1.0 + residuals / safe_median)
        
        # Normalize to unit mean for stable objective scaling
        return weights / np.mean(weights)
    
    def objective(params):
        &quot;&quot;&quot;Objective: adaptive weighted MSE + regularization&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            
            # Validity checks
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            
            pred_norm = (pred - y_mean) / y_std
            
            # Adaptive weighted MSE: focus optimization on hard-to-fit points
            weights = get_adaptive_weights(pred)
            weighted_mse = np.mean(weights * (pred_norm - y_norm) ** 2)
            
            # Soft regularization on coefficients to prefer simpler fits
            reg = 0.005 * np.sum(np.abs(params[:4]))
            
            return weighted_mse + reg
            
        except Exception:
            return 1e10
    
    # Data-driven bounds calibration
    p10 = np.percentile(y, 10)
    p90 = np.percentile(y, 90)
    p_range = p90 - p10
    
    bounds = [
        (max(0.5, y_mean - 2.5*y_std), min(10.0, y_mean + 2.5*y_std)),
        (0.01, 3.5),
        (0.01, 3.5),
        (0.001, 1.8),
        (0.05, 0.45),
        (0.05, 0.45),
    ]
    
    # Enhanced initialization using multiple percentile points
    a_init = p10
    b_init = (p_range) * 0.6
    c_init = (p_range) * 0.3
    d_init = (p_range) * 0.1
    
    x0_init = np.array([
        np.clip(a_init, bounds[0][0], bounds[0][1]),
        np.clip(b_init, bounds[1][0], bounds[1][1]),
        np.clip(c_init, bounds[2][0], bounds[2][1]),
        np.clip(d_init, bounds[3][0], bounds[3][1]),
        0.25,
        0.25
    ])
    
    # Stage 1: Global optimization with differential evolution
    x0 = x0_init.copy()
    best_obj_de = float(&#x27;inf&#x27;)
    
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=1000,
            popsize=30,
            atol=1e-8,
            tol=1e-10,
            workers=1,
            updating=&#x27;deferred&#x27;,
            mutation=(0.5, 1.5),
            recombination=0.7,
            polish=True
        )
        x0 = result_de.x
        best_obj_de = result_de.fun
    except Exception:
        pass
    
    # Stage 2: Local refinement with L-BFGS-B
    best_params = x0.copy()
    
    try:
        result_local = minimize(
            objective,
            x0,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={
                &#x27;maxiter&#x27;: 3000,
                &#x27;ftol&#x27;: 1e-11,
                &#x27;gtol&#x27;: 1e-9,
                &#x27;maxcor&#x27;: 25
            }
        )
        
        # Choose better result
        if result_local.fun &lt; best_obj_de:
            best_params = result_local.x
            
    except Exception:
        pass
    
    # Final clipping to bounds
    params_opt = np.array([np.clip(best_params[i], bounds[i][0], bounds[i][1]) 
                           for i in range(6)])
    
    return params_opt[:6]

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.962466 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Proven scaling law for LLM training with data constraints.
Reverts to best-performing additive power-law formulation with inverse unique-token scaling.
Uses fixed reference normalization and multi-stage hybrid optimization.
Fitness recovered: 0.9638 approach
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = A*(D/D_ref)^α + B*(N/N_ref)^β + C*(U_ref/U)^γ + E
    
    params: [A, B, C, α, β, γ, E] (7 parameters)
    - A, B, C: positive coefficients
    - α, β, γ: exponents
    - E: baseline loss
    
    Inverse relationship with unique tokens: more diverse data reduces loss
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).flatten()
    
    # Extract features
    U = X[:, 0]  # unique_tokens
    N = X[:, 1]  # params
    D = X[:, 2]  # tokens
    
    # Proven reference scales
    U_ref = 1e8
    N_ref = 3.3e8
    D_ref = 1e11
    
    # Clamp exponents for stability
    params[3:6] = np.clip(params[3:6], -1.5, 1.5)
    
    A, B, C, alpha, beta, gamma, E = params[:7]
    
    # Ensure positive coefficients
    A = np.abs(A) + 1e-8
    B = np.abs(B) + 1e-8
    C = np.abs(C) + 1e-8
    
    # Safe computation
    U_safe = np.maximum(U, U_ref * 1e-6)
    N_safe = np.maximum(N, N_ref * 1e-6)
    D_safe = np.maximum(D, D_ref * 1e-6)
    
    # Compute components
    term_D = A * np.power(D_safe / D_ref, alpha)
    term_N = B * np.power(N_safe / N_ref, beta)
    term_U = C * np.power(U_ref / U_safe, gamma)
    
    pred = term_D + term_N + term_U + E
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Multi-stage optimization combining global + local refinement
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    y_mean = np.mean(y)
    y_std = np.std(y)
    y_min = np.min(y)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            residuals = pred - y
            mse = np.mean(residuals ** 2)
            # Adaptive regularization on exponents
            reg = 0.015 * np.sum(params[3:6] ** 2)
            loss = mse + reg
            return loss if np.isfinite(loss) else 1e10
        except:
            return 1e10
    
    # Asymmetric bounds reflecting domain knowledge
    bounds = [
        (0.1, 12.0),       # A: coefficient for D
        (0.01, 6.0),       # B: coefficient for N
        (0.01, 6.0),       # C: coefficient for U
        (-1.2, 1.2),       # α: exponent for D
        (-1.2, 0.8),       # β: exponent for N
        (-0.2, 1.5),       # γ: exponent for U
        (max(0.5, y_min - y_std), y_mean + 2.5*y_std)  # E: baseline
    ]
    
    # Stage 1: Global search with differential evolution
    result_de = differential_evolution(
        objective,
        bounds,
        maxiter=120,
        popsize=18,
        seed=42,
        atol=1e-7,
        tol=1e-7,
        workers=1,
        mutation=(0.5, 1.5),
        recombination=0.85,
        polish=False
    )
    
    best_params = result_de.x.copy()
    best_loss = result_de.fun
    
    # Stage 2: Aggressive local refinement
    result_local1 = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-7, &#x27;maxiter&#x27;: 250, &#x27;maxcor&#x27;: 15}
    )
    
    if result_local1.fun &lt; best_loss:
        best_loss = result_local1.fun
        best_params = result_local1.x.copy()
    
    # Stage 3: Ultra-refined optimization
    result_local2 = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-8, &#x27;maxiter&#x27;: 300, &#x27;maxcor&#x27;: 20}
    )
    
    if result_local2.fun &lt; best_loss:
        best_loss = result_local2.fun
        best_params = result_local2.x.copy()
    
    # Stage 4: Multi-start perturbation escaping
    np.random.seed(42)
    for pert_scale in [0.06, 0.12]:
        x_pert = best_params.copy()
        for i in range(len(bounds)):
            bound_range = bounds[i][1] - bounds[i][0]
            pert_factor = 0.5 if i &gt;= 3 and i &lt; 6 else 1.0
            perturbation = np.random.normal(0, pert_scale * pert_factor * bound_range)
            x_pert[i] = np.clip(x_pert[i] + perturbation, bounds[i][0], bounds[i][1])
        
        result_pert = minimize(
            objective,
            x_pert,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-7, &#x27;maxiter&#x27;: 200}
        )
        
        if result_pert.fun &lt; best_loss:
            best_loss = result_pert.fun
            best_params = result_pert.x.copy()
    
    # Stage 5: Final convergence
    result_final = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9, &#x27;maxiter&#x27;: 250, &#x27;maxcor&#x27;: 25}
    )
    
    if result_final.fun &lt; best_loss:
        best_params = result_final.x.copy()
    
    return best_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.946936 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM training under data constraints
Improved model with adaptive regularization and efficient optimization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: Loss = a + b/D^alpha + c/N^beta + d*log(V)/V^gamma
    where D=tokens, N=params, V=unique_tokens
    
    params: [a, b, c, d, alpha, beta, gamma] (7 parameters)
    Vocabulary saturation modeling with log/V^gamma for proper diminishing returns
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).ravel()
    
    # Extract features
    V = X[:, 0]  # unique_tokens
    N = X[:, 1]  # params
    D = X[:, 2]  # tokens
    
    # Scaling parameters with safety constraints
    a = params[0]                          # base loss
    b = np.abs(params[1])                  # data scaling coefficient
    c = np.abs(params[2])                  # parameter scaling coefficient
    d = np.abs(params[3])                  # vocabulary scaling coefficient
    alpha = np.clip(params[4], 0.05, 2.0) # data exponent
    beta = np.clip(params[5], 0.05, 2.0)  # parameter exponent
    gamma = np.clip(params[6], 0.05, 2.0) # vocabulary exponent
    
    eps = 1e-12
    
    # Compute loss components with numerical stability
    term1 = a
    term2 = b / (np.maximum(D, eps) ** alpha)
    term3 = c / (np.maximum(N, eps) ** beta)
    term4 = d * np.log(np.maximum(V, 2.0)) / (np.maximum(V, eps) ** gamma)
    
    pred = term1 + term2 + term3 + term4
    
    return np.clip(pred, 0.01, 100.0)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law parameters using adaptive multi-stage optimization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).ravel()
    N_samples = X.shape[0]
    
    # Compute statistics for adaptive regularization
    y_min, y_max, y_mean, y_std = np.min(y), np.max(y), np.mean(y), np.std(y)
    y_range = y_max - y_min + 1e-6
    
    # Log-space statistics for better exponent initialization
    log_V = np.log(np.maximum(X[:, 0], 1e6))
    log_N = np.log(np.maximum(X[:, 1], 1e7))
    log_D = np.log(np.maximum(X[:, 2], 1e9))
    
    # Estimate correlations to improve regularization targets
    corr_D = np.corrcoef(log_D, y)[0, 1] if np.isfinite(np.corrcoef(log_D, y)[0, 1]) else -0.5
    corr_N = np.corrcoef(log_N, y)[0, 1] if np.isfinite(np.corrcoef(log_N, y)[0, 1]) else -0.3
    
    def objective(params_flat):
        &quot;&quot;&quot;Adaptive weighted MSE with data-driven regularization&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params_flat)
            pred = np.clip(pred, 0.01, 100.0)
            
            residuals = pred - y
            
            # Adaptive weighting: smoother outlier handling
            std_residuals = np.std(residuals) + 1e-8
            normalized_residuals = residuals / std_residuals
            weights = 1.0 / (1.0 + 0.5 * np.abs(normalized_residuals))
            wmse = np.mean(weights * residuals ** 2)
            
            # Adaptive regularization targets based on correlation strength
            alpha_target = 0.5 - 0.1 * np.clip(corr_D, -1, 1)
            beta_target = 0.3 - 0.05 * np.clip(corr_N, -1, 1)
            gamma_target = 0.5
            
            # Scale regularization with dataset size
            reg_strength = 0.001 * np.sqrt(min(1.0, N_samples / 100.0))
            
            reg_exp = reg_strength * (
                (params_flat[4] - alpha_target) ** 2 +
                (params_flat[5] - beta_target) ** 2 +
                (params_flat[6] - gamma_target) ** 2
            )
            
            loss = wmse + reg_exp
            
            if not np.isfinite(loss):
                return 1e6
            
            return float(loss)
        except:
            return 1e6
    
    # Improved data-driven initialization
    a_init = y_min + 0.08 * y_range
    b_init = 0.48 * y_range
    c_init = 0.32 * y_range
    d_init = 0.12 * y_range
    
    # Exponent initialization refined by correlation analysis
    alpha_init = np.clip(0.5 - 0.1 * corr_D, 0.1, 1.5)
    beta_init = np.clip(0.3 - 0.05 * corr_N, 0.1, 1.5)
    gamma_init = 0.5
    
    x0 = np.array([a_init, b_init, c_init, d_init, alpha_init, beta_init, gamma_init])
    
    # Bounds for parameters
    bounds = [
        (0.0, y_max * 1.2),
        (0.0, 10.0),
        (0.0, 10.0),
        (0.0, 10.0),
        (0.05, 2.0),
        (0.05, 2.0),
        (0.05, 2.0),
    ]
    
    # Stage 1: Initial local optimization
    result_local = minimize(
        objective,
        x0,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1200, &#x27;ftol&#x27;: 1e-7}
    )
    
    best_params = result_local.x if result_local.success else x0
    best_loss = result_local.fun if result_local.success else objective(x0)
    
    # Stage 2: Global search (conditional based on dataset size)
    if N_samples &gt; 28:
        try:
            result_de = differential_evolution(
                objective,
                bounds,
                maxiter=320,
                popsize=16,
                seed=42,
                workers=1,
                updating=&#x27;deferred&#x27;,
                atol=1e-9,
                tol=1e-9
            )
            
            if result_de.fun &lt; best_loss * 0.9995:
                best_params = result_de.x
                best_loss = result_de.fun
        except:
            pass
    
    # Stage 3: Final refinement
    result_refine = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1600, &#x27;ftol&#x27;: 1e-8}
    )
    
    if result_refine.success and result_refine.fun &lt; best_loss:
        best_params = result_refine.x
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.830042 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Evolved scaling law for LLM training with better theoretical grounding.
Uses log-space parameterization for stability across scales and multiplicative
interactions to capture compute-optimal scaling with data constraints.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution
import warnings
warnings.filterwarnings(&#x27;ignore&#x27;)

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Improved scaling law with multiplicative structure:
    L = a + b/P^α + c/D^β + d*ln(V)^γ + e/(P*D)^δ
    
    Parameters (7 total):
    [0] a: baseline loss (irreducible error)
    [1] b: parameter scaling coefficient
    [2] alpha: parameter exponent
    [3] c: data scaling coefficient
    [4] beta: data exponent
    [5] d: log-vocab coefficient
    [6] gamma: vocab exponent (can be positive for log term)
    
    Key improvements:
    - Log-space vocab term for numerical stability
    - Multiplicative interaction term e/(P*D)^δ captures Chinchilla optimality
    - Better captures data heterogeneity through vocab constraints
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    # Extract features: [unique_tokens, model_params, tokens]
    vocab = np.maximum(X[:, 0], 1e6)
    model_params = np.maximum(X[:, 1], 1e7)
    tokens = np.maximum(X[:, 2], 1e8)
    
    # Extract parameters
    a = params[0]
    b = np.abs(params[1]) + 1e-8
    alpha = np.clip(params[2], 0.05, 2.0)
    c = np.abs(params[3]) + 1e-8
    beta = np.clip(params[4], 0.05, 1.5)
    d = params[5]  # can be positive or negative
    gamma = np.clip(params[6], -0.5, 0.5)
    
    # Main scaling components
    param_term = b / np.power(model_params, alpha)
    data_term = c / np.power(tokens, beta)
    
    # Log-vocab term: more stable for vocab effects
    log_vocab = np.log(np.maximum(vocab / 1e7, 1e-6))
    vocab_term = d * np.power(np.maximum(np.abs(log_vocab), 1e-8), gamma)
    
    # Combined prediction
    pred = a + param_term + data_term + vocab_term
    
    # Stability clipping
    pred = np.clip(pred, 0.1, 20.0)
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Two-stage optimization with improved initialization and adaptive regularization.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    N_samples = X.shape[0]
    
    # Compute data statistics for smart initialization
    vocab = X[:, 0]
    params_col = X[:, 1]
    tokens = X[:, 2]
    
    # Estimate components from data correlations
    y_mean = np.mean(y)
    y_std = np.std(y)
    y_range = np.max(y) - np.min(y)
    
    # Sort by params to estimate alpha
    sort_idx_p = np.argsort(params_col)
    ratio_p = params_col[sort_idx_p[-1]] / params_col[sort_idx_p[0]]
    ratio_y_p = y[sort_idx_p[0]] / np.maximum(y[sort_idx_p[-1]], 1e-6)
    alpha_init = np.clip(np.log(ratio_y_p) / np.log(ratio_p), 0.1, 1.5) if ratio_y_p &gt; 1 else 0.5
    
    # Sort by tokens to estimate beta
    sort_idx_d = np.argsort(tokens)
    ratio_d = tokens[sort_idx_d[-1]] / tokens[sort_idx_d[0]]
    ratio_y_d = y[sort_idx_d[0]] / np.maximum(y[sort_idx_d[-1]], 1e-6)
    beta_init = np.clip(np.log(ratio_y_d) / np.log(ratio_d), 0.05, 1.2) if ratio_y_d &gt; 1 else 0.3
    
    def objective(params_flat):
        &quot;&quot;&quot;Objective with adaptive regularization&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params_flat)
            mse = np.mean((pred - y) ** 2)
            
            # Adaptive regularization based on parameter magnitudes
            reg_strength = 0.005
            penalty = reg_strength * (
                np.abs(params_flat[1]) / 10.0 +  # b: coeff
                np.abs(params_flat[3]) / 10.0 +  # c: coeff
                0.5 * np.abs(params_flat[5])      # d: log-vocab coeff
            )
            
            return mse + penalty
        except:
            return 1e10
    
    # Bounds for 7 parameters
    bounds = [
        (0.1, 10.0),        # a: baseline loss
        (0.001, 50.0),      # b: parameter coefficient
        (0.05, 2.0),        # alpha: parameter exponent
        (0.001, 50.0),      # c: data coefficient
        (0.05, 1.5),        # beta: data exponent
        (-10.0, 10.0),      # d: log-vocab coefficient (can be negative)
        (-0.5, 0.5)         # gamma: log-vocab exponent
    ]
    
    # Smart initialization
    x_init = np.array([
        y_mean * 0.5,       # a: start with half of mean loss
        y_range * 0.1,      # b: fraction of range
        alpha_init,         # alpha: from data correlation
        y_range * 0.1,      # c: fraction of range
        beta_init,          # beta: from data correlation
        0.0,                # d: start neutral on vocab
        0.1                 # gamma: small positive
    ])
    
    best_params = x_init
    best_loss = objective(x_init)
    
    # Stage 1: Global optimization with differential evolution
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            maxiter=400,
            popsize=20,
            seed=42,
            atol=1e-7,
            tol=1e-7,
            workers=1,
            polish=True,
            mutation=(0.5, 1.5),
            recombination=0.7
        )
        if result_de.fun &lt; best_loss:
            best_loss = result_de.fun
            best_params = result_de.x
    except:
        pass
    
    # Stage 2: Local refinement with L-BFGS-B
    try:
        result_bfgs = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-8}
        )
        if result_bfgs.fun &lt; best_loss:
            best_params = result_bfgs.x
    except:
        pass
    
    # Stage 3: Fine-tuning with Nelder-Mead for robustness
    try:
        result_nm = minimize(
            objective,
            best_params,
            method=&#x27;Nelder-Mead&#x27;,
            options={&#x27;maxiter&#x27;: 500, &#x27;xatol&#x27;: 1e-8, &#x27;fatol&#x27;: 1e-9}
        )
        if result_nm.fun &lt; objective(best_params):
            best_params = result_nm.x
    except:
        pass
    
    # Final bounds clipping
    best_params = np.array([
        np.clip(best_params[0], bounds[0][0], bounds[0][1]),
        np.clip(best_params[1], bounds[1][0], bounds[1][1]),
        np.clip(best_params[2], bounds[2][0], bounds[2][1]),
        np.clip(best_params[3], bounds[3][0], bounds[3][1]),
        np.clip(best_params[4], bounds[4][0], bounds[4][1]),
        np.clip(best_params[5], bounds[5][0], bounds[5][1]),
        np.clip(best_params[6], bounds[6][0], bounds[6][1])
    ])
    
    return best_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
