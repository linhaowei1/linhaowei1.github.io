<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - SFT Scaling Law - opencode + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>SFT Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">opencode</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">GPT-5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        0.893342
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.845450</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.756889</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.893342
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0">from __future__ import annotations
from typing import List, Dict

# Discovered scaling law (shared functional form across groups):
#   sft_loss(N) = L_inf + A * N^(-alpha)
# Coefficients are fitted per experimental group.

# Per-group parameters fitted from /app/data
PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 7.4655976596, &#x27;alpha&#x27;: 0.0893833639},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 6.9696265408, &#x27;alpha&#x27;: 0.1248879757},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.8453873318, &#x27;alpha&#x27;: 0.0660024991},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 5.7130408378, &#x27;alpha&#x27;: 0.0782922582},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 6.2931215297, &#x27;alpha&#x27;: 0.1291848119},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.8960123299, &#x27;alpha&#x27;: 0.0544754427},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.7633686465, &#x27;alpha&#x27;: 0.0524564535},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.8571354747, &#x27;alpha&#x27;: 0.0957503004},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.1982974523, &#x27;alpha&#x27;: 0.0512269552},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.5820265379, &#x27;alpha&#x27;: 0.0513362827},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 5.7873187312, &#x27;alpha&#x27;: 0.1119673779},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.5087532133, &#x27;alpha&#x27;: 0.0704142022},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 7.1611436323, &#x27;alpha&#x27;: 0.0916473833},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 9.2960910354, &#x27;alpha&#x27;: 0.1581352715},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.2857551447, &#x27;A&#x27;: 5.8306038938, &#x27;alpha&#x27;: 0.1217045037},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 5.2395659867, &#x27;alpha&#x27;: 0.0767344267},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 9.5069117910, &#x27;alpha&#x27;: 0.1693705958},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7850518893, &#x27;A&#x27;: 2.6271353620, &#x27;alpha&#x27;: 0.1159319757},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.2428955975, &#x27;alpha&#x27;: 0.0499613896},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 5.6934577617, &#x27;alpha&#x27;: 0.1182278832},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.3523591081, &#x27;alpha&#x27;: 0.0419183827},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.5858563672, &#x27;alpha&#x27;: 0.0606207735},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 7.4768918755, &#x27;alpha&#x27;: 0.1403389747},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.2500719821, &#x27;alpha&#x27;: 0.0557536570},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.2344284065, &#x27;alpha&#x27;: 0.0191717060},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.1808236698, &#x27;alpha&#x27;: 0.0146927813},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.2715471457, &#x27;A&#x27;: 1.7859035415, &#x27;alpha&#x27;: 0.0422749399},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.6211698165, &#x27;alpha&#x27;: 0.0648699072},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.4542367430, &#x27;alpha&#x27;: 0.0321327577},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.8802698557, &#x27;alpha&#x27;: 0.0961777019},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.5193429654, &#x27;alpha&#x27;: 0.0536772855},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.5889984642, &#x27;alpha&#x27;: 0.0381545424},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.9537507864, &#x27;alpha&#x27;: 0.0789696371},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 7.7432486686, &#x27;alpha&#x27;: 0.0903489876},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 7.4481233493, &#x27;alpha&#x27;: 0.1368351252},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.0053815966, &#x27;alpha&#x27;: 0.0698352636},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 3.5927162124, &#x27;alpha&#x27;: 0.0538530120},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4187926280, &#x27;A&#x27;: 1.8293722612, &#x27;alpha&#x27;: 0.1684931683},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.2734524248, &#x27;alpha&#x27;: 0.0452549072},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 4.1031621457, &#x27;alpha&#x27;: 0.0541145502},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4134344504, &#x27;A&#x27;: 1.7782418386, &#x27;alpha&#x27;: 0.1373669844},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0000000000, &#x27;A&#x27;: 2.8159260594, &#x27;alpha&#x27;: 0.0518360448},
}

# Fallback parameters (simple robust averages) in case an unseen group is requested
# Computed as medians over fitted groups to be more stable for extrapolation
_FALLBACK = {&#x27;L_inf&#x27;: 0.0, &#x27;A&#x27;: 4.0, &#x27;alpha&#x27;: 0.07}


def _predict_one(n: float, params: Dict[str, float]) -&gt; float:
    # Guard against non-positive sizes
    n = max(float(n), 1.0)
    L_inf = params[&#x27;L_inf&#x27;]
    A = params[&#x27;A&#x27;]
    alpha = params[&#x27;alpha&#x27;]
    return L_inf + A * (n ** (-alpha))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = PARAMS.get(group, _FALLBACK)
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        if &#x27;sft_data_size&#x27; not in row:
            raise KeyError(&quot;Missing required input key &#x27;sft_data_size&#x27;&quot;)
        n = row[&#x27;sft_data_size&#x27;]
        y = _predict_one(n, params)
        outputs.append({&#x27;sft_loss&#x27;: float(y)})
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.887104
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1">from __future__ import annotations
from typing import Dict, Tuple, List

# Scaling law form (shared across groups):
#   sft_loss(N) = L_inf[group] + K[group] * N ** (-alpha[group])
# If an unknown group is requested, fall back to DEFAULT params (median across groups).

PARAMS: Dict[str, Tuple[float, float, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: (0.21512777, 7.40276909, 0.09605112),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.12094358, 7.07353050, 0.13298803),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: (0.15948628, 3.73425333, 0.07154606),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: (0.18870053, 5.61973142, 0.08431313),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.10434121, 6.39938739, 0.13747389),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: (0.14218530, 2.78010796, 0.05931183),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: (0.18812161, 3.60793698, 0.05713703),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: (0.13158650, 4.83288193, 0.10278902),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: (0.16175015, 3.06328378, 0.05580373),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: (0.22111855, 4.39985898, 0.05576369),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.12292923, 5.82391597, 0.11962378),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: (0.17848771, 4.39227977, 0.07626124),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: (0.20510091, 7.10558756, 0.09850559),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.10697243, 9.57773349, 0.16738350),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: (0.29683060, 5.83349926, 0.12242032),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: (0.18532633, 5.13665293, 0.08282034),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: (0.09571155, 9.83827740, 0.17897799),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: (0.78537576, 2.62717817, 0.11597420),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: (0.16717492, 3.10153397, 0.05445005),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: (0.11621607, 5.74514767, 0.12633081),
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: (0.13683784, 2.22914054, 0.04584659),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: (0.19638808, 4.44078637, 0.06568707),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: (0.11186047, 7.63870448, 0.14917509),
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: (0.15737898, 3.12308331, 0.06068140),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: (0.17414991, 2.06339910, 0.02113603),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: (0.17870209, 2.00412362, 0.01622301),
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: (0.27470871, 1.78302812, 0.04238552),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: (0.19688726, 4.48030612, 0.07038140),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.22450341, 3.24262697, 0.03523111),
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (0.13712846, 4.85046137, 0.10340951),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: (0.17416998, 3.37665074, 0.05845846),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: (0.21287811, 3.39496642, 0.04172983),
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: (0.14186325, 3.87623996, 0.08536726),
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: (0.21928342, 7.68484059, 0.09705297),
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: (0.11301806, 7.59986948, 0.14541089),
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: (0.15971244, 3.90044753, 0.07565736),
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: (0.17152739, 3.45373820, 0.05852187),
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.41825821, 1.82806061, 0.16825325),
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (0.12472066, 2.16396984, 0.04939527),
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: (0.19776244, 3.94292758, 0.05885328),
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: (0.40021184, 1.76842378, 0.13379426),
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: (0.14060546, 2.69903350, 0.05642527),
}

DEFAULT: Tuple[float, float, float] = (0.17283865, 3.92168755, 0.07595930)


def _predict_single(n: float, coeffs: Tuple[float, float, float]) -&gt; float:
    L_inf, K, alpha = coeffs
    # Guard against non-positive sizes
    if n is None or n &lt;= 0:
        return float(&#x27;nan&#x27;)
    return L_inf + K * (float(n) ** (-alpha))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coeffs = PARAMS.get(group, DEFAULT)
    out: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&#x27;sft_data_size&#x27;, float(&#x27;nan&#x27;)))
        pred = _predict_single(n, coeffs)
        out.append({&#x27;sft_loss&#x27;: float(pred)})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.866421
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2">from __future__ import annotations
from typing import List, Dict

# Discovered scaling law: sft_loss(N) = c + a * N^{-b}
# Same functional form across groups; (a,b,c) differ per group.

PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 7.79709296, &#x27;b&#x27;: 0.14366504, &#x27;c&#x27;: 1.15127774},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 7.18065251, &#x27;b&#x27;: 0.13966749, &#x27;c&#x27;: 0.20943581},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.50492382, &#x27;b&#x27;: 0.09301953, &#x27;c&#x27;: 0.59486280},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 5.55096616, &#x27;b&#x27;: 0.11852902, &#x27;c&#x27;: 0.88700526},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 6.33245067, &#x27;b&#x27;: 0.13250068, &#x27;c&#x27;: 0.04341208},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.57317908, &#x27;b&#x27;: 0.07191404, &#x27;c&#x27;: 0.42185299},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.13641643, &#x27;b&#x27;: 0.08539028, &#x27;c&#x27;: 0.88121614},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 4.84266305, &#x27;b&#x27;: 0.11468163, &#x27;c&#x27;: 0.31586501},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.72740881, &#x27;b&#x27;: 0.07469495, &#x27;c&#x27;: 0.61750148},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.77416454, &#x27;b&#x27;: 0.09141127, &#x27;c&#x27;: 1.21118553},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 5.88436542, &#x27;b&#x27;: 0.12720263, &#x27;c&#x27;: 0.22929230},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 4.15198671, &#x27;b&#x27;: 0.10663355, &#x27;c&#x27;: 0.78487709},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 7.45393345, &#x27;b&#x27;: 0.14374514, &#x27;c&#x27;: 1.05100910},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 9.47113095, &#x27;b&#x27;: 0.16403119, &#x27;c&#x27;: 0.06972433},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 5.97887685, &#x27;b&#x27;: 0.14140928, &#x27;c&#x27;: 0.54784037},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.99689388, &#x27;b&#x27;: 0.11656868, &#x27;c&#x27;: 0.85326329},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 9.38268749, &#x27;b&#x27;: 0.16542420, &#x27;c&#x27;: -0.04288451},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.62592585, &#x27;b&#x27;: 0.11454169, &#x27;c&#x27;: 0.77424909},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 2.72492496, &#x27;b&#x27;: 0.07484074, &#x27;c&#x27;: 0.67174921},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 5.77555355, &#x27;b&#x27;: 0.12986758, &#x27;c&#x27;: 0.16216071},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.02894972, &#x27;b&#x27;: 0.05450637, &#x27;c&#x27;: 0.36837840},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.04942870, &#x27;b&#x27;: 0.09803201, &#x27;c&#x27;: 0.96388081},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 7.65025332, &#x27;b&#x27;: 0.14974638, &#x27;c&#x27;: 0.11860471},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.83147284, &#x27;b&#x27;: 0.07932386, &#x27;c&#x27;: 0.57378977},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 1.51363810, &#x27;b&#x27;: 0.03174040, &#x27;c&#x27;: 0.74149915},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 1.40790357, &#x27;b&#x27;: 0.02514156, &#x27;c&#x27;: 0.78702090},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 1.75335535, &#x27;b&#x27;: 0.04356588, &#x27;c&#x27;: 0.30744922},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 4.12803145, &#x27;b&#x27;: 0.10607214, &#x27;c&#x27;: 0.96887260},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 2.35048143, &#x27;b&#x27;: 0.06291060, &#x27;c&#x27;: 1.24503412},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 4.86327770, &#x27;b&#x27;: 0.11878338, &#x27;c&#x27;: 0.37128462},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 2.98458153, &#x27;b&#x27;: 0.08255616, &#x27;c&#x27;: 0.74169984},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 2.64709116, &#x27;b&#x27;: 0.07015078, &#x27;c&#x27;: 1.12878106},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.77516678, &#x27;b&#x27;: 0.10151783, &#x27;c&#x27;: 0.41863249},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 8.16685603, &#x27;b&#x27;: 0.14630670, &#x27;c&#x27;: 1.19283420},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 7.62765686, &#x27;b&#x27;: 0.14681575, &#x27;c&#x27;: 0.13018064},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 3.69790048, &#x27;b&#x27;: 0.09825580, &#x27;c&#x27;: 0.59712436},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.08194773, &#x27;b&#x27;: 0.08089143, &#x27;c&#x27;: 0.71527394},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 1.82066852, &#x27;b&#x27;: 0.16688442, &#x27;c&#x27;: 0.41517682},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.06003085, &#x27;b&#x27;: 0.05427983, &#x27;c&#x27;: 0.24720659},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;a&#x27;: 3.44637525, &#x27;b&#x27;: 0.09027300, &#x27;c&#x27;: 0.97762445},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;a&#x27;: 1.77232820, &#x27;b&#x27;: 0.13527281, &#x27;c&#x27;: 0.40577385},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;a&#x27;: 2.49570434, &#x27;b&#x27;: 0.06779296, &#x27;c&#x27;: 0.40605462},
}

DEFAULT: Dict[str, float] = {&#x27;a&#x27;: 4.33379279, &#x27;b&#x27;: 0.10382283, &#x27;c&#x27;: 0.60135888}


def _predict_sft_loss(n: float, a: float, b: float, c: float) -&gt; float:
    if n &lt;= 0:
        # Guard against invalid inputs; fallback to asymptote
        return float(c)
    return float(c + a * (n ** (-b)))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = PARAMS.get(group, DEFAULT)
    a, b, c = params[&#x27;a&#x27;], params[&#x27;b&#x27;], params[&#x27;c&#x27;]
    out: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&#x27;sft_data_size&#x27;, 0.0))
        y = _predict_sft_loss(n, a, b, c)
        out.append({&#x27;sft_loss&#x27;: y})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.823497
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3">from __future__ import annotations

from typing import List, Dict

# Discovered scaling law (same functional form for all groups):
#   sft_loss(N) = L_inf + A * N**(-alpha)
# Parameters (L_inf, A, alpha) are fitted per group.

_PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.65127773818122, &quot;A&quot;: 9.627636110542012, &quot;alpha&quot;: 0.20027924459754728},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7094358089325561, &quot;A&quot;: 8.865653607508344, &quot;alpha&quot;: 0.19800335463595245},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.0948627966954982, &quot;A&quot;: 3.6373715928919688, &quot;alpha&quot;: 0.14381793206862253},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.3870052565309616, &quot;A&quot;: 6.371080257535652, &quot;alpha&quot;: 0.17042255735621067},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.5434120777731637, &quot;A&quot;: 7.662993628357628, &quot;alpha&quot;: 0.1907173509488867},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.9218529855948283, &quot;A&quot;: 2.3923404329438163, &quot;alpha&quot;: 0.11699630370758805},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.3812161407059724, &quot;A&quot;: 3.1436469109241303, &quot;alpha&quot;: 0.13450336630697737},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8158650085611785, &quot;A&quot;: 5.487940292687338, &quot;alpha&quot;: 0.1695013996929711},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.1175014790256352, &quot;A&quot;: 2.5900727104038723, &quot;alpha&quot;: 0.12007897055693705},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.7111855314883684, &quot;A&quot;: 3.9127469491683713, &quot;alpha&quot;: 0.1373625593896704},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7292923040500483, &quot;A&quot;: 6.967656124781269, &quot;alpha&quot;: 0.18375362369530945},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.2848770931344928, &quot;A&quot;: 4.547213001678072, &quot;alpha&quot;: 0.1613832872213043},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.5510091039471074, &quot;A&quot;: 9.20693494347863, &quot;alpha&quot;: 0.2020388784974948},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.56972433379916, &quot;A&quot;: 12.386988214376414, &quot;alpha&quot;: 0.22750047436629098},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.0478403671811507, &quot;A&quot;: 7.361477195148514, &quot;alpha&quot;: 0.20816541761373086},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.3532632855814968, &quot;A&quot;: 5.673444648443512, &quot;alpha&quot;: 0.1709408294855965},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.4571154854619428, &quot;A&quot;: 12.3368654340819, &quot;alpha&quot;: 0.2303296007217699},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8607566080420259, &quot;A&quot;: 2.6491880068855447, &quot;alpha&quot;: 0.1267697399761743},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.171749209401148, &quot;A&quot;: 2.5854684335593436, &quot;alpha&quot;: 0.12026549590036285},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.6621607125882694, &quot;A&quot;: 6.891642217643802, &quot;alpha&quot;: 0.18923641573799493},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8683783962588203, &quot;A&quot;: 1.6914437727399052, &quot;alpha&quot;: 0.09261012525490318},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.463880812146748, &quot;A&quot;: 4.316968915592505, &quot;alpha&quot;: 0.14681389474891918},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.6186047097097414, &quot;A&quot;: 9.673609194142475, &quot;alpha&quot;: 0.21207146970020188},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.0737897722315954, &quot;A&quot;: 2.735922068950953, &quot;alpha&quot;: 0.12707149006301569},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.2414991489274696, &quot;A&quot;: 1.0612135613446296, &quot;alpha&quot;: 0.05707558230929925},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.287020903980636, &quot;A&quot;: 0.9392426366225727, &quot;alpha&quot;: 0.04596316888651769},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8074492198132941, &quot;A&quot;: 1.3483282412522588, &quot;alpha&quot;: 0.07607955772238753},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.4688725993825085, &quot;A&quot;: 4.5144583405879555, &quot;alpha&quot;: 0.16068467806986383},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.745034122529899, &quot;A&quot;: 2.088823825388248, &quot;alpha&quot;: 0.1032880939465572},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.871284615442321, &quot;A&quot;: 5.573787647372328, &quot;alpha&quot;: 0.17658744292398043},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.2416998420079974, &quot;A&quot;: 2.9427595790138255, &quot;alpha&quot;: 0.1311995007063413},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.628781064016902, &quot;A&quot;: 2.4743566125089473, &quot;alpha&quot;: 0.11295717154926284},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.9186324900396516, &quot;A&quot;: 4.042512579943654, &quot;alpha&quot;: 0.15627210509048312},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.6928341966172602, &quot;A&quot;: 10.147228891615065, &quot;alpha&quot;: 0.20292675913739364},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.630180644744962, &quot;A&quot;: 9.567826000822086, &quot;alpha&quot;: 0.20724319711759384},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.0971243613545192, &quot;A&quot;: 3.9195477918894746, &quot;alpha&quot;: 0.15125310943992998},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.2152739441287244, &quot;A&quot;: 3.0295421285723143, &quot;alpha&quot;: 0.126287878102745},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.41052785055309204, &quot;A&quot;: 1.8100469427145163, &quot;alpha&quot;: 0.16486480202937612},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7472065942824018, &quot;A&quot;: 1.7242228951910559, &quot;alpha&quot;: 0.09151959741229326},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.477624445415, &quot;A&quot;: 3.5382374857245042, &quot;alpha&quot;: 0.1393167297150124},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.4064502225063129, &quot;A&quot;: 1.7728248579304224, &quot;alpha&quot;: 0.13545498163891634},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.906054616524141, &quot;A&quot;: 2.2765381401929234, &quot;alpha&quot;: 0.11015919967664096},
}

# Reasonable fallback if an unknown group is requested
_FALLBACK = {&quot;L_inf&quot;: 1.0, &quot;A&quot;: 4.0, &quot;alpha&quot;: 0.15}


def _predict_loss(n: float, p: Dict[str, float]) -&gt; float:
    # Ensure numerical stability and reasonable domain
    n = max(float(n), 1.0)
    L_inf, A, alpha = p[&quot;L_inf&quot;], p[&quot;A&quot;], p[&quot;alpha&quot;]
    y = L_inf + A * (n ** (-alpha))
    # Loss should be positive
    return max(y, 0.0)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = _PARAMS.get(group, _FALLBACK)
    out: List[Dict[str, float]] = []
    for row in input_data:
        if &quot;sft_data_size&quot; not in row:
            raise KeyError(&quot;Each input row must contain &#x27;sft_data_size&#x27;.&quot;)
        n = float(row[&quot;sft_data_size&quot;])  # number of SFT examples
        y = _predict_loss(n, params)
        out.append({&quot;sft_loss&quot;: float(y)})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.756889
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4">from __future__ import annotations
from typing import List, Dict
import math

# Scaling law: sft_loss(n) = a_g + b_g * n^(-c_g)
# Parameters fitted per group using constrained least squares (a &gt;= 0).

_PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.5756231571491163, &quot;b&quot;: 6.788800109382551, &quot;c&quot;: 0.1},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 5.891449115811081, &quot;c&quot;: 0.10452261306532663},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.7581735338374717, &quot;b&quot;: 3.3253897665429006, &quot;c&quot;: 0.1},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.765913597391628, &quot;b&quot;: 5.037698133931885, &quot;c&quot;: 0.1},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 5.330435864010007, &quot;c&quot;: 0.10904522613065327},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.8162796218426965, &quot;b&quot;: 2.3246370433858927, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.14040467953001, &quot;b&quot;: 2.9286389094129244, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.19153901917179617, &quot;b&quot;: 4.5738879298641795, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 1.00490771189317, &quot;b&quot;: 2.455204059651987, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.465031427159416, &quot;b&quot;: 3.450654874709355, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 5.23569628002393, &quot;c&quot;: 0.1},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.7360076887487385, &quot;b&quot;: 4.040263014645736, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.4183721326985148, &quot;b&quot;: 6.680930867444866, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 7.49839505175648, &quot;c&quot;: 0.13165829145728644},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.21487574349143937, &quot;b&quot;: 5.859678336782164, &quot;c&quot;: 0.11809045226130654},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.6711606109410349, &quot;b&quot;: 4.779116715335894, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 7.8086157931066555, &quot;c&quot;: 0.14522613065326634},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.7679524826460833, &quot;b&quot;: 2.6213614567063552, &quot;c&quot;: 0.1135678391959799},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.0525147738249625, &quot;b&quot;: 2.463773688599949, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 5.271300023177653, &quot;c&quot;: 0.10904522613065327},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.937971229084469, &quot;b&quot;: 1.633385429484853, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.121012709357284, &quot;b&quot;: 3.739557598350229, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 6.463565783906934, &quot;c&quot;: 0.12261306532663317},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.8751090655481231, &quot;b&quot;: 2.6540006198573116, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.518261849277188, &quot;b&quot;: 0.8703088623580859, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 1.6362956915613212, &quot;b&quot;: 0.6587676975419687, &quot;c&quot;: 0.1},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.9748633662659345, &quot;b&quot;: 1.2519888790332818, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.9339671741790373, &quot;b&quot;: 4.002666584784875, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 1.7683001697852798, &quot;b&quot;: 1.9748465379724371, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.13475564355322378, &quot;b&quot;: 4.715649810362228, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.0227022336320242, &quot;b&quot;: 2.787572784403897, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 1.6097662515198514, &quot;b&quot;: 2.263220520187441, &quot;c&quot;: 0.1},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.42186966026041917, &quot;b&quot;: 3.7178336557836027, &quot;c&quot;: 0.1},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 0.5753079426107529, &quot;b&quot;: 7.024797595579225, &quot;c&quot;: 0.1},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.0, &quot;b&quot;: 6.380661194775806, &quot;c&quot;: 0.11809045226130654},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.6762981369243037, &quot;b&quot;: 3.5611403317650283, &quot;c&quot;: 0.1},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.0544435197152868, &quot;b&quot;: 2.8125301742547397, &quot;c&quot;: 0.1},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.41767974252640383, &quot;b&quot;: 1.8250438104494973, &quot;c&quot;: 0.1678391959798995},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.8377530762048058, &quot;b&quot;: 1.6349563093453587, &quot;c&quot;: 0.1},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;a&quot;: 1.1904004060064355, &quot;b&quot;: 3.2313189729433955, &quot;c&quot;: 0.1},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;a&quot;: 0.4079594296474219, &quot;b&quot;: 1.7792784858202433, &quot;c&quot;: 0.13618090452261306},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;a&quot;: 0.8676415086385475, &quot;b&quot;: 2.180107757631235, &quot;c&quot;: 0.1},
}

# Global fallback if an unknown group is provided
_FALLBACK = {&quot;a&quot;: 0.5860710641486957, &quot;b&quot;: 3.8977261691915013, &quot;c&quot;: 0.1}


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = _PARAMS.get(group, _FALLBACK)
    a = float(params[&quot;a&quot;])  # asymptotic loss floor (&gt;= 0)
    b = float(params[&quot;b&quot;])  # scale factor
    c = float(params[&quot;c&quot;])  # decay exponent (&gt; 0)

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&quot;sft_data_size&quot;, 0.0))
        # Guard against non-positive n
        if n &lt;= 0:
            y = float(&quot;nan&quot;)
        else:
            y = a + b * (n ** (-c))
        outputs.append({&quot;sft_loss&quot;: float(y)})
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>