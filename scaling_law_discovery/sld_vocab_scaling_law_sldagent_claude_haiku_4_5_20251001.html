<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Vocabulary Scaling Law - SLDAgent + Claude Haiku 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Vocabulary Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Haiku 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.972263 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.957408</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.949266</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.972263 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for LLM vocabulary and parameter scaling
Uses normalized log-space features, adaptive regularization, and intelligent initialization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Log-space scaling law with 7 parameters using normalized features:
    Loss = a + b*log_P_norm + c*log_V_norm + d*log_D_norm + 
           e*log_P_norm*log_V_norm + f*log_V_norm^2 + g*log_D_norm^2
    
    Normalization improves numerical stability and parameter interpretability.
    
    params: [a, b, c, d, e, f, g] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    # Extract features with safety bounds
    P_nonvocab = np.maximum(X[:, 0], 1e-10)  # non-vocab parameters
    V_vocab = np.maximum(X[:, 1], 1e-10)     # vocabulary size
    D_chars = np.maximum(X[:, 2], 1e-10)     # number of characters
    
    # Compute log features
    log_P = np.log(P_nonvocab)
    log_V = np.log(V_vocab)
    log_D = np.log(D_chars)
    
    # Normalize log features to unit variance range for stability
    # This prevents numerical issues from large feature ranges
    log_P_norm = log_P / 20.0  # ~[-0.5, 1.5] typical range
    log_V_norm = log_V / 11.0  # ~[-0.3, 1.0] typical range
    log_D_norm = log_D / 27.0  # ~[-0.5, 2.0] typical range
    
    # Compute all terms with normalized features
    term_a = params[0]                                    # intercept
    term_b = params[1] * log_P_norm                       # parameter scaling
    term_c = params[2] * log_V_norm                       # vocabulary scaling
    term_d = params[3] * log_D_norm                       # data scaling
    term_e = params[4] * log_P_norm * log_V_norm          # param-vocab interaction
    term_f = params[5] * (log_V_norm ** 2)                # vocab quadratic
    term_g = params[6] * (log_D_norm ** 2)                # data quadratic
    
    pred = term_a + term_b + term_c + term_d + term_e + term_f + term_g
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit using adaptive log-linear model with intelligent initialization and refinement
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    N = X.shape[0]
    n_params = 7
    
    # Protect features for log computation
    P_nonvocab = np.maximum(X[:, 0], 1e-10)
    V_vocab = np.maximum(X[:, 1], 1e-10)
    D_chars = np.maximum(X[:, 2], 1e-10)
    
    log_P = np.log(P_nonvocab)
    log_V = np.log(V_vocab)
    log_D = np.log(D_chars)
    
    # Normalize features for design matrix
    log_P_norm = log_P / 20.0
    log_V_norm = log_V / 11.0
    log_D_norm = log_D / 27.0
    
    # Compute statistics for adaptive regularization
    y_mean = np.mean(y)
    y_std = np.std(y)
    y_range = np.max(y) - np.min(y)
    
    # Stage 1: Build linear design matrix with normalized features
    design_matrix = np.column_stack([
        np.ones(N),                  # a: intercept
        log_P_norm,                  # b: normalized log(P)
        log_V_norm,                  # c: normalized log(V)
        log_D_norm,                  # d: normalized log(D)
        log_P_norm * log_V_norm,     # e: interaction term
        log_V_norm ** 2,             # f: log(V)^2 coefficient
        log_D_norm ** 2              # g: log(D)^2 coefficient
    ])
    
    # Compute initial least squares solution
    try:
        params_init, residuals, rank, s = np.linalg.lstsq(design_matrix, y, rcond=None)
        # Check for numerical issues
        if np.any(np.isnan(params_init)) or np.any(np.isinf(params_init)):
            params_init = np.zeros(n_params)
    except:
        params_init = np.zeros(n_params)
    
    # Compute initial residual for adaptive regularization
    try:
        pred_init = scaling_law_func(X, params_init)
        residual_std = np.std(y - pred_init)
    except:
        residual_std = y_std
    
    # Adaptive regularization weight based on fit quality
    reg_weight = 1e-8 * (residual_std / (y_std + 1e-10))
    
    # Stage 2: Define objective with adaptive regularization
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            
            mse = np.mean((pred - y) ** 2)
            
            # Adaptive L2 regularization
            reg = reg_weight * np.sum(params ** 2)
            
            # Additional penalty for extreme parameter values
            penalty = 1e-9 * np.sum(np.abs(params) ** 3)
            
            return mse + reg + penalty
        except:
            return 1e10
    
    # Adaptive bounds based on data characteristics
    bounds = [
        (y_mean - 2*y_range, y_mean + 2*y_range),  # a: intercept with wide range
        (-15, 15),    # b: log(P) coefficient
        (-15, 15),    # c: log(V) coefficient
        (-15, 15),    # d: log(D) coefficient
        (-15, 15),    # e: interaction coefficient
        (-15, 15),    # f: log(V)^2 coefficient
        (-15, 15)     # g: log(D)^2 coefficient
    ]
    
    best_params = params_init.copy()
    best_loss = objective(best_params)
    
    # Stage 3: Local refinement from least-squares initialization
    try:
        result_local = minimize(
            objective,
            params_init,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1500, &#x27;ftol&#x27;: 1e-10}
        )
        if result_local.fun &lt; best_loss:
            best_params = result_local.x
            best_loss = result_local.fun
    except:
        pass
    
    # Stage 4: Global search with differential evolution
    try:
        result_de = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=300,
            popsize=20,
            atol=1e-10,
            tol=1e-10,
            workers=1,
            updating=&#x27;deferred&#x27;,
            polish=True
        )
        if result_de.fun &lt; best_loss:
            best_params = result_de.x
            best_loss = result_de.fun
    except:
        pass
    
    # Stage 5: Final aggressive local refinement with tighter tolerance
    try:
        result_final = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-11}
        )
        if result_final.fun &lt; best_loss:
            best_params = result_final.x
            best_loss = result_final.fun
    except:
        pass
    
    # Stage 6: Try Nelder-Mead from best point for robustness
    try:
        result_nm = minimize(
            objective,
            best_params,
            method=&#x27;Nelder-Mead&#x27;,
            options={&#x27;maxiter&#x27;: 500, &#x27;xatol&#x27;: 1e-9, &#x27;fatol&#x27;: 1e-11}
        )
        if result_nm.fun &lt; best_loss:
            best_params = result_nm.x
    except:
        pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.965092 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law for LLM vocabulary trade-offs
Enhanced three-stage optimization with improved feature handling and tighter convergence
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Log-linear scaling law with vocabulary-parameter synergy:
    Lossu = a0 + a1*log(P_nv) + a2*log(V) + a3*log(C) + a4*log(V)*log(P_nv) + a5*log(V)^2 + a6*log(C)^2
    
    Uses exactly 7 parameters capturing:
    - Main effects of non-vocab parameters, vocabulary size, and data
    - Vocabulary-parameter synergy interaction (key scaling trade-off)
    - Vocabulary and data scaling non-linearities
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    # Safe feature extraction
    P_nv = np.maximum(X[:, 0], 1e-10)
    V = np.maximum(X[:, 1], 1e-10)
    C = np.maximum(X[:, 2], 1e-10)
    
    # Log-transformed features
    log_P = np.log(P_nv)
    log_V = np.log(V)
    log_C = np.log(C)
    
    # Unpack parameters
    a0, a1, a2, a3, a4, a5, a6 = params
    
    # Compute prediction with synergy interaction
    pred = (a0 + 
            a1 * log_P + 
            a2 * log_V + 
            a3 * log_C + 
            a4 * log_V * log_P + 
            a5 * log_V * log_V + 
            a6 * log_C * log_C)
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Enhanced three-stage optimization with centered features and aggressive convergence.
    Combines global search, local refinement, and final polishing for best results.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    # Compute normalization statistics
    y_mean = np.mean(y)
    y_std = np.std(y) + 1e-10
    y_norm = (y - y_mean) / y_std
    
    # Extract log features
    log_P = np.log(np.maximum(X[:, 0], 1e-10))
    log_V = np.log(np.maximum(X[:, 1], 1e-10))
    log_C = np.log(np.maximum(X[:, 2], 1e-10))
    
    # Center features for improved numerical stability and correlation computation
    log_P_mean = np.mean(log_P)
    log_V_mean = np.mean(log_V)
    log_C_mean = np.mean(log_C)
    
    log_P_c = log_P - log_P_mean
    log_V_c = log_V - log_V_mean
    log_C_c = log_C - log_C_mean
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            pred_norm = (pred - y_mean) / y_std
            residuals = pred_norm - y_norm
            mse = np.mean(residuals ** 2)
            # Fine-tuned L1 regularization for better generalization
            reg = 0.0002 * np.sum(np.abs(params))
            return mse + reg
        except:
            return 1e10
    
    # Adaptive bounds based on physical constraints and feature ranges
    bounds = [
        (-6.0, 6.0),     # a0: bias
        (-4.0, 4.0),     # a1: log(P_nv) effect
        (-4.0, 4.0),     # a2: log(V) effect
        (-4.0, 4.0),     # a3: log(C) effect
        (-1.5, 1.5),     # a4: V-P interaction (synergy)
        (-1.5, 1.5),     # a5: vocab quadratic
        (-1.0, 1.0),     # a6: data quadratic
    ]
    
    # Compute correlations using centered features for robust initialization
    try:
        corr_P = np.corrcoef(log_P_c, y_norm)[0, 1]
        corr_V = np.corrcoef(log_V_c, y_norm)[0, 1]
        corr_C = np.corrcoef(log_C_c, y_norm)[0, 1]
    except:
        corr_P = corr_V = corr_C = -0.35
    
    # Improved initialization with stronger correlation-based estimates
    x0 = np.array([
        y_mean / y_std,
        np.clip(corr_P * 0.85, -1.2, 1.2),
        np.clip(corr_V * 0.85, -1.2, 1.2),
        np.clip(corr_C * 0.85, -1.2, 1.2),
        0.08,
        -0.18,
        -0.09
    ])
    
    # Stage 1: Global search with Differential Evolution (optimized parameters)
    result_de = differential_evolution(
        objective, 
        bounds, 
        seed=42,
        maxiter=520,
        popsize=21,
        atol=1e-11,
        tol=1e-11,
        mutation=(0.58, 1.42),
        recombination=0.77,
        workers=1,
        updating=&#x27;deferred&#x27;,
        polish=False
    )
    
    # Stage 2: L-BFGS-B refinement with tighter tolerances
    result_local = minimize(
        objective,
        result_de.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={
            &#x27;maxiter&#x27;: 1350,
            &#x27;ftol&#x27;: 1e-13,
            &#x27;gtol&#x27;: 1e-12,
            &#x27;maxcor&#x27;: 26
        }
    )
    
    # Stage 3: Final polish with Powell method for robust multi-parameter refinement
    best_x = result_local.x if result_local.fun &lt; result_de.fun else result_de.x
    result_final = minimize(
        objective,
        best_x,
        method=&#x27;Powell&#x27;,
        options={
            &#x27;maxiter&#x27;: 400,
            &#x27;xtol&#x27;: 1e-12,
            &#x27;ftol&#x27;: 1e-12
        }
    )
    
    # Return best result among all stages
    candidates = [
        (result_de.fun, result_de.x),
        (result_local.fun, result_local.x),
        (result_final.fun, result_final.x)
    ]
    best_fun, best_params = min(candidates, key=lambda x: x[0])
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.950638 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Evolved scaling law for LLM vocabulary and parameter scaling
Simplified log-linear model with vocabulary-parameter-data interactions
Optimized for both accuracy and numerical stability
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Simplified scaling law with 7 parameters using log-linear interactions
    
    Form: Lossu = a0 + a1*log(P) + a2*log(V) + a3*log(C) 
                  + a4*log(P)*log(V) + a5*log(V)*log(C) + a6*log(P)*log(C)
    
    where P=non_vocab_params, V=vocab_size, C=num_characters
    
    This model captures:
    - Base loss (a0)
    - Individual feature scaling (a1, a2, a3)
    - Cross-feature interactions (a4, a5, a6)
    
    Parameters: [a0, a1, a2, a3, a4, a5, a6] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    N_samples, F = X.shape
    params = np.asarray(params, dtype=np.float64).flatten()
    
    # Ensure we have exactly 7 parameters
    if len(params) &lt; 7:
        params = np.pad(params, (0, 7 - len(params)), mode=&#x27;constant&#x27;, constant_values=0.0)
    elif len(params) &gt; 7:
        params = params[:7]
    
    # Extract features
    P_nonvocab = X[:, 0]  # non-vocabulary parameters
    V_vocab = X[:, 1]     # vocabulary size
    N_chars = X[:, 2]     # number of characters
    
    # Ensure positive values for log transformation
    eps = 1e-10
    P_nonvocab = np.maximum(P_nonvocab, eps)
    V_vocab = np.maximum(V_vocab, eps)
    N_chars = np.maximum(N_chars, eps)
    
    # Log-transform features
    log_P = np.log(P_nonvocab)
    log_V = np.log(V_vocab)
    log_C = np.log(N_chars)
    
    # Unpack parameters
    a0, a1, a2, a3, a4, a5, a6 = params[:7]
    
    # Main scaling law: linear combination of log features and their interactions
    pred = (a0 + 
            a1 * log_P + 
            a2 * log_V + 
            a3 * log_C +
            a4 * log_P * log_V +
            a5 * log_V * log_C +
            a6 * log_P * log_C)
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law parameters using robust hybrid optimization approach:
    1. Linear regression initialization on log-transformed features
    2. Global search with differential evolution
    3. Local refinement with L-BFGS-B
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    N_samples, F = X.shape
    
    # Extract and log-transform features
    eps = 1e-10
    P_nonvocab = np.maximum(X[:, 0], eps)
    V_vocab = np.maximum(X[:, 1], eps)
    N_chars = np.maximum(X[:, 2], eps)
    
    log_P = np.log(P_nonvocab)
    log_V = np.log(V_vocab)
    log_C = np.log(N_chars)
    
    # Linear regression initialization on log-transformed features
    def init_linear_fit():
        &quot;&quot;&quot;Fit linear model to get initial parameter estimates&quot;&quot;&quot;
        A = np.column_stack([
            np.ones(N_samples),
            log_P,
            log_V,
            log_C,
            log_P * log_V,
            log_V * log_C,
            log_P * log_C
        ])
        
        try:
            # Least squares solution
            coeffs, _, _, _ = np.linalg.lstsq(A, y, rcond=None)
            return coeffs
        except:
            # Fallback: simple initialization
            return np.array([np.mean(y), -0.5, -0.3, -0.2, 0.05, 0.05, 0.05])
    
    init_params = init_linear_fit()
    
    def objective(params):
        &quot;&quot;&quot;MSE objective function&quot;&quot;&quot;
        try:
            pred = scaling_law_func(X, params)
            # Check for numerical issues
            if np.any(np.isnan(pred)) or np.any(np.isinf(pred)):
                return 1e10
            mse = np.mean((pred - y) ** 2)
            return mse if np.isfinite(mse) else 1e10
        except:
            return 1e10
    
    # Parameter bounds based on domain knowledge
    bounds = [
        (-10.0, 10.0),    # a0: bias term
        (-2.0, 0.5),      # a1: log(P) coefficient
        (-2.0, 0.5),      # a2: log(V) coefficient
        (-2.0, 0.5),      # a3: log(C) coefficient
        (-1.0, 1.0),      # a4: log(P)*log(V) interaction
        (-1.0, 1.0),      # a5: log(V)*log(C) interaction
        (-1.0, 1.0),      # a6: log(P)*log(C) interaction
    ]
    
    # Global optimization with differential evolution
    best_params = init_params
    best_loss = objective(init_params)
    
    try:
        result_global = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=400,
            popsize=20,
            atol=1e-7,
            tol=1e-7,
            workers=1,
            updating=&#x27;deferred&#x27;,
            polish=True
        )
        if result_global.fun &lt; best_loss:
            best_params = result_global.x
            best_loss = result_global.fun
    except:
        pass
    
    # Local refinement with L-BFGS-B
    try:
        result_local = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-8}
        )
        if result_local.fun &lt; best_loss:
            best_params = result_local.x
            best_loss = result_local.fun
    except:
        pass
    
    return best_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.949779 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law for LLM vocabulary and parameter trade-offs
Refined optimization with tighter bounds and multi-path convergence strategy
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: Loss = α + β₁·(P_nv)^(-a) + β₂·(D)^(-b) + β₃·log(V)·(V^(-d))
    
    params: [α, β₁, β₂, β₃, a, b, d] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    if X.shape[1] != 3:
        raise ValueError(f&quot;Expected 3 features, got {X.shape[1]}&quot;)
    
    P_nv, V, D = X[:, 0], X[:, 1], X[:, 2]
    
    params = np.asarray(params, dtype=np.float64)
    if len(params) &lt; 7:
        params = np.pad(params, (0, 7 - len(params)))
    
    alpha, beta1, beta2, beta3, a, b, d = params[:7]
    
    # Clip exponents for stability
    a = np.clip(a, 0.05, 2.5)
    b = np.clip(b, 0.05, 2.5)
    d = np.clip(d, -0.5, 1.0)
    
    # Safe bases
    P_nv_safe = np.maximum(P_nv, 1e5)
    D_safe = np.maximum(D, 1e5)
    V_safe = np.maximum(V, 5)
    
    # Scaling terms
    term1 = beta1 * np.power(P_nv_safe, -a)
    term2 = beta2 * np.power(D_safe, -b)
    term3 = beta3 * np.log(V_safe) * np.power(V_safe, -np.abs(d))
    
    return alpha + term1 + term2 + term3


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law with refined multi-stage optimization:
    Tighter bounds + smarter initialization + aggressive local refinement
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if not np.all(np.isfinite(pred)):
                return 1e10
            mse = np.mean((pred - y) ** 2)
            return mse if np.isfinite(mse) else 1e10
        except:
            return 1e10
    
    # Data statistics for smarter initialization
    y_min, y_max, y_mean, y_std = np.min(y), np.max(y), np.mean(y), np.std(y)
    y_range = y_max - y_min
    
    # Tighter, more informed bounds
    bounds = [
        (y_mean - 2.5*y_range, y_mean + 2.5*y_range),  # α: intercept
        (-250, 250),                                     # β₁: parameter effect
        (-250, 250),                                     # β₂: data effect
        (-150, 150),                                     # β₃: vocab effect
        (0.05, 2.5),                                     # a: parameter exponent
        (0.05, 2.5),                                     # b: data exponent
        (-0.5, 1.0),                                     # d: vocab exponent
    ]
    
    # Smarter initialization with refined β estimates
    init_params = np.array([
        y_mean,
        -65.0,
        -65.0,
        2.5,
        0.5,
        0.5,
        0.15,
    ])
    
    # Global optimization with differential evolution
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=700,
        popsize=20,
        tol=1e-10,
        atol=1e-12,
        mutation=(0.5, 1.5),
        recombination=0.8,
        polish=False,
        workers=1,
    )
    
    best_params = result_de.x.copy()
    best_loss = result_de.fun
    
    # Multi-path local refinement (from Inspiration 2 approach)
    # Path 1: From DE solution
    result1 = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-14, &#x27;gtol&#x27;: 1e-11}
    )
    
    if result1.fun &lt; best_loss:
        best_params = result1.x.copy()
        best_loss = result1.fun
    
    # Path 2: From initial guess
    result2 = minimize(
        objective,
        init_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-14, &#x27;gtol&#x27;: 1e-11}
    )
    
    if result2.fun &lt; best_loss:
        best_params = result2.x.copy()
        best_loss = result2.fun
    
    # Path 3: From perturbed DE solution
    perturbed = best_params + np.random.RandomState(123).normal(0, 0.1, 7)
    for i in range(7):
        perturbed[i] = np.clip(perturbed[i], bounds[i][0], bounds[i][1])
    
    result3 = minimize(
        objective,
        perturbed,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-14, &#x27;gtol&#x27;: 1e-11}
    )
    
    if result3.fun &lt; best_loss:
        best_params = result3.x.copy()
    
    return np.asarray(best_params[:7], dtype=np.float64)
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.949266 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law for LLM vocabulary and parameter trade-offs
Optimized centered log features with robust dual-method fitting
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with centered log features and interaction terms
    params: [a, b, c, d, e, f, g] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    
    if X.shape[1] != 3:
        raise ValueError(f&quot;Expected 3 features, got {X.shape[1]}&quot;)
    
    # Extract and log-transform features
    P = np.maximum(X[:, 0], 1e-10)
    V = np.maximum(X[:, 1], 1e-10)
    C = np.maximum(X[:, 2], 1e-10)
    
    log_P = np.log(P)
    log_V = np.log(V)
    log_C = np.log(C)
    
    params = np.asarray(params, dtype=np.float64)
    if len(params) &lt; 7:
        params = np.concatenate([params, np.zeros(7 - len(params))])
    
    a, b, c, d, e, f, g = params[:7]
    
    # Center features for numerical stability
    log_P_c = log_P - np.mean(log_P)
    log_V_c = log_V - np.mean(log_V)
    log_C_c = log_C - np.mean(log_C)
    
    # Main effects and all interaction terms
    pred = a + b * log_P_c + c * log_V_c + d * log_C_c
    pred += e * log_P_c * log_V_c + f * log_P_c * log_C_c + g * log_V_c * log_C_c
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law with efficient dual-stage optimization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    if X.shape[1] != 3 or len(y) != len(X):
        raise ValueError(&quot;Invalid input dimensions&quot;)
    
    # Log-transform features
    P = np.maximum(X[:, 0], 1e-10)
    V = np.maximum(X[:, 1], 1e-10)
    C = np.maximum(X[:, 2], 1e-10)
    
    log_P = np.log(P)
    log_V = np.log(V)
    log_C = np.log(C)
    
    # Center features
    log_P_m = np.mean(log_P)
    log_V_m = np.mean(log_V)
    log_C_m = np.mean(log_C)
    
    log_P_c = log_P - log_P_m
    log_V_c = log_V - log_V_m
    log_C_c = log_C - log_C_m
    
    # Design matrix for least squares
    X_feat = np.column_stack([
        np.ones(len(X)),
        log_P_c, log_V_c, log_C_c,
        log_P_c * log_V_c, log_P_c * log_C_c, log_V_c * log_C_c
    ])
    
    # Initialize with least squares
    try:
        init_params = np.linalg.lstsq(X_feat, y, rcond=None)[0]
    except:
        init_params = np.array([np.mean(y), -0.15, -0.15, -0.15, 0.005, 0.005, 0.005])
    
    init_params = np.clip(init_params, -5, 5)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            residuals = pred - y
            mse = np.mean(residuals ** 2)
            reg = 0.005 * np.sum(params ** 2)
            return mse + reg
        except:
            return 1e10
    
    best_result = None
    best_loss = float(&#x27;inf&#x27;)
    
    # Stage 1: L-BFGS-B (primary, very reliable for this problem)
    try:
        bounds = [(-5, 5)] * 7
        result = minimize(objective, init_params, method=&#x27;L-BFGS-B&#x27;,
                         bounds=bounds, options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-7})
        if result.fun &lt; best_loss:
            best_loss = result.fun
            best_result = result.x
    except:
        pass
    
    # Stage 2: SLSQP refinement (polish the solution)
    if best_result is not None:
        try:
            bounds = [(-5, 5)] * 7
            result = minimize(objective, best_result, method=&#x27;SLSQP&#x27;,
                             bounds=bounds, options={&#x27;maxiter&#x27;: 1500, &#x27;ftol&#x27;: 1e-10})
            if result.fun &lt; best_loss:
                best_loss = result.fun
                best_result = result.x
        except:
            pass
    
    # Stage 3: Nelder-Mead escape (derivative-free backup)
    if best_result is not None:
        try:
            result = minimize(objective, best_result, method=&#x27;Nelder-Mead&#x27;,
                             options={&#x27;maxiter&#x27;: 3000, &#x27;xatol&#x27;: 1e-8, &#x27;fatol&#x27;: 1e-9, &#x27;adaptive&#x27;: True})
            if result.fun &lt; best_loss:
                best_loss = result.fun
                best_result = result.x
        except:
            pass
    
    return np.clip(best_result if best_result is not None else init_params, -5, 5)

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
