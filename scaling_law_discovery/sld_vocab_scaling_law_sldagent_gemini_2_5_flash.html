<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Vocabulary Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Vocabulary Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Gemini 2.5 Flash</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.987151 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.767992</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.016960</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.987151 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Calculates predicted Lossu values based on input features and parameters.
    The functional form is an additive bias plus a multiplicative power law,
    with an additional quadratic term for log(vocab_size) to model more complex scaling behaviors:

    Lossu = bias_K + exp(log_c0 + e_P * log(P_non_vocab) + e_V1 * log(vocab_size) + e_V2 * (log(vocab_size))^2 + e_C * log(num_characters))

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - params: Array of 6 parameters [log_c0, e_P, e_V1, e_V2, e_C, bias_K]

    Returns:
    - Predicted Lossu values (N,)
    &quot;&quot;&quot;
    X_raw = np.atleast_2d(np.asarray(data_points))

    # Add a small epsilon to inputs to prevent log(0) and for numerical stability.
    # Data characteristics suggest inputs are always positive and large, but this is good practice.
    P_non_vocab_log = np.log(X_raw[:, 0] + 1e-10)
    vocab_size_log = np.log(X_raw[:, 1] + 1e-10)
    num_characters_log = np.log(X_raw[:, 2] + 1e-10)

    # Unpack parameters for the 6-parameter model:
    # [log_c0, e_P, e_V1, e_V2, e_C, bias_K]
    log_c0, e_P, e_V1, e_V2, e_C, bias_K = params

    # Calculate the combined exponent for the exponential term.
    # The (log(vocab_size))^2 term allows for non-monotonic effects or saturation
    # with respect to vocabulary size, capturing potential &quot;trade-offs&quot;.
    log_term_exponent = log_c0 + \
                        e_P * P_non_vocab_log + \
                        e_V1 * vocab_size_log + \
                        e_V2 * (vocab_size_log**2) + \
                        e_C * num_characters_log

    # Compute the multiplicative term and add the bias.
    # np.exp is used to reverse the log-transform, ensuring the base scaling term is positive.
    pred = bias_K + np.exp(log_term_exponent)

    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the generalized scaling law function to the given data using bounded optimization.

    The model now includes a quadratic term for log(vocab_size) to better capture
    &quot;vocabulary scaling trade-offs&quot; as suggested by the problem description.

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - loss_values: Array of corresponding Lossu values

    Returns:
    - Optimized parameters (6 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # This model uses 6 parameters: [log_c0, e_P, e_V1, e_V2, e_C, bias_K]
    P_total = 6 

    # --- Initial parameter guess ---
    init_params = np.zeros(P_total)

    min_y, max_y = np.min(y), np.max(y)
    y_range = max_y - min_y

    # Initial guess for bias_K (asymptotic minimum loss).
    # It should be lower (more negative) than the minimum observed Lossu.
    init_params[5] = min_y - 0.1 * y_range 
    # Fallback for very small or zero y_range to ensure a meaningful bias.
    if y_range &lt; 1e-6:
        init_params[5] = min_y - 0.1 

    # Initial exponents. Typical scaling law exponents are negative.
    # e_V2 is initialized to 0, implying a starting point of no quadratic effect.
    init_params[1] = -0.5 # e_P (non-vocab parameters exponent)
    init_params[2] = -0.5 # e_V1 (vocab size linear log exponent)
    init_params[3] = 0.0  # e_V2 (vocab size quadratic log exponent)
    init_params[4] = -0.5 # e_C (num characters exponent)

    # Initial log_c0.
    # Estimate `c0` such that the multiplicative term at minimum resource levels
    # (where loss is typically highest) accounts for `max_y - bias_K`.
    target_multiplicative_term_at_min_resources = max_y - init_params[5]
    # Ensure this target is positive before attempting to derive `c0`.
    if target_multiplicative_term_at_min_resources &lt;= 0:
        target_multiplicative_term_at_min_resources = 0.1 

    # Calculate log of minimum input values for initial c0 estimation
    min_P_val = np.min(X[:, 0]) + 1e-10
    min_V_val = np.min(X[:, 1]) + 1e-10
    min_C_val = np.min(X[:, 2]) + 1e-10

    log_min_P_val = np.log(min_P_val)
    log_min_V_val = np.log(min_V_val)
    log_min_C_val = np.log(min_C_val)

    # Reconstruct the log_term_exponent with initial exponent guesses
    log_term_at_min_resources = init_params[1] * log_min_P_val + \
                                init_params[2] * log_min_V_val + \
                                init_params[3] * (log_min_V_val**2) + \
                                init_params[4] * log_min_C_val

    # Calculate `c0` based on the target multiplicative term and initial exponents.
    exp_log_term = np.exp(log_term_at_min_resources)
    # Avoid numerical issues if denominator is extremely small or zero
    if exp_log_term &lt; 1e-100: 
        init_c0_val = 1.0 # Default if problematic
    else:
        init_c0_val = target_multiplicative_term_at_min_resources / exp_log_term

    # Ensure init_c0_val is positive and not extremely small before taking its logarithm.
    init_c0_val = max(1e-8, init_c0_val) 
    init_params[0] = np.log(init_c0_val)

    # --- Bounds for parameters using L-BFGS-B ---
    bounds = []
    # Bounds for log_c0: A wide range to allow for varying base scales.
    bounds.append((-20.0, 20.0)) 
    # Bounds for e_P and e_C: Typically negative for loss reduction with increased resources.
    bounds.append((-5.0, -1e-8)) # e_P
    bounds.append((-2.0, 2.0))   # e_V1: Can be positive or negative to interact with e_V2 for U-shape.
    bounds.append((-0.1, 0.1))   # e_V2: Small range for quadratic coefficient to avoid extreme behavior.
    bounds.append((-5.0, -1e-8)) # e_C
    # Bounds for bias_K: Must be negative and asymptotically below the minimum observed Lossu.
    bounds.append((-10.0, np.min(y) - 1e-6)) 

    def objective(flat_params):
        &quot;&quot;&quot;Objective function for minimization (Mean Squared Error).&quot;&quot;&quot;
        pred = scaling_law_func(X, flat_params)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Perform the optimization using L-BFGS-B, which handles bounds effectively.
    result = minimize(
        objective,
        init_params,  # Initial parameters (1D array)
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 5000, &#x27;ftol&#x27;: 1e-9, &#x27;gtol&#x27;: 1e-7} # Tighter tolerances for precision
    )

    # Return optimized parameters if successful, otherwise return the initial parameters as a fallback.
    return result.x if result.success else init_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.970944 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Calculates predicted Lossu values based on input features and parameters.
    The functional form models an interaction where vocab_size modulates the
    exponent of non_vocab_parameters:
    Lossu = bias + c_P * P_non_vocab^(e_P_base + e_PV * vocab_size^e_V_coeff) + c_C * num_characters^e_C

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - params: Array of 7 parameters [c_P, e_P_base, e_PV, e_V_coeff, c_C, e_C, bias]

    Returns:
    - Predicted Lossu values (N,)
    &quot;&quot;&quot;
    X_raw = np.atleast_2d(np.asarray(data_points))
    P_non_vocab = X_raw[:, 0]
    vocab_size = X_raw[:, 1]
    num_characters = X_raw[:, 2]

    # Add a small epsilon for numerical stability before taking log or raising to negative powers.
    # This prevents issues with zero or near-zero inputs.
    epsilon = 1e-10
    P_non_vocab_safe = P_non_vocab + epsilon
    vocab_size_safe = vocab_size + epsilon
    num_characters_safe = num_characters + epsilon

    # Unpack the 7 parameters: [c_P, e_P_base, e_PV, e_V_coeff, c_C, e_C, bias]
    c_P, e_P_base, e_PV, e_V_coeff, c_C, e_C, bias = params

    # Calculate terms using log-exp for numerical stability: X^e = exp(e * log(X))

    # 1. Vocab modulation term for exponent: vocab_size^e_V_coeff
    # This term quantifies how vocabulary size modulates the exponent of P_non_vocab.
    # It must be non-negative.
    vocab_exponent_modulation = np.exp(e_V_coeff * np.log(vocab_size_safe))

    # 2. Combined exponent for P_non_vocab: (e_P_base + e_PV * vocab_exponent_modulation)
    combined_e_P = e_P_base + e_PV * vocab_exponent_modulation

    # 3. P_non_vocab term: c_P * P_non_vocab^combined_e_P
    term_P_non_vocab = c_P * np.exp(combined_e_P * np.log(P_non_vocab_safe))

    # 4. num_characters term: c_C * num_characters^e_C
    term_num_characters = c_C * np.exp(e_C * np.log(num_characters_safe))

    # 5. Total prediction = bias + P_non_vocab_term + num_characters_term
    pred = bias + term_P_non_vocab + term_num_characters

    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the evolved scaling law function to the given data using bounded optimization.
    The functional form is Lossu = bias + c_P * P_non_vocab^(e_P_base + e_PV * vocab_size^e_V_coeff) + c_C * num_characters^e_C.

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - loss_values: Array of corresponding Lossu values

    Returns:
    - Optimized parameters (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    P_total = 7 # Total number of parameters for the new functional form

    min_y, max_y = np.min(y), np.max(y)
    y_range = max_y - min_y

    # Initial parameter guess for: [c_P, e_P_base, e_PV, e_V_coeff, c_C, e_C, bias]
    init_params = np.zeros(P_total)

    # --- Initial Guess for Bias (params[6]) ---
    # The bias term represents the asymptotic minimum Lossu, so it should be
    # slightly lower (more negative) than the minimum observed Lossu.
    init_params[6] = min_y - 0.05 * y_range
    if y_range &lt; 1e-6: # Fallback for near-constant loss values
        init_params[6] = min_y - 0.1

    # --- Initial Guess for Exponents ---
    # e_P_base (params[1]) and e_C (params[5]) are expected to be negative for typical scaling laws.
    init_e_typical = -0.1
    init_params[1] = init_e_typical # e_P_base (base exponent for P_non_vocab)
    init_params[5] = init_e_typical # e_C (exponent for num_characters)

    # e_PV (params[2]) and e_V_coeff (params[3]) define the interaction term.
    # Start with neutral values (close to zero) to allow the optimizer to determine the direction
    # and magnitude of the interaction, given the broadened bounds.
    init_params[2] = 0.0 # e_PV (interaction coefficient for vocab_size modulation)
    init_params[3] = 0.0 # e_V_coeff (exponent for vocab_size modulation factor)

    # --- Initial Guess for Coefficients (params[0], params[4]) ---
    # c_P and c_C should be positive.
    target_total_contribution_from_terms = max_y - init_params[6]
    if target_total_contribution_from_terms &lt;= 0:
        target_total_contribution_from_terms = 0.1 # Ensure a positive target

    # Heuristic: split the target contribution 50/50 between P and C terms initially.
    contribution_per_term = target_total_contribution_from_terms * 0.5

    # Estimate c_C (params[4]):
    min_num_characters = np.min(X[:, 2]) + 1e-10
    term_C_factor_at_min = np.exp(init_params[5] * np.log(min_num_characters))
    init_params[4] = contribution_per_term / term_C_factor_at_min # c_C

    # Estimate c_P (params[0]):
    min_P_non_vocab = np.min(X[:, 0]) + 1e-10
    min_vocab_size = np.min(X[:, 1]) + 1e-10 # Using min_vocab_size as it&#x27;s part of the interaction

    # Calculate the combined exponent for P_non_vocab using initial guesses at min values.
    # With init_params[2] and init_params[3] set to 0, the interaction term &#x27;e_PV * vocab_size^e_V_coeff&#x27;
    # becomes 0 * vocab_size^0 = 0. So combined_e_P starts as e_P_base.
    init_vocab_exponent_modulation = np.exp(init_params[3] * np.log(min_vocab_size)) # This will be ~1 if init_params[3] is 0
    init_combined_e_P = init_params[1] + init_params[2] * init_vocab_exponent_modulation # This will be init_params[1]
    
    term_P_factor_at_min = np.exp(init_combined_e_P * np.log(min_P_non_vocab))
    init_params[0] = contribution_per_term / term_P_factor_at_min # c_P

    # --- Bounds for Parameters ---
    # Parameters: [c_P, e_P_base, e_PV, e_V_coeff, c_C, e_C, bias]
    bounds = [
        (1e-9, 1e8),    # c_P: Must be positive. Broad range for flexibility.
        (-0.5, -0.005), # e_P_base: Negative, typical exponent range for scaling laws.
        (-0.5, 0.5),    # e_PV: Broadened to allow both positive and negative values.
        (-0.5, 0.5),    # e_V_coeff: Broadened to allow both positive and negative values.
        (1e-9, 1e8),    # c_C: Must be positive. Broad range.
        (-0.5, -0.005), # e_C: Negative, typical exponent range.
        (-20.0, -0.001) # bias: Negative, theoretically represents the minimum achievable loss.
    ]

    def objective(flat_params):
        &quot;&quot;&quot;Objective function for minimization (Mean Squared Error).&quot;&quot;&quot;
        pred = scaling_law_func(X, flat_params)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Use &#x27;L-BFGS-B&#x27; for bounded optimization, known for robustness.
    result = minimize(
        objective,
        init_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 10000, &#x27;ftol&#x27;: 1e-10} # Increased max_iter and tighter tolerance for precision
    )

    # Return optimized parameters if successful, otherwise the refined initial parameters.
    return result.x if result.success else init_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.933802 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.

This evolved program refines the multiplicative power law model by introducing saturation terms
for both `vocab_size` and `P_non_vocab`. This aims to better capture the &quot;vocabulary scaling trade-offs&quot;
mentioned in the problem, explicitly addressing parameter and vocabulary dimensions where the benefits
of increasing these resources might diminish or saturate. It uses 7 parameters, maximizing the allowed limit.

The `scaling_law_func` models Lossu as:
Lossu = bias + C_total * (P_non_vocab / (P_non_vocab + P_saturation))^e_P * (vocab_size / (vocab_size + V_saturation))^e_V * num_characters^e_C

Key characteristics of this model and its optimization:
1.  **Model Form:** A multiplicative power law for `num_characters`, combined with modified power laws
    for `P_non_vocab` and `vocab_size` that include saturation parameters (`P_saturation`, `V_saturation`),
    plus an additive `bias` term.
    The `(X / (X + X_saturation))` term ensures that the effective contribution of a resource `X`
    to loss reduction saturates as `X` becomes much larger than its respective `X_saturation`.
    If `X_saturation` is very small, `X` has little effect. If `X_saturation` is very large,
    the term approximates `(X / X_saturation)^e_X`, effectively reducing to a standard power law.
    This form allows the model to capture diminishing returns for both parameters and vocabulary size.
2.  **Parameter Efficiency:** Uses 7 parameters (`C_total, e_P, e_V, e_C, bias, V_saturation, P_saturation`),
    which is the maximum allowed and efficiently uses the parameter budget to capture more complex dynamics.
3.  **Numerical Stability:** Employs `np.log` and `np.exp` for robust computation of power laws
    and products, especially with large input values, preventing underflow/overflow.
    Care is taken to ensure arguments to `log` are positive by adding a small epsilon (`1e-10`).
4.  **Optimization (`fit_scaling_law`):**
    *   **L-BFGS-B:** Utilizes `scipy.optimize.minimize` with the &#x27;L-BFGS-B&#x27; method for bounded optimization.
    *   **Initial Parameters:** Carefully chosen initial guesses:
        *   `C_total`: A small positive value.
        *   `Exponents (e_P, e_V, e_C)`: Negative values as increasing resources should decrease loss.
        *   `Bias`: Initialized slightly below the minimum observed Lossu, representing the asymptotic
            (irreducible) loss as resources become infinite.
        *   `V_saturation`, `P_saturation`: Initialized to the median `vocab_size` and `P_non_vocab`
            from the dataset, respectively, providing reasonable starting points within the range
            where saturation effects might become noticeable.
    *   **Parameter Bounds:** Tightly defined but sufficiently wide bounds to guide the optimizer
        towards physically meaningful solutions:
        *   `C_total`: Positive, wide range.
        *   `Exponents`: Strictly negative to ensure loss decreases with increasing resources.
        *   `Bias`: Limited between a value below `np.min(y)` and `0.0`.
        *   `V_saturation`, `P_saturation`: Strictly positive, covering observed ranges and beyond.
    *   **Tolerances:** Increased `maxiter` and tighter `ftol`/`gtol` for higher precision in fitting.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Calculates predicted Lossu values based on input features and parameters.
    The functional form is a product of power laws plus a bias, with saturation terms for P_non_vocab and vocab_size:
    Lossu = bias + C_total * (P_non_vocab / (P_non_vocab + P_saturation))^e_P * (vocab_size / (vocab_size + V_saturation))^e_V * num_characters^e_C

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - params: Array of 7 parameters [C_total, e_P, e_V, e_C, bias, V_saturation, P_saturation]

    Returns:
    - Predicted Lossu values (N,)
    &quot;&quot;&quot;
    X_raw = np.atleast_2d(np.asarray(data_points))

    P_non_vocab = X_raw[:, 0]
    vocab_size = X_raw[:, 1]
    # num_characters is still a simple power law, so we take its log directly
    num_characters_log = np.log(X_raw[:, 2] + 1e-10) 

    C_total = params[0]
    e_P, e_V, e_C = params[1:4] # Exponents for P_non_vocab saturation, vocab_size saturation, num_characters
    bias = params[4]
    V_saturation = params[5] # Saturation parameter for vocab_size
    P_saturation = params[6] # New saturation parameter for P_non_vocab

    # Calculate P_non_vocab term with saturation logic
    # P_non_vocab and P_saturation are positive, so ratio is between 0 and 1.
    P_ratio_term = P_non_vocab / (P_non_vocab + P_saturation)
    # Apply exponent e_P. Add epsilon to prevent log(0) if ratio is ever exactly 0.
    P_term = np.exp(e_P * np.log(P_ratio_term + 1e-10))

    # Calculate vocab_size term with saturation logic
    # vocab_size and V_saturation are positive, so ratio is between 0 and 1.
    V_ratio_term = vocab_size / (vocab_size + V_saturation)
    # Apply exponent e_V. Add epsilon to prevent log(0).
    V_term = np.exp(e_V * np.log(V_ratio_term + 1e-10))

    # Calculate num_characters term (simple power law)
    C_term = np.exp(e_C * num_characters_log)

    # Combine all terms multiplicatively
    product_term = P_term * V_term * C_term

    # Predicted Lossu = bias + C_total * product_term
    pred_lossu = bias + C_total * product_term

    return pred_lossu


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function (product form with P_non_vocab and vocab_size saturation)
    to the given data using bounded optimization.

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - loss_values: Array of corresponding Lossu values

    Returns:
    - Optimized parameters (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    # Total number of parameters: [C_total, e_P, e_V, e_C, bias, V_saturation, P_saturation]
    P_total = 7 

    # --- Initial parameter guess ---
    init_params = np.zeros(P_total)

    init_params[0] = 1e-2 # C_total: Small positive coefficient
    init_params[1:4] = -0.1 # e_P, e_V, e_C: Negative exponents for decreasing loss
    init_params[4] = np.min(y) - 0.01 # bias: Slightly below min observed Lossu
    init_params[5] = np.median(X[:, 1]) # V_saturation: Median vocab_size from data as an initial guess
    init_params[6] = np.median(X[:, 0]) # P_saturation: Median P_non_vocab from data as an initial guess

    # --- Bounds for parameters using L-BFGS-B ---
    bounds = []
    bounds.append((1e-10, 1e10)) # C_total: Positive, wide range for scaling factor
    for _ in range(3): # e_P, e_V, e_C: Negative exponents for inverse scaling (loss decreases with resource)
        bounds.append((-5.0, -1e-9)) # Exponents must be negative, avoiding zero
    bounds.append((np.min(y) - 1.0, 0.0)) # bias: Should be less than or equal to 0 (unigram loss), allowing for asymptotic improvement
    # V_saturation: Must be positive. Range covering and potentially exceeding observed vocab_sizes (4k to 96k).
    bounds.append((1.0, 1e7))
    # P_saturation: Must be positive. Range covering and potentially exceeding observed P_non_vocab (3.3e7 to 1.1e9).
    bounds.append((1.0, 1e11)) # Upper bound allows P_saturation to be much larger than observed P if needed

    def objective(params):
        &quot;&quot;&quot;Objective function for minimization (Mean Squared Error).&quot;&quot;&quot;
        pred = scaling_law_func(X, params)
        mse = np.mean((pred - y) ** 2)
        return mse

    # Perform optimization using L-BFGS-B, which supports bounds.
    result = minimize(
        objective,
        init_params,  # Initial parameters (1D array)
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={
            &#x27;maxiter&#x27;: 10000, # Increased max iterations for potentially better convergence
            &#x27;ftol&#x27;: 1e-10,    # Tighter function tolerance for more precise fitting
            &#x27;gtol&#x27;: 1e-7      # Tighter gradient tolerance
        }
    )

    # Return optimized parameters if the optimization was successful, otherwise return initial parameters.
    return result.x if result.success else init_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.931103 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
This evolved program introduces a new scaling law function based on a multiplicative
power-law model, which is often more theoretically aligned with observed scaling
behaviors in machine learning, especially when multiple resource dimensions
(like parameters, vocabulary size, and data) interact. The optimization strategy
is also refined with more appropriate initial parameter guesses and *loosened, yet still
theoretically sound, bounds for exponents* to allow greater flexibility in fitting.
Additionally, a subtle but important bug in the Mean Squared Error (MSE) calculation
within the `objective` function is fixed to ensure correct broadcasting.

The `scaling_law_func` models Lossu as:
Lossu = K + A * P_non_vocab^e_P * vocab_size^e_V * num_characters^e_C
This form has 5 parameters: `[K, A, e_P, e_V, e_C]`, well within the 7-parameter limit.
- `K`: Represents the irreducible or asymptotic minimum loss.
- `A`: A global scaling coefficient.
- `e_P, e_V, e_C`: Exponents for non-vocabulary parameters, vocabulary size, and number of characters, respectively.

This multiplicative form implies that the different resource dimensions interact synergistically to reduce loss,
which is a common assumption in many scaling law formulations (e.g., related to total compute).
Numerical stability is maintained by performing power calculations in log-space:
`X^e = exp(e * log(X))`, with a small epsilon added to inputs to prevent `log(0)`.

The `fit_scaling_law` function continues to use the L-BFGS-B algorithm, but with significant
improvements tailored to the new model and data characteristics:

1.  **New Functional Form**: Switched from an additive sum of power laws to a multiplicative power law,
    reducing the parameter count from 7 to 5 and potentially better capturing interactions. (This change was introduced in the previous iteration and is maintained).
2.  **Refined Initial Parameter Guesses**:
    -   **K (irreducible loss)**: Initialized slightly below the minimum observed Lossu (`np.min(y) - 0.05`), providing a robust starting point for the asymptotic floor.
    -   **A (scaling coefficient)**: The initial guess for `A` is now more precisely estimated. It&#x27;s calculated to bridge the gap between the expected irreducible loss (`K_init`) and the maximum observed loss (`np.max(y)`) given the maximum product of power terms from the initial exponent guesses. This ensures a more physically meaningful starting point for the optimization, allowing the model to better span the range of observed Lossu values.
    -   **Exponents (e_P, e_V, e_C)**: Initialized to `-0.2`, a common magnitude for exponents in LLM scaling laws.
3.  **Loosened and More Realistic Exponent Bounds**:
    -   **K**: Bounds remain `[np.min(y) - 1.0, 0.0]`, allowing for a deep asymptotic minimum while respecting the unigram normalization.
    -   **A**: Bounds set to `(1e-9, 1e5)`. `A` must be positive. This range is chosen to accommodate the required scaling given the potential range of the multiplicative term and target Lossu values.
    -   **Exponents (e_P, e_V, e_C)**: Bounds are now `(-1.0, -1e-9)`. This significantly loosens the previous `(-0.5, -0.01)` restriction. It allows exponents to explore a wider range of negative values, which is common in scaling laws (e.g., up to -1.0), and ensures they are not exactly zero (which would imply no scaling effect). This change aims to allow the model to capture stronger scaling effects if present in the data, potentially improving fit.
4.  **Robust Optimization Settings**: `maxiter`, `ftol`, and `gtol` are retained at high rigor (`10000`, `1e-10`, `1e-9`) to ensure convergence to a precise solution.
5.  **MSE Calculation Fix**: Corrected the `objective` function&#x27;s MSE calculation to ensure proper broadcasting when subtracting `pred` (which is `(N,)`) from `y` (which is also `(N,)`), avoiding potential incorrect results from `(N,) - (N,1)` broadcasting.

These changes are designed to address the previous model&#x27;s negative R2 score by providing a more suitable functional form, guiding the optimization towards a more accurate and stable region of the parameter space, and correcting a potential numerical issue, ultimately improving fitness and generalization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Calculates predicted Lossu values based on input features and parameters.
    The functional form is a multiplicative power law plus a bias:
    Lossu = K + A * P_non_vocab^e_P * vocab_size^e_V * num_characters^e_C

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - params: Array of 5 parameters [K, A, e_P, e_V, e_C]

    Returns:
    - Predicted Lossu values (N,)
    &quot;&quot;&quot;
    X_raw = np.atleast_2d(np.asarray(data_points))
    
    # Ensure inputs are positive before taking log.
    # Given data characteristics (minimum values are large), adding epsilon is primarily for theoretical robustness.
    # It also helps prevent log(0) if any data point could theoretically be zero, though for this problem they are large.
    P_non_vocab = X_raw[:, 0] + 1e-10
    vocab_size = X_raw[:, 1] + 1e-10
    num_characters = X_raw[:, 2] + 1e-10

    params = np.asarray(params)

    # Handle single or multiple parameter sets (T=1 for this problem)
    if params.ndim == 1:
        params = params[None, :]  # Reshape to (1, P_total)
    T, P_total = params.shape

    # P_total must be 5 for this specific model: [K, A, e_P, e_V, e_C]
    if P_total != 5:
        raise ValueError(f&quot;Expected 5 parameters but got {P_total}&quot;)

    K = params[:, 0]       # Irreducible loss (bias)
    A = params[:, 1]       # Overall scaling coefficient
    e_P = params[:, 2]     # Exponent for P_non_vocab
    e_V = params[:, 3]     # Exponent for vocab_size
    e_C = params[:, 4]     # Exponent for num_characters

    # Compute the multiplicative term: A * P^e_P * V^e_V * C^e_C
    # Using log-exp for numerical stability: term = A * exp(e_P*log(P) + e_V*log(V) + e_C*log(C))
    # Each log(X) term is (N, 1), each e is (1, T) due to broadcasting, sum to (N, T)
    log_prod_term = (
        e_P[None, :] * np.log(P_non_vocab[:, None]) +
        e_V[None, :] * np.log(vocab_size[:, None]) +
        e_C[None, :] * np.log(num_characters[:, None])
    )
    
    # K and A are (T,), log_prod_term is (N, T)
    # The result A * exp(log_prod_term) will be (N, T) due to broadcasting
    scaling_term = A[None, :] * np.exp(log_prod_term)
    
    # The final prediction is K + scaling_term. K is (T,), scaling_term is (N, T)
    # Result is (N, T)
    pred = K[None, :] + scaling_term

    # Return predictions for the single parameter set (T=1)
    return pred[:, 0]


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the new multiplicative scaling law function to the given data using bounded optimization.

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - loss_values: Array of corresponding Lossu values

    Returns:
    - Optimized parameters (5 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values) # y is (N,)
    # F is the number of features (3: non_vocab_parameters, vocab_size, num_characters)
    # P_total is the number of model parameters (5: K, A, e_P, e_V, e_C)
    P_total = 5

    # --- Initial parameter guess ---
    # params structure: [K, A, e_P, e_V, e_C]
    init_params = np.zeros(P_total)

    # K (irreducible loss): index 0
    # Start slightly below the minimum observed Lossu to allow for asymptotic improvement.
    K_init = np.min(y) - 0.05
    init_params[0] = K_init

    # Exponents (e_P, e_V, e_C): indices 2, 3, 4
    # Start with negative values, e.g., -0.2, typical for scaling laws.
    e_init = -0.2
    init_params[2:] = e_init

    # A (scaling coefficient): index 1
    # Estimate A based on the observed loss range and the initial exponent guesses.
    # Lossu_max = K_init + A_init * (P_min^e_init * V_min^e_init * C_min^e_init)
    # So A_init = (Lossu_max - K_init) / (P_min^e_init * V_min^e_init * C_min^e_init)
    # P_min, V_min, C_min are the minimum values of the resources, which yield the maximum term value
    # because exponents are negative.
    min_P_val = np.min(X[:,0]) + 1e-10
    min_V_val = np.min(X[:,1]) + 1e-10
    min_C_val = np.min(X[:,2]) + 1e-10
    
    max_power_term_at_init = (min_P_val**e_init) * (min_V_val**e_init) * (min_C_val**e_init)
    
    # Ensure denominator is not zero or extremely small to avoid division by zero/inf.
    # Given the problem&#x27;s large resource values and negative exponents, max_power_term_at_init will be positive and non-zero.
    if max_power_term_at_init &gt; 1e-15: # Safety check
        A_init = (np.max(y) - K_init) / max_power_term_at_init
        # Cap A_init within reasonable bounds if the estimation is extreme
        init_params[1] = np.clip(A_init, 1e-9, 1e5)
    else:
        init_params[1] = 1e3 # Fallback to a default large positive value if calculation is problematic


    # --- Bounds for parameters using L-BFGS-B ---
    bounds = []
    # K (index 0): [np.min(y) - 1.0, 0.0]
    # Lossu values range from approx -5.34 to -0.51.
    # Allow K to be lower than min observed loss, and up to 0 (unigram loss).
    bounds.append((np.min(y) - 1.0, 0.0))

    # A (index 1): [1e-9, 1e5]
    # A must be positive. Upper bound adjusted based on expected term magnitudes.
    bounds.append((1e-9, 1e5))

    # Exponents (indices 2, 3, 4): [-1.0, -1e-9] - Loosened bounds
    # Exponents must be negative for loss to decrease with increasing resources.
    # This range is common for scaling law exponents in LLMs, allowing for stronger scaling effects.
    # Avoids values too close to zero (implying no effect) and excessively negative values that can cause instability.
    for _ in range(3): # There are 3 exponents
        bounds.append((-1.0, -1e-9))


    def objective(flat_params):
        &quot;&quot;&quot;Objective function for minimization (Mean Squared Error).&quot;&quot;&quot;
        # params_for_func is (1, P_total) as expected by scaling_law_func
        params_for_func = flat_params[None, :] 
        pred = scaling_law_func(X, params_for_func) # pred is (N,)
        mse = np.mean((pred - y) ** 2) # y is (N,). Direct subtraction is correct.
        return mse

    # Use &#x27;L-BFGS-B&#x27; which supports bounds, generally more robust for complex non-linear problems
    result = minimize(
        objective,
        init_params.ravel(),  # Pass initial parameters as a flat array
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 10000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-9} # Tighter gtol for higher precision convergence
    )

    # Return optimized parameters if successful, otherwise return initial parameters
    # The result.x is already a 1D array of the optimized parameters.
    return result.x if result.success else init_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.016960 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Calculates predicted Lossu values based on input features and parameters.
    The functional form is a sum of power laws plus a bias:
    Lossu = bias + c_P * P_non_vocab^e_P + c_V * vocab_size^e_V + c_C * num_characters^e_C

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - params: Array of 7 parameters [c_P, c_V, c_C, e_P, e_V, e_C, bias]

    Returns:
    - Predicted Lossu values (N,)
    &quot;&quot;&quot;
    X_raw = np.atleast_2d(np.asarray(data_points))
    # Use np.maximum to ensure strictly positive values before taking log, adding robustness.
    X_log = np.log(np.maximum(X_raw, 1e-10))

    N, F = X_log.shape  # F is 3 (non_vocab_parameters, vocab_size, num_characters)
    params = np.asarray(params)

    # Handle single or multiple parameter sets (T=1 for this problem)
    if params.ndim == 1:
        params = params[None, :]  # Reshape to (1, P_total)
    T, P_total = params.shape

    # P_total = F (coeffs) + F (exponents) + 1 (bias) = 2*F + 1 = 7
    if P_total != 2 * F + 1:
        raise ValueError(f&quot;Expected {2*F+1} parameters but got {P_total}&quot;)

    coeffs = params[:, :F]       # Coefficients for each feature (c_P, c_V, c_C)
    exponents = params[:, F:2*F] # Exponents for each feature (e_P, e_V, e_C)
    bias = params[:, -1]         # Overall bias (K)

    # Compute each power law term: coeffs[i] * (X_raw[j, i] ** exponents[i])
    # This is numerically implemented as coeffs * exp(exponents * log(X_raw)) for stability.
    # The broadcasting ensures correct element-wise multiplication across data points (N),
    # parameter sets (T, which is 1 here), and features (F).
    # Clip the argument to np.exp to prevent potential overflow/underflow for very large/small inputs.
    term_contributions_log = exponents[None, :, :] * X_log[:, None, :]
    # Clipping exp argument to prevent issues like np.exp(710) -&gt; inf or np.exp(-710) -&gt; 0.0
    term_contributions_exp = np.exp(np.fmax(np.fmin(term_contributions_log, 700), -700))
    term_contributions = coeffs[None, :, :] * term_contributions_exp

    # Sum contributions of each feature for each data point and add the bias
    pred = term_contributions.sum(axis=2) + bias[None, :]

    # Return predictions for the single parameter set (T=1) as a 1D array
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fits the scaling law function to the given data using bounded optimization.
    This version includes refined initial parameter guesses and expanded bounds
    to better suit the large dynamic range of input features and the negative Lossu values.
    It also specializes initial exponent guesses and bounds for the vastly different scale
    of &#x27;num_characters&#x27;.

    Parameters:
    - data_points: (N,3) array with columns [P_non_vocab, vocab_size, num_characters]
    - loss_values: Array of corresponding Lossu values

    Returns:
    - Optimized parameters (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    N, F = X.shape
    P_total = 2 * F + 1  # Total parameters for scaling_law_func (7 for F=3)

    # Ensure y is 2D for consistency with scaling_law_func&#x27;s output format
    if y.ndim == 1:
        y2d = y[:, None]
    else:
        y2d = y

    # --- Initial parameter guess ---
    # params = [c_P, c_V, c_C, e_P, e_V, e_C, bias]
    init_params = np.zeros(P_total)

    min_y, max_y = np.min(y), np.max(y)
    y_range = max_y - min_y

    # Initial guess for bias (K): index 6
    # It should be slightly lower (more negative) than the minimum observed Lossu to act as an asymptote.
    init_params[-1] = min_y - 0.1 * y_range 
    # Fallback for degenerate cases where y_range is zero or tiny, ensure bias is meaningfully lower than min_y
    if y_range &lt; 1e-6: 
        init_params[-1] = min_y - 0.1 

    # Initial exponents (e_P, e_V, e_C): indices 3, 4, 5
    # P_non_vocab and vocab_size typically have similar exponent magnitudes.
    # num_characters has a much larger range (up to 5e12), so its exponent might need to be less negative
    # to prevent its term from vanishing too quickly and allow it to contribute meaningfully.
    initial_exponents_guess = np.array([-0.5, -0.5, -0.1]) # P_non_vocab, vocab_size, num_characters
    init_params[F:2*F] = initial_exponents_guess

    # Initial coefficients (c_P, c_V, c_C): indices 0, 1, 2
    # Estimate based on distributing the &#x27;y_range&#x27; across terms.
    # The target total contribution from the power-law terms (at their maximum, i.e., min resource levels)
    # should cover the observed range of Lossu values from &#x27;bias&#x27; up to &#x27;max_y&#x27;.
    target_total_contribution_from_terms = max_y - init_params[-1]
    if target_total_contribution_from_terms &lt;= 0:
        # Fallback for degenerate cases, ensure a positive target for coefficients
        target_total_contribution_from_terms = 0.1 

    # Robust calculation of min_X_val for each feature, ensuring values are positive for exponentiation.
    min_X_vals_robust = np.maximum(np.min(X, axis=0), 1e-10)

    for i in range(F):
        # Calculate the base contribution of this feature at its minimum value with the initial exponent
        term_val_at_min_X = min_X_vals_robust[i] ** initial_exponents_guess[i]
        
        # We want each term c_i * (min_X_val^init_e) to contribute roughly
        # (target_total_contribution_from_terms / F) to the total range.
        # So, c_i = (target_total_contribution_from_terms / F) / (term_val_at_min_X)
        init_params[i] = (target_total_contribution_from_terms / F) / term_val_at_min_X
        # Ensure initial coefficient is positive
        init_params[i] = np.maximum(init_params[i], 1e-8)


    # --- Bounds for parameters using L-BFGS-B ---
    bounds = []
    # Coeffs (0, 1, 2): [1e-8, 1e10]
    # Allow positive coefficients. The upper bound is significantly increased from 1e6 to 1e10
    # to account for very small X^e terms needing large coefficients to contribute to the loss range.
    for _ in range(F):
        bounds.append((1e-8, 1e10)) 

    # Exponents (3, 4, 5): Specific bounds for each feature type based on range characteristics.
    # P_non_vocab and vocab_size can have steeper decays.
    bounds.append((-5.0, -1e-8)) # e_P
    bounds.append((-5.0, -1e-8)) # e_V
    # num_characters: much larger scale, thus often a shallower exponent (less negative).
    # Its lower bound is adjusted from -5.0 to -0.5 to match this expectation.
    bounds.append((-0.5, -1e-8)) # e_C

    # Bias (6): Refined bound.
    # This ensures the bias is always lower than any observed Lossu value,
    # consistent with its role as an asymptotic irreducible minimum.
    bounds.append((-10.0, np.min(y) - 1e-3)) 

    def objective(flat_params):
        &quot;&quot;&quot;Objective function for minimization (Mean Squared Error).&quot;&quot;&quot;
        # Reshape flat parameters to (T, P_total) for scaling_law_func (T=1 here)
        params_reshaped = flat_params.reshape(y2d.shape[1], P_total)
        pred = scaling_law_func(X, params_reshaped)
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Use &#x27;L-BFGS-B&#x27; which supports bounds and is generally robust for non-linear optimization.
    result = minimize(
        objective,
        init_params.ravel(),  # Pass initial parameters as a flat array
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 7000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-8} # Increased max iterations and tighter tolerances for precision
    )

    # Return optimized parameters if the optimization was successful; otherwise, return the initial parameters.
    # The final output should be a 1D array of parameters.
    # If optimization fails, the carefully constructed initial parameters (within bounds) are returned as fallback.
    params_opt = result.x.reshape(y2d.shape[1], P_total) if result.success else init_params.reshape(y2d.shape[1], P_total)
    return params_opt[0] if y2d.shape[1] == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
