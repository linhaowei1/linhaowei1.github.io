<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - SFT Scaling Law - goose + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>SFT Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">goose</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">GPT-5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.967556 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.898627</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.731620</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.967556 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0">def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, each with key &quot;sft_data_size&quot; (number of SFT examples).
        group: The experimental group identifier (string).
                The functional form is identical across groups; only constants differ.

    Returns:
        A list of dictionaries with key &quot;sft_loss&quot; for each input row.
    &quot;&quot;&quot;
    # Parameters fitted per group for the law:
    _PARAMS = {&quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1512777381812203, &#x27;c&#x27;: 23.1398245994615, &#x27;alpha&#x27;: 0.2381458233930208, &#x27;N0&#x27;: 4876.374238039832}, &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.6238498708909732, &#x27;c&#x27;: 99.99999997352305, &#x27;alpha&#x27;: 0.3976078215060757, &#x27;N0&#x27;: 11558.491226820295}, &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.5948627966954984, &#x27;c&#x27;: 4.169583454195764, &#x27;alpha&#x27;: 0.10842983259539116, &#x27;N0&#x27;: 620.9294592458539}, &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.8870052565309617, &#x27;c&#x27;: 12.589156065262879, &#x27;alpha&#x27;: 0.18965261036492076, &#x27;N0&#x27;: 4292.202626480964}, &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4915945363758296, &#x27;c&#x27;: 53.72376110471119, &#x27;alpha&#x27;: 0.3538490433901832, &#x27;N0&#x27;: 8208.076049162994}, &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.42185298559482837, &#x27;c&#x27;: 2.721922686370415, &#x27;alpha&#x27;: 0.07696425226399932, &#x27;N0&#x27;: 201.57413275984604}, &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.8812161407059725, &#x27;c&#x27;: 3.7124906639274893, &#x27;alpha&#x27;: 0.1004347493339725, &#x27;N0&#x27;: 651.176470457412}, &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.31586500856117855, &#x27;c&#x27;: 6.968142395099821, &#x27;alpha&#x27;: 0.1466144207037816, &#x27;N0&#x27;: 1335.72212381429}, &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.6175014790256353, &#x27;c&#x27;: 3.075530975514127, &#x27;alpha&#x27;: 0.08535072362606437, &#x27;N0&#x27;: 514.7047933000287}, &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2111855314883686, &#x27;c&#x27;: 5.298400333867949, &#x27;alpha&#x27;: 0.12086049837063219, &#x27;N0&#x27;: 1771.2506644133234}, &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.2292923040500484, &#x27;c&#x27;: 12.618964826980536, &#x27;alpha&#x27;: 0.1941056864931099, &#x27;N0&#x27;: 3295.2428246856243}, &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7848770931344929, &#x27;c&#x27;: 4.6985988833591, &#x27;alpha&#x27;: 0.11755460303872274, &#x27;N0&#x27;: 346.47811059894894}, &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.0510091039471077, &#x27;c&#x27;: 13.36609532140565, &#x27;alpha&#x27;: 0.19459739902099005, &#x27;N0&#x27;: 1966.148414736923}, &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.5697621647603599, &#x27;c&#x27;: 99.99999999074515, &#x27;alpha&#x27;: 0.40937876666094647, &#x27;N0&#x27;: 6244.642734497064}, &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.224143146950251, &#x27;c&#x27;: 14.59973132909446, &#x27;alpha&#x27;: 0.2968364050528966, &#x27;N0&#x27;: 550.5561027660805}, &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.8532632855814969, &#x27;c&#x27;: 5.769313072202102, &#x27;alpha&#x27;: 0.128688687479602, &#x27;N0&#x27;: 441.5873172322458}, &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.43621335949856677, &#x27;c&#x27;: 61.030635791043764, &#x27;alpha&#x27;: 0.3684503046572159, &#x27;N0&#x27;: 4178.038107530485}, &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7814637080160759, &#x27;c&#x27;: 2.6207508287947126, &#x27;alpha&#x27;: 0.11520376239509583, &#x27;N0&#x27;: 1.0760076089567364e-11}, &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.671749209401148, &#x27;c&#x27;: 3.05173942731373, &#x27;alpha&#x27;: 0.08495191517966257, &#x27;N0&#x27;: 462.61062401679254}, &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.30332635730211316, &#x27;c&#x27;: 10.781981318062071, &#x27;alpha&#x27;: 0.19556333080725272, &#x27;N0&#x27;: 1844.53853486769}, &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.36837839625882035, &#x27;c&#x27;: 2.064791523499473, &#x27;alpha&#x27;: 0.05607439316988944, &#x27;N0&#x27;: 75.9172379877138}, &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.963880812146748, &#x27;c&#x27;: 6.13315838607694, &#x27;alpha&#x27;: 0.1342216717881915, &#x27;N0&#x27;: 2089.526726264657}, &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.3170696281489177, &#x27;c&#x27;: 21.451868485184864, &#x27;alpha&#x27;: 0.25484008412733566, &#x27;N0&#x27;: 2967.5386314330094}, &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.5737897722315956, &#x27;c&#x27;: 2.8913494105073343, &#x27;alpha&#x27;: 0.08109527270865684, &#x27;N0&#x27;: 73.79570557646554}, &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1209766956751008, &#x27;c&#x27;: 1.1915986753620234, &#x27;alpha&#x27;: 0.05000000000000001, &#x27;N0&#x27;: 119.23012333120555}, &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.63395079142584, &#x27;c&#x27;: 1.8526750238010694, &#x27;alpha&#x27;: 0.1921537870023617, &#x27;N0&#x27;: 5578.439427129741}, &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.8797129884870536, &#x27;c&#x27;: 1.3801607965652851, &#x27;alpha&#x27;: 0.09031120236583308, &#x27;N0&#x27;: 150.71481335433086}, &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.9688725993825086, &#x27;c&#x27;: 4.846574318664137, &#x27;alpha&#x27;: 0.12037211014476513, &#x27;N0&#x27;: 464.2117601422507}, &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2450341225298993, &#x27;c&#x27;: 2.728697598311802, &#x27;alpha&#x27;: 0.07621781787029036, &#x27;N0&#x27;: 829.0089044374079}, &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.3712846154423211, &#x27;c&#x27;: 5.908144391720105, &#x27;alpha&#x27;: 0.1361472675488146, &#x27;N0&#x27;: 509.95077973946974}, &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7416998420079975, &#x27;c&#x27;: 3.3821432143495036, &#x27;alpha&#x27;: 0.09379307542347348, &#x27;N0&#x27;: 447.8764747996571}, &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1287810640169023, &#x27;c&#x27;: 3.965604717364322, &#x27;alpha&#x27;: 0.10568116035833683, &#x27;N0&#x27;: 3076.09487758598}, &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4186324900396543, &#x27;c&#x27;: 4.004193555148838, &#x27;alpha&#x27;: 0.10676480802307665, &#x27;N0&#x27;: 147.73082755798134}, &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1928341966172604, &#x27;c&#x27;: 30.064659143310216, &#x27;alpha&#x27;: 0.2591663719980929, &#x27;N0&#x27;: 6252.163345032034}, &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.47259514594176505, &#x27;c&#x27;: 41.02620921518222, &#x27;alpha&#x27;: 0.31908945776498765, &#x27;N0&#x27;: 5570.914686476627}, &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.597124361354747, &#x27;c&#x27;: 4.375526622935792, &#x27;alpha&#x27;: 0.113331946856678, &#x27;N0&#x27;: 529.9943149466569}, &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.7152739441287245, &#x27;c&#x27;: 3.5717730527435636, &#x27;alpha&#x27;: 0.09379847746856583, &#x27;N0&#x27;: 643.8465620868365}, &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4167409910882705, &#x27;c&#x27;: 1.8233794046650496, &#x27;alpha&#x27;: 0.16745997535522983, &#x27;N0&#x27;: 1.9948450922754098e-07}, &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.24720659428707983, &#x27;c&#x27;: 2.2042419001082694, &#x27;alpha&#x27;: 0.06036996753569996, &#x27;N0&#x27;: 349.6769779624643}, &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.9776244454150002, &#x27;c&#x27;: 4.108372726387373, &#x27;alpha&#x27;: 0.10572258394634945, &#x27;N0&#x27;: 685.4601409524844}, &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.5585500951317257, &#x27;c&#x27;: 2.424817826690159, &#x27;alpha&#x27;: 0.20909810514967986, &#x27;N0&#x27;: 173.8274655166467}, &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4060546165241737, &#x27;c&#x27;: 2.753567690217471, &#x27;alpha&#x27;: 0.07659591725848344, &#x27;N0&#x27;: 431.1701418362203}}
    _FALLBACK = {&#x27;L_inf&#x27;: 0.6477995401460606, &#x27;c&#x27;: 4.272555038565778, &#x27;alpha&#x27;: 0.12061630425769866, &#x27;N0&#x27;: 632.3880106663452}

    p = _PARAMS.get(group, _FALLBACK)
    out: list[dict[str, float]] = []
    for row in input_data:
        if &quot;sft_data_size&quot; not in row:
            raise KeyError(&quot;Input row missing required key: sft_data_size&quot;)
        N = float(row[&quot;sft_data_size&quot;])
        L_inf = float(p[&quot;L_inf&quot;])
        c = float(p[&quot;c&quot;])
        alpha = float(p[&quot;alpha&quot;])
        N0 = float(p[&quot;N0&quot;])
        if N0 &lt; 0.0:
            N0 = 0.0
        y = L_inf + c * (N + N0) ** (-alpha)
        out.append({&quot;sft_loss&quot;: float(y)})
    return out</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.960281 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># Auto-generated scaling law implementation for SFT loss
# Formula: sft_loss(n) = c_g + a_g * n**(-b_g)
# Where parameters (a_g, b_g, c_g) depend on the experimental group `group`.
# If an unknown group is provided, a robust fallback using median parameters across groups is used.

from typing import List, Dict

# Per-group parameters fitted on the provided dataset
PARAMS: Dict[str, Dict[str, float]] = {
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 8647.31721981341,
    &quot;b&quot;: 3.3063841377017955e-05,
    &quot;c&quot;: -8641.328598263062
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8557.877640701434,
    &quot;b&quot;: 3.078509515275162e-05,
    &quot;c&quot;: -8553.139575273637
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 5170.546672610577,
    &quot;b&quot;: 2.6711498018263207e-05,
    &quot;c&quot;: -5167.154633119724
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 7826.224298004896,
    &quot;b&quot;: 2.7124359556650597e-05,
    &quot;c&quot;: -7821.440497277876
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8415.320549742071,
    &quot;b&quot;: 2.829728260010143e-05,
    &quot;c&quot;: -8411.115753993647
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 10.059892214078516,
    &quot;b&quot;: 0.010502118801849692,
    &quot;c&quot;: -7.366858158510148
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 4167.788093140718,
    &quot;b&quot;: 2.920434619018147e-05,
    &quot;c&quot;: -4164.3273281576985
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 5644.6338439025485,
    &quot;b&quot;: 3.3778618347786454e-05,
    &quot;c&quot;: -5640.813241965669
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 3326.0456526599573,
    &quot;b&quot;: 3.0689071103015705e-05,
    &quot;c&quot;: -3323.0952003378807
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 4878.603252124826,
    &quot;b&quot;: 2.966209177782707e-05,
    &quot;c&quot;: -4874.392460064771
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8307.215165566979,
    &quot;b&quot;: 2.716869941643151e-05,
    &quot;c&quot;: -8302.996489510351
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 22.836923635993642,
    &quot;b&quot;: 0.007851507602704935,
    &quot;c&quot;: -18.856671249950633
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 8676.53254076723,
    &quot;b&quot;: 3.2147659436668996e-05,
    &quot;c&quot;: -8670.809397175384
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8403.636556708127,
    &quot;b&quot;: 3.9844941189324075e-05,
    &quot;c&quot;: -8398.225321840873
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 5.861962046164737,
    &quot;b&quot;: 0.12018623910929609,
    &quot;c&quot;: 0.2505260920524569
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 4100.869680169394,
    &quot;b&quot;: 4.833849889036928e-05,
    &quot;c&quot;: -4096.4157975816925
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8927.943733814629,
    &quot;b&quot;: 3.741431076796748e-05,
    &quot;c&quot;: -8922.700732356094
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 2.6207508297172257,
    &quot;b&quot;: 0.11520374997088315,
    &quot;c&quot;: 0.7814636074750416
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 3410.6762678503906,
    &quot;b&quot;: 2.999309327533415e-05,
    &quot;c&quot;: -3407.672624260463
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 5270.395400311453,
    &quot;b&quot;: 4.271505935321989e-05,
    &quot;c&quot;: -5266.327652830642
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 3.745411480088363,
    &quot;b&quot;: 0.02195677648506152,
    &quot;c&quot;: -1.4568822328003974
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 5319.127232480517,
    &quot;b&quot;: 2.9481823937002092e-05,
    &quot;c&quot;: -5315.030641572232
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8672.617705998246,
    &quot;b&quot;: 3.277489202411034e-05,
    &quot;c&quot;: -8667.828138426317
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 4.260560944608557,
    &quot;b&quot;: 0.035323941244454864,
    &quot;c&quot;: -1.1261246855437828
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 8.84031525707576,
    &quot;b&quot;: 0.004229085352640586,
    &quot;c&quot;: -6.628301483551928
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 825.4524052668343,
    &quot;b&quot;: 3.375864235705277e-05,
    &quot;c&quot;: -823.2897371425845
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 1.7881025825750823,
    &quot;b&quot;: 0.042208386127589134,
    &quot;c&quot;: 0.269331498642009
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 1923.9868909345048,
    &quot;b&quot;: 8.614983584285064e-05,
    &quot;c&quot;: -1919.887533567419
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 2292.138334477352,
    &quot;b&quot;: 3.593819009738095e-05,
    &quot;c&quot;: -2288.8026683660664
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 37.947268995336835,
    &quot;b&quot;: 0.005395627879697212,
    &quot;c&quot;: -34.044344515916116
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 2384.0900712105713,
    &quot;b&quot;: 4.846823372978741e-05,
    &quot;c&quot;: -2380.8616253192226
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 3286.7719399660746,
    &quot;b&quot;: 2.9014248974787798e-05,
    &quot;c&quot;: -3283.3571700176385
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 4.866217953659758,
    &quot;b&quot;: 0.048834241972019514,
    &quot;c&quot;: -1.180044505513471
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 10030.632472189145,
    &quot;b&quot;: 2.9534701000729295e-05,
    &quot;c&quot;: -10024.452312684838
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 8220.885240152042,
    &quot;b&quot;: 3.417647218553115e-05,
    &quot;c&quot;: -8216.067683072208
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 2919.6020805080443,
    &quot;b&quot;: 5.054366184556883e-05,
    &quot;c&quot;: -2916.108417313347
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 4182.118663568445,
    &quot;b&quot;: 2.802441935072382e-05,
    &quot;c&quot;: -4178.833026596572
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 1.8233793964016372,
    &quot;b&quot;: 0.16745997355463152,
    &quot;c&quot;: 0.4167409866932137
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 1958.284262865906,
    &quot;b&quot;: 3.465851622310035e-05,
    &quot;c&quot;: -1956.1518736561584
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;a&quot;: 4198.848239126039,
    &quot;b&quot;: 3.202832135414763e-05,
    &quot;c&quot;: -4195.095998292536
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;a&quot;: 1.775741756518827,
    &quot;b&quot;: 0.13439792658089325,
    &quot;c&quot;: 0.40091761886668614
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;a&quot;: 2237.846721238536,
    &quot;b&quot;: 4.047117434071229e-05,
    &quot;c&quot;: -2235.2521851047054
  }
}

# Fallback (median across fitted groups)
FALLBACK = {&#x27;a&#x27;: 3368.360960255174, &#x27;b&#x27;: 3.529835316024065e-05, &#x27;c&#x27;: -3365.383912299172}


def _predict_single(n: float, a: float, b: float, c: float) -&gt; float:
    # Guard against non-positive dataset sizes
    n = float(n)
    if not (n &gt; 0):
        n = 1.0
    return c + a * (n ** (-b))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = PARAMS.get(group, FALLBACK)
    a = float(params[&#x27;a&#x27;]); b = float(params[&#x27;b&#x27;]); c = float(params[&#x27;c&#x27;])
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&#x27;sft_data_size&#x27;, 1.0))
        y = _predict_single(n, a, b, c)
        outputs.append({&#x27;sft_loss&#x27;: float(y)})
    return outputs</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.940323 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># Auto-generated scaling law implementation
# Formula: sft_loss(N) = L_inf + k * N**(-alpha)
from __future__ import annotations

# Per-group parameters
PARAMS: dict[str, dict[str, float]] = {
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.84872226181878,
    &quot;alpha&quot;: 0.04693921400417043,
    &quot;k&quot;: 9.497535378634526
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.790564191067444,
    &quot;alpha&quot;: 0.04410150185670217,
    &quot;k&quot;: 9.105789601382336
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.405137203304502,
    &quot;alpha&quot;: 0.025030929087892846,
    &quot;k&quot;: 6.945360297551526
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.1129947434690384,
    &quot;alpha&quot;: 0.03623548015303827,
    &quot;k&quot;: 8.263467907393492
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.956587922226836,
    &quot;alpha&quot;: 0.04055923481177196,
    &quot;k&quot;: 8.628322678679417
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.5781470144051717,
    &quot;alpha&quot;: 0.017940235346751876,
    &quot;k&quot;: 6.301385169297382
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.1187838592940276,
    &quot;alpha&quot;: 0.0223244844231755,
    &quot;k&quot;: 6.695142198457086
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.6841349914388215,
    &quot;alpha&quot;: 0.03334474965323042,
    &quot;k&quot;: 7.790419125340186
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.382498520974365,
    &quot;alpha&quot;: 0.0189465581623249,
    &quot;k&quot;: 6.4145211823610495
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.7888144685116316,
    &quot;alpha&quot;: 0.02573360751945762,
    &quot;k&quot;: 7.1685528664069595
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.7707076959499517,
    &quot;alpha&quot;: 0.03862018836989847,
    &quot;k&quot;: 8.398951219616288
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.215122906865507,
    &quot;alpha&quot;: 0.029764350302425833,
    &quot;k&quot;: 7.355400097137995
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.9489908960528926,
    &quot;alpha&quot;: 0.04619962742399895,
    &quot;k&quot;: 9.28290266402412
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.9302756662008402,
    &quot;alpha&quot;: 0.05436575101488506,
    &quot;k&quot;: 10.235285784884718
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.2821754599129802,
    &quot;alpha&quot;: 0.12147501331757041,
    &quot;k&quot;: 5.829718775142702
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.1467367144185032,
    &quot;alpha&quot;: 0.034403106857395824,
    &quot;k&quot;: 7.901373817811698
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -4.042884514538057,
    &quot;alpha&quot;: 0.054442310427141344,
    &quot;k&quot;: 10.164235207037365
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.7717832947588175,
    &quot;alpha&quot;: 0.1142292263849532,
    &quot;k&quot;: 2.6257079431735018
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.328250790598852,
    &quot;alpha&quot;: 0.018987979800346985,
    &quot;k&quot;: 6.413249669199695
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.8378392874117306,
    &quot;alpha&quot;: 0.03881323039803364,
    &quot;k&quot;: 8.30245528884783
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -1.4511709120118566,
    &quot;alpha&quot;: 0.02199761766628879,
    &quot;k&quot;: 3.7398136132117465
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.0361191878532523,
    &quot;alpha&quot;: 0.02778767997253752,
    &quot;k&quot;: 7.331010326048531
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.881395290290259,
    &quot;alpha&quot;: 0.04744089080855877,
    &quot;k&quot;: 9.308918407577007
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -1.132977603708254,
    &quot;alpha&quot;: 0.035247932113577574,
    &quot;k&quot;: 4.267077557325106
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.2585008510725304,
    &quot;alpha&quot;: 0.007003789638826263,
    &quot;k&quot;: 5.474501111426712
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.212979096019364,
    &quot;alpha&quot;: 0.0054518183435738835,
    &quot;k&quot;: 5.382150698438362
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.26734817720677784,
    &quot;alpha&quot;: 0.042128965416046646,
    &quot;k&quot;: 1.7897251112906631
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.0311274006174918,
    &quot;alpha&quot;: 0.0295556688942115,
    &quot;k&quot;: 7.339597895281673
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.754965877470101,
    &quot;alpha&quot;: 0.015472046649958514,
    &quot;k&quot;: 6.144404673374608
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.6287153845576787,
    &quot;alpha&quot;: 0.03422960947337014,
    &quot;k&quot;: 7.779427664586992
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.258300157992003,
    &quot;alpha&quot;: 0.021301010777072714,
    &quot;k&quot;: 6.589549881862163
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.871218935983098,
    &quot;alpha&quot;: 0.017731461190845753,
    &quot;k&quot;: 6.361166351429151
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -1.1753529535693708,
    &quot;alpha&quot;: 0.0489005510100014,
    &quot;k&quot;: 4.861778368571223
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -2.80716580338274,
    &quot;alpha&quot;: 0.04832113090143747,
    &quot;k&quot;: 9.69977295636641
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.869819355255038,
    &quot;alpha&quot;: 0.04677581604567699,
    &quot;k&quot;: 9.324068047000075
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.402875638645481,
    &quot;alpha&quot;: 0.026644500564620817,
    &quot;k&quot;: 7.063580020109555
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.2847260558712756,
    &quot;alpha&quot;: 0.02140274300422177,
    &quot;k&quot;: 6.678646710059947
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.4127485489941094,
    &quot;alpha&quot;: 0.16582280132153118,
    &quot;k&quot;: 1.8150428474927893
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.7527934057175982,
    &quot;alpha&quot;: 0.012900547039701041,
    &quot;k&quot;: 5.920992518798672
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.022375554585,
    &quot;alpha&quot;: 0.024351213683578964,
    &quot;k&quot;: 6.916266312880347
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;L_inf&quot;: 0.40333556866832776,
    &quot;alpha&quot;: 0.13462039679218443,
    &quot;k&quot;: 1.77057749748915
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;L_inf&quot;: -3.593945383475859,
    &quot;alpha&quot;: 0.016917955818556193,
    &quot;k&quot;: 6.2521647710599755
  }
}

# Fallback parameters (median across groups) will be computed at import time
from statistics import median

_LINF = [float(v[&#x27;L_inf&#x27;]) for v in PARAMS.values()]
_K = [float(v[&#x27;k&#x27;]) for v in PARAMS.values()]
_ALPHA = [float(v[&#x27;alpha&#x27;]) for v in PARAMS.values()]
DEFAULT_PARAMS = {
    &#x27;L_inf&#x27;: median(_LINF),
    &#x27;k&#x27;: median(_K),
    &#x27;alpha&#x27;: median(_ALPHA),
}

def _predict_loss(N: float, p: dict[str, float]) -&gt; float:
    N = max(float(N), 1e-9)
    return float(p[&#x27;L_inf&#x27;] + p[&#x27;k&#x27;] * (N ** (-p[&#x27;alpha&#x27;])))

def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    p = PARAMS.get(group, DEFAULT_PARAMS)
    out: list[dict[str, float]] = []
    for row in input_data:
        if &#x27;sft_data_size&#x27; not in row:
            raise KeyError(&quot;Each input row must include &#x27;sft_data_size&#x27;.&quot;)
        N = row[&#x27;sft_data_size&#x27;]
        y = _predict_loss(N, p)
        out.append({&#x27;sft_loss&#x27;: y})
    return out</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.893357 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3">from __future__ import annotations
from typing import Dict, List

# Discovered scaling law (shared functional form across groups):
#   sft_loss = c_g + a_g * (sft_data_size) ** (-b_g)
# where parameters (a_g, b_g, c_g) depend on the experimental group g.
# The parameters below were fit via log-linear regression with an offset grid
# search for c_g on the provided dataset located at /app/data.

PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 7.465597659576915, &quot;b&quot;: 0.08938336391209903},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 6.969626540790526, &quot;b&quot;: 0.12488797569428049},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.8453873317979803, &quot;b&quot;: 0.06600249911783039},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 5.713040837756619, &quot;b&quot;: 0.078292258243444},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 6.293121529736768, &quot;b&quot;: 0.12918481189914838},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.8960123299335487, &quot;b&quot;: 0.054475442708765934},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.7633686464634066, &quot;b&quot;: 0.05245645350446028},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.857135474728793, &quot;b&quot;: 0.09575030044686336},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.1982974522931986, &quot;b&quot;: 0.0512269552341686},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.582026537858248, &quot;b&quot;: 0.051336282730936586},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 5.7873187311771686, &quot;b&quot;: 0.11196737789112272},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.508753213336002, &quot;b&quot;: 0.07041420217384924},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 7.161143632325522, &quot;b&quot;: 0.09164738334291282},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 9.29609103538407, &quot;b&quot;: 0.1581352715189433},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.2877892290738823, &quot;a&quot;: 5.831117801407008, &quot;b&quot;: 0.12183531134399322},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 5.2395659866752196, &quot;b&quot;: 0.0767344266956308},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 9.506911791025216, &quot;b&quot;: 0.16937059578718588},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.7726903703957233, &quot;a&quot;: 2.625785639789138, &quot;b&quot;: 0.11434396216041307},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.2428955974836873, &quot;b&quot;: 0.04996138964643028},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 5.693457761695681, &quot;b&quot;: 0.1182278831830303},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.3523591080867368, &quot;b&quot;: 0.04191838265818281},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.5858563671544825, &quot;b&quot;: 0.060620773494188006},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 7.476891875498462, &quot;b&quot;: 0.1403389747361836},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.250071982062786, &quot;b&quot;: 0.05575365697201484},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.2344284064674045, &quot;b&quot;: 0.019171706023120298},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.180823669815048, &quot;b&quot;: 0.014692781292003692},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.2693737538308797, &quot;a&quot;: 1.7878812167903022, &quot;b&quot;: 0.04219925650771196},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.621169816481482, &quot;b&quot;: 0.06486990722366653},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.4542367430363656, &quot;b&quot;: 0.032132757710130806},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.880269855685565, &quot;b&quot;: 0.09617770187230991},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.5193429654174033, &quot;b&quot;: 0.053677285460916976},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.5889984641635824, &quot;b&quot;: 0.03815454241188795},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.953750786401135, &quot;b&quot;: 0.07896963709680072},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 7.743248668603332, &quot;b&quot;: 0.09034898764189159},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 7.448123349328373, &quot;b&quot;: 0.13683512515646537},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.005381596620445, &quot;b&quot;: 0.06983526356123393},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 3.5927162123671033, &quot;b&quot;: 0.05385301204930487},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.4150909728394073, &quot;a&quot;: 1.8204666644793315, &quot;b&quot;: 0.16684663164101404},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.273452424763712, &quot;b&quot;: 0.04525490715783886},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 4.103162145653623, &quot;b&quot;: 0.05411455023213695},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;c&quot;: 0.4069241943999308, &quot;a&quot;: 1.7731757737185632, &quot;b&quot;: 0.13558294804986845},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;c&quot;: 0.0, &quot;a&quot;: 2.8159260594072455, &quot;b&quot;: 0.05183604484401081},
}

# Fallback parameters (global median-ish) in case an unseen group name is used.
FALLBACK = {&quot;c&quot;: 0.0, &quot;a&quot;: 4.0542718711370345, &quot;b&quot;: 0.07012473286754159}


def _predict_one(x: float, params: Dict[str, float]) -&gt; float:
    # Ensure numerical stability and monotonicity in x
    if x &lt;= 0:
        # Use tiny positive to avoid non-physical behavior
        x = 1e-12
    return params[&quot;c&quot;] + params[&quot;a&quot;] * (x ** (-params[&quot;b&quot;]))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    # Normalize group key to match our parameter dictionary keys
    key = str(group)
    params = PARAMS.get(key, FALLBACK)

    results: List[Dict[str, float]] = []
    for row in input_data:
        x = float(row.get(&quot;sft_data_size&quot;))
        yhat = _predict_one(x, params)
        results.append({&quot;sft_loss&quot;: float(yhat)})
    return results</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #daa520; color: white"> R² = 0.731620 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># Autogenerated scaling law implementation
from __future__ import annotations
from typing import List, Dict
import math
import ast

# Parameters fitted from training data
_PARAMS_BY_GROUP: Dict[str, Dict[str, float]] = {
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 6.551027034471107,
    &quot;L_inf&quot;: 1.0512777381812202,
    &quot;alpha&quot;: 0.11745762711864405,
    &quot;sse&quot;: 0.4808298153556973
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 5.765933611487401,
    &quot;L_inf&quot;: 0.10943580893255611,
    &quot;alpha&quot;: 0.10745762711864404,
    &quot;sse&quot;: 0.6245798014382321
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 3.376395921350376,
    &quot;L_inf&quot;: 0.5548627966954982,
    &quot;alpha&quot;: 0.08563389830508474,
    &quot;sse&quot;: 0.022893087373991938
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 4.952993658538474,
    &quot;L_inf&quot;: 0.7870052565309618,
    &quot;alpha&quot;: 0.09908474576271184,
    &quot;sse&quot;: 0.2400542702584885
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 5.263660623382425,
    &quot;L_inf&quot;: 0.0,
    &quot;alpha&quot;: 0.10745762711864404,
    &quot;sse&quot;: 0.44323280410802207
  },
  &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.5700770849364734,
    &quot;L_inf&quot;: 0.40185298559482835,
    &quot;alpha&quot;: 0.07011457627118642,
    &quot;sse&quot;: 0.002812062716955058
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 3.0671660328777723,
    &quot;L_inf&quot;: 0.8665890220619048,
    &quot;alpha&quot;: 0.08167457627118642,
    &quot;sse&quot;: 0.01832322318723353
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 4.476965914016248,
    &quot;L_inf&quot;: 0.21586500856117852,
    &quot;alpha&quot;: 0.09885593220338983,
    &quot;sse&quot;: 0.09055411163147822
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.6958514343065025,
    &quot;L_inf&quot;: 0.5775014790256352,
    &quot;alpha&quot;: 0.07011457627118642,
    &quot;sse&quot;: 0.011516760278950221
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 3.634706072945405,
    &quot;L_inf&quot;: 1.1111855314883685,
    &quot;alpha&quot;: 0.08029016949152541,
    &quot;sse&quot;: 0.07819663306349012
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 5.243225803366305,
    &quot;L_inf&quot;: 0.12929230405004832,
    &quot;alpha&quot;: 0.10745762711864404,
    &quot;sse&quot;: 0.22934069038976262
  },
  &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 4.028614188892544,
    &quot;L_inf&quot;: 0.724877093134493,
    &quot;alpha&quot;: 0.09885593220338983,
    &quot;sse&quot;: 0.01833369729089268
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 6.254726539846071,
    &quot;L_inf&quot;: 0.9510091039471076,
    &quot;alpha&quot;: 0.11694915254237281,
    &quot;sse&quot;: 0.2540788846983032
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 7.809486898113445,
    &quot;L_inf&quot;: 0.0,
    &quot;alpha&quot;: 0.13694915254237283,
    &quot;sse&quot;: 0.6790616782057259
  },
  &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 5.848232850566735,
    &quot;L_inf&quot;: 0.5101285027743712,
    &quot;alpha&quot;: 0.13593220338983047,
    &quot;sse&quot;: 0.046485498566010953
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 4.72749327185259,
    &quot;L_inf&quot;: 0.813263285581497,
    &quot;alpha&quot;: 0.10745762711864404,
    &quot;sse&quot;: 0.06443381176516555
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 7.9111202924205815,
    &quot;L_inf&quot;: 0.0,
    &quot;alpha&quot;: 0.14694915254237284,
    &quot;sse&quot;: 0.5847511268238709
  },
  &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.6358531287238205,
    &quot;L_inf&quot;: 0.7976718622793141,
    &quot;alpha&quot;: 0.11796610169491523,
    &quot;sse&quot;: 0.0008035057119018064
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 2.690458418113715,
    &quot;L_inf&quot;: 0.6317492094011479,
    &quot;alpha&quot;: 0.07011457627118642,
    &quot;sse&quot;: 0.00896705369896697
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 5.055017104788688,
    &quot;L_inf&quot;: 0.06216071258826933,
    &quot;alpha&quot;: 0.10745762711864405,
    &quot;sse&quot;: 0.15896734156520373
  },
  &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 1.9989082093584953,
    &quot;L_inf&quot;: 0.3883783962588203,
    &quot;alpha&quot;: 0.05463796610169492,
    &quot;sse&quot;: 0.0005966937577908683
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 3.7735300303987582,
    &quot;L_inf&quot;: 0.9038808121467481,
    &quot;alpha&quot;: 0.08563389830508474,
    &quot;sse&quot;: 0.08866213184912522
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 6.633179880504192,
    &quot;L_inf&quot;: 0.01860470970974132,
    &quot;alpha&quot;: 0.12694915254237282,
    &quot;sse&quot;: 0.349162962231544
  },
  &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.7612543989445277,
    &quot;L_inf&quot;: 0.6360779078248159,
    &quot;alpha&quot;: 0.08167457627118642,
    &quot;sse&quot;: 0.0027850673476467027
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 1.365125617665658,
    &quot;L_inf&quot;: 0.8999059285884865,
    &quot;alpha&quot;: 0.03699364406779661,
    &quot;sse&quot;: 0.0004314444689843593
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 1.2787823367249933,
    &quot;L_inf&quot;: 0.9146819209297886,
    &quot;alpha&quot;: 0.027880000000000002,
    &quot;sse&quot;: 0.0034799487006857556
  },
  &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 1.6558332978869559,
    &quot;L_inf&quot;: 0.408940745237023,
    &quot;alpha&quot;: 0.04719830508474577,
    &quot;sse&quot;: 0.0002784725035281276
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 4.023037948558858,
    &quot;L_inf&quot;: 0.9088725993825086,
    &quot;alpha&quot;: 0.09885593220338983,
    &quot;sse&quot;: 0.02482242593421794
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 2.287289667115906,
    &quot;L_inf&quot;: 1.2250341225298993,
    &quot;alpha&quot;: 0.058017288135593224,
    &quot;sse&quot;: 0.013476308002459835
  },
  &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 4.673677233571926,
    &quot;L_inf&quot;: 0.2712846154423211,
    &quot;alpha&quot;: 0.10745762711864405,
    &quot;sse&quot;: 0.035593744683382535
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 2.932396661171257,
    &quot;L_inf&quot;: 0.7555303504825739,
    &quot;alpha&quot;: 0.08167457627118642,
    &quot;sse&quot;: 0.010985283372535848
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 2.5411857996356715,
    &quot;L_inf&quot;: 1.1479844538474107,
    &quot;alpha&quot;: 0.06697525423728813,
    &quot;sse&quot;: 0.039205759001219365
  },
  &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 3.736579330087355,
    &quot;L_inf&quot;: 0.39863249003965173,
    &quot;alpha&quot;: 0.09885593220338983,
    &quot;sse&quot;: 0.005007817242997015
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 6.712728732449793,
    &quot;L_inf&quot;: 1.0928341966172603,
    &quot;alpha&quot;: 0.11745762711864405,
    &quot;sse&quot;: 0.5785501985298458
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 6.245576858034009,
    &quot;L_inf&quot;: 0.030180644744962015,
    &quot;alpha&quot;: 0.11694915254237281,
    &quot;sse&quot;: 0.4917946060647004
  },
  &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 3.5477422717951628,
    &quot;L_inf&quot;: 0.6678701240663837,
    &quot;alpha&quot;: 0.09885593220338983,
    &quot;sse&quot;: 0.023337582935558168
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 2.9579816171965967,
    &quot;L_inf&quot;: 0.6552739441287245,
    &quot;alpha&quot;: 0.07150389830508473,
    &quot;sse&quot;: 0.024602104806572565
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 1.8289527080465815,
    &quot;L_inf&quot;: 0.41334140987512596,
    &quot;alpha&quot;: 0.16694915254237283,
    &quot;sse&quot;: 0.002126327534880776
  },
  &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.020115701049203,
    &quot;L_inf&quot;: 0.27566422140104585,
    &quot;alpha&quot;: 0.05463796610169492,
    &quot;sse&quot;: 0.002761225698689312
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {
    &quot;A&quot;: 3.3885713791993206,
    &quot;L_inf&quot;: 0.8860820725336442,
    &quot;alpha&quot;: 0.08167457627118642,
    &quot;sse&quot;: 0.026660987451640007
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {
    &quot;A&quot;: 1.7756175381171875,
    &quot;L_inf&quot;: 0.41731462928597385,
    &quot;alpha&quot;: 0.13796610169491524,
    &quot;sse&quot;: 0.002043258408125567
  },
  &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {
    &quot;A&quot;: 2.4527737444128093,
    &quot;L_inf&quot;: 0.41988512499871733,
    &quot;alpha&quot;: 0.06697525423728813,
    &quot;sse&quot;: 0.00958337967879773
  }
}
_PARAMS_BY_DATASET: Dict[str, Dict[str, float]] = {
  &quot;flan&quot;: {
    &quot;A&quot;: 4.215660115624637,
    &quot;L_inf&quot;: 0.571749209401148,
    &quot;alpha&quot;: 0.08029016949152541,
    &quot;sse&quot;: 43.25799096145912
  },
  &quot;gigaword&quot;: {
    &quot;A&quot;: 4.64950666811947,
    &quot;L_inf&quot;: 0.0,
    &quot;alpha&quot;: 0.09333599576271186,
    &quot;sse&quot;: 54.41332124583563
  },
  &quot;wikiword&quot;: {
    &quot;A&quot;: 3.219329078800361,
    &quot;L_inf&quot;: 0.20103710275697806,
    &quot;alpha&quot;: 0.07011457627118642,
    &quot;sse&quot;: 15.138361813331931
  }
}
_GLOBAL_PARAMS: Dict[str, float] = {
  &quot;A&quot;: 4.1350908249146565,
  &quot;L_inf&quot;: 0.12178898498171782,
  &quot;alpha&quot;: 0.07745491525423727,
  &quot;sse&quot;: 164.34461735246174
}

def _select_params(group: str) -&gt; Dict[str, float]:
    # Exact group match first
    if group in _PARAMS_BY_GROUP:
        return _PARAMS_BY_GROUP[group]
    # Try to parse tuple-like string to extract dataset fallback
    try:
        tpl = ast.literal_eval(group)
        if isinstance(tpl, (list, tuple)) and len(tpl) &gt;= 2:
            dataset = tpl[1]
            if dataset in _PARAMS_BY_DATASET:
                return _PARAMS_BY_DATASET[dataset]
    except Exception:
        pass
    # Fallback to global parameters
    return _GLOBAL_PARAMS

def _predict_loss(n: float, params: Dict[str, float]) -&gt; float:
    # Scaling law: L(N) = L_inf + A * N^{-alpha}
    L_inf = float(params[&#x27;L_inf&#x27;])
    A = float(params[&#x27;A&#x27;])
    alpha = float(params[&#x27;alpha&#x27;])
    # Safety: ensure positive N
    n = float(n)
    if not math.isfinite(n) or n &lt;= 0:
        return float(&#x27;nan&#x27;)
    return L_inf + A * (n ** (-alpha))

def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = _select_params(group)
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = row.get(&#x27;sft_data_size&#x27;)
        y = _predict_loss(n, params)
        outputs.append({&#x27;sft_loss&#x27;: float(y)})
    return outputs</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
