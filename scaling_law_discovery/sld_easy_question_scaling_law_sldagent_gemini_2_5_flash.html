<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - U-shaped Scaling Law - SLDAgent + Gemini 2.5 Flash</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>U-shaped Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Gemini 2.5 Flash</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #d2691e; color: white"> 0.360919 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.317830</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.265633</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.360919 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Evolved to capture U-shaped/double descent patterns with an exponential sum model.
Focus on refined parameter bounds and initial guesses for numerical stability and better fit.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts brier_score (negative, more negative = better) based on log_flops.
    Models a U-shaped (convex) pattern for the negative brier score,
    characteristic of performance initially worsening with scale before improving again
    (i.e., brier score first becomes less negative, then more negative).

    The model used is: y = p0 * exp(p1 * x) + p2 * exp(p3 * x) + p4 + p5 * x
    (a sum of two exponentials, a bias, and a linear term)

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
                   log_flops: log10(FLOPs in 1E21 units).
    - params: Array of 6 parameters [p0, p1, p2, p3, p4, p5].
              If params is (T, P), it iterates through T sets of parameters.

    Returns:
    - Predicted brier_score values (negative). If T=1, returns (N,) array, else (N, T) array.
    &quot;&quot;&quot;
    X = np.asarray(data_points).flatten() # Ensure 1D log_flops array (N,)

    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :] # Ensure params is (1, P) if single set of params
    T, P = params.shape # T: number of tasks, P: number of parameters

    # Initialize prediction array
    pred = np.zeros((len(X), T))

    for t_idx in range(T):
        # Unpack the 6 parameters for the current task
        p0, p1, p2, p3, p4, p5 = params[t_idx, :]
        
        # Calculate the prediction using the evolved scaling law function
        # This form (sum of two exponentials + linear + bias) can effectively model
        # a U-shaped (convex) curve. With appropriate negative bias (p4), this forms
        # a valley in the negative Brier score space.
        pred[:, t_idx] = p0 * np.exp(p1 * X) + p2 * np.exp(p3 * X) + p4 + p5 * X
    
    # Return predictions based on the original structure (N,T) or (N,) if T=1
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func to best fit the provided data.
    Uses L-BFGS-B for bounded optimization, providing robustness for exponential terms
    and guiding the search towards the desired U-shaped (valley) pattern for negative Brier scores.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
    - loss_values: Array of corresponding brier_score values (negative). Can be (N,) or (N, T).

    Returns:
    - Optimized parameters (a 1D array of 6 parameters for a single task, or (T, 6) for multiple tasks).
    &quot;&quot;&quot;
    X = np.asarray(data_points) # (N, F) where F=1
    y = np.asarray(loss_values) # (N,) or (N, T)

    N, F = X.shape
    
    # Standardize y to (N, T) format for consistent processing
    if y.ndim == 1:
        y2d = y[:, None] # (N, 1)
    else:
        y2d = y          # (N, T)
    T = y2d.shape[1]     # Number of tasks

    P = 6 # Number of parameters for the chosen function: [p0, p1, p2, p3, p4, p5]

    # Initial guess for parameters.
    # To model a &quot;U-shape&quot; or &quot;valley&quot; for negative Brier scores (initially less negative,
    # then more negative, then less negative again):
    # - p0, p2: Positive coefficients to create the upward-sloping arms of the U-shape.
    # - p1: A positive exponent for exponential growth (right arm).
    # - p3: A negative exponent for exponential decay (left arm).
    # - p4: A negative bias term, initialized to the mean of the observed scores,
    #       to shift the U-shape downwards so its minimum is negative.
    # - p5: A small linear term for asymmetry.
    mean_y = np.mean(y)
    
    # Adjusted initial guesses for p0, p2 to be slightly larger than 0.05,
    # to allow for a more pronounced U-shape if needed, within the new tighter bounds.
    # Increased p0, p2 initial guess from 0.05 to 0.1 for potentially stronger initial exponential influence.
    init_params_for_one_task = np.array([0.1, 1.0, 0.1, -1.0, mean_y, 0.0]) # Coefficients for exponentials, exponents, bias, linear.
    
    # Replicate initial parameters for each task
    init = np.tile(init_params_for_one_task, (T, 1))

    # Bounds for parameters for robust optimization with L-BFGS-B.
    # These bounds encourage the desired U-shape (valley) for negative brier scores,
    # prevent numerical instability, and keep parameters within reasonable ranges.
    # Increased upper bounds for p0 and p2 from 0.5 to 2.0 to allow for deeper U-shapes.
    # Also extended bounds for exponents (p1, p3) and linear term (p5) to allow more flexibility.
    bounds_for_one_task = [
        (0.0, 2.0),     # p0 (coefficient for right arm): Expect positive, wider upper bound.
        (0.0, 10.0),    # p1 (exponent for right arm): Must be positive, wider upper bound for steeper growth.
        (0.0, 2.0),     # p2 (coefficient for left arm): Expect positive, wider upper bound.
        (-10.0, 0.0),   # p3 (exponent for left arm): Must be negative, wider lower bound for steeper decay.
        (-1.0, 0.0),    # p4 (bias): Brier scores are negative, so bias ensures the U-minimum is negative.
        (-2.0, 2.0)     # p5 (linear term coefficient): Allows for a larger linear tilt if needed.
    ]
    # For minimize, bounds must be a list of P*T tuples.
    bounds = bounds_for_one_task * T

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function for minimization (calculates Mean Squared Error).
        &quot;&quot;&quot;
        params = flat_params.reshape(T, P)
        pred = scaling_law_func(X, params) # (N, T) predictions
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Perform optimization using L-BFGS-B, which handles bounds effectively.
    # Added ftol and gtol for potentially earlier convergence if oscillations occur and increased maxiter.
    result = minimize(objective, init.ravel(), method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                      options={&#x27;ftol&#x27;: 1e-7, &#x27;gtol&#x27;: 1e-5, &#x27;maxiter&#x27;: 1500}) # Increased maxiter for more thorough search

    # Return optimized parameters. If optimization fails, return initial guess.
    params_opt = result.x.reshape(T, P) if result.success else init

    # Match the expected return format: (P,) for T=1, or (T, P) for T&gt;1
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.354773 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts brier_score (negative, more negative = better) based on log_flops.
    Models an inverted U-shaped pattern in the brier score (a peak), which corresponds
    to performance initially worsening with scale before improving again.

    The model used is: y = p0 * exp(-((x - p1) / p2)**2) + p3 * x**2 + p4 * x + p5
    (a Gaussian term for the peak/trough, plus a quadratic function for the overall trend and bias)

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
                   log_flops: log10(FLOPs in 1E21 units).
    - params: Array of 6 parameters [p0, p1, p2, p3, p4, p5].
              If params is (T, P), it iterates through T sets of parameters.

    Returns:
    - Predicted brier_score values (negative). If T=1, returns (N,) array, else (N, T) array.
    &quot;&quot;&quot;
    X = np.asarray(data_points).flatten() # Ensure 1D log_flops array (N,)

    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :] # Ensure params is (1, P) if single set of params
    T, P = params.shape # T: number of tasks, P: number of parameters

    # Initialize prediction array
    pred = np.zeros((len(X), T))

    for t_idx in range(T):
        # Unpack the 6 parameters for the current task
        p0, p1, p2, p3, p4, p5 = params[t_idx, :]
        
        # Ensure p2 is positive and not too small for numerical stability
        p2_safe = np.maximum(p2, 1e-6) 
        
        # Calculate the prediction using the evolved scaling law function
        # y = p0 * exp(-((x - p1) / p2)**2) + p3 * X**2 + p4 * X + p5
        pred[:, t_idx] = p0 * np.exp(-((X - p1) / p2_safe)**2) + p3 * X**2 + p4 * X + p5
    
    # Return predictions based on the original structure (N,T) or (N,) if T=1
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func to best fit the provided data.
    Uses L-BFGS-B for bounded optimization, providing robustness.
    Refined initial guesses and tighter bounds guide the optimizer towards the expected
    inverted U-shape pattern for negative brier scores.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
    - loss_values: Array of corresponding brier_score values (negative). Can be (N,) or (N, T).

    Returns:
    - Optimized parameters (a 1D array of 6 parameters for a single task, or (T, 6) for multiple tasks).
    &quot;&quot;&quot;
    X = np.asarray(data_points).flatten() # Ensure 1D log_flops array (N,)
    y = np.asarray(loss_values) # (N,) or (N, T)

    N = X.shape[0]
    
    # Standardize y to (N, T) format for consistent processing
    if y.ndim == 1:
        y2d = y[:, None] # (N, 1)
    else:
        y2d = y          # (N, T)
    T = y2d.shape[1]     # Number of tasks

    P = 6 # Number of parameters: [p0, p1, p2, p3, p4, p5]

    # Calculate initial values for adaptive initial guesses and bounds
    min_X, max_X = np.min(X), np.max(X)
    min_y, max_y = np.min(y2d), np.max(y2d) 

    # --- Adaptive Initial Guesses for Quadratic Part (p3, p4, p5) ---
    # Sort data by X to robustly select the &quot;right tail&quot; for quadratic trend estimation
    sort_idx = np.argsort(X)
    X_sorted = X[sort_idx]
    y_sorted_first_task = y2d[sort_idx, 0] # Use the first task&#x27;s data for initial guess

    # Take the upper percentage of the data to estimate the long-term trend,
    # where the Gaussian peak is assumed to have minimal influence.
    # At least 3 points are needed for a quadratic fit.
    num_points_for_poly = max(3, int(len(X_sorted) * 0.35)) 
    X_tail = X_sorted[-num_points_for_poly:]
    y_tail = y_sorted_first_task[-num_points_for_poly:]

    # Default initial guesses for quadratic part
    p3_init_quad, p4_init_quad, p5_init_quad = -0.08, -0.08, max_y - (max_y - min_y) * 0.1

    if len(X_tail) &gt;= 3:
        try:
            # Fit a quadratic: y = a*x^2 + b*x + c
            # (p3, p4, p5) corresponds to (a, b, c)
            quad_coeffs = np.polyfit(X_tail, y_tail, 2)
            p3_init_quad = quad_coeffs[0]
            p4_init_quad = quad_coeffs[1]
            p5_init_quad = quad_coeffs[2]
            
            # Heuristic: Clip p3 and p4 to ensure initial guesses are reasonable
            # A negative p3 is generally expected for improving performance with scale.
            p3_init_quad = np.clip(p3_init_quad, -0.5, 0.05) 
            p4_init_quad = np.clip(p4_init_quad, -1.0, 0.5)

        except np.linalg.LinAlgError:
            # If polyfit fails (e.g., singular matrix), fall back to defaults.
            pass

    # Initial guess for all 6 parameters, combining adaptive quadratic estimates
    # with robust estimates for the Gaussian peak.
    init_params_for_one_task = np.array([
        (max_y - min_y) * 1.2,      # p0 (Gaussian amplitude): Range of y, slightly above. Positive for peak.
        min_X + (max_X - min_X) * 0.25, # p1 (Gaussian center): Peak slightly towards lower X.
        (max_X - min_X) * 0.35,    # p2 (Gaussian width): Reasonable fraction of data range.
        p3_init_quad,              # p3 (Quadratic x^2 coeff): From polyfit or default.
        p4_init_quad,              # p4 (Linear x coeff): From polyfit or default.
        p5_init_quad               # p5 (Bias/Intercept): From polyfit or default.
    ])
    
    # Replicate initial parameters for each task
    init = np.tile(init_params_for_one_task, (T, 1))

    # Bounds for parameters for robust optimization with L-BFGS-B.
    bounds_for_one_task = [
        (0.01, -min_y * 3.0),       # p0 (amplitude): Must be positive for a peak. Max 3x magnitude of best score.
        (min_X - 0.5, max_X + 0.5), # p1 (center): Allow slightly outside the data range.
        (0.05, max_X - min_X),      # p2 (width): Must be positive, reasonable range.
        (-0.5, 0.1),                # p3 (quadratic coeff): Favors negative to aid improvement at ends.
        (-1.0, 0.5),                # p4 (linear coeff): Wider range.
        (min_y * 2.0, max_y + 1e-6) # p5 (bias): Baseline for negative brier scores.
    ]
    # For minimize, bounds must be a list of P*T tuples.
    bounds = bounds_for_one_task * T

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function for minimization (calculates Mean Squared Error).
        &quot;&quot;&quot;
        params = flat_params.reshape(T, P)
        # data_points for scaling_law_func expects (N,1) but X is (N,), so pass X[:, None]
        pred = scaling_law_func(X[:, None], params) 
        
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Perform optimization using L-BFGS-B, which handles bounds effectively.
    result = minimize(objective, init.ravel(), method=&#x27;L-BFGS-B&#x27;, bounds=bounds)

    # Return optimized parameters. If optimization fails, return initial guess.
    params_opt = result.x.reshape(T, P) if result.success else init

    # Ensure p2 remains positive (bounds should enforce this, but as a final safeguard)
    # and not too small for future predictions
    params_opt[:, 2] = np.maximum(params_opt[:, 2], 1e-6)

    # Match the expected return format: (P,) for T=1, or (T, P) for T&gt;1
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.341499 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Evolved to capture U-shaped/double descent patterns with a Gaussian peak on a quadratic background.
This approach aims to model the characteristic &quot;worsening then improving&quot; performance
on easy questions as compute scales.

The model is: y = A * exp(-((x - B)^2) / (2 * C^2)) + D * x^2 + E * x + F
- The Gaussian term (A * exp(...)) models the initial performance dip/peak (worsening).
- The quadratic term (D*x^2 + E*x + F) models the underlying scaling trend, typically
  showing improvement at higher compute.

Improvements in this evolution:
1. Numerical stability: Added explicit clipping for exponent arguments and safeguarding
   of the Gaussian width parameter &#x27;C&#x27; to prevent overflow/underflow and division by zero.
2. Refined initial guesses: Based on domain understanding and previous successful models,
   initial parameters are set to guide the optimizer towards the expected U-shaped pattern
   in negative brier score (a &quot;hill&quot; shape).
3. Tighter and more informed bounds: Optimization bounds for each parameter are adjusted
   to be more specific to the expected behavior, preventing the optimizer from exploring
   unrealistic or unstable regions of the parameter space. This includes a stronger bias
   for the quadratic coefficient (D) to be negative, supporting long-term performance improvement.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts brier_score (negative, more negative = better) based on log_flops.
    Models a characteristic &quot;hill&quot; or &quot;peak&quot; shape for negative brier_score,
    where performance initially worsens with scale (brier_score moves towards zero)
    before improving again (brier_score becomes more negative).
    
    The model used is: y = A * exp(-((x - B)^2) / (2 * C^2)) + D * x^2 + E * x + F

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
                   log_flops: log10(FLOPs in 1E21 units).
    - params: Array of 6 parameters [A, B, C, D, E, F].
              If params is (T, P), it iterates through T sets of parameters.

    Returns:
    - Predicted brier_score values (negative). If T=1, returns (N,) array, else (N, T) array.
    &quot;&quot;&quot;
    X = np.asarray(data_points, dtype=np.float64).flatten() # Ensure 1D log_flops array (N,)

    params_arr = np.asarray(params, dtype=np.float64)
    if params_arr.ndim == 1:
        params_arr = params_arr[None, :] # Ensure params_arr is (1, P) if single set of params
    T, P = params_arr.shape # T: number of tasks, P: number of parameters (must be 6)

    # Initialize prediction array
    pred = np.zeros((len(X), T), dtype=np.float64)

    # Max and min safe values for np.exp() argument to prevent overflow/underflow
    MAX_EXP_ARG = 700.0 # np.exp(709) is inf for float64
    MIN_EXP_ARG = -700.0 # np.exp(-709) is 0 for float64

    for t_idx in range(T):
        # Unpack the 6 parameters for the current task
        # A: Amplitude of Gaussian peak (positive for a peak in negative brier_score, i.e., worsening performance)
        # B: Center (mean) of Gaussian peak
        # C: Width (standard deviation) of Gaussian peak. C should be positive.
        # D: Quadratic coefficient for background
        # E: Linear coefficient for background
        # F: Constant offset for background
        A, B, C, D, E, F = params_arr[t_idx, :]
        
        # Safeguard C to be positive and not too small to prevent division by zero or numerical instability.
        C_stable = np.maximum(C, 1e-6) 
        
        # Calculate the exponent argument and clip it for numerical stability
        exponent_arg = -((X - B)**2) / (2 * C_stable**2)
        clipped_exponent_arg = np.clip(exponent_arg, MIN_EXP_ARG, 0.0) # Max value of exponent_arg is 0

        # Calculate the prediction using the evolved scaling law function
        pred[:, t_idx] = A * np.exp(clipped_exponent_arg) + D * X**2 + E * X + F
    
    # Return predictions based on the original structure (N,T) or (N,) if T=1
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func to best fit the provided data.
    Uses L-BFGS-B for bounded optimization, providing robustness for the new functional form.
    Improved with more dynamically informed initial guesses and tighter bounds based on data characteristics.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
    - loss_values: Array of corresponding brier_score values (negative). Can be (N,) or (N, T).

    Returns:
    - Optimized parameters (a 1D array of 6 parameters for a single task, or (T, 6) for multiple tasks).
    &quot;&quot;&quot;
    X = np.asarray(data_points, dtype=np.float64) # (N, F) where F=1
    y = np.asarray(loss_values, dtype=np.float64) # (N,) or (N, T)

    # Standardize y to (N, T) format for consistent processing
    if y.ndim == 1:
        y2d = y[:, None] # (N, 1)
    else:
        y2d = y          # (N, T)
    T = y2d.shape[1]     # Number of tasks

    P = 6 # Number of parameters for the chosen function: [A, B, C, D, E, F]

    # Calculate data ranges for informed initial guesses and bounds
    X_flat = X.flatten()
    y_flat = y2d.flatten() 

    min_x, max_x = np.min(X_flat), np.max(X_flat)
    x_range = max_x - min_x
    mean_x = np.mean(X_flat)
    
    # Ensure x_range is not too small to avoid issues with division by zero
    x_range_safe = max(x_range, 1e-3)
    
    min_y, max_y = np.min(y_flat), np.max(y_flat) # min_y is more negative (better), max_y is closer to 0 (worse)
    mean_y = np.mean(y_flat)

    # Initial guess for parameters [A, B, C, D, E, F]
    # These are designed to guide the optimizer towards an inverted U-shape for negative brier_score
    init_params_for_one_task = np.array([
        0.15,                               # A (amplitude of Gaussian peak): Positive for a peak in brier_score (worsening).
                                            #    Slightly increased from 0.1 in previous versions.
        (min_x + max_x) / 2,                # B (center of Gaussian peak): Roughly mid-range of log_flops.
        x_range_safe / 3.0,                 # C (width/std dev of Gaussian peak): Approx 1/3 of the log_flops range.
        -0.02,                              # D (quadratic background coefficient): Slightly more negative than -0.01,
                                            #    to encourage long-term performance improvement.
        0.0,                                # E (linear background coefficient): Start at zero.
        mean_y                              # F (constant offset): Around the average brier_score.
    ], dtype=np.float64)
    
    # Replicate initial parameters for each task
    init = np.tile(init_params_for_one_task, (T, 1))

    # Bounds for parameters for robust optimization with L-BFGS-B.
    # These bounds encourage the desired shape and prevent numerical instability.
    bounds_for_one_task = [
        (1e-4, 1.5),                            # A: Amplitude. Must be positive for a hill. Max increased to 1.5.
        (min_x - x_range_safe*0.2, max_x + x_range_safe*0.2), # B: Peak center. Allowed to be slightly outside data range.
        (0.05, x_range_safe * 2),             # C: Peak width (std dev). Must be positive. Min decreased to 0.05 for sharper peaks.
        (-0.2, 0.01),                           # D: Quadratic background coeff. More biased to negative, upper bound closer to 0.01.
        (-0.2, 0.2),                            # E: Linear background coeff. Tighter range compared to -0.5 to 0.5.
        (-1.0, -1e-4)                           # F: Constant offset. Brier scores are negative, so F must be negative. Fixed bounds.
    ]
    # For minimize, bounds must be a list of P*T tuples.
    bounds = bounds_for_one_task * T

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function for minimization (calculates Mean Squared Error).
        &quot;&quot;&quot;
        params = flat_params.reshape(T, P)
        pred = scaling_law_func(X, params) # (N, T) predictions
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Perform optimization using L-BFGS-B, which handles bounds effectively.
    result = minimize(objective, init.ravel(), method=&#x27;L-BFGS-B&#x27;, bounds=bounds)

    # Return optimized parameters. If optimization fails, return initial guess.
    params_opt = result.x.reshape(T, P) if result.success else init

    # Match the expected return format: (P,) for T=1, or (T, P) for T&gt;1
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.266326 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios.
Evolved to capture U-shaped/double descent patterns explicitly, where performance
initially worsens (brier_score becomes more negative) before improving
(brier_score becomes less negative). This means the brier_score itself should
exhibit a U-shape when plotted against log_flops.

The model used is: y = p0 * exp(p1 * x) + p2 * exp(p3 * x) + p4 + p5 * x
(a sum of two exponentials, a bias, and a linear term)
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts brier_score (negative, more negative = better) based on log_flops.
    Models a U-shaped pattern for the brier_score.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
                   log_flops: log10(FLOPs in 1E21 units).
    - params: Array of 6 parameters [p0, p1, p2, p3, p4, p5].
              If params is (T, P), it iterates through T sets of parameters.

    Returns:
    - Predicted brier_score values (negative). If T=1, returns (N,) array, else (N, T) array.
    &quot;&quot;&quot;
    X = np.asarray(data_points).flatten() # Ensure 1D log_flops array (N,)

    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :] # Ensure params is (1, P) if single set of params
    T, P = params.shape # T: number of tasks, P: number of parameters

    pred = np.zeros((len(X), T))

    for t_idx in range(T):
        # Unpack the 6 parameters for the current task
        p0, p1, p2, p3, p4, p5 = params[t_idx, :]
        
        # Calculate the prediction using the evolved scaling law function
        # For a U-shape in y (brier_score):
        # p0 &gt; 0, p1 &lt; 0 (decays from positive to 0, lifts left arm)
        # p2 &gt; 0, p3 &gt; 0 (grows from positive, lifts right arm)
        # p4 is the base (lowest/most negative point) of the U
        # p5 * x adds a slight tilt/linear trend
        pred[:, t_idx] = p0 * np.exp(p1 * X) + p2 * np.exp(p3 * X) + p4 + p5 * X
    
    # Return predictions based on the original structure (N,T) or (N,) if T=1
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func to best fit the provided data.
    Uses L-BFGS-B for bounded optimization, providing robustness for exponential terms.
    Initial guesses and bounds are dynamically set based on the observed data range
    to specifically encourage a U-shaped brier_score curve.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
    - loss_values: Array of corresponding brier_score values (negative). Can be (N,) or (N, T).

    Returns:
    - Optimized parameters (a 1D array of 6 parameters for a single task, or (T, 6) for multiple tasks).
    &quot;&quot;&quot;
    X = np.asarray(data_points) # (N, F) where F=1
    y = np.asarray(loss_values) # (N,) or (N, T)

    if y.ndim == 1:
        y2d = y[:, None] # (N, 1)
    else:
        y2d = y          # (N, T)
    T = y2d.shape[1]     # Number of tasks

    P = 6 # Number of parameters: [p0, p1, p2, p3, p4, p5]

    init_params = np.zeros((T, P))
    all_bounds = []

    # Common bounds for exponents and linear term to prevent numerical instability
    # These are general reasonable ranges for the log_flops input range [-0.9, 2.9]
    exp_decay_bounds = (-3.0, -0.1) # p1 (negative exponent for left arm decay)
    exp_growth_bounds = (0.1, 3.0)   # p3 (positive exponent for right arm growth)
    linear_bounds = (-0.1, 0.1)      # p5 (small linear trend)

    for t_idx in range(T):
        task_y = y2d[:, t_idx]
        y_min, y_max = np.min(task_y), np.max(task_y)
        y_range = y_max - y_min # Range of observed brier scores for the current task

        # --- Initial Guesses ---
        # p4 (base of U-shape): Initialized slightly below the observed minimum brier score.
        p4_init_val = y_min - y_range * 0.1

        # p0, p2 (coefficients for exponential terms): Must be positive to lift the curve.
        # Initialized proportionally to y_range to adapt to different scales of brier scores.
        coeff_init_val = max(0.01, y_range * 0.15) 

        init_params[t_idx, :] = [
            coeff_init_val,   # p0: Coefficient for the left arm exponential (decaying positive)
            -1.5,             # p1: Exponent for the left arm (negative for decay)
            coeff_init_val,   # p2: Coefficient for the right arm exponential (growing positive)
            1.5,              # p3: Exponent for the right arm (positive for growth)
            p4_init_val,      # p4: Constant bias, forms the base (minimum) of the U-shape
            0.0               # p5: Coefficient for the linear term (neutral starting point)
        ]

        # --- Bounds for L-BFGS-B ---
        # p0, p2 bounds: Must be positive. Upper bound is scaled by y_range.
        coeff_bounds_val = (1e-6, y_range * 0.8) # Ensures positive coefficients, prevents overly large terms

        # p4 bounds: Allows it to be below observed min (to form the U&#x27;s base) but not wildly negative,
        # and not positive (brier scores are negative).
        p4_bounds_val = (max(-1.0, y_min - y_range * 0.75), min(-1e-6, y_max + y_range * 0.1))

        all_bounds.extend([
            coeff_bounds_val,  # p0
            exp_decay_bounds,  # p1
            coeff_bounds_val,  # p2
            exp_growth_bounds, # p3
            p4_bounds_val,     # p4
            linear_bounds      # p5
        ])

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function for minimization (calculates Mean Squared Error).
        &quot;&quot;&quot;
        params = flat_params.reshape(T, P)
        pred = scaling_law_func(X, params) # (N, T) predictions
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Perform optimization using L-BFGS-B, which handles bounds effectively.
    result = minimize(objective, init_params.ravel(), method=&#x27;L-BFGS-B&#x27;, bounds=all_bounds)

    # Return optimized parameters. If optimization fails, return the initial guess
    # which is already structured for the desired U-shape.
    params_opt = result.x.reshape(T, P) if result.success else init_params

    # Match the expected return format: (P,) for T=1, or (T, P) for T&gt;1
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #d2691e; color: white"> R² = 0.265633 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law discovery for LLM finetuning scenarios
Evolved to capture U-shaped/double descent patterns with a Gaussian peak model.
Uses differential_evolution for robust global optimization, which is better suited
for non-linear functions with potential local minima.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Predicts brier_score (negative, more negative = better) based on log_flops.
    Models an inverted U-shaped pattern for brier score (a &quot;hill&quot;), where performance
    initially worsens (brier score becomes less negative) and then improves (more negative).
    This corresponds to a U-shaped pattern for &quot;performance&quot;.

    The model used is a Gaussian peak combined with a linear term and a bias:
    y = p0 * exp(-(X - p1)**2 / (2 * p2**2)) + p3 * X + p4

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
                   log_flops: log10(FLOPs in 1E21 units).
    - params: Array of 5 parameters [p0, p1, p2, p3, p4].
              If params is (T, P), it iterates through T sets of parameters.

    Returns:
    - Predicted brier_score values (negative). If T=1, returns (N,) array, else (N, T) array.
    &quot;&quot;&quot;
    X = np.asarray(data_points).flatten() # Ensure 1D log_flops array (N,)

    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :] # Ensure params is (1, P) if single set of params
    T, P = params.shape # T: number of tasks, P: number of parameters (should be 5)

    pred = np.zeros((len(X), T))

    for t_idx in range(T):
        p0, p1, p2, p3, p4 = params[t_idx, :]
        
        # Ensure p2 (width) is strictly positive to avoid division by zero or numerical issues.
        # It&#x27;s squared in the Gaussian, so np.abs is a safeguard for robustness,
        # but primarily enforced by bounds in fit_scaling_law.
        p2_safe = np.maximum(1e-6, p2) # p2 is expected to be positive from bounds

        # A positive p0 creates a &#x27;hill&#x27; in the negative brier score:
        # starting low (more negative), rising to a peak (less negative = worse performance),
        # then falling again (more negative = better performance).
        pred[:, t_idx] = p0 * np.exp(-(X - p1)**2 / (2 * p2_safe**2)) + p3 * X + p4
    
    return pred[:, 0] if T == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimizes the parameters for the scaling_law_func to best fit the provided data.
    Uses differential_evolution for robust global optimization, which is better suited
    for non-linear functions with potential local minima and aims to capture the
    U-shaped pattern more accurately.

    Parameters:
    - data_points: (N,1) array with columns [log_flops].
    - loss_values: Array of corresponding brier_score values (negative). Can be (N,) or (N, T).

    Returns:
    - Optimized parameters (a 1D array of 5 parameters for a single task, or (T, 5) for multiple tasks).
    &quot;&quot;&quot;
    X = np.asarray(data_points) # (N, F) where F=1
    y = np.asarray(loss_values) # (N,) or (N, T)

    # Standardize y to (N, T) format for consistent processing
    if y.ndim == 1:
        y2d = y[:, None] # (N, 1)
    else:
        y2d = y          # (N, T)
    T = y2d.shape[1]     # Number of tasks

    P = 5 # Number of parameters for the chosen function: [p0, p1, p2, p3, p4]

    min_x, max_x = X.min(), X.max()
    
    # Define bounds for a single set of parameters for the Gaussian model.
    # These bounds are chosen to guide the optimizer towards meaningful U-shaped solutions
    # for negative brier scores (where performance worsens then improves).
    # p0 (amplitude of Gaussian peak): Must be positive to create a &#x27;hill&#x27; (worsening score).
    #                                  Max 0.5 because brier scores are in range -1 to 0.
    # p1 (center of peak): Can be slightly outside the observed log_flops range to capture
    #                      U-shapes that peak early or late.
    # p2 (width of peak): Must be positive, small minimum for stability, and reasonable maximum.
    # p3 (linear term): Allows for a slight linear trend, but not to dominate the U-shape.
    # p4 (bias): Baseline brier score, typically negative.
    
    bounds_for_one_task = [
        (0.0, 0.5),                      # p0 (amplitude): Positive for &#x27;hill&#x27; (less negative brier score).
        (min_x - 0.5, max_x + 0.5),      # p1 (center): Slightly wider than data range.
        (0.01, (max_x - min_x) * 0.8),   # p2 (width): Positive, up to 80% of data range.
        (-0.2, 0.2),                     # p3 (linear term): Small linear trend.
        (-1.0, 0.0)                      # p4 (bias): Brier scores are negative, so bias must be negative or zero.
    ]
    
    # differential_evolution expects a list of (min, max) tuples for each parameter.
    all_bounds_flat = bounds_for_one_task * T

    def objective(flat_params):
        &quot;&quot;&quot;
        Objective function for minimization (calculates Mean Squared Error).
        &quot;&quot;&quot;
        params_current = flat_params.reshape(T, P)
        pred = scaling_law_func(X, params_current)
        mse = np.mean((pred - y2d) ** 2)
        return mse

    # Perform optimization using differential_evolution for global search.
    # It does not require an initial guess; it uses the bounds to initialize its population.
    # `polish=True` runs a local optimization method (like L-BFGS-B) at the end to refine the result.
    # `seed` ensures reproducibility.
    result = differential_evolution(objective, bounds=all_bounds_flat, strategy=&#x27;best1bin&#x27;,
                                    maxiter=2000, popsize=20, tol=0.001, polish=True, seed=42)

    # differential_evolution returns result.x containing the optimized parameters.
    params_opt = result.x.reshape(T, P)

    # Match the expected return format: (P,) for T=1, or (T, P) for T&gt;1
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
