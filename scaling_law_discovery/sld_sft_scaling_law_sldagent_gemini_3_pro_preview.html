<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - SFT Scaling Law - SLDAgent + Gemini 3 Pro Preview</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>SFT Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Gemini 3 Pro Preview</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.993071
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.992024</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.991557</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.993071
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Improved scaling law discovery using an Inverse Power Law: L(D) = A + 1/(B + C*D^alpha).
Key improvements:
1. Robust normalization of both inputs (data size) and outputs (loss) to ~O(1).
2. Grid-search initialization on linearized model z = 1/(y-A) to find global basin.
3. Soft-L1 loss minimization to handle outliers and noise.
4. Numerical safeguards (epsilon, bounds, absolute values) for stability.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    # data_points: (N, 1) or (N,)
    # params: (4,) or (T, 4) -&gt; [A, B, C, alpha]
    
    X = np.asarray(data_points, dtype=np.float64)
    if X.ndim == 2:
        X = X[:, 0]
    else:
        X = X.ravel()
        
    params = np.asarray(params, dtype=np.float64)
    if params.ndim == 1:
        params = params[None, :]
        
    # Extract parameters with broadcasting
    # Use abs() to ensure physical validity (decaying loss, positive constants)
    # A: Asymptote (irreducible loss)
    # B: Inverse Bias (1 / (Initial Loss - A))
    # C: Inverse Scaling factor
    # alpha: Power law exponent
    A = params[:, 0][None, :]
    B = np.abs(params[:, 1][None, :])
    C = np.abs(params[:, 2][None, :])
    alpha = np.abs(params[:, 3][None, :])
    
    X_col = X[:, None]
    
    # Model: A + 1 / (B + C * X^alpha)
    # 1e-12 epsilon prevents division by zero
    denom = B + C * (X_col ** alpha)
    pred = A + 1.0 / (denom + 1e-12)
    
    return pred[:, 0] if pred.shape[1] == 1 else pred

def fit_scaling_law(data_points, loss_values):
    # data_points: (N, 1)
    # loss_values: (N,) or (N, T)
    
    X = np.asarray(data_points, dtype=np.float64).ravel()
    Y = np.asarray(loss_values, dtype=np.float64)
    if Y.ndim == 1:
        Y = Y[:, None]
    
    N, T = Y.shape
    params_opt = np.zeros((T, 4))
    
    # Normalize X (geometric mean centers the data in log-space)
    x_min = np.min(X)
    x_max = np.max(X)
    x_scale = np.sqrt(x_min * x_max) if x_min &gt; 0 else 1.0
    X_n = X / x_scale
    
    for i in range(T):
        y = Y[:, i]
        y_min = np.min(y)
        
        # Normalize Y by minimum value (puts values in [1.0, R])
        y_scale = y_min if y_min &gt; 1e-9 else 1.0
        y_n = y / y_scale
        
        min_yn = np.min(y_n) # Should be 1.0
        
        # Residuals for normalized model
        def residuals(p, x, y_target):
            # p = [a, b, c, alpha]
            return p[0] + 1.0 / (p[1] + p[2] * (x ** p[3]) + 1e-12) - y_target
            
        # Initialization Strategy
        # Linearize: Z = 1/(y - A) = B + C * x^alpha
        best_mse = np.inf
        best_p0 = [0.0, 1.0, 1.0, 0.5] # Fallback
        
        # Grid search for A (asymptote)
        # A must be &lt; min(y). We try values relative to min(y).
        # We include 0.0 for pure power law behavior
        a_ratios = [0.0, 0.5, 0.8, 0.9, 0.95, 0.99]
        alpha_grid = [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]
        
        for r in a_ratios:
            a_try = min_yn * r
            diff = y_n - a_try
            
            mask = diff &gt; 1e-5
            if np.sum(mask) &lt; 3: continue
            
            z = 1.0 / diff[mask]
            x_sub = X_n[mask]
            
            for alf in alpha_grid:
                # Linear Regression: z = b + c * x^alpha
                u = x_sub ** alf
                
                u_mean = np.mean(u)
                z_mean = np.mean(z)
                
                cov = np.sum((u - u_mean) * (z - z_mean))
                var = np.sum((u - u_mean)**2)
                
                if var &lt; 1e-12: continue
                
                c_est = cov / var
                b_est = z_mean - c_est * u_mean
                
                # Physical constraints: C&gt;0 (decaying loss), B&gt;0 (finite initial)
                if c_est &lt;= 0: continue
                
                b_clamped = max(b_est, 1e-6)
                c_clamped = max(c_est, 1e-6)
                
                # Check MSE
                pred = a_try + 1.0 / (b_clamped + c_clamped * (x_sub ** alf) + 1e-12)
                mse = np.mean((pred - y_n[mask])**2)
                
                if mse &lt; best_mse:
                    best_mse = mse
                    best_p0 = [a_try, b_clamped, c_clamped, alf]
        
        # Constrained Optimization
        # Bounds: A &lt; min_yn, B,C,alpha &gt;= 0
        lb = [0.0, 0.0, 0.0, 0.0]
        ub = [min_yn - 1e-9, np.inf, np.inf, 5.0]
        
        p0 = np.clip(best_p0, lb, ub)
        p0[1] = max(p0[1], 1e-5)
        p0[2] = max(p0[2], 1e-5)
        
        try:
            res = least_squares(residuals, p0, args=(X_n, y_n),
                                bounds=(lb, ub),
                                method=&#x27;trf&#x27;, loss=&#x27;soft_l1&#x27;, f_scale=0.05,
                                max_nfev=500)
            p_final = res.x
        except Exception:
            p_final = p0
            
        # Denormalize parameters
        an, bn, cn, alfn = p_final
        
        A_real = an * y_scale
        B_real = bn / y_scale
        alpha_real = alfn
        
        # Safe C calculation
        try:
            scale_factor = y_scale * (x_scale ** alfn)
            C_real = cn / scale_factor
        except:
            C_real = 0.0
            
        params_opt[i] = [A_real, B_real, C_real, alpha_real]

    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.992130
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law discovery using a Separable Non-linear Least Squares (VarPro) approach.
Form: L(D) = A + B * (D/Scale + delta)^(-alpha).
Improvements:
1. Variable Projection: Optimizes nonlinear parameters (alpha, delta) while solving for linear parameters (A, B) exactly.
   This significantly reduces the search space and improves convergence robustness.
2. Input scaling (Scale=10000) for numerical stability.
3. Dense grid search initialization for nonlinear parameters.
4. Robust handling of physical constraints (B&gt;=0, alpha&gt;0, delta&gt;=0).
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

# Global scaling factor to normalize input data range (200 - 800k) to approx (0.02 - 80)
X_SCALE = 10000.0

def scaling_law_func(data_points, params):
    # data_points: (N, 1) or (N,) array
    # params: (4,) or (T, 4) array [A, B, alpha, delta]
    
    # 1. Input Handling
    X = np.asarray(data_points)
    if X.ndim == 2:
        X = X[:, 0]
    else:
        X = X.ravel()
        
    # Scale inputs
    X_s = X / X_SCALE
        
    # 2. Parameter Handling
    params = np.asarray(params)
    single_curve = False
    if params.ndim == 1:
        params = params[None, :]
        single_curve = True
    
    # Extract parameters
    # A: Asymptotic loss (can be any value)
    # B: Scaling factor (must be positive for decaying loss)
    # alpha: Decay rate (must be positive)
    # delta: Offset (must be positive)
    A = params[:, 0]
    B = np.abs(params[:, 1])
    alpha = np.abs(params[:, 2])
    delta = np.abs(params[:, 3])
    
    # 3. Computation
    # Broadcast dimensions: X is (N,), Params are (T,)
    # Result should be (N, T)
    X_col = X_s[:, None]
    A_row = A[None, :]
    B_row = B[None, :]
    alpha_row = alpha[None, :]
    delta_row = delta[None, :]
    
    # Compute power law term
    # Ensure base is positive. delta is abs(), X_s is positive.
    base = X_col + delta_row
    # Numerical safety
    base = np.maximum(base, 1e-10)
    
    term = base ** (-alpha_row)
    predictions = A_row + B_row * term
    
    if single_curve:
        return predictions[:, 0]
    return predictions

def fit_scaling_law(data_points, loss_values):
    # data_points: (N,) or (N, 1)
    # loss_values: (N,) or (N, T)
    
    X = np.asarray(data_points).ravel()
    # Pre-scale X for optimization stability
    X_s = X / X_SCALE
    
    Y = np.asarray(loss_values)
    if Y.ndim == 1:
        Y = Y[:, None]
        
    N_data, N_curves = Y.shape
    results = np.zeros((N_curves, 4))
    
    # Grid for initialization of nonlinear parameters
    # alpha: power law exponent
    grid_alpha = [0.05, 0.15, 0.3, 0.5, 0.7, 1.0, 1.5]
    # delta: offset in scaled units. 
    # 0.001 is ~10 samples, 10.0 is ~100k samples
    grid_delta = [0.0, 0.001, 0.01, 0.1, 0.5, 2.0, 10.0]
    
    for i in range(N_curves):
        yi = Y[:, i]
        
        # --- Helper: Solve Linear Parameters (A, B) given Nonlinear (alpha, delta) ---
        # Returns A, B and the residual vector
        def solve_linear_params(alpha_val, delta_val):
            # Enforce positive nonlinear params via abs
            a = np.abs(alpha_val)
            d = np.abs(delta_val)
            
            # Construct feature z = (x + delta)^-alpha
            base = np.maximum(X_s + d, 1e-10)
            z = base ** -a
            
            # Linear Regression: y = A + B*z
            # B = Cov(y,z) / Var(z)
            z_mean = np.mean(z)
            y_mean = np.mean(yi)
            
            z_centered = z - z_mean
            y_centered = yi - y_mean
            
            var_z = np.sum(z_centered**2)
            cov = np.sum(z_centered * y_centered)
            
            if var_z &lt; 1e-14:
                # Degenerate case (z is constant), fit constant A
                return y_mean, 0.0, (yi - y_mean)
                
            B_est = cov / var_z
            
            # Constraint: Scaling law must decay -&gt; B &gt;= 0
            if B_est &lt; 0:
                B_est = 0.0
                
            A_est = y_mean - B_est * z_mean
            
            residual = yi - (A_est + B_est * z)
            return A_est, B_est, residual

        # --- Residual function for Optimizer ---
        def optim_resid(p):
            # p = [alpha, delta]
            _, _, r = solve_linear_params(p[0], p[1])
            return r

        # 1. Initialization via Grid Search
        best_p_nl = [0.5, 0.0] # [alpha, delta]
        best_mse = np.inf
        
        for a_try in grid_alpha:
            for d_try in grid_delta:
                _, _, r = solve_linear_params(a_try, d_try)
                mse = np.mean(r**2)
                if mse &lt; best_mse:
                    best_mse = mse
                    best_p_nl = [a_try, d_try]
                    
        # 2. Refinement using Levenberg-Marquardt
        # Optimizes (alpha, delta), implicitly finding best (A, B) at each step.
        try:
            res = least_squares(optim_resid, best_p_nl, method=&#x27;lm&#x27;,
                                max_nfev=1000, ftol=1e-8, xtol=1e-8)
            final_alpha, final_delta = np.abs(res.x)
        except:
            final_alpha, final_delta = np.abs(best_p_nl)
            
        # 3. Final Parameter Reconstruction
        final_A, final_B, _ = solve_linear_params(final_alpha, final_delta)
        
        results[i] = [final_A, final_B, final_alpha, final_delta]

    if N_curves == 1:
        return results[0]
    return results
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.991784
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Robust scaling law discovery with multi-start Levenberg-Marquardt optimization.
Model: L(D) = A + |B| * (D + |delta|)^(-|alpha|)
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    # data_points: Input data sizes. Converted to (N, 1)
    X = np.asarray(data_points).reshape(-1, 1)
    
    # params: (T, 4) or (4,)
    P = np.asarray(params)
    is_single = (P.ndim == 1)
    if is_single:
        P = P[None, :] # (1, 4)
        
    # Parameters: A, B, alpha, delta
    # Prediction = A + |B| * (X + |delta|)^(-|alpha|)
    # Broadcasting: (N,1) and (T,) -&gt; (N,T)
    
    A = P[:, 0]
    B = P[:, 1]
    alpha = P[:, 2]
    delta = P[:, 3]
    
    term = (X + np.abs(delta)) ** (-np.abs(alpha))
    pred = A + np.abs(B) * term
    
    if is_single:
        return pred.ravel()
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points).flatten()
    Y = np.asarray(loss_values)
    if Y.ndim == 1:
        Y = Y[:, None]
        
    N, T = Y.shape
    params_opt = []
    
    def residuals(p, x, y):
        # p: (4,)
        # Returns: (N,)
        return scaling_law_func(x, p) - y

    for i in range(T):
        y_col = Y[:, i]
        ymin, ymax = np.min(y_col), np.max(y_col)
        
        # Candidate initializations to avoid local minima
        candidates = []
        
        # 1. Standard convergence: A slightly below min
        candidates.append([ymin * 0.95, ymax - ymin, 0.5, 0.0])
        
        # 2. Power law with zero asymptote (A=0)
        try:
            # log(y) ~ log(B) - alpha * log(x)
            lx, ly = np.log(X), np.log(y_col)
            # Simple linear regression
            s, intercept = np.polyfit(lx, ly, 1)
            # if s is slope, alpha = -s
            candidates.append([0.0, np.exp(intercept), -s, 0.0])
        except:
            candidates.append([0.0, 10.0, 0.5, 0.0])
            
        # 3. High offset / Pretrained-like behavior
        candidates.append([ymin * 0.8, ymax, 0.5, np.median(X)])
        
        best_p = None
        best_cost = float(&#x27;inf&#x27;)
        
        for p0 in candidates:
            try:
                # Use Levenberg-Marquardt
                res = least_squares(residuals, p0, args=(X, y_col), 
                                    method=&#x27;lm&#x27;, max_nfev=1000)
                if res.cost &lt; best_cost:
                    best_cost = res.cost
                    best_p = res.x
            except:
                pass
                
        if best_p is None:
            best_p = np.array(candidates[0])
            
        params_opt.append(best_p)
            
    res = np.array(params_opt)
    return res[0] if T == 1 else res
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.991580
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Improved scaling law discovery using a 4-parameter power law with Top-K Grid Search Initialization.
Form: L(D) = A + B * (D + delta)^(-alpha)
Optimization uses Levenberg-Marquardt (LM) on normalized data. Robustness is achieved by 
performing a comprehensive grid search over Asymptote (A) and Offset (delta), estimating 
(B, alpha) via linear regression, and refining the top 3 candidates to avoid local minima.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    # data_points: (N, 1) or (N,) array of data sizes
    # params: (4,) or (T, 4) array [A, B, alpha, delta]
    
    x = np.array(data_points, copy=False).ravel()
    p = np.array(params, copy=False)
    
    # Handle broadcasting for params
    if p.ndim == 1:
        p = p[None, :] # (1, 4)
    
    # Extract parameters
    # A: Asymptote (unconstrained)
    # B: Scale (positive)
    # alpha: Decay rate (positive)
    # delta: Data offset (positive)
    A = p[:, 0][None, :]
    B = p[:, 1][None, :]
    alpha = p[:, 2][None, :]
    delta = p[:, 3][None, :]
    
    x_col = x[:, None]
    
    # Model: L = A + |B| * (x + |delta|)^(-|alpha|)
    # Use abs() to enforce physical constraints during prediction
    # Add epsilon to base to prevent division by zero or complex numbers
    base = x_col + np.abs(delta)
    base = np.maximum(base, 1e-10)
    
    # Calculate prediction
    pred = A + np.abs(B) * (base ** (-np.abs(alpha)))
    
    # Return (N,) if single param set, else (N, T)
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    # data_points: (N, 1)
    # loss_values: (N,) or (N, T)
    
    x_raw = np.array(data_points, copy=False).ravel()
    y_raw = np.array(loss_values, copy=False)
    
    # Ensure y is 2D for consistent processing
    if y_raw.ndim == 1:
        y_raw = y_raw[:, None]
        
    N, T = y_raw.shape
    results = np.zeros((T, 4))
    
    # Normalize inputs to [0, 1] range to improve optimization conditioning
    x_max = np.max(x_raw) if x_raw.size &gt; 0 else 1.0
    x_norm = x_raw / x_max
    
    # Pre-calculate validity mask (x &gt; 0)
    x_pos_mask = x_norm &gt; 1e-9
    
    # Residual function for optimizer
    # Parameters p are: [A, B_norm, alpha, delta_norm]
    def residuals(p, x, y):
        # Matches scaling_law_func logic with abs() constraints
        return (p[0] + np.abs(p[1]) * ((x + np.abs(p[3])) ** (-np.abs(p[2])))) - y
    
    for i in range(T):
        yi = y_raw[:, i]
        min_y = np.min(yi)
        max_y = np.max(yi)
        range_y = max_y - min_y if max_y &gt; min_y else 1.0
        
        # --- Grid Search Initialization ---
        # Evaluate a grid of (A, delta) assumptions to find good basins of attraction.
        # We select the top K candidates to refine, reducing the chance of getting 
        # stuck in a local minimum (e.g., suboptimal A).
        
        candidates = []
        
        # A candidates: Scan the range below min_y, including saturation regimes
        A_grid = [
            0.0, 
            min_y * 0.5, 
            min_y * 0.9, 
            min_y * 0.99,
            min_y - 0.05 * range_y,
            min_y - 1e-4 * range_y
        ]
        
        # Delta candidates (normalized): Log-spaced from very small to large
        D_grid = [0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.0]
        
        for A_try in A_grid:
            y_shift = yi - A_try
            
            # Filter points where shifted loss is positive
            mask = (y_shift &gt; 1e-9) &amp; x_pos_mask
            
            # Need sufficient points for linear regression
            if np.sum(mask) &lt; 3:
                continue
                
            log_y = np.log(y_shift[mask])
            x_sub = x_norm[mask]
            
            for D_try in D_grid:
                # Transform x with candidate offset
                log_x = np.log(x_sub + D_try)
                
                try:
                    # Linear fit: log(y - A) = log(B) - alpha * log(x + delta)
                    slope, intercept = np.polyfit(log_x, log_y, 1)
                    
                    alpha_est = -slope
                    # Constraints: alpha must be positive and reasonable
                    if not (1e-5 &lt; alpha_est &lt; 10.0):
                        continue
                    
                    B_est = np.exp(intercept)
                    
                    # Compute MSE on full dataset for this candidate
                    # Note: Using x_norm and D_try
                    pred = A_try + B_est * ((x_norm + D_try) ** (-alpha_est))
                    mse = np.mean((pred - yi) ** 2)
                    
                    candidates.append((mse, [A_try, B_est, alpha_est, D_try]))
                        
                except Exception:
                    continue
        
        # Fallback if grid search finds nothing
        if not candidates:
            # [A, B, alpha, delta]
            candidates.append((np.inf, [min_y * 0.9, range_y, 0.5, 0.0]))
            
        # Select top 3 distinct candidates to refine
        candidates.sort(key=lambda x: x[0])
        top_k = candidates[:3]
        
        # --- Refinement ---
        best_final_mse = np.inf
        best_p_final = None
        
        for _, p0 in top_k:
            try:
                # Run optimization starting from this grid point
                res = least_squares(residuals, p0, args=(x_norm, yi), 
                                    method=&#x27;lm&#x27;, max_nfev=500, ftol=1e-8, xtol=1e-8)
                
                # Check result MSE
                mse = np.mean(res.fun**2)
                if mse &lt; best_final_mse:
                    best_final_mse = mse
                    best_p_final = res.x
            except Exception:
                continue
        
        # If all refinements failed (unlikely), take best initialization
        if best_p_final is None:
            best_p_final = top_k[0][1]
        
        # --- Denormalize Parameters ---
        # Model: A + B_n * (x_n + d_n)^-a 
        #      = A + B_n * (x/S + d_n)^-a 
        #      = A + B_n * S^a * (x + d_n*S)^-a
        A_opt = best_p_final[0]
        alpha_opt = np.abs(best_p_final[2])
        d_n = np.abs(best_p_final[3])
        B_n = np.abs(best_p_final[1])
        
        delta_real = d_n * x_max
        B_real = B_n * (x_max ** alpha_opt)
        
        results[i] = [A_opt, B_real, alpha_opt, delta_real]
        
    return results[0] if T == 1 else results
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.991557
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Robust scaling law discovery with geometric-mean normalization and grid-search initialization.
Model: L(D) = A + B * (D + delta)^(-alpha)
Features:
1. Geometric mean normalization of input D centers the data in log-space, optimal for power law fitting.
2. Comprehensive initialization grid search:
   - Scans multiple candidates for asymptote A.
   - Scans multiple candidates for offset delta (0, small, medium).
   - Uses linearized log-log regression to estimate B and alpha for each (A, delta) pair.
3. Levenberg-Marquardt optimization with internal absolute value constraints for physical plausibility.
4. Correct parameter denormalization to ensure the prediction function works on raw data.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import least_squares

def scaling_law_func(data_points, params):
    # data_points: (N, 1) or (N,) array
    # params: (4,) or (T, 4) array [A, B, alpha, delta]
    
    X = np.asarray(data_points)
    if X.ndim == 2:
        X = X[:, 0]
    else:
        X = X.ravel()
        
    p = np.asarray(params)
    if p.ndim == 1:
        p = p[None, :]
        
    # Extract parameters
    # L = A + B * (D + delta)^-alpha
    # We use abs() on B, alpha, delta to enforce physical scaling law constraints:
    # B &gt; 0: Loss decreases with data
    # alpha &gt; 0: Loss decreases with data
    # delta &gt;= 0: Positive data offset
    # A is unconstrained (usually &gt; 0, but allowed to fit noise freely)
    A = p[:, 0]
    B = np.abs(p[:, 1])
    alpha = np.abs(p[:, 2])
    delta = np.abs(p[:, 3])
    
    # Broadcast dimensions: X is (N,), Params are (T,) -&gt; Result (N, T)
    # base = x + delta
    base = X[:, None] + delta[None, :]
    
    # Numerical safety for power operation
    base = np.maximum(base, 1e-10)
    
    pred = A[None, :] + B[None, :] * (base ** -alpha[None, :])
    
    if pred.shape[1] == 1:
        return pred[:, 0]
    return pred

def fit_scaling_law(data_points, loss_values):
    X = np.asarray(data_points)
    if X.ndim == 2:
        X = X[:, 0]
    else:
        X = X.ravel()
        
    Y = np.asarray(loss_values)
    return_1d = False
    if Y.ndim == 1:
        Y = Y[:, None]
        return_1d = True
        
    N, T = Y.shape
    params_opt = np.zeros((T, 4))
    
    # Normalize X for better optimization landscape
    # Geometric mean centering is optimal for power laws in log-space
    # Shifts log(x) to be centered around 0
    log_x = np.log(np.maximum(X, 1e-9))
    x_geom = np.exp(np.mean(log_x)) if N &gt; 0 else 1.0
    X_norm = X / x_geom
    
    # Residual function for optimizer
    # p = [A, B_norm, alpha, delta_norm]
    def residual(p, x, y):
        # Mirror the logic in scaling_law_func
        term = np.abs(p[1]) * ((x + np.abs(p[3])) ** -np.abs(p[2]))
        return (p[0] + term) - y

    for i in range(T):
        yi = Y[:, i]
        ymin = np.min(yi)
        
        # --- Initialization Strategy ---
        # Grid search over A and delta to find a basin of attraction
        best_p0 = None
        best_mse = np.inf
        
        # A candidates: 0, 50%, 90%, 99% of min_loss, and slightly below min
        A_grid = [0.0, 0.5 * ymin, 0.9 * ymin, 0.99 * ymin, ymin - 1e-4]
        
        # Delta candidates (normalized scale): 0, 0.1, 1.0
        # This helps if the curve is not a straight line in log-log
        delta_grid = [0.0, 0.1, 1.0]
        
        candidates = []
        
        for A_try in A_grid:
            y_shift = yi - A_try
            mask = y_shift &gt; 1e-8
            if np.sum(mask) &lt; 3:
                continue
                
            ly = np.log(y_shift[mask])
            
            for d_try in delta_grid:
                try:
                    lx = np.log(X_norm[mask] + d_try)
                    
                    # Linear regression: log(y-A) = log(B) - alpha * log(x+delta)
                    slope, intercept = np.polyfit(lx, ly, 1)
                    
                    alpha_est = -slope
                    B_est = np.exp(intercept)
                    
                    # Basic sanity check
                    if alpha_est &gt; 0.001 and alpha_est &lt; 10.0:
                        p_cand = np.array([A_try, B_est, alpha_est, d_try])
                        
                        # Evaluate initial MSE
                        mse = np.mean(residual(p_cand, X_norm, yi)**2)
                        
                        if mse &lt; best_mse:
                            best_mse = mse
                            best_p0 = p_cand
                except:
                    pass
        
        # Fallback if grid search found nothing valid
        if best_p0 is None:
            B_guess = np.mean(yi)
            best_p0 = np.array([0.0, B_guess, 0.5, 0.0])
            
        # --- Optimization ---
        try:
            # Levenberg-Marquardt (method=&#x27;lm&#x27;) is efficient for unconstrained least squares
            res = least_squares(residual, best_p0, args=(X_norm, yi), 
                                method=&#x27;lm&#x27;, max_nfev=2000, ftol=1e-8, xtol=1e-8)
            p_fit = res.x
        except:
            p_fit = best_p0
            
        # --- Denormalization ---
        # Map back to original scale
        # Model: y = A + B_n * (x/S + d_n)^-a
        #          = A + B_n * S^a * (x + d_n*S)^-a
        
        A_final = p_fit[0]
        alpha_final = np.abs(p_fit[2])
        delta_norm = np.abs(p_fit[3])
        B_norm = np.abs(p_fit[1])
        
        B_final = B_norm * (x_geom ** alpha_final)
        delta_final = delta_norm * x_geom
        
        params_opt[i] = [A_final, B_final, alpha_final, delta_final]

    if return_1d:
        return params_opt[0]
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>