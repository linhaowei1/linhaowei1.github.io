<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - SFT Scaling Law - openhands + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>SFT Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">openhands</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">GPT-5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.980774
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.639659</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.264702</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.980774
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0">from __future__ import annotations

from typing import Dict, List

# Discovered functional form (same across groups):
#   sft_loss(N) = L_inf + A * (N + N0) ** (-alpha)
# Parameters (L_inf, A, alpha, N0) are fitted per group.

COEFS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.5135371154521521e-19, &#x27;A&#x27;: 12.637662723245858, &#x27;alpha&#x27;: 0.13564229463083571, &#x27;N0&#x27;: 3172.8234615970255},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.69370841915913439, &#x27;A&#x27;: 138.47586436118499, &#x27;alpha&#x27;: 0.43197144948922223, &#x27;N0&#x27;: 12511.93839001269},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.0206229440881137e-17, &#x27;A&#x27;: 4.2334890591214069, &#x27;alpha&#x27;: 0.074604106141066315, &#x27;N0&#x27;: 436.68578725705436},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.7711852652167247e-13, &#x27;A&#x27;: 8.9222402023374769, &#x27;alpha&#x27;: 0.11739594638060982, &#x27;N0&#x27;: 3069.4072808413994},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.49159480556550028, &#x27;A&#x27;: 53.723814153106851, &#x27;alpha&#x27;: 0.35384915515563858, &#x27;N0&#x27;: 8208.078494045174},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.9028322596153102e-12, &#x27;A&#x27;: 2.9896485354858799, &#x27;alpha&#x27;: 0.057353092134821475, &#x27;N0&#x27;: 140.71016365962777},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.9228407146983363e-23, &#x27;A&#x27;: 4.0628784034233334, &#x27;alpha&#x27;: 0.059345006379399601, &#x27;N0&#x27;: 426.03406297221312},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.773561525935189e-21, &#x27;A&#x27;: 6.3361847733287249, &#x27;alpha&#x27;: 0.11920127411802653, &#x27;N0&#x27;: 1084.1135998708885},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.517421441147456e-13, &#x27;A&#x27;: 3.4101334333949072, &#x27;alpha&#x27;: 0.056959955133795447, &#x27;N0&#x27;: 363.71063540276225},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.4271757528963296e-22, &#x27;A&#x27;: 5.319361930050178, &#x27;alpha&#x27;: 0.064500318286134922, &#x27;N0&#x27;: 1162.8526629118423},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.9096827040254358e-18, &#x27;A&#x27;: 10.792521127129737, &#x27;alpha&#x27;: 0.16678589880904315, &#x27;N0&#x27;: 2909.7266453907},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.5242433354929804e-14, &#x27;A&#x27;: 4.7563293804047371, &#x27;alpha&#x27;: 0.075206341138201682, &#x27;N0&#x27;: 197.06923273179166},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.5286971660491316e-21, &#x27;A&#x27;: 9.4669899393848862, &#x27;alpha&#x27;: 0.11633031124644934, &#x27;N0&#x27;: 1218.0919778829946},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.58946598776300896, &#x27;A&#x27;: 108.9287557326785, &#x27;alpha&#x27;: 0.41880696190951294, &#x27;N0&#x27;: 6405.9291211063764},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2241278764977892, &#x27;A&#x27;: 14.59891742710114, &#x27;alpha&#x27;: 0.296828658414929, &#x27;N0&#x27;: 550.52167029708596},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.832015840256426e-16, &#x27;A&#x27;: 5.6114061665463435, &#x27;alpha&#x27;: 0.082693064391887061, &#x27;N0&#x27;: 269.41968725510077},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.43621297514778762, &#x27;A&#x27;: 61.030549178502987, &#x27;alpha&#x27;: 0.36845014177095686, &#x27;N0&#x27;: 4178.0357004377929},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.78146336733638189, &#x27;A&#x27;: 2.6207508360795408, &#x27;alpha&#x27;: 0.11520372047236672, &#x27;N0&#x27;: 5.180818493558634e-14},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.1316583743515148e-22, &#x27;A&#x27;: 3.4371667100475456, &#x27;alpha&#x27;: 0.055193141005927544, &#x27;N0&#x27;: 323.52070958714017},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.30332705190985243, &#x27;A&#x27;: 10.781988597837117, &#x27;alpha&#x27;: 0.1955634435440943, &#x27;N0&#x27;: 1844.5396893552465},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.9141285827544074e-07, &#x27;A&#x27;: 2.3710328503714968, &#x27;alpha&#x27;: 0.042629161467379273, &#x27;N0&#x27;: 42.530108010942897},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.2674637946839605e-16, &#x27;A&#x27;: 5.6274498109833084, &#x27;alpha&#x27;: 0.078679983159364975, &#x27;N0&#x27;: 1427.2928357832131},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.31706165374107204, &#x27;A&#x27;: 21.45158456370562, &#x27;alpha&#x27;: 0.25483831221668413, &#x27;N0&#x27;: 2967.5137614439668},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.5590982935776697e-22, &#x27;A&#x27;: 3.2578144041623611, &#x27;alpha&#x27;: 0.055926984195821164, &#x27;N0&#x27;: 15.871413989109369},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.2644970195159805e-14, &#x27;A&#x27;: 2.2398829331455485, &#x27;alpha&#x27;: 0.019392179443575755, &#x27;N0&#x27;: 27.449781010659894},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.6339449437277602, &#x27;A&#x27;: 1.8526050324609111, &#x27;alpha&#x27;: 0.19214798532267319, &#x27;N0&#x27;: 5578.3067117739565},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.87971515066227735, &#x27;A&#x27;: 1.3801605110048245, &#x27;alpha&#x27;: 0.090311571163732174, &#x27;N0&#x27;: 150.71617039785826},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.2324185014386889e-13, &#x27;A&#x27;: 4.9361247204147887, &#x27;alpha&#x27;: 0.070825564721401529, &#x27;N0&#x27;: 268.26577562611078},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.4803500876947523e-18, &#x27;A&#x27;: 3.6572076010535848, &#x27;alpha&#x27;: 0.037261245000811988, &#x27;N0&#x27;: 549.53715116267517},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.3503734240167499e-20, &#x27;A&#x27;: 5.5586712508331431, &#x27;alpha&#x27;: 0.10787587016761085, &#x27;N0&#x27;: 388.26351955071812},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.7920010957389166e-18, &#x27;A&#x27;: 3.7361033508459114, &#x27;alpha&#x27;: 0.059085413754668289, &#x27;N0&#x27;: 296.79196124297391},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.8314919901603799e-18, &#x27;A&#x27;: 4.3017011973884225, &#x27;alpha&#x27;: 0.054175363187733493, &#x27;N0&#x27;: 2255.1426458690694},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.6999584666614079e-18, &#x27;A&#x27;: 4.0703650010924015, &#x27;alpha&#x27;: 0.081594995337680942, &#x27;N0&#x27;: 84.735744356004844},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.7452280964370062e-19, &#x27;A&#x27;: 14.341235592859249, &#x27;alpha&#x27;: 0.14433096493029524, &#x27;N0&#x27;: 3987.9517915485044},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.47259512781074609, &#x27;A&#x27;: 41.026206907119928, &#x27;alpha&#x27;: 0.31908945133616412, &#x27;N0&#x27;: 5570.9145376585493},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.6976723637732368e-21, &#x27;A&#x27;: 4.3889355931475347, &#x27;alpha&#x27;: 0.078088125901744368, &#x27;N0&#x27;: 365.99983215367581},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 7.7110040059956664e-14, &#x27;A&#x27;: 3.8842460457017833, &#x27;alpha&#x27;: 0.060766168148407823, &#x27;N0&#x27;: 454.69993288306603},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.4167409915611956, &#x27;A&#x27;: 1.8233794054589314, &#x27;alpha&#x27;: 0.16745997554820544, &#x27;N0&#x27;: 1.3061379770078428e-12},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.9931463428500972e-08, &#x27;A&#x27;: 2.3917549317042992, &#x27;alpha&#x27;: 0.049831237916825061, &#x27;N0&#x27;: 303.99768386012391},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.6946017191960408e-16, &#x27;A&#x27;: 4.4288718323457097, &#x27;alpha&#x27;: 0.060922238935669323, &#x27;N0&#x27;: 428.39342065201339},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.55855037746740699, &#x27;A&#x27;: 2.4248206335240212, &#x27;alpha&#x27;: 0.20909832005384368, &#x27;N0&#x27;: 173.82805296646535},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.3866123781231927e-14, &#x27;A&#x27;: 3.0054681182413341, &#x27;alpha&#x27;: 0.057697528021941544, &#x27;N0&#x27;: 352.66019690405182},
}

MEDIAN_PARAMS = {&#x27;L_inf&#x27;: 4.3943701775044804e-14, &#x27;A&#x27;: 4.4089037127466222, &#x27;alpha&#x27;: 0.082144029864784002, &#x27;N0&#x27;: 432.53960395453385}


def _predict_n(n: float, params: Dict[str, float]) -&gt; float:
    # Guard against bad inputs
    if n is None or n &lt;= 0:
        n = 1.0
    L_inf = float(params[&#x27;L_inf&#x27;])
    A = float(params[&#x27;A&#x27;])
    alpha = float(params[&#x27;alpha&#x27;])
    N0 = float(params[&#x27;N0&#x27;])
    return L_inf + A * ((n + N0) ** (-alpha))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = COEFS.get(group, MEDIAN_PARAMS)
    outputs: List[Dict[str, float]] = []
    for row in input_data:
        n = float(row.get(&#x27;sft_data_size&#x27;, 0.0))
        y = _predict_n(n, params)
        outputs.append({&#x27;sft_loss&#x27;: float(y)})
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.881513
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1">from typing import List, Dict
import math

# Discovered scaling law (shared form across groups):
#   sft_loss(N) = L_inf + A * N**(-alpha)
# Parameters (L_inf, A, alpha) are fitted per experimental group.
# Keys match the dataset&#x27;s string representation of the group.
_PARAMS: Dict[str, Dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.47411344881239503, &quot;A&quot;: 7.374808337249902, &quot;alpha&quot;: 0.10559131746215066},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.27165359489412566, &quot;A&quot;: 6.860897755882119, &quot;alpha&quot;: 0.11000429377966452},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7548157825020428, &quot;A&quot;: 3.4707438590784974, &quot;alpha&quot;: 0.10469167102305689},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.5986396543309487, &quot;A&quot;: 5.504238879900254, &quot;alpha&quot;: 0.10138910303291937},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.26571541893347317, &quot;A&quot;: 6.163958957295546, &quot;alpha&quot;: 0.11217152849430964},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8463866229511964, &quot;A&quot;: 2.3861909601450515, &quot;alpha&quot;: 0.10675039464871704},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.1532050628799038, &quot;A&quot;: 3.0593391008291815, &quot;alpha&quot;: 0.1063286666070229},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.13731630548678075, &quot;A&quot;: 4.832333492547224, &quot;alpha&quot;: 0.10312003781282773},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.9903338078822375, &quot;A&quot;: 2.5737102631364244, &quot;alpha&quot;: 0.10382388972999458},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.2655018787222039, &quot;A&quot;: 3.7596472555051346, &quot;alpha&quot;: 0.09479040630151708},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.09998631088564425, &quot;A&quot;: 5.777782392650072, &quot;alpha&quot;: 0.10645628309837579},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7785420197141477, &quot;A&quot;: 4.152352522191937, &quot;alpha&quot;: 0.10618673866174984},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.36459417457512533, &quot;A&quot;: 7.085796493184775, &quot;alpha&quot;: 0.10462298211809325},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.7686886755625248, &quot;A&quot;: 8.529332274933846, &quot;alpha&quot;: 0.11414571125661148},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.28736890285933836, &quot;A&quot;: 5.831010953341128, &quot;alpha&quot;: 0.12180825697034668},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.6377771788560049, &quot;A&quot;: 4.983169060709315, &quot;alpha&quot;: 0.1029142315676206},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.8480423693355799, &quot;A&quot;: 8.450682070471329, &quot;alpha&quot;: 0.11613923216094764},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7738835957171751, &quot;A&quot;: 2.6258922117119616, &quot;alpha&quot;: 0.11449526082979924},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.0516216885374654, &quot;A&quot;: 2.569460860025876, &quot;alpha&quot;: 0.10478186744092736},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.1170624514587777, &quot;A&quot;: 5.670422892511086, &quot;alpha&quot;: 0.11109463193971202},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.9604445677062592, &quot;A&quot;: 1.670448216347929, &quot;alpha&quot;: 0.1065558735650616},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.9725591969482548, &quot;A&quot;: 4.048032141370493, &quot;alpha&quot;: 0.09858854088474828},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.48081699720904036, &quot;A&quot;: 7.193524925842425, &quot;alpha&quot;: 0.11226593209674694},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8957143243445762, &quot;A&quot;: 2.7053104006048057, &quot;alpha&quot;: 0.10442868141102707},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.5178695146140102, &quot;A&quot;: 0.8963908163255226, &quot;alpha&quot;: 0.10330062084991393},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.6271899761579776, &quot;A&quot;: 0.7228639238613288, &quot;alpha&quot;: 0.10714822966510242},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.9976291700548863, &quot;A&quot;: 1.2701553495194946, &quot;alpha&quot;: 0.10692842591735088},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.9909552392011973, &quot;A&quot;: 4.127108041394403, &quot;alpha&quot;: 0.10765554735842911},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.738144527000284, &quot;A&quot;: 2.0894910201254104, &quot;alpha&quot;: 0.10236881852083911},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.219940755393925, &quot;A&quot;: 4.844151110122103, &quot;alpha&quot;: 0.10835078116396765},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.0582629427618657, &quot;A&quot;: 2.88488578563406, &quot;alpha&quot;: 0.10765599678024225},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.5590598696645523, &quot;A&quot;: 2.4667672986940827, &quot;alpha&quot;: 0.10397451979276673},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.4997614924770106, &quot;A&quot;: 3.764600236381742, &quot;alpha&quot;: 0.10752696949469541},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.438056337367148, &quot;A&quot;: 7.6634206051294, &quot;alpha&quot;: 0.10485804635532844},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: -0.42946933611862526, &quot;A&quot;: 7.19942996960907, &quot;alpha&quot;: 0.11212218230377852},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.721914712650132, &quot;A&quot;: 3.6805767946541, &quot;alpha&quot;: 0.10751695465396897},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 0.9738756683322746, &quot;A&quot;: 2.9858467916178224, &quot;alpha&quot;: 0.09915764181480334},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.4160154272945322, &quot;A&quot;: 1.8226518092580497, &quot;alpha&quot;: 0.16725452198815022},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8384227713655806, &quot;A&quot;: 1.7031116858599484, &quot;alpha&quot;: 0.10486402319956421},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.1429779868192445, &quot;A&quot;: 3.4039902908755177, &quot;alpha&quot;: 0.10198959701342969},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.40646821534634897, &quot;A&quot;: 1.772838135879755, &quot;alpha&quot;: 0.1354598347974606},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.8687266845909188, &quot;A&quot;: 2.2762678472908346, &quot;alpha&quot;: 0.10517519218258724},
}

# Reasonable default if an unknown group is requested (median-ish across groups)
_DEFAULT = {&quot;L_inf&quot;: 0.9, &quot;A&quot;: 3.5, &quot;alpha&quot;: 0.106}


def _get_params(group: str) -&gt; Dict[str, float]:
    # Try direct match, then a simple normalization replacing double with single quotes
    if group in _PARAMS:
        return _PARAMS[group]
    key = group.replace(&#x27;&quot;&#x27;, &quot;&#x27;&quot;)
    return _PARAMS.get(key, _DEFAULT)


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    p = _get_params(group)
    L_inf, A, alpha = p[&quot;L_inf&quot;], p[&quot;A&quot;], p[&quot;alpha&quot;]

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        N = float(row.get(&quot;sft_data_size&quot;, 0.0))
        if N &lt;= 0:
            # Fallback for non-positive sizes
            pred = float(L_inf)
        else:
            pred = L_inf + A * (N ** (-alpha))
        outputs.append({&quot;sft_loss&quot;: float(pred)})
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #DAA520; color: white;">
                        R² = 0.787239
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2">def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    The discovered scaling law for the final SFT loss as a function of the number of
    fine-tuning examples N is:
        sft_loss(N) = L_inf + A * N^(-alpha)

    The functional form is shared across groups, while (L_inf, A, alpha) depend on the
    experimental group.
    &quot;&quot;&quot;
    # Per-group parameters fitted from the provided dataset
    params = {
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 5.559881562578104e-23, &quot;A&quot;: 6.973887795673848, &quot;alpha&quot;: 0.08134157828268436},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 4.327079500922691e-14, &quot;A&quot;: 5.923748173995491, &quot;alpha&quot;: 0.10520870609722754},
        &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.012696544433358e-20, &quot;A&quot;: 3.7815959353426396, &quot;alpha&quot;: 0.06407311263806709},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 8.817175226395397e-17, &quot;A&quot;: 5.416194246903303, &quot;alpha&quot;: 0.07203581857213417},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.1212202956484596e-15, &quot;A&quot;: 5.3506885622596645, &quot;alpha&quot;: 0.10952375080346688},
        &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.7498503534003776e-14, &quot;A&quot;: 2.8815067645852346, &quot;alpha&quot;: 0.05390255280816395},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 4.4213376653851305e-14, &quot;A&quot;: 3.7266874983524167, &quot;alpha&quot;: 0.051336229389850475},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.0553780091788161e-17, &quot;A&quot;: 4.601802147881311, &quot;alpha&quot;: 0.0893794232774253},
        &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 2.5651860391815964e-23, &quot;A&quot;: 3.167052213096539, &quot;alpha&quot;: 0.050104822840154946},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.0430715861904326e-11, &quot;A&quot;: 4.497775910160822, &quot;alpha&quot;: 0.04919730153376939},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 8.299683396004994e-20, &quot;A&quot;: 5.2374267924570335, &quot;alpha&quot;: 0.10004122927271739},
        &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 5.5178220303264e-15, &quot;A&quot;: 4.456019626758435, &quot;alpha&quot;: 0.06905741523740135},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 9.967688172321561e-24, &quot;A&quot;: 6.79659479623705, &quot;alpha&quot;: 0.08550170313618997},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.1582514825954972e-14, &quot;A&quot;: 7.594470247698282, &quot;alpha&quot;: 0.1333098737650711},
        &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.2505082588152107, &quot;A&quot;: 5.8619605021880545, &quot;alpha&quot;: 0.12018517344350915},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.7722344623948618e-21, &quot;A&quot;: 5.1213143331737045, &quot;alpha&quot;: 0.07407829953498384},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.189571119296465e-16, &quot;A&quot;: 7.68391016722078, &quot;alpha&quot;: 0.14310642748853092},
        &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.7814641269275161, &quot;A&quot;: 2.620750815857531, &quot;alpha&quot;: 0.11520381374808832},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 4.412424863407121e-19, &quot;A&quot;: 3.217103535577476, &quot;alpha&quot;: 0.0490507140893502},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.946713648801423e-16, &quot;A&quot;: 5.229994536997381, &quot;alpha&quot;: 0.10805297833097156},
        &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 4.931261534682388e-14, &quot;A&quot;: 2.3491291177019287, &quot;alpha&quot;: 0.04176287734274415},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 6.169762118090173e-17, &quot;A&quot;: 4.468174332740433, &quot;alpha&quot;: 0.05761166165573999},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 8.658086070458824e-16, &quot;A&quot;: 6.499200759590741, &quot;alpha&quot;: 0.12331846053711241},
        &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 7.720579158922494e-13, &quot;A&quot;: 3.2418388182870084, &quot;alpha&quot;: 0.05546399769034253},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 7.293554654846449e-05, &quot;A&quot;: 2.2338960147401603, &quot;alpha&quot;: 0.019149109674392775},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 1.565293992962462e-13, &quot;A&quot;: 2.1770799496039057, &quot;alpha&quot;: 0.0144985269914185},
        &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 0.2693295870152577, &quot;A&quot;: 1.7881043289159282, &quot;alpha&quot;: 0.04220832008572985},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.2519378570990074e-20, &quot;A&quot;: 4.571149009162612, &quot;alpha&quot;: 0.0636171898266009},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 6.12442057141287e-13, &quot;A&quot;: 3.4396037441224725, &quot;alpha&quot;: 0.031650622094009276},
        &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.3848462896415916e-18, &quot;A&quot;: 4.734453356289708, &quot;alpha&quot;: 0.0926126263841603},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 7.833658894818595e-23, &quot;A&quot;: 3.4927985709331155, &quot;alpha&quot;: 0.05281181461038294},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.9078937035736547e-12, &quot;A&quot;: 3.5492578609621224, &quot;alpha&quot;: 0.03688126685016739},
        &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.4601392172786045e-20, &quot;A&quot;: 3.926297381838874, &quot;alpha&quot;: 0.07816268962859528},
        &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 6.881729106134914e-20, &quot;A&quot;: 7.198398075176432, &quot;alpha&quot;: 0.08172044839859696},
        &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 2.7587330526782323e-12, &quot;A&quot;: 6.339063440387514, &quot;alpha&quot;: 0.11725622939708774},
        &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 4.910092417326488e-22, &quot;A&quot;: 3.946267217667862, &quot;alpha&quot;: 0.06811657158668484},
        &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 1.9384460869442965e-21, &quot;A&quot;: 3.543052164952107, &quot;alpha&quot;: 0.05225511788632308},
        &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.41674098826209743, &quot;A&quot;: 1.8233793979734063, &quot;alpha&quot;: 0.1674599741068092},
        &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 1.9399139733409873e-15, &quot;A&quot;: 2.2598720912644974, &quot;alpha&quot;: 0.044573869804594164},
        &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&quot;L_inf&quot;: 7.821952737934886e-19, &quot;A&quot;: 4.052493003053307, &quot;alpha&quot;: 0.05269103060656371},
        &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&quot;L_inf&quot;: 0.40091826346029086, &quot;A&quot;: 1.7757420657725895, &quot;alpha&quot;: 0.13439808776804488},
        &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&quot;L_inf&quot;: 2.8846765809929266e-19, &quot;A&quot;: 2.7905737209445953, &quot;alpha&quot;: 0.05079953105765396},
    }

    # Fallbacks (if an unseen group appears): medians by task and overall
    per_task_median = {
        &#x27;flan&#x27;: {&quot;L_inf&quot;: 2.550298887010306e-19, &quot;A&quot;: 4.482975121450627, &quot;alpha&quot;: 0.05521173813306146},
        &#x27;gigaword&#x27;: {&quot;L_inf&quot;: 3.242665491759094e-14, &quot;A&quot;: 5.233710664727207, &quot;alpha&quot;: 0.10878836456721921},
        &#x27;wikiword&#x27;: {&quot;L_inf&quot;: 3.7288680018336935e-15, &quot;A&quot;: 3.2044455156917735, &quot;alpha&quot;: 0.059768555164204804},
    }
    overall_median = {&quot;L_inf&quot;: 5.802399859630123e-16, &quot;A&quot;: 3.9993801103605846, &quot;alpha&quot;: 0.06858699341204309}

    # Resolve coefficients for the requested group
    if group in params:
        p = params[group]
    else:
        # Try to parse the task name from a string like &quot;(&#x27;model&#x27;, &#x27;task&#x27;)&quot;
        task = None
        try:
            # Split by comma and take last segment, strip ) and quotes
            task = group.split(&#x27;,&#x27;)[-1].strip().strip(&#x27;) &#x27;).strip(&quot;&#x27;&quot;)
        except Exception:
            task = None
        p = per_task_median.get(task, overall_median)

    L_inf = float(p[&quot;L_inf&quot;])  # asymptotic loss floor
    A = float(p[&quot;A&quot;])          # amplitude
    alpha = float(p[&quot;alpha&quot;])  # scaling exponent

    out: List[Dict[str, float]] = []
    for item in input_data:
        N = float(item.get(&#x27;sft_data_size&#x27;, 0.0))
        if N &lt;= 0:
            # Graceful fallback: use minimal positive value to avoid division by zero
            N = 1.0
        pred = L_inf + A * (N ** (-alpha))
        out.append({&#x27;sft_loss&#x27;: float(pred)})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.284069
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3">from __future__ import annotations
from typing import List, Dict

# Parameters fitted per experimental group for the scaling law
# L(N) = L_inf + A * (N + N0)^(-alpha)
GROUP_PARAMS = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: (1.74334372845e-14, 12.6376966889, 0.135642527036, 3172.84813899),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.693703651144, 138.472285153, 0.431968782432, 12511.858243),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: (1.01586399934e-16, 4.2334921402, 0.0746041722245, 436.68969118),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: (1.19409314659e-18, 8.92224180358, 0.117395961813, 3069.40919683),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: (0.491595039277, 53.7238611705, 0.353849253703, 8208.08072935),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: (5.27990777079e-16, 2.98964855216, 0.0573530926488, 140.710189355),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: (3.76889628326e-16, 4.06287843965, 0.0593450071786, 426.034125852),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: (6.18853897564e-19, 6.33621255116, 0.119201667319, 1084.13661129),
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: (8.67298183717e-15, 3.41013139136, 0.0569598662977, 363.706821923),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: (5.61099380572e-23, 4.09129228086, 0.0655956807586, 381.324214495),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: (2.33071354039e-21, 6.20496986049, 0.0898142124765, 421.971295556),
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: (7.17491357681e-12, 2.97543402284, 0.10948596302, 1540.59235749),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: (4.59171206171e-23, 3.92826799844, 0.0572921215978, 513.889289197),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.331145263898, 1.80249328365, 0.161681097935, 1.38996740228e-08),
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: (7.54240118853e-08, 2.40134088564, 0.046208695099, 368.565361701),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: (5.08695784056e-19, 3.85220096633, 0.053905137389, 522.232192014),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: (0.333615694269, 2.10767061513, 0.152700192794, 2.67294535273e-08),
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: (0.0102249525968, 2.66899639933, 0.0733424237895, 125.118536989),
    &quot;(&#x27;google/flan-t5-base&#x27;, &#x27;flan&#x27;)&quot;: (4.91247757583e-23, 3.60117304024, 0.0572465810795, 417.875313569),
    &quot;(&#x27;google/flan-t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.405005274224, 1.61373265073, 0.164013157084, 3.1548076111e-09),
    &quot;(&#x27;google/flan-t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (9.15667282415e-06, 2.32680485136, 0.0503996647358, 283.080905921),
    &quot;(&#x27;google/flan-t5-small&#x27;, &#x27;flan&#x27;)&quot;: (1.00714948887e-18, 3.83772842519, 0.0591848781206, 455.282652859),
    &quot;(&#x27;google/flan-t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: (0.535288121688, 2.08203051743, 0.178595557119, 22.4200612776),
    &quot;(&#x27;google/flan-t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: (1.03689287158e-14, 2.85083455163, 0.056333131327, 301.738465789),
    &quot;(&#x27;google/gemma-1.1-2b-it&#x27;, &#x27;flan&#x27;)&quot;: (6.8915143895e-21, 3.00848271538, 0.0480777398665, 390.795415165),
    &quot;(&#x27;google/gemma-1.1-2b-it&#x27;, &#x27;gigaword&#x27;)&quot;: (0.625782070821, 1.82234052821, 0.175295164625, 1.42379766592e-08),
    &quot;(&#x27;google/gemma-1.1-2b-it&#x27;, &#x27;wikiword&#x27;)&quot;: (2.05488645576e-08, 2.04820492761, 0.0343357892473, 304.231733022),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-alpha&#x27;, &#x27;flan&#x27;)&quot;: (4.9049381785e-22, 2.43999566801, 0.0515771748292, 215.32550852),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-alpha&#x27;, &#x27;gigaword&#x27;)&quot;: (0.663107576466, 2.05155751097, 0.187721428568, 26.8042322803),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-alpha&#x27;, &#x27;wikiword&#x27;)&quot;: (4.12506384686e-15, 1.75811857156, 0.04720378107, 278.035272188),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-beta&#x27;, &#x27;flan&#x27;)&quot;: (3.09249230834e-20, 1.99774222836, 0.0462512339271, 212.068394081),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-beta&#x27;, &#x27;gigaword&#x27;)&quot;: (0.645108253463, 1.31823579619, 0.185613813767, 1.64484516925e-07),
    &quot;(&#x27;HuggingFaceH4/zephyr-7b-beta&#x27;, &#x27;wikiword&#x27;)&quot;: (3.41599139834e-18, 1.50808886401, 0.0431953430781, 270.828819886),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan+synthetic&#x27;)&quot;: (5.48508537431e-09, 3.44038957741, 0.10143885973, 522.878784361),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword+synthetic&#x27;)&quot;: (0.6503504776, 43.3219835398, 0.388166622966, 6800.69906577),
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword+synthetic&#x27;)&quot;: (7.2807427126e-09, 2.37643665333, 0.0904549693201, 583.1756373),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan+synthetic&#x27;)&quot;: (3.14075461673e-10, 3.34192480141, 0.0956885199672, 644.350510751),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword+synthetic&#x27;)&quot;: (0.487120603132, 25.5290146884, 0.295662400974, 4101.34778253),
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword+synthetic&#x27;)&quot;: (1.61549400971e-08, 2.19369617312, 0.0864794164689, 589.042812831),
    &quot;(&#x27;meta-llama/Llama-2-7b-chat-hf&#x27;, &#x27;flan&#x27;)&quot;: (3.82222214223e-10, 2.17654619575, 0.0736122258328, 316.310451114),
    &quot;(&#x27;meta-llama/Llama-2-7b-chat-hf&#x27;, &#x27;gigaword&#x27;)&quot;: (0.65265599579, 1.61221334699, 0.203963866387, 7.77256658209e-08),
    &quot;(&#x27;meta-llama/Llama-2-7b-chat-hf&#x27;, &#x27;wikiword&#x27;)&quot;: (1.10820500056e-16, 1.42413350177, 0.0494370037209, 181.81689969),
    &quot;(&#x27;openchat/openchat_3.5&#x27;, &#x27;flan&#x27;)&quot;: (7.48101007959e-13, 2.14041757993, 0.0610714316899, 291.439271718),
    &quot;(&#x27;openchat/openchat_3.5&#x27;, &#x27;gigaword&#x27;)&quot;: (0.442685808441, 1.59285815147, 0.190752780851, 62.6334333168),
    &quot;(&#x27;openchat/openchat_3.5&#x27;, &#x27;wikiword&#x27;)&quot;: (1.608493018e-14, 1.44339217538, 0.0506461652539, 231.637812601),
    &quot;(&#x27;Qwen/Qwen1.5-1.8B-Chat&#x27;, &#x27;flan&#x27;)&quot;: (6.27129779661e-22, 2.42847553091, 0.057062250832, 313.402069889),
    &quot;(&#x27;Qwen/Qwen1.5-1.8B-Chat&#x27;, &#x27;gigaword&#x27;)&quot;: (0.672573369247, 1.68982128645, 0.195871473078, 1.58312141503e-07),
    &quot;(&#x27;Qwen/Qwen1.5-1.8B-Chat&#x27;, &#x27;wikiword&#x27;)&quot;: (1.15387868584e-16, 1.78180769947, 0.0558031745887, 246.147766024),
    &quot;(&#x27;Qwen/Qwen1.5-7B-Chat&#x27;, &#x27;flan&#x27;)&quot;: (2.41403848954e-21, 1.7337576363, 0.0522215109255, 287.46828319),
    &quot;(&#x27;Qwen/Qwen1.5-7B-Chat&#x27;, &#x27;gigaword&#x27;)&quot;: (0.701829784636, 1.54187766402, 0.206394932847, 3.91240722118e-08),
    &quot;(&#x27;Qwen/Qwen1.5-7B-Chat&#x27;, &#x27;wikiword&#x27;)&quot;: (5.58834583656e-19, 1.35838705992, 0.0471114172229, 255.717687844),
    &quot;(&#x27;tiiuae/falcon-1b&#x27;, &#x27;flan&#x27;)&quot;: (2.93496793134e-17, 4.38619638046, 0.0803432225412, 635.63611150),
    &quot;(&#x27;tiiuae/falcon-1b&#x27;, &#x27;gigaword&#x27;)&quot;: (0.642779525838, 7.68985835857, 0.246590363741, 1906.41074333),
    &quot;(&#x27;tiiuae/falcon-1b&#x27;, &#x27;wikiword&#x27;)&quot;: (3.18649499919e-12, 3.10653686584, 0.103057831142, 1573.84972758),
    &quot;(&#x27;tiiuae/falcon-7b-instruct&#x27;, &#x27;flan&#x27;)&quot;: (2.60111608485e-16, 2.22355106974, 0.0722324800102, 397.155408139),
    &quot;(&#x27;tiiuae/falcon-7b-instruct&#x27;, &#x27;gigaword&#x27;)&quot;: (0.669375089848, 1.61392768813, 0.203131910042, 2.41640431048e-08),
    &quot;(&#x27;tiiuae/falcon-7b-instruct&#x27;, &#x27;wikiword&#x27;)&quot;: (1.6813344452e-18, 1.29755740218, 0.0449371852927, 230.006648709),
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: (1.20686838657e-20, 3.57216973557, 0.0528291592115, 454.455171846),
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: (0.416740988169, 1.82337939819, 0.167459974094, 5.3249802882e-09),
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: (8.88575837592e-06, 2.39174846722, 0.0498315754901, 304.000915976),
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: (1.04548316353e-21, 4.42886795655, 0.0609221599197, 428.387583993),
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: (0.558546921255, 2.42478473544, 0.209095628426, 173.820139152),
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: (1.89469511102e-15, 3.00546805873, 0.0576975262125, 352.660073561),
}

# Robust fallback parameters (median across groups), used if an unknown group is requested
FALLBACK_PARAMS = (4.73642516861e-15, 4.40890174984, 0.0821440711414, 432.538637586)


def _predict_loss(N: float, params: tuple[float, float, float, float]) -&gt; float:
    L0, A, alpha, N0 = params
    x = float(N) + float(N0)
    if x &lt; 1e-9:
        x = 1e-9
    return float(L0) + float(A) * (x ** (-float(alpha)))


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    params = GROUP_PARAMS.get(group, FALLBACK_PARAMS)
    return [{&quot;sft_loss&quot;: _predict_loss(row.get(&quot;sft_data_size&quot;, 0.0), params)} for row in input_data]</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #D2691E; color: white;">
                        R² = 0.264702
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4">from __future__ import annotations
import math, re
from typing import List, Dict

# Coefficients fitted on the provided dataset using the law:
# L(N) = L_inf + k * N**(-alpha)
# Functional form is shared across groups; only coefficients differ.

COEFFS: dict[str, dict[str, float]] = {
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.32602981455e-19, &#x27;k&#x27;: 6.97388770412, &#x27;alpha&#x27;: 0.0813415766836},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 9.01714251449e-18, &#x27;k&#x27;: 5.9237450148, &#x27;alpha&#x27;: 0.105208639131},
    &quot;(&#x27;MBZUAI/LaMini-GPT-124M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.97921363876e-13, &#x27;k&#x27;: 3.78159598424, &#x27;alpha&#x27;: 0.0640731141802},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.75683775834e-22, &#x27;k&#x27;: 5.41619406169, &#x27;alpha&#x27;: 0.0720358144505},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.59755209321e-16, &#x27;k&#x27;: 5.35068812271, &#x27;alpha&#x27;: 0.109523740458},
    &quot;(&#x27;MBZUAI/LaMini-GPT-774M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.7406071092e-16, &#x27;k&#x27;: 2.88150676493, &#x27;alpha&#x27;: 0.0539025528231},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.07589754583e-15, &#x27;k&#x27;: 3.7266874915, &#x27;alpha&#x27;: 0.0513362291721},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.33979003212e-22, &#x27;k&#x27;: 4.60180205858, &#x27;alpha&#x27;: 0.0893794208949},
    &quot;(&#x27;cerebras/Cerebras-GPT-1.3B&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.08399294226e-13, &#x27;k&#x27;: 3.16705223304, &#x27;alpha&#x27;: 0.0501048235748},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 5.00467018061e-20, &#x27;k&#x27;: 4.49777573647, &#x27;alpha&#x27;: 0.0491972970206},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.66802653456e-13, &#x27;k&#x27;: 5.23742845695, &#x27;alpha&#x27;: 0.100041268932},
    &quot;(&#x27;cerebras/Cerebras-GPT-256M&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.44788262041e-24, &#x27;k&#x27;: 4.45601962703, &#x27;alpha&#x27;: 0.0690574152446},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.81763239739e-24, &#x27;k&#x27;: 6.79659479072, &#x27;alpha&#x27;: 0.0855017030342},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.89609277179e-20, &#x27;k&#x27;: 7.594466362, &#x27;alpha&#x27;: 0.133309807287},
    &quot;(&#x27;facebook/bart-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.250521154378, &#x27;k&#x27;: 5.86196164531, &#x27;alpha&#x27;: 0.120185944723},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.69858816566e-15, &#x27;k&#x27;: 5.12131355301, &#x27;alpha&#x27;: 0.0740782957819},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.7736342501e-13, &#x27;k&#x27;: 7.68393185227, &#x27;alpha&#x27;: 0.14310680226},
    &quot;(&#x27;facebook/bart-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.781463763912, &#x27;k&#x27;: 2.62075089349, &#x27;alpha&#x27;: 0.115203588592},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.11456520733e-15, &#x27;k&#x27;: 3.21710442803, &#x27;alpha&#x27;: 0.0490513209021},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.28428679752e-24, &#x27;k&#x27;: 5.22999701626, &#x27;alpha&#x27;: 0.108052731066},
    &quot;(&#x27;facebook/opt-1.3b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.39484227831e-17, &#x27;k&#x27;: 2.34912920881, &#x27;alpha&#x27;: 0.0417630255941},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.12896705802e-13, &#x27;k&#x27;: 4.46817476021, &#x27;alpha&#x27;: 0.0576120681765},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.44205901232e-14, &#x27;k&#x27;: 6.49920240563, &#x27;alpha&#x27;: 0.123318370565},
    &quot;(&#x27;facebook/opt-350m&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.16479244482e-17, &#x27;k&#x27;: 3.24183866898, &#x27;alpha&#x27;: 0.0554642626476},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.21510504514e-18, &#x27;k&#x27;: 2.23396791349, &#x27;alpha&#x27;: 0.0191484545848},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 6.16775164569e-13, &#x27;k&#x27;: 2.17707962767, &#x27;alpha&#x27;: 0.0144989474484},
    &quot;(&#x27;facebook/opt-6.7b&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.175919014957, &#x27;k&#x27;: 2.74005136883, &#x27;alpha&#x27;: 0.10447327565},
    &quot;(&#x27;gpt2&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.5e-323, &#x27;k&#x27;: 3.72739393067, &#x27;alpha&#x27;: 0.0547825189783},
    &quot;(&#x27;gpt2&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.36550147093e-24, &#x27;k&#x27;: 4.77140298652, &#x27;alpha&#x27;: 0.0885343305552},
    &quot;(&#x27;gpt2&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.11505657262e-18, &#x27;k&#x27;: 2.47064530041, &#x27;alpha&#x27;: 0.0392326880497},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.85373464182e-17, &#x27;k&#x27;: 6.32490044618, &#x27;alpha&#x27;: 0.10705970048},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.09467361407e-22, &#x27;k&#x27;: 7.23146392424, &#x27;alpha&#x27;: 0.134074989853},
    &quot;(&#x27;google/mt5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0060702888029, &#x27;k&#x27;: 3.91259799635, &#x27;alpha&#x27;: 0.0828354050287},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 8.78500761998e-21, &#x27;k&#x27;: 5.90447421308, &#x27;alpha&#x27;: 0.0953409619567},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 4.07696061715e-16, &#x27;k&#x27;: 7.11451773393, &#x27;alpha&#x27;: 0.136344443788},
    &quot;(&#x27;google/mt5-large&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.335229341613, &#x27;k&#x27;: 2.54590405026, &#x27;alpha&#x27;: 0.103779566203},
    &quot;(&#x27;t5-base&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 2.6467349184e-20, &#x27;k&#x27;: 6.08769684037, &#x27;alpha&#x27;: 0.0932843181384},
    &quot;(&#x27;t5-base&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.68496645579e-20, &#x27;k&#x27;: 6.8125614773, &#x27;alpha&#x27;: 0.116536448907},
    &quot;(&#x27;t5-base&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 0.0, &#x27;k&#x27;: 4.67827633118, &#x27;alpha&#x27;: 0.114076823388},
    &quot;(&#x27;t5-small&#x27;, &#x27;flan&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.35247273446e-21, &#x27;k&#x27;: 7.06663234781, &#x27;alpha&#x27;: 0.121259374307},
    &quot;(&#x27;t5-small&#x27;, &#x27;gigaword&#x27;)&quot;: {&#x27;L_inf&#x27;: 1.19919213784e-19, &#x27;k&#x27;: 7.38374240969, &#x27;alpha&#x27;: 0.150851024609},
    &quot;(&#x27;t5-small&#x27;, &#x27;wikiword&#x27;)&quot;: {&#x27;L_inf&#x27;: 3.28440821836e-21, &#x27;k&#x27;: 5.01746548505, &#x27;alpha&#x27;: 0.109593599131},
}

MEDIAN_BY_DATASET: dict[str, dict[str, float]] = {
    &#x27;flan&#x27;: {&#x27;L_inf&#x27;: 1.26146861678e-15, &#x27;k&#x27;: 4.48297517797, &#x27;alpha&#x27;: 0.0552117421974},
    &#x27;gigaword&#x27;: {&#x27;L_inf&#x27;: 1.34386175918e-16, &#x27;k&#x27;: 5.23371253922, &#x27;alpha&#x27;: 0.108788384497},
    &#x27;wikiword&#x27;: {&#x27;L_inf&#x27;: 2.53160329051e-13, &#x27;k&#x27;: 3.20444552644, &#x27;alpha&#x27;: 0.0597685559862},
}

GLOBAL_MEDIAN = {&#x27;L_inf&#x27;: 1.29103697185e-15, &#x27;k&#x27;: 3.99938013481, &#x27;alpha&#x27;: 0.0685869934535}

def _predict_one(N: float, coeffs: dict[str, float]) -&gt; float:
    if N &lt;= 0:
        N = 1.0
    L_inf = float(coeffs[&#x27;L_inf&#x27;])
    k = float(coeffs[&#x27;k&#x27;])
    alpha = float(coeffs[&#x27;alpha&#x27;])
    return max(0.0, L_inf + k * (N ** (-alpha)))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    coeffs = COEFFS.get(group)
    if coeffs is None:
        m = re.search(r&quot;, &#x27;([^&#x27;]+)&#x27;\)$&quot;, str(group))
        if m:
            ds = m.group(1)
            coeffs = MEDIAN_BY_DATASET.get(ds, GLOBAL_MEDIAN)
        else:
            coeffs = GLOBAL_MEDIAN
    out = []
    for row in input_data:
        N = float(row.get(&#x27;sft_data_size&#x27;, row.get(&#x27;N&#x27;, 0.0)))
        y = _predict_one(N, coeffs)
        out.append({&#x27;sft_loss&#x27;: float(y)})
    return out</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>