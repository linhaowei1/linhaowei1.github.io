<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - Domain Mixture Scaling Law - opencode + GPT-5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>Domain Mixture Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">opencode</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">GPT-5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.989423
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.960246</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.899201</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.989423
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0">from __future__ import annotations

import math
from typing import List, Dict

# Fixed feature ordering
PROP_COLS = [
    &quot;proportion_domain_1&quot;,
    &quot;proportion_domain_2&quot;,
    &quot;proportion_domain_3&quot;,
    &quot;proportion_domain_4&quot;,
    &quot;proportion_domain_5&quot;,
]
LOSS_COLS = [
    &quot;loss_domain_1&quot;,
    &quot;loss_domain_2&quot;,
    &quot;loss_domain_3&quot;,
    &quot;loss_domain_4&quot;,
    &quot;loss_domain_5&quot;,
]

EPS = 1e-6

# Coefficients learned per group for the law:
# loss_i = intercept[i] + sum_j coef_linear[i][j] * p_j + sum_j coef_log[i][j] * log(p_j + EPS)
COEFFS: Dict[str, Dict[str, list]] = {
    &quot;160M&quot;: {
        &quot;intercept&quot;: [
            2.469311683337708,
            3.3141620411008277,
            2.5975875154705848,
            1.3440867180535057,
            3.2488739962835567,
        ],
        &quot;coef_linear&quot;: [
            [-0.39242870031019833, 0.1449840040105368, 0.20870621607378334, 0.012988956774962533, 0.02574952345091407],
            [0.2073851738757125, -0.5034944849958123, 0.08099673956846185, 0.2119097462425706, 0.0032028253090607807],
            [0.4222507474605135, 0.33048349239799873, -1.3184032511886987, 0.3397661471062194, 0.22590286422400102],
            [0.1075280031010361, 0.3278752202366596, 0.018326424467131473, -0.5403846909284411, 0.08665504312361116],
            [0.1224578633506513, -0.06992992306569604, 0.0648541522733341, 0.08654508830086936, -0.20392718085915945],
        ],
        &quot;coef_log&quot;: [
            [-0.039451752022555374, -0.0003854857497984469, -2.3239743517545694e-05, 9.268231255609287e-06, -0.0006293642768779598],
            [-0.0015843455126219829, -0.00597505925571199, -0.00010878237745062993, -0.0007202157067082326, -0.0012285972839189082],
            [-0.0009734332588850447, -0.001936822498506686, -0.027443305577813045, -0.00024645647285300213, -0.00019926772803499236],
            [-0.0006024744943890134, -0.002147785787884586, 0.001399812773972361, -0.036472059131277504, 0.00012750772191223904],
            [-0.001567815576140436, 0.0013055621917748808, 0.0002487312848513498, -0.0008614874408401778, -0.019870896443806487],
        ],
    },
    &quot;305M&quot;: {
        &quot;intercept&quot;: [
            2.3392247012746834,
            3.1651345666056483,
            2.471987105632863,
            1.2404678308980266,
            3.0887017193916093,
        ],
        &quot;coef_linear&quot;: [
            [-0.3945995646360234, 0.04212797569256443, 0.3597852823915539, 0.004507385349434609, -0.011821078797535627],
            [0.18765244585849242, -0.5607080638027755, 0.16385032928665508, 0.22772936777546302, -0.018524079117843765],
            [0.36498559947643294, 0.36326950855260254, -1.247281529045098, 0.3544474379638183, 0.16457898305227767],
            [0.11479489142933053, 0.2241274675743544, 0.07534052854957383, -0.4984903999878992, 0.08422751243463553],
            [0.1034302676552572, -0.1442429588936119, 0.1542111292796102, 0.10260754874495605, -0.2160059867862149],
        ],
        &quot;coef_log&quot;: [
            [-0.0389843240976756, 0.0003898475662999871, -0.0012326552175473988, -0.0008170951320506675, -0.0006305864869774297],
            [-0.0018382099319297328, -0.004966654576883016, -0.0008004862412949112, -0.0016726743862113481, -0.0014239105552697226],
            [-0.0013605116194238868, -0.0029875971857020777, -0.029138080972677064, -0.0016317402099057068, 0.001163162472447215],
            [-0.0011594434613557832, -0.0010890215347730992, 0.0008814829783619934, -0.035207303872518685, 0.00014797726343401387],
            [-0.0015994149367300917, 0.002183961698325075, -0.0005510279268070304, -0.00175006550618083, -0.020723679414693993],
        ],
    },
    &quot;410M&quot;: {
        &quot;intercept&quot;: [
            2.2845576475924543,
            3.10221083581893,
            2.4040537489237623,
            1.2320388989073703,
            3.0194029194493215,
        ],
        &quot;coef_linear&quot;: [
            [-0.40161868178180443, 0.04851096556048266, 0.37552617435827934, -0.007771674366947659, -0.014646783770016363],
            [0.16564665878501697, -0.5418667012877614, 0.19196166965559713, 0.21461472108487065, -0.030356348237732297],
            [0.3827566078799856, 0.34563333912754424, -1.207292188578679, 0.2962894338651403, 0.18261280770604105],
            [0.054055822096378214, 0.18257490953749397, 0.25515869822947196, -0.5390509589695227, 0.04726152910616942],
            [0.08444528786706501, -0.1235789613695045, 0.1680640099151793, 0.09513264245956578, -0.22406297887230905],
        ],
        &quot;coef_log&quot;: [
            [-0.03838578515244451, 0.0010474524802569906, -0.0020612475600514644, 0.0001902706294946067, -0.0012861227733191377],
            [-0.0012829345286925373, -0.004688819508647834, -0.0016346691987556602, -0.0009769593878491815, -0.002091652532429498],
            [-0.0012514981651361474, -0.0022513525226212174, -0.03034764820962916, -0.00021614146982995423, -0.00017869542804964955],
            [-0.0010235048622236945, -7.088093693356411e-05, -0.0007898197931760238, -0.033703719578066345, -0.0007674227907133403],
            [-0.001315884970387432, 0.0024028568720025913, -0.0013333912060313298, -0.0011109928776418308, -0.021811398074324508],
        ],
    },
    &quot;70M&quot;: {
        &quot;intercept&quot;: [
            2.7857859114105317,
            3.631804815517477,
            2.8681805224896912,
            1.5890762093073625,
            3.585150303901379,
        ],
        &quot;coef_linear&quot;: [
            [-0.4416445868640977, 0.07076027431299382, 0.302121611856706, 0.021520016473356454, 0.04724268422103695],
            [0.21834311045644716, -0.553392474651338, 0.04818153280501451, 0.24033451148528265, 0.04653331990458732],
            [0.4651202418236946, 0.2657143234624336, -1.3670739924126776, 0.36683688818700816, 0.2694025389395758],
            [0.0850252828881378, 0.2990024146623459, 0.16822113994472984, -0.6239023579571726, 0.07165352046195206],
            [0.14123915472576296, -0.13497556494772603, 0.03364295958750099, 0.12770730941446795, -0.16761385878000656],
        ],
        &quot;coef_log&quot;: [
            [-0.041246477328631105, 0.0006531144880363961, -0.0006596475145338669, -0.00019599814522888677, -0.0015631188541267603],
            [-0.0009803943328683558, -0.005672467237098692, -8.71136475631502e-05, -0.0009074144501494191, -0.0019413115294711764],
            [-0.0006290227608540234, -0.0005120063062147314, -0.02905249764872596, -0.0007835652353273532, -0.000662736071510837],
            [-0.0008408582373940847, -0.0019902435925866755, 0.00039188729846795716, -0.0409361614036341, -0.0005053380487605633],
            [-0.0009055762689076869, 0.0025986071507895507, 0.00015711172405491434, -0.0011903203768379186, -0.019717110434476673],
        ],
    },
}


def _predict_point(p: Dict[str, float], coeff: Dict[str, list]) -&gt; Dict[str, float]:
    # Build feature vectors in fixed order
    P = [float(p.get(k, 0.0)) for k in PROP_COLS]
    logP = [math.log(x + EPS) for x in P]

    y = []
    for i in range(5):
        val = coeff[&quot;intercept&quot;][i]
        # linear terms
        for j in range(5):
            val += coeff[&quot;coef_linear&quot;][i][j] * P[j]
        # log terms
        for j in range(5):
            val += coeff[&quot;coef_log&quot;][i][j] * logP[j]
        y.append(val)

    return {LOSS_COLS[i]: y[i] for i in range(5)}


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    if group not in COEFFS:
        # Fallback: use the average of available groups if unknown label is passed
        # This keeps functional form identical while remaining robust.
        # Precompute simple average coefficients on the fly.
        groups = list(COEFFS.values())
        avg = {
            &quot;intercept&quot;: [sum(g[&quot;intercept&quot;][i] for g in groups) / len(groups) for i in range(5)],
            &quot;coef_linear&quot;: [
                [sum(g[&quot;coef_linear&quot;][i][j] for g in groups) / len(groups) for j in range(5)]
                for i in range(5)
            ],
            &quot;coef_log&quot;: [
                [sum(g[&quot;coef_log&quot;][i][j] for g in groups) / len(groups) for j in range(5)]
                for i in range(5)
            ],
        }
        coeff = avg
    else:
        coeff = COEFFS[group]

    outputs: List[Dict[str, float]] = []
    for row in input_data:
        outputs.append(_predict_point(row, coeff))
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.971145
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1">from __future__ import annotations
from typing import List, Dict
import math

# Log-scaling law per group and domain:
# loss_domain_i = a + b * log(proportion_domain_i + eps)
# The functional form is identical across groups; only (a, b, eps) differ.

COEFFS: Dict[str, Dict[str, Dict[str, float]]] = {
    &quot;160M&quot;: {
        &quot;loss_domain_1&quot;: {&quot;a&quot;: 2.2532615943868906, &quot;b&quot;: -0.1337057136545849, &quot;eps&quot;: 0.0023901343745956435},
        &quot;loss_domain_2&quot;: {&quot;a&quot;: 3.2631919363951556, &quot;b&quot;: -0.03558508713341269, &quot;eps&quot;: 0.0028180383737978713},
        &quot;loss_domain_3&quot;: {&quot;a&quot;: 2.617689579679002, &quot;b&quot;: -0.09252884608244097, &quot;eps&quot;: 0.000733714154677849},
        &quot;loss_domain_4&quot;: {&quot;a&quot;: 1.1948582730723922, &quot;b&quot;: -0.12312317609847837, &quot;eps&quot;: 0.00195132901468823},
        &quot;loss_domain_5&quot;: {&quot;a&quot;: 3.0846493236033896, &quot;b&quot;: -0.14154552771058287, &quot;eps&quot;: 0.027307052357303876},
    },
    &quot;305M&quot;: {
        &quot;loss_domain_1&quot;: {&quot;a&quot;: 2.1166886286314748, &quot;b&quot;: -0.12677944713435335, &quot;eps&quot;: 0.002106041895388586},
        &quot;loss_domain_2&quot;: {&quot;a&quot;: 3.0993803822936004, &quot;b&quot;: -0.0385714100210861, &quot;eps&quot;: 0.00469352643305276},
        &quot;loss_domain_3&quot;: {&quot;a&quot;: 2.498051621939985, &quot;b&quot;: -0.08371626119596315, &quot;eps&quot;: 0.0003880182070929469},
        &quot;loss_domain_4&quot;: {&quot;a&quot;: 1.0998534285937238, &quot;b&quot;: -0.11664800642521109, &quot;eps&quot;: 0.001864178447017992},
        &quot;loss_domain_5&quot;: {&quot;a&quot;: 2.918795681535315, &quot;b&quot;: -0.13713177186385297, &quot;eps&quot;: 0.023253743472940992},
    },
    &quot;410M&quot;: {
        &quot;loss_domain_1&quot;: {&quot;a&quot;: 2.0637659220766347, &quot;b&quot;: -0.12150735957421108, &quot;eps&quot;: 0.0017973751820540681},
        &quot;loss_domain_2&quot;: {&quot;a&quot;: 3.0488279748294658, &quot;b&quot;: -0.027049899121057236, &quot;eps&quot;: 0.0012212145195230296},
        &quot;loss_domain_3&quot;: {&quot;a&quot;: 2.431248081266068, &quot;b&quot;: -0.08424573100842837, &quot;eps&quot;: 0.00036447069913629604},
        &quot;loss_domain_4&quot;: {&quot;a&quot;: 1.0617583927188603, &quot;b&quot;: -0.11564152762782738, &quot;eps&quot;: 0.0020181562979846223},
        &quot;loss_domain_5&quot;: {&quot;a&quot;: 2.8417472866298, &quot;b&quot;: -0.13918681081923803, &quot;eps&quot;: 0.02183585057099095},
    },
    &quot;70M&quot;: {
        &quot;loss_domain_1&quot;: {&quot;a&quot;: 2.536901150373465, &quot;b&quot;: -0.1533997450030275, &quot;eps&quot;: 0.0032688441418023727},
        &quot;loss_domain_2&quot;: {&quot;a&quot;: 3.568955616082981, &quot;b&quot;: -0.05120827266924541, &quot;eps&quot;: 0.007660059062331851},
        &quot;loss_domain_3&quot;: {&quot;a&quot;: 2.886179438825381, &quot;b&quot;: -0.10197118388034185, &quot;eps&quot;: 0.0009064844324661106},
        &quot;loss_domain_4&quot;: {&quot;a&quot;: 1.4039456278566775, &quot;b&quot;: -0.13849029661959558, &quot;eps&quot;: 0.0019748899726039115},
        &quot;loss_domain_5&quot;: {&quot;a&quot;: 3.435925433424547, &quot;b&quot;: -0.1375046497033619, &quot;eps&quot;: 0.02690444281470573},
    },
}

_PROP_COLS = [
    &quot;proportion_domain_1&quot;,
    &quot;proportion_domain_2&quot;,
    &quot;proportion_domain_3&quot;,
    &quot;proportion_domain_4&quot;,
    &quot;proportion_domain_5&quot;,
]
_LOSS_COLS = [
    &quot;loss_domain_1&quot;,
    &quot;loss_domain_2&quot;,
    &quot;loss_domain_3&quot;,
    &quot;loss_domain_4&quot;,
    &quot;loss_domain_5&quot;,
]


def _predict_for_group(row: Dict[str, float], group: str) -&gt; Dict[str, float]:
    coeffs = COEFFS.get(group)
    if coeffs is None:
        raise ValueError(f&quot;Unknown group &#x27;{group}&#x27;. Known groups: {sorted(COEFFS)}&quot;)
    out: Dict[str, float] = {}
    for i, loss_key in enumerate(_LOSS_COLS):
        prop_key = _PROP_COLS[i]
        p = float(row.get(prop_key, 0.0))
        a = coeffs[loss_key][&quot;a&quot;]
        b = coeffs[loss_key][&quot;b&quot;]
        eps = coeffs[loss_key][&quot;eps&quot;]
        # Guard against tiny negatives from numerical issues
        val = a + b * math.log(max(p + eps, 1e-12))
        out[loss_key] = float(val)
    return out


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    return [_predict_for_group(row, group) for row in input_data]</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.971000
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2">from __future__ import annotations
from typing import Dict, List

# Discovered scaling law (same functional form across groups):
#   loss_domain_i = a_{g,i} + b_{g,i} * (proportion_domain_i + c_{g,i}) ** (-alpha_{g,i})
# Parameters (a, b, alpha, c) are fitted per group g and domain i.

_PARAMS: Dict[str, Dict[int, Dict[str, float]]] = {
    &quot;160M&quot;: {
        1: {&quot;a&quot;: -0.11147193199439992, &quot;b&quot;: 2.3787691982958217, &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        2: {&quot;a&quot;: 3.278500369142649,   &quot;b&quot;: 0.01306711991811282, &quot;alpha&quot;: 0.8034482758620689, &quot;c&quot;: 0.034895833333333334},
        3: {&quot;a&quot;: 0.8506733951924391,  &quot;b&quot;: 1.7622754024333271, &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.0015625},
        4: {&quot;a&quot;: -1.072627406051137,  &quot;b&quot;: 2.2748208460279304, &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        5: {&quot;a&quot;: 0.3144198553952366,  &quot;b&quot;: 2.772096693493694,  &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.034895833333333334},
    },
    &quot;305M&quot;: {
        1: {&quot;a&quot;: -0.18733739807046243, &quot;b&quot;: 2.312924189576437,  &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        2: {&quot;a&quot;: 3.0583843406593463,  &quot;b&quot;: 0.05867924234335276, &quot;alpha&quot;: 0.3672413793103448, &quot;c&quot;: 0.019791666666666666},
        3: {&quot;a&quot;: 2.037098981893403,   &quot;b&quot;: 0.48486293207181885, &quot;alpha&quot;: 0.1293103448275862, &quot;c&quot;: 0.0015625},
        4: {&quot;a&quot;: -1.0727458525278075, &quot;b&quot;: 2.177270392291824,  &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        5: {&quot;a&quot;: 1.465292570110724,   &quot;b&quot;: 1.4569415752353443, &quot;alpha&quot;: 0.0896551724137931, &quot;c&quot;: 0.034895833333333334},
    },
    &quot;410M&quot;: {
        1: {&quot;a&quot;: -0.21860503605523499, &quot;b&quot;: 2.2855798194974324, &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        2: {&quot;a&quot;: 3.068706883350695,    &quot;b&quot;: 0.005605907282164662, &quot;alpha&quot;: 1.0017241379310344, &quot;c&quot;: 0.034895833333333334},
        3: {&quot;a&quot;: 1.958790095134566,    &quot;b&quot;: 0.4939182316775028,  &quot;alpha&quot;: 0.1293103448275862, &quot;c&quot;: 0.0015625},
        4: {&quot;a&quot;: -1.050236886559304,   &quot;b&quot;: 2.1205339628269058, &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.003125},
        5: {&quot;a&quot;: 1.3466574766335897,   &quot;b&quot;: 1.4981064795012045,  &quot;alpha&quot;: 0.0896551724137931, &quot;c&quot;: 0.034895833333333334},
    },
    &quot;70M&quot;: {
        1: {&quot;a&quot;: 2.1270757115611185,  &quot;b&quot;: 0.4395199897525094,  &quot;alpha&quot;: 0.2482758620689655, &quot;c&quot;: 0.013194444444444443},
        2: {&quot;a&quot;: 3.552025532207962,   &quot;b&quot;: 0.04562819235160885, &quot;alpha&quot;: 0.5258620689655172, &quot;c&quot;: 0.034895833333333334},
        3: {&quot;a&quot;: 1.0433037320725291,  &quot;b&quot;: 1.8510839638322976,  &quot;alpha&quot;: 0.05, &quot;c&quot;: 0.0015625},
        4: {&quot;a&quot;: 1.3203779743559676,  &quot;b&quot;: 0.16265793539818846, &quot;alpha&quot;: 0.4068965517241379, &quot;c&quot;: 0.013194444444444443},
        5: {&quot;a&quot;: 3.192722657529413,   &quot;b&quot;: 0.2586205571225181,  &quot;alpha&quot;: 0.446551724137931, &quot;c&quot;: 0.09531249999999998},
    },
}

_DOMAIN_KEYS = {
    1: (&quot;proportion_domain_1&quot;, &quot;loss_domain_1&quot;),
    2: (&quot;proportion_domain_2&quot;, &quot;loss_domain_2&quot;),
    3: (&quot;proportion_domain_3&quot;, &quot;loss_domain_3&quot;),
    4: (&quot;proportion_domain_4&quot;, &quot;loss_domain_4&quot;),
    5: (&quot;proportion_domain_5&quot;, &quot;loss_domain_5&quot;),
}

_DEF_EPS = 1e-8  # numerical floor to avoid zero to negative-power


def _predict_for_group_row(row: Dict[str, float], params_g: Dict[int, Dict[str, float]]) -&gt; Dict[str, float]:
    out: Dict[str, float] = {}
    for i in range(1, 6):
        p_key, y_key = _DOMAIN_KEYS[i]
        p = float(row.get(p_key, 0.0))
        par = params_g[i]
        # Apply shifted power law with small numerical floor
        val = par[&quot;a&quot;] + par[&quot;b&quot;] * ((max(p, 0.0) + max(par[&quot;c&quot;], _DEF_EPS)) ** (-par[&quot;alpha&quot;]))
        out[y_key] = float(val)
    return out


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    if group not in _PARAMS:
        raise ValueError(f&quot;Unknown group &#x27;{group}&#x27;. Known: {sorted(_PARAMS.keys())}&quot;)
    params_g = _PARAMS[group]
    return [_predict_for_group_row(row, params_g) for row in input_data]</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.970459
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3">from typing import List, Dict
import math

# Offset power-law with asymptote per group and domain:
# loss_domain_i = a + b * (proportion_domain_i + d) ** c
# Parameters were fitted on the provided dataset.
_PARAMS = {
    &quot;70M&quot;: {
        1: {&quot;a&quot;: 2.1328, &quot;b&quot;: 0.4344, &quot;c&quot;: -0.25, &quot;d&quot;: 0.01320},
        2: {&quot;a&quot;: 3.5859, &quot;b&quot;: 0.0212, &quot;c&quot;: -0.80, &quot;d&quot;: 0.05000},
        3: {&quot;a&quot;: 2.6248, &quot;b&quot;: 0.3145, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00349},
        4: {&quot;a&quot;: 1.3108, &quot;b&quot;: 0.1693, &quot;c&quot;: -0.40, &quot;d&quot;: 0.01320},
        5: {&quot;a&quot;: 2.8438, &quot;b&quot;: 0.6006, &quot;c&quot;: -0.20, &quot;d&quot;: 0.05000},
    },
    &quot;160M&quot;: {
        1: {&quot;a&quot;: 1.7899, &quot;b&quot;: 0.4896, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00847},
        2: {&quot;a&quot;: 3.2759, &quot;b&quot;: 0.0149, &quot;c&quot;: -0.75, &quot;d&quot;: 0.03208},
        3: {&quot;a&quot;: 2.4432, &quot;b&quot;: 0.2486, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00224},
        4: {&quot;a&quot;: 0.9249, &quot;b&quot;: 0.3149, &quot;c&quot;: -0.25, &quot;d&quot;: 0.00847},
        5: {&quot;a&quot;: 2.4777, &quot;b&quot;: 0.6157, &quot;c&quot;: -0.20, &quot;d&quot;: 0.05000},
    },
    &quot;305M&quot;: {
        1: {&quot;a&quot;: 1.6614, &quot;b&quot;: 0.4760, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00847},
        2: {&quot;a&quot;: 3.1208, &quot;b&quot;: 0.0108, &quot;c&quot;: -0.95, &quot;d&quot;: 0.05000},
        3: {&quot;a&quot;: 2.3548, &quot;b&quot;: 0.2163, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00143},
        4: {&quot;a&quot;: 0.8390, &quot;b&quot;: 0.3014, &quot;c&quot;: -0.25, &quot;d&quot;: 0.00847},
        5: {&quot;a&quot;: 2.3085, &quot;b&quot;: 0.6178, &quot;c&quot;: -0.20, &quot;d&quot;: 0.05000},
    },
    &quot;410M&quot;: {
        1: {&quot;a&quot;: 1.6083, &quot;b&quot;: 0.4704, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00847},
        2: {&quot;a&quot;: 3.0372, &quot;b&quot;: 0.0275, &quot;c&quot;: -0.45, &quot;d&quot;: 0.01320},
        3: {&quot;a&quot;: 2.2824, &quot;b&quot;: 0.2203, &quot;c&quot;: -0.20, &quot;d&quot;: 0.00143},
        4: {&quot;a&quot;: 0.8117, &quot;b&quot;: 0.2936, &quot;c&quot;: -0.25, &quot;d&quot;: 0.00847},
        5: {&quot;a&quot;: 2.2138, &quot;b&quot;: 0.6352, &quot;c&quot;: -0.20, &quot;d&quot;: 0.05000},
    },
}

# Fallback parameters: average across known groups per domain.
# This is used if an unknown group is requested.
_avg_params = {}
for dom in range(1, 6):
    vals = [g[dom] for g in _PARAMS.values()]
    _avg_params[dom] = {
        &quot;a&quot;: sum(v[&quot;a&quot;] for v in vals) / len(vals),
        &quot;b&quot;: sum(v[&quot;b&quot;] for v in vals) / len(vals),
        &quot;c&quot;: sum(v[&quot;c&quot;] for v in vals) / len(vals),
        &quot;d&quot;: sum(v[&quot;d&quot;] for v in vals) / len(vals),
    }


def _predict_for_group(row: Dict[str, float], group: str) -&gt; Dict[str, float]:
    params = _PARAMS.get(group, _avg_params)
    out: Dict[str, float] = {}
    for i in range(1, 6):
        p = float(row.get(f&quot;proportion_domain_{i}&quot;, 0.0))
        if p &lt; 0.0:
            p = 0.0
        pr = params[i]
        a = pr[&quot;a&quot;]
        b = pr[&quot;b&quot;]
        c = pr[&quot;c&quot;]
        d = pr[&quot;d&quot;]
        y = a + b * ((p + d) ** c)
        out[f&quot;loss_domain_{i}&quot;] = float(y)
    return out


def law(input_data: List[Dict[str, float]], group: str) -&gt; List[Dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    return [_predict_for_group(row, group) for row in input_data]</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #228B22; color: white;">
                        R² = 0.899201
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4">from __future__ import annotations

import math
from typing import Dict, List

# Small epsilon to avoid log(0) when a domain proportion is zero
_EPS = 1e-6

# Per-group, per-output coefficients for the multi-log linear law.
# For each group and each output loss_domain_k, the list is:
#   [intercept, b1, b2, b3, b4, b5]
_COEFS: Dict[str, Dict[str, List[float]]] = {
    &quot;160M&quot;: {
        &quot;loss_domain_1&quot;: [
            2.4042025803406553,
            -0.04878456902733,
            -0.001374141990715709,
            0.00425143168670042,
            -0.004036425589256114,
            0.007660516801797778,
        ],
        &quot;loss_domain_2&quot;: [
            3.329935868696654,
            0.002613273350161564,
            -0.011891996557419415,
            2.5572674116154508e-05,
            0.003256629390122456,
            -0.005952205705195364,
        ],
        &quot;loss_domain_3&quot;: [
            2.8006343825165314,
            0.0038463371713819104,
            0.0010271048724917586,
            -0.03834514983761013,
            0.003370871701657902,
            -0.004992444549614571,
        ],
        &quot;loss_domain_4&quot;: [
            1.3842254371292473,
            0.0001770125184695076,
            0.0019008052563317707,
            0.00040985897229494966,
            -0.04321725706040489,
            0.005203775943861618,
        ],
        &quot;loss_domain_5&quot;: [
            3.22070848376419,
            0.005681276339156933,
            0.004826630941776033,
            -0.00156340623248917,
            0.005473022220321205,
            -0.030039287316337895,
        ],
    },
    &quot;305M&quot;: {
        &quot;loss_domain_1&quot;: [
            2.254097761686427,
            -0.047618308465165196,
            -0.0013550046237553216,
            0.004003192579882288,
            -0.0042909223510698554,
            0.006758159630738282,
        ],
        &quot;loss_domain_2&quot;: [
            3.166848403701224,
            0.0023474329227610603,
            -0.011422287483401284,
            4.890168638858063e-06,
            0.0026987505976661923,
            -0.006498741023938888,
        ],
        &quot;loss_domain_3&quot;: [
            2.6500359123770685,
            0.0035548451771600445,
            0.0011888369505355075,
            -0.03964115398330717,
            0.0028959099742507494,
            -0.004828358464409514,
        ],
        &quot;loss_domain_4&quot;: [
            1.2751944568968825,
            -0.00024923215104993846,
            0.0015693015233153626,
            0.0004352251902864513,
            -0.041431801189773,
            0.00487564464080058,
        ],
        &quot;loss_domain_5&quot;: [
            3.0477226649403506,
            0.005424541586794049,
            0.004752807485933067,
            -0.0015473014680650466,
            0.004775541162988269,
            -0.030879417992197157,
        ],
    },
    &quot;410M&quot;: {
        &quot;loss_domain_1&quot;: [
            2.1969109104720936,
            -0.04711053566759858,
            -0.0005929598868873091,
            0.0032935545744610275,
            -0.0034299883497122844,
            0.006245418652814477,
        ],
        &quot;loss_domain_2&quot;: [
            3.096774846113359,
            0.002683578481767577,
            -0.01079532012122322,
            -0.0006061709329291659,
            0.003289915492216653,
            -0.007059325448456723,
        ],
        &quot;loss_domain_3&quot;: [
            2.5854698484676923,
            0.0036493802056689617,
            0.0014981486567392278,
            -0.04057579585320029,
            0.003422309711002145,
            -0.005304641068008599,
        ],
        &quot;loss_domain_4&quot;: [
            1.2374283294627488,
            -0.0007019754734980677,
            0.0023220834389344196,
            0.00022483249867104974,
            -0.040255057354183485,
            0.0043998574943246395,
        ],
        &quot;loss_domain_5&quot;: [
            2.9734625677861892,
            0.005474979456664107,
            0.005292433569369652,
            -0.0021983339298417347,
            0.005328130514437292,
            -0.031848404298810445,
        ],
    },
    &quot;70M&quot;: {
        &quot;loss_domain_1&quot;: [
            2.709753097020149,
            -0.05223448519596348,
            -0.002096832291136182,
            0.0049309629806633245,
            -0.005121637241168704,
            0.008582633603168494,
        ],
        &quot;loss_domain_2&quot;: [
            3.6589940901202946,
            0.0024594854221674217,
            -0.013037306540415955,
            0.00018013301869309603,
            0.0025694674548725075,
            -0.0055192991657138785,
        ],
        &quot;loss_domain_3&quot;: [
            3.0882519411499176,
            0.004138978755961604,
            0.000999612740187218,
            -0.040138664176247195,
            0.002610113875372775,
            -0.00489486237526904,
        ],
        &quot;loss_domain_4&quot;: [
            1.6123830068952152,
            -0.0002626923872205353,
            0.001840626589019361,
            0.0004871656033250259,
            -0.048580221686945084,
            0.0053741041526707105,
        ],
        &quot;loss_domain_5&quot;: [
            3.567736623439916,
            0.005906206376564464,
            0.00463119283202685,
            -0.0015851069219014631,
            0.004999662780701288,
            -0.029220774259506147,
        ],
    },
}


def _predict_one(input_point: Dict[str, float], beta: List[float]) -&gt; float:
    # Build feature vector: [1, log(p1+eps), ..., log(p5+eps)]
    logs = [math.log(max(input_point.get(f&quot;proportion_domain_{i}&quot;, 0.0), 0.0) + _EPS) for i in range(1, 6)]
    return beta[0] + sum(b * x for b, x in zip(beta[1:], logs))


def law(input_data: list[dict[str, float]], group: str) -&gt; list[dict[str, float]]:
    &quot;&quot;&quot;
    Predicts output variables based on input variables according to a discovered scaling law.

    Args:
        input_data: A list of dictionaries, where each dictionary is a single data
                    point containing input variable names as keys and their
                    corresponding values.
        group: The name of the experimental group for which to make predictions.
                The functional form of the law must be the same for all groups,
                but the constant parameters/coefficients can differ per group.

    Returns:
        A list of dictionaries, corresponding to the input_data list, with each
        dictionary containing the predicted output variable(s).
    &quot;&quot;&quot;
    if group not in _COEFS:
        raise ValueError(f&quot;Unknown group &#x27;{group}&#x27;. Available groups: {sorted(_COEFS.keys())}&quot;)

    group_coefs = _COEFS[group]
    outputs: List[Dict[str, float]] = []
    for point in input_data:
        pred: Dict[str, float] = {}
        for k in range(1, 6):
            key = f&quot;loss_domain_{k}&quot;
            beta = group_coefs[key]
            pred[key] = _predict_one(point, beta)
        outputs.append(pred)
    return outputs</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>