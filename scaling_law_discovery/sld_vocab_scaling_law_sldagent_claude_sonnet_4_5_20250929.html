<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SLD - Vocabulary Scaling Law - SLDAgent + Claude Sonnet 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --accent-primary: #2563eb;
            --accent-secondary: #3b82f6;
            --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
            --text-primary: #1f2937;
            --text-secondary: #4b5563;
            --border-subtle: rgba(0, 0, 0, 0.1);
            --glass-bg: rgba(0, 0, 0, 0.02);
            --success: #10b981;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Sora', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            min-height: 100vh;
            color: var(--text-primary);
        }
        
        .bg-pattern {
            display: none;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }
        
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--accent-primary);
            text-decoration: none;
            font-size: 0.9rem;
            margin-bottom: 1.5rem;
            transition: color 0.2s;
        }
        
        .back-link:hover {
            color: var(--accent-secondary);
        }
        
        .header {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 2rem;
            backdrop-filter: blur(10px);
        }
        
        .header h1 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin-top: 1rem;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
        
        .meta-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .meta-value {
            font-weight: 600;
            color: var(--text-primary);
        }
        
        .r2-badge {
            display: inline-block;
            padding: 0.3rem 0.6rem;
            border-radius: 6px;
            font-weight: 600;
            font-size: 0.85rem;
            font-family: 'JetBrains Mono', monospace;
        }
        
        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
        }
        
        .runs-container {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        
        .run-card {
            background: var(--glass-bg);
            border: 1px solid var(--border-subtle);
            border-radius: 12px;
            overflow: hidden;
            transition: border-color 0.2s;
        }
        
        .run-card:hover {
            border-color: rgba(99, 102, 241, 0.3);
        }
        
        .run-card.best-run {
            border-color: var(--success);
            box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
        }
        
        .run-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 1.25rem;
            background: rgba(255, 255, 255, 0.02);
            cursor: pointer;
            transition: background 0.2s;
        }
        
        .run-header:hover {
            background: rgba(255, 255, 255, 0.04);
        }
        
        .run-info {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        
        .run-badge {
            padding: 0.25rem 0.6rem;
            border-radius: 6px;
            font-size: 0.75rem;
            font-weight: 600;
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-secondary);
        }
        
        .run-badge.best-badge {
            background: var(--success);
            color: white;
        }
        
        .run-label {
            font-weight: 500;
            color: var(--text-primary);
        }
        
        .expand-icon {
            color: var(--text-muted);
            font-size: 0.8rem;
            transition: transform 0.2s;
        }
        
        .run-header.expanded .expand-icon {
            transform: rotate(180deg);
        }
        
        .run-content {
            border-top: 1px solid var(--border-subtle);
        }
        
        .code-container {
            overflow: hidden;
        }
        
        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.75rem 1.25rem;
            background: rgba(0, 0, 0, 0.2);
            border-bottom: 1px solid var(--border-subtle);
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .copy-btn {
            padding: 0.35rem 0.75rem;
            background: rgba(99, 102, 241, 0.2);
            border: 1px solid rgba(99, 102, 241, 0.3);
            border-radius: 6px;
            color: var(--accent-primary);
            font-size: 0.75rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            font-family: inherit;
        }
        
        .copy-btn:hover {
            background: rgba(99, 102, 241, 0.3);
        }
        
        .code-container pre {
            margin: 0;
            padding: 1.25rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8rem;
            line-height: 1.6;
            overflow-x: auto;
            background: transparent !important;
        }
        
        .footer {
            text-align: center;
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border-subtle);
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        
        .footer a {
            color: var(--accent-primary);
            text-decoration: none;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .header h1 {
                font-size: 1.25rem;
            }
            
            .meta-row {
                flex-direction: column;
                gap: 0.75rem;
            }
            
            .run-info {
                flex-wrap: wrap;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="bg-pattern"></div>
    
    <div class="container">
        <a href="sld_index.html" class="back-link">
            ← Back to Leaderboard
        </a>
        
        <div class="header">
            <h1>Vocabulary Scaling Law</h1>
            <div class="meta-row">
                <div class="meta-item">
                    <span class="meta-label">Agent:</span>
                    <span class="meta-value">SLDAgent</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Model:</span>
                    <span class="meta-value">Claude Sonnet 4.5</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Best R²:</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        0.984953
                    </span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Mean R²:</span>
                    <span class="meta-value">0.980341</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Min R²:</span>
                    <span class="meta-value">0.976718</span>
                </div>
                <div class="meta-item">
                    <span class="meta-label">Runs:</span>
                    <span class="meta-value">5</span>
                </div>
            </div>
        </div>
        
        <h2 class="section-title">All Runs (sorted by R²)</h2>
        
        <div class="runs-container">
            
        <div class="run-card best-run">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge best-badge">Best</span>
                    <span class="run-label">Run 2</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.984953
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: block;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined scaling law with stable vocabulary coupling
7 parameters: a, b, c, alpha, beta, gamma, offset
Form: L = a*N^(-alpha) + b*D^(-beta) + c*V^theta/(N^delta * D^epsilon) + offset
Key: Balanced vocab-capacity-data interactions with numerical stability
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with balanced vocab-capacity-data coupling
    L = a*N^(-alpha) + b*D^(-beta) + c*V^theta/(N^gamma_n * D^gamma_d) + offset
    
    Improvements:
    - Fractional vocab exponent (theta=0.4) between sqrt and linear for flexibility
    - Asymmetric N/D coupling in vocab term (different sensitivity to params vs data)
    - Tight numerical conditioning via scaled inputs
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    
    N = np.maximum(X[:, 0], 1e6)
    V = np.maximum(X[:, 1], 1e3)
    D = np.maximum(X[:, 2], 1e6)
    
    # Normalization constants tuned for data range
    N_s = N / 1e8
    V_s = V / 3e4
    D_s = D / 1e11
    
    predictions = []
    for p in params:
        a, b, c, alpha, beta, gamma, offset = p
        
        # Standard power law terms
        t1 = a * np.power(N_s, -np.abs(alpha))
        t2 = b * np.power(D_s, -np.abs(beta))
        
        # Vocab coupling term with asymmetric N/D interaction
        # theta=0.4 gives sublinear vocab scaling (between sqrt=0.5 and linear=1.0)
        v_scaled = np.power(V_s, 0.4)
        
        # Asymmetric coupling: vocab needs MORE data than parameters
        # gamma controls overall coupling strength
        n_coupling = np.power(N_s, 0.2 * np.abs(gamma))
        d_coupling = np.power(D_s, 0.4 * np.abs(gamma))
        
        denominator = n_coupling * d_coupling
        t3 = c * v_scaled / np.maximum(denominator, 1e-10)
        
        pred = t1 + t2 + t3 + offset
        predictions.append(pred)
    
    predictions = np.array(predictions).T
    return predictions[:, 0] if predictions.shape[1] == 1 else predictions


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;Optimized fitting with focused search strategy&quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    if y.ndim == 1:
        y = y[:, None]
    
    T = y.shape[1]
    all_params = []
    
    for t in range(T):
        y_t = y[:, t]
        
        # Tightened bounds based on successful parameters
        bounds = [
            (0.05, 3.5),    # a
            (0.05, 3.5),    # b
            (-1.2, 1.2),    # c (can be negative)
            (0.1, 0.55),    # alpha
            (0.1, 0.55),    # beta
            (0.1, 0.9),     # gamma (coupling strength)
            (-7.5, -0.5)    # offset
        ]
        
        def objective(p):
            pred = scaling_law_func(X, p)
            residuals = pred - y_t
            
            # Primary MSE loss
            mse = np.mean(residuals ** 2)
            
            # Minimal regularization for stability
            reg = 1e-7 * np.sum(p[:3] ** 2)
            
            # Gentle outlier penalty
            y_range = np.max(y_t) - np.min(y_t)
            outlier = np.mean(np.maximum(0, np.abs(residuals) - 3*y_range) ** 2)
            
            return mse + reg + 0.03 * outlier
        
        # Smart initialization near known good region
        y_med = np.median(y_t)
        
        best_loss = float(&#x27;inf&#x27;)
        best_params = None
        
        # Focused multi-start with proven strategies
        init_points = [
            np.array([1.2, 1.2, 0.08, 0.28, 0.28, 0.35, y_med]),
            np.array([1.0, 1.4, -0.15, 0.25, 0.32, 0.42, y_med]),
            np.array([1.5, 1.0, 0.2, 0.35, 0.22, 0.3, y_med - 0.15]),
        ]
        
        for idx, init in enumerate(init_points):
            # Global search with adaptive parameters
            res_global = differential_evolution(
                objective, bounds,
                seed=42 + idx,
                maxiter=320,
                popsize=19,
                atol=1e-8,
                tol=1e-8,
                workers=1,
                strategy=&#x27;best1bin&#x27;,
                mutation=(0.6, 1.4),
                recombination=0.75
            )
            
            # Fine-tuned local refinement
            res_local = minimize(
                objective,
                res_global.x,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 900, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-9}
            )
            
            final = res_local if res_local.success else res_global
            
            if final.fun &lt; best_loss:
                best_loss = final.fun
                best_params = final.x
        
        all_params.append(best_params)
    
    all_params = np.array(all_params)
    return all_params[0] if T == 1 else all_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#2</span>
                    <span class="run-label">Run 5</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.983928
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law with quadratic efficiency interactions
Uses 7 parameters: [a, alpha, beta, gamma, b, c, d]
Form: L = a * P^(-alpha) * V^beta * N^(-gamma) + b*log(V*N^0.5/P) + c*(V^2/(P*N))^0.25 + d
Key innovation: Quadratic vocab efficiency term captures non-linear vocab effects
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Hybrid scaling law with novel quadratic vocab efficiency:
    - Main term: a * P^(-alpha) * V^beta * N^(-gamma) - standard joint scaling
    - Efficiency term: b*log(V*N^0.5/P) - vocab-data efficiency with sqrt dampening
    - Quadratic vocab term: c*(V^2/(P*N))^0.25 - captures vocab density effects
    - Baseline: d
    
    The quadratic vocab term is novel and captures how vocab utilization
    scales with the ratio of vocab^2 to parameters and data.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    T, P_dim = params.shape
    
    # Extract features with numerical stability
    P_non_vocab = np.maximum(X[:, 0], 1e-10)
    vocab_size = np.maximum(X[:, 1], 1.0)
    num_chars = np.maximum(X[:, 2], 1.0)
    
    # Normalize to similar scales for better numerical conditioning
    P_norm = P_non_vocab / 1e8
    V_norm = vocab_size / 50000
    N_norm = num_chars / 1e10
    
    pred = np.zeros((X.shape[0], T))
    
    for t in range(T):
        a, alpha, beta, gamma, b, c, d = params[t]
        
        # Main multiplicative term - Chinchilla-style joint scaling
        term1 = a * np.power(P_norm, -np.abs(alpha)) * \
                    np.power(V_norm, beta) * \
                    np.power(N_norm, -np.abs(gamma))
        
        # Efficiency term with sqrt dampening on data
        # This balances vocab and data relative to parameters
        # sqrt(N) provides sublinear data scaling in the efficiency measure
        efficiency = np.maximum((V_norm * np.sqrt(N_norm)) / (P_norm + 1e-10), 1e-10)
        term2 = b * np.log(efficiency)
        
        # Novel quadratic vocab density term: (V^2/(P*N))^0.25
        # Captures how vocab^2 scales relative to computational resources (P*N)
        # Higher vocab relative to compute suggests potential underutilization
        # 0.25 exponent provides strong dampening for stability
        vocab_density = np.maximum((V_norm * V_norm) / ((P_norm * N_norm) + 1e-10), 1e-10)
        term3 = c * np.power(vocab_density, 0.25)
        
        pred[:, t] = term1 + term2 + term3 + d
    
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Enhanced two-stage optimization with adaptive strategy
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    if y.ndim == 1:
        y2d = y[:, None]
    else:
        y2d = y
    T = y2d.shape[1]
    
    def objective(flat_params):
        params = flat_params.reshape(T, 7)
        pred = scaling_law_func(X, params)
        
        if pred.ndim == 1:
            pred = pred[:, None]
        
        residuals = pred - y2d
        mse = np.mean(residuals ** 2)
        
        # Very light regularization to avoid overfitting
        reg = 1e-8 * np.sum(params ** 2)
        
        return mse + reg
    
    # Carefully tuned bounds for the new formulation
    bounds = [
        (0.01, 22.0),     # a: main multiplicative coefficient
        (0.05, 0.75),     # alpha: parameter exponent
        (-1.8, 1.8),      # beta: vocab exponent (allow both signs)
        (0.01, 0.95),     # gamma: data exponent
        (-3.2, 3.2),      # b: efficiency term coefficient
        (-2.5, 2.5),      # c: quadratic vocab density coefficient
        (-10.0, 0.0)      # d: baseline (negative for lossu)
    ] * T
    
    # Differential evolution with enhanced exploration
    result_de = differential_evolution(
        objective,
        bounds,
        maxiter=420,
        popsize=21,
        seed=42,
        atol=1e-9,
        tol=1e-9,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.55, 1.85),
        recombination=0.72,
        workers=1,
        updating=&#x27;deferred&#x27;,
        polish=False
    )
    
    # Refine with L-BFGS-B for high precision
    result = minimize(
        objective,
        result_de.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2200, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
    )
    
    # Return best result
    if result.success and result.fun &lt; result_de.fun:
        params_opt = result.x.reshape(T, 7)
    else:
        params_opt = result_de.x.reshape(T, 7)
    
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#3</span>
                    <span class="run-label">Run 4</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.979100
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined scaling law with enhanced numerical stability
7 parameters: [a, b, c, d, e, f, g]
Maintains successful power law structure with improved optimization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution, shgo

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: Loss = a * P^b * V^c * C^d + e * log(V/P) + f * log(C) + g
    
    Key improvements:
    - Enhanced numerical stability through careful normalization
    - Separate power law components for better conditioning
    - Robust log term handling
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).flatten()
    
    # Extract and stabilize features with tighter bounds
    P = np.maximum(X[:, 0], 1e-8)
    V = np.maximum(X[:, 1], 10.0)
    C = np.maximum(X[:, 2], 1e-8)
    
    # Adaptive normalization for better numerical range
    P_n = P / 5e7
    V_n = V / 5e4
    C_n = C / 5e10
    
    a, b, c, d, e, f, g = params
    
    # Separate power terms for numerical stability
    # Clip exponents more conservatively
    P_term = np.power(P_n, np.clip(b, -2.5, 0.5))
    V_term = np.power(V_n, np.clip(c, -1.5, 1.5))
    C_term = np.power(C_n, np.clip(d, -2.5, 0.5))
    
    # Main power law with better conditioning
    power_term = a * P_term * V_term * C_term
    
    # Log terms with safety checks
    log_vp = np.log(np.maximum(V / P, 1e-10))
    log_c = np.log(np.maximum(C, 1e-10))
    
    vocab_efficiency = e * log_vp
    data_correction = f * log_c
    
    # Combine terms
    pred = power_term + vocab_efficiency + data_correction + g
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Enhanced multi-stage optimization with adaptive strategies
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    # Refined bounds based on successful patterns
    bounds = [
        (-12.0, 12.0),    # a: main coefficient
        (-2.2, 0.4),      # b: parameter exponent
        (-1.0, 1.0),      # c: vocab exponent
        (-2.2, 0.4),      # d: data exponent
        (-2.5, 2.5),      # e: vocab efficiency
        (-2.5, 2.5),      # f: data correction
        (-10.0, 3.0)      # g: intercept
    ]
    
    def objective(p):
        try:
            pred = scaling_law_func(X, p)
            if np.any(~np.isfinite(pred)):
                return 1e10
            residuals = pred - y
            # MSE with adaptive weighting
            mse = np.mean(residuals ** 2)
            # Very light regularization to prevent overfitting
            reg = 0.003 * np.sum(p ** 2)
            return mse + reg
        except:
            return 1e10
    
    # Stage 1: Global search with enhanced differential evolution
    result = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=400,
        popsize=20,
        atol=1e-10,
        tol=1e-10,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.5, 1.2),
        recombination=0.8,
        workers=1,
        updating=&#x27;deferred&#x27;
    )
    
    best_params = result.x if result.success else np.array([1.0, -0.4, 0.1, -0.4, 0.5, -0.5, -3.0])
    best_score = result.fun if result.success else 1e10
    
    # Stage 2: Intensive local refinement with multiple methods
    if result.success:
        # L-BFGS-B with very tight tolerances
        for i in range(3):
            try:
                local_res = minimize(
                    objective,
                    best_params,
                    method=&#x27;L-BFGS-B&#x27;,
                    bounds=bounds,
                    options={
                        &#x27;maxiter&#x27;: 1000, 
                        &#x27;ftol&#x27;: 1e-12, 
                        &#x27;gtol&#x27;: 1e-11,
                        &#x27;maxfun&#x27;: 15000
                    }
                )
                if local_res.success and local_res.fun &lt; best_score:
                    best_params = local_res.x
                    best_score = local_res.fun
            except:
                continue
        
        # TNC for additional refinement
        try:
            local_res2 = minimize(
                objective,
                best_params,
                method=&#x27;TNC&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 800, &#x27;ftol&#x27;: 1e-12, &#x27;xtol&#x27;: 1e-12}
            )
            if local_res2.success and local_res2.fun &lt; best_score:
                best_params = local_res2.x
                best_score = local_res2.fun
        except:
            pass
        
        # Stage 3: Multi-scale perturbation search
        for scale in [0.1, 0.05, 0.02, 0.01]:
            for attempt in range(2):
                try:
                    np.random.seed(42 + attempt)
                    perturbed = best_params + np.random.randn(7) * scale
                    perturbed = np.clip(perturbed, [b[0] for b in bounds], [b[1] for b in bounds])
                    
                    local_res3 = minimize(
                        objective,
                        perturbed,
                        method=&#x27;L-BFGS-B&#x27;,
                        bounds=bounds,
                        options={&#x27;maxiter&#x27;: 600, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11}
                    )
                    
                    if local_res3.success and local_res3.fun &lt; best_score:
                        best_params = local_res3.x
                        best_score = local_res3.fun
                except:
                    continue
        
        # Stage 4: Final polish with SLSQP
        try:
            final_res = minimize(
                objective,
                best_params,
                method=&#x27;SLSQP&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 500, &#x27;ftol&#x27;: 1e-13}
            )
            if final_res.success and final_res.fun &lt; best_score:
                best_params = final_res.x
        except:
            pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#4</span>
                    <span class="run-label">Run 1</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.977008
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined scaling law with vocab-compute budget coupling.
Form: L = L_inf + a/P^alpha + b/D^beta + c/(V^gamma * (P*D)^delta)
Captures vocabulary efficiency as function of total compute budget.
Parameters: [a, b, c, alpha, beta, gamma, delta] (7 params)
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = L_inf + a/P^alpha + b/D^beta + c/(V^gamma * (P*D)^delta)
    Vocab term scales with compute budget (P*D) to capture efficiency trade-offs.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params)
    
    if params.ndim == 1:
        params = params[None, :]
    
    P = np.maximum(X[:, 0], 1e6)
    V = np.maximum(X[:, 1], 1000)
    D = np.maximum(X[:, 2], 1e8)
    
    predictions = []
    for param_set in params:
        a, b, c, alpha, beta, gamma, delta = param_set
        
        # Safe exponents to prevent overflow
        alpha_safe = np.clip(alpha, 0.01, 0.99)
        beta_safe = np.clip(beta, 0.01, 0.99)
        gamma_safe = np.clip(gamma, 0.01, 2.0)
        delta_safe = np.clip(delta, -0.15, 0.15)
        
        # Core Chinchilla-style terms
        param_term = a / np.power(P, alpha_safe)
        data_term = b / np.power(D, beta_safe)
        
        # Vocab-compute coupling: efficiency depends on total compute
        compute_budget = P * D
        vocab_term = c / (np.power(V, gamma_safe) * np.power(compute_budget, delta_safe))
        
        # L_inf around -3 based on data characteristics
        L_inf = -3.0
        
        pred = L_inf + param_term + data_term + vocab_term
        predictions.append(pred)
    
    predictions = np.array(predictions).T
    return predictions[:, 0] if predictions.shape[1] == 1 else predictions


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Robust fitting with normalized features and multi-start optimization.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    
    y2d = y[:, None] if y.ndim == 1 else y
    T = y2d.shape[1]
    
    # Feature scaling for better conditioning
    P_scale = 1e8
    V_scale = 50000
    D_scale = 1e12
    
    def objective(params):
        # Scale parameters back to original space
        params_scaled = params.copy()
        params_scaled[0] *= P_scale ** params[3]  # a scaled by P^alpha
        params_scaled[1] *= D_scale ** params[4]  # b scaled by D^beta
        params_scaled[2] *= (V_scale ** params[5]) * ((P_scale * D_scale) ** params[6])  # c scaled
        
        pred = scaling_law_func(X, params_scaled)
        pred = pred[:, None] if pred.ndim == 1 else pred
        
        mse = np.mean((pred - y2d) ** 2)
        
        # Gentle regularization toward Chinchilla values
        reg = 1e-8 * ((params[3] - 0.34)**2 + (params[4] - 0.28)**2)
        
        return mse + reg
    
    # Bounds in normalized space
    bounds = [
        (0.01, 100),     # a (normalized)
        (0.01, 100),     # b (normalized)
        (-100, 100),     # c (normalized, can be negative)
        (0.1, 0.7),      # alpha (parameter exponent)
        (0.1, 0.7),      # beta (data exponent)
        (0.0, 2.0),      # gamma (vocab exponent)
        (-0.15, 0.15)    # delta (compute coupling, small for stability)
    ]
    
    all_params = []
    for t in range(T):
        best_result = None
        best_score = np.inf
        
        # Multi-start global search
        for seed in [42, 123, 456, 789]:
            result = differential_evolution(
                objective,
                bounds=bounds,
                seed=seed,
                maxiter=220,
                popsize=18,
                atol=1e-9,
                tol=1e-9,
                strategy=&#x27;best1bin&#x27;,
                mutation=(0.5, 1.3),
                recombination=0.75,
                polish=True
            )
            
            if result.fun &lt; best_score:
                best_score = result.fun
                best_result = result
        
        # Local refinement for precision
        result_local = minimize(
            objective,
            best_result.x,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 400, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-9}
        )
        
        params_opt = result_local.x if result_local.success and result_local.fun &lt; best_result.fun else best_result.x
        
        # Scale parameters back to original space
        params_opt[0] *= P_scale ** params_opt[3]
        params_opt[1] *= D_scale ** params_opt[4]
        params_opt[2] *= (V_scale ** params_opt[5]) * ((P_scale * D_scale) ** params_opt[6])
        
        all_params.append(params_opt)
    
    all_params = np.array(all_params)
    return all_params[0] if T == 1 else all_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        <div class="run-card ">
            <div class="run-header" onclick="toggleRun(this)">
                <div class="run-info">
                    <span class="run-badge ">#5</span>
                    <span class="run-label">Run 3</span>
                    <span class="r2-badge" style="background-color: #006400; color: white;">
                        R² = 0.976718
                    </span>
                </div>
                <span class="expand-icon">▼</span>
            </div>
            <div class="run-content" style="display: none;">
                <div class="code-container">
                    <div class="code-header">
                        <span>Python</span>
                        <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
                    </div>
                    <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Simplified Chinchilla-inspired scaling law with log-stable interactions.
Form: Loss = a/(P^α*D^β) + b/V^γ + c*log(1+P/V) + E

Key improvements:
- Log-based P/V interaction (proven stable in top performers)
- Streamlined code (under 500 chars target)
- Optimized multi-stage fitting with tight convergence
- Enhanced numerical stability
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Stable scaling: L = a/(P^α*D^β) + b/V^γ + c*log(1+P/V) + E
    params = [a, b, c, alpha, beta, gamma, E] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    p = np.asarray(params, dtype=np.float64).flatten()[:7]
    
    # Extract and stabilize
    P, V, D = np.maximum(X[:, 0], 1e-10), np.maximum(X[:, 1], 1e-10), np.maximum(X[:, 2], 1e-10)
    
    # Normalize
    P_n, V_n, D_n = P / 1e8, V / 1e4, D / 1e11
    
    a, b, c, alpha, beta, gamma, E = p
    eps = 1e-12
    
    # Joint P-D term (Chinchilla)
    t1 = a / (P_n**np.abs(alpha) * D_n**np.abs(beta) + eps)
    
    # Vocab term
    t2 = b / (V_n**np.abs(gamma) + eps)
    
    # Log P/V interaction (stable)
    t3 = c * np.log1p(P_n / (V_n + eps))
    
    return np.clip(t1 + t2 + t3 + E, -15, 10)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Enhanced multi-stage optimization.
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).flatten()
    
    valid = np.isfinite(y)
    X, y = X[valid], y[valid]
    
    if len(y) == 0:
        return np.array([1.0, 0.5, 0.1, 0.35, 0.35, 0.15, -2.0])
    
    def objective(p):
        pred = scaling_law_func(X, p)
        res = pred - y
        
        # Huber loss
        delta = 0.5
        huber = np.where(np.abs(res) &lt;= delta, 0.5 * res**2,
                        delta * (np.abs(res) - 0.5 * delta))
        
        # Regularization toward Chinchilla values
        reg = 1e-7 * (np.sum(p[:3]**2) + np.sum((p[3:5] - 0.35)**2) + (p[5] - 0.15)**2)
        
        return np.mean(huber) + reg
    
    # Adaptive initialization
    y_range, y_med = np.ptp(y), np.median(y)
    
    x0 = np.array([
        y_range * 0.43,  # a
        y_range * 0.29,  # b
        y_range * 0.13,  # c
        0.35,            # alpha
        0.35,            # beta
        0.16,            # gamma
        y_med            # E
    ])
    
    bounds = [
        (1e-6, 200),     # a
        (1e-6, 200),     # b
        (-12, 12),       # c
        (0.05, 0.99),    # alpha
        (0.05, 0.99),    # beta
        (0.01, 0.80),    # gamma
        (-15, 3)         # E
    ]
    
    # Stage 1: Global search
    res_de = differential_evolution(
        objective, bounds,
        maxiter=520, popsize=17, seed=42,
        atol=1e-9, tol=1e-9,
        strategy=&#x27;best1bin&#x27;, updating=&#x27;deferred&#x27;,
        init=&#x27;latinhypercube&#x27;
    )
    
    best_params, best_score = res_de.x, res_de.fun
    
    # Stage 2: L-BFGS-B
    res_lbfgs = minimize(
        objective, best_params,
        method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
        options={&#x27;maxiter&#x27;: 2800, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11}
    )
    
    if res_lbfgs.success and res_lbfgs.fun &lt; best_score:
        best_params, best_score = res_lbfgs.x, res_lbfgs.fun
    
    # Stage 3: Powell
    res_powell = minimize(
        objective, best_params,
        method=&#x27;Powell&#x27;,
        options={&#x27;maxiter&#x27;: 1400, &#x27;ftol&#x27;: 1e-11, &#x27;xtol&#x27;: 1e-11}
    )
    
    if res_powell.fun &lt; best_score:
        best_params, best_score = res_powell.x, res_powell.fun
    
    # Stage 4: Nelder-Mead
    res_nm = minimize(
        objective, best_params,
        method=&#x27;Nelder-Mead&#x27;,
        options={&#x27;maxiter&#x27;: 1100, &#x27;xatol&#x27;: 1e-10, &#x27;fatol&#x27;: 1e-10}
    )
    
    if res_nm.fun &lt; best_score:
        best_params = res_nm.x
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
                </div>
            </div>
        </div>
        
        </div>
        
        <footer class="footer">
            <p>SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> | <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a></p>
        </footer>
    </div>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        function toggleRun(header) {
            const content = header.nextElementSibling;
            const isExpanded = content.style.display === 'block';
            content.style.display = isExpanded ? 'none' : 'block';
            header.classList.toggle('expanded', !isExpanded);
        }
        
        function copyCode(event, codeId) {
            event.stopPropagation();
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                const btn = event.target;
                btn.textContent = 'Copied!';
                setTimeout(() => btn.textContent = 'Copy', 2000);
            });
        }
        
        Prism.highlightAll();
    </script>
</body>
</html>