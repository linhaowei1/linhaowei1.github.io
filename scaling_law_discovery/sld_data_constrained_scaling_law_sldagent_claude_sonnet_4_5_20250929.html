<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Data-Constrained Scaling Law - SLDAgent + Claude Sonnet 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Data-Constrained Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Sonnet 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #228b22; color: white"> 0.919475 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.894453</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.852179</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.919475 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined scaling law with data efficiency modeling
Key innovations:
- Standard power law base: A/P^alpha + B/D^beta + C/U^gamma
- Data efficiency term: F/(D/U)^delta to model repetition effects
- The D/U ratio captures how much data is &quot;recycled&quot; vs unique
- Simpler than log corrections, more interpretable
- Direct modeling of the intuition that loss depends on unique content fraction
Uses 7 parameters: [A, alpha, B, beta, C, gamma, F, delta]
Actually uses 7: We&#x27;ll use F*U^delta/D^delta = F*(U/D)^delta
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with data efficiency:
    L = A/P^alpha + B/D^beta + C/U^gamma + F*(U/D)^delta
    The (U/D)^delta term captures data repetition effects
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    
    U = X[:, 0]  # unique_tokens
    P = X[:, 1]  # params
    D = X[:, 2]  # tokens
    
    eps = 1e-12
    U = np.maximum(U, eps)
    P = np.maximum(P, eps)
    D = np.maximum(D, eps)
    
    A, alpha, B, beta, C, gamma, F = params[0]
    
    # Standard power law terms
    term1 = A / (P ** alpha)
    term2 = B / (D ** beta)
    term3 = C / (U ** gamma)
    
    # Data efficiency term: models unique content fraction
    # When U/D is small (high repetition), this term is small
    # delta fixed at 0.15 to save a parameter (empirically good value)
    delta = 0.15
    efficiency_ratio = U / D
    efficiency_term = F * (efficiency_ratio ** delta)
    
    pred = term1 + term2 + term3 + efficiency_term
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Streamlined two-stage optimization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    U = X[:, 0]
    P = X[:, 1]
    D = X[:, 2]
    
    # Log-median normalization
    U_scale = np.exp(np.median(np.log(U + 1e-12)))
    P_scale = np.exp(np.median(np.log(P + 1e-12)))
    D_scale = np.exp(np.median(np.log(D + 1e-12)))
    y_scale = np.median(y)
    
    U_norm = U / U_scale
    P_norm = P / P_scale
    D_norm = D / D_scale
    y_norm = y / y_scale
    
    delta = 0.15
    
    def objective(params):
        A, alpha, B, beta, C, gamma, F = params
        
        eps = 1e-12
        P_safe = np.maximum(P_norm, eps)
        D_safe = np.maximum(D_norm, eps)
        U_safe = np.maximum(U_norm, eps)
        
        term1 = A / (P_safe ** alpha)
        term2 = B / (D_safe ** beta)
        term3 = C / (U_safe ** gamma)
        
        efficiency_ratio = U_safe / D_safe
        efficiency_term = F * (efficiency_ratio ** delta)
        
        pred = term1 + term2 + term3 + efficiency_term
        
        residuals = pred - y_norm
        mse = np.mean(residuals ** 2)
        
        # Balanced regularization
        reg = 0.007 * (np.abs(alpha - 0.37) + 
                       np.abs(beta - 0.37) + 
                       np.abs(gamma - 0.28))
        reg += 0.004 * np.abs(F)
        
        return mse + reg
    
    bounds = [
        (0.001, 135),   # A
        (0.07, 1.05),   # alpha
        (0.001, 135),   # B
        (0.07, 1.05),   # beta
        (0.001, 135),   # C
        (0.07, 0.88),   # gamma
        (-8, 8)         # F
    ]
    
    # Data-driven initialization
    y_min = np.min(y_norm)
    y_range = np.max(y_norm) - y_min
    
    init_guess = [
        y_range * 0.34,
        0.37,
        y_range * 0.32,
        0.37,
        y_range * 0.25,
        0.28,
        y_range * 0.06
    ]
    
    # Global search
    result = differential_evolution(
        objective, 
        bounds, 
        seed=42,
        maxiter=400,
        popsize=16,
        atol=1e-8,
        tol=1e-8,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.5, 1.2),
        recombination=0.75
    )
    
    params_opt = result.x if result.success else np.array(init_guess)
    
    # Local refinement
    result_local = minimize(
        objective, 
        params_opt, 
        method=&#x27;L-BFGS-B&#x27;, 
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 550, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-9}
    )
    
    if result_local.success and result_local.fun &lt; objective(params_opt):
        params_opt = result_local.x
    
    # Scale back to original space
    params_scaled = params_opt.copy()
    params_scaled[0] *= y_scale * (P_scale ** params_opt[1])
    params_scaled[2] *= y_scale * (D_scale ** params_opt[3])
    params_scaled[4] *= y_scale * (U_scale ** params_opt[5])
    
    # Scale F: (U_norm/D_norm)^delta = (U/D)^delta * (D_scale/U_scale)^delta
    params_scaled[6] = params_opt[6] * y_scale * ((D_scale / U_scale) ** delta)
    
    return params_scaled
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.916660 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Advanced scaling law with learnable log efficiency modulation
L = A/N^α + B/D^β + C/U^γ * (1 + δ*log(D/U)) + E
Makes δ learnable within the 7-parameter constraint via coefficient scaling
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = A/N^α + B/D^β + C/U^γ * (1 + implicit_δ*log(D/U)) + E
    params = [A, α, B, β, C, γ, E] (7 parameters)
    δ effect is absorbed into C scaling through log modulation strength
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).ravel()
    
    if len(params) != 7:
        raise ValueError(f&quot;Expected 7 parameters, got {len(params)}&quot;)
    
    U = X[:, 0]  # unique_tokens
    N = X[:, 1]  # model_params
    D = X[:, 2]  # tokens
    
    A, alpha, B, beta, C, gamma, E = params
    
    eps = 1e-15
    
    # Core power-law terms
    term1 = A / np.power(np.maximum(N, eps), alpha)
    term2 = B / np.power(np.maximum(D, eps), beta)
    
    # Enhanced unique token term with adaptive log modulation
    # Use variable log coefficient based on gamma magnitude
    # When gamma is larger, log effect is proportionally scaled
    ratio = np.maximum(D / np.maximum(U, eps), 1.0)
    log_strength = 0.12 + 0.08 * np.clip(gamma, 0, 0.5)  # Range: 0.12 to 0.16
    efficiency_factor = 1.0 + log_strength * np.log(ratio)
    term3 = C / np.power(np.maximum(U, eps), gamma) * efficiency_factor
    
    return term1 + term2 + term3 + E


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Multi-restart optimization with adaptive regularization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    y_min, y_max = np.min(y), np.max(y)
    y_std = np.std(y)
    y_median = np.median(y)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            
            if np.any(~np.isfinite(pred)):
                return 1e10
            
            residuals = pred - y
            
            # Adaptive Huber loss with median-based threshold
            delta = 0.28 * y_std
            abs_res = np.abs(residuals)
            huber = np.where(
                abs_res &lt;= delta,
                0.5 * residuals**2,
                delta * (abs_res - 0.5 * delta)
            )
            
            base_loss = np.mean(huber)
            
            # Adaptive regularization with cross-term balance
            # Encourage balanced exponents
            exp_balance = 0.0025 * ((params[1] - params[3])**2 + 
                                    (params[3] - params[5])**2)
            
            # Light magnitude regularization
            coef_reg = 0.00008 * (np.log1p(params[0])**2 + 
                                   np.log1p(params[2])**2 + 
                                   np.log1p(params[4])**2)
            
            # Encourage E near lower bound
            E_penalty = 0.0005 * max(0, y_min * 0.5 - params[6])**2
            
            return base_loss + exp_balance + coef_reg + E_penalty
        except:
            return 1e10
    
    # Slightly tighter bounds for more focused search
    bounds = [
        (0.01, 1800),                    # A
        (0.05, 0.88),                    # α
        (0.01, 1800),                    # B
        (0.04, 0.68),                    # β
        (0.01, 1800),                    # C
        (0.04, 0.68),                    # γ
        (y_min * 0.35, y_max * 1.15)    # E
    ]
    
    best_result = None
    best_score = 1e10
    
    # Multi-restart with different seeds
    for seed_offset in [0, 17, 99]:
        result = differential_evolution(
            objective,
            bounds,
            seed=42 + seed_offset,
            maxiter=450,
            popsize=24,
            atol=1e-9,
            tol=1e-9,
            polish=True,
            workers=1,
            updating=&#x27;deferred&#x27;,
            strategy=&#x27;best1bin&#x27;,
            mutation=(0.55, 1.75),
            recombination=0.82,
            init=&#x27;latinhypercube&#x27;
        )
        
        if result.fun &lt; best_score:
            best_score = result.fun
            best_result = result
    
    return best_result.x
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.906971 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined Chinchilla-style scaling law with data efficiency modeling
Form: L = E + A/P^α + B/D^β + C/U^γ + I*log(D/U)
Where P=params, D=tokens, U=unique_tokens (normalized)
7 parameters: [E, A, α, B, β, C, γ] with I derived from B,C
Key: Tighter bounds, streamlined optimization, data efficiency term
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with data efficiency: L = E + A/P^α + B/D^β + C/U^γ + I*log(D/U)
    params = [E, A, α, B, β, C, γ] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).ravel()
    
    # Extract and normalize features: [unique_tokens, params, tokens]
    U = np.maximum(X[:, 0], 1e6) / 1e8
    P = np.maximum(X[:, 1], 1e7) / 1e8
    D = np.maximum(X[:, 2], 1e8) / 1e10
    
    E, A, alpha, B, beta, C, gamma = params
    
    # Clip exponents for stability
    alpha_safe = np.clip(alpha, 0.05, 1.0)
    beta_safe = np.clip(beta, 0.05, 1.0)
    gamma_safe = np.clip(gamma, 0.05, 0.9)
    
    # Main scaling terms
    term_P = A / np.power(P, alpha_safe)
    term_D = B / np.power(D, beta_safe)
    term_U = C / np.power(U, gamma_safe)
    
    # Data efficiency interaction: penalize D &gt;&gt; U
    ratio = np.clip(D * 1e10 / (U * 1e8), 1.0, 1e4)
    efficiency = 0.02 * (B + C) * np.log(ratio) / 10.0
    
    loss = E + term_P + term_D + term_U + efficiency
    
    return np.clip(loss, 0.5, 15.0)


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Streamlined hybrid optimization with informed initialization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64).ravel()
    
    # Clean data
    valid_mask = np.isfinite(y) &amp; (y &gt; 0) &amp; (y &lt; 50)
    X, y = X[valid_mask], y[valid_mask]
    
    if len(y) &lt; 3:
        return np.array([1.69, 2.9, 0.39, 2.4, 0.34, 1.9, 0.29])
    
    def objective(params):
        pred = scaling_law_func(X, params)
        mse = np.mean((pred - y) ** 2)
        # Light L2 regularization on exponents
        reg = 0.002 * np.sum(params[2::2] ** 2)
        return mse + reg
    
    # Tighter bounds informed by top performers
    bounds = [
        (1.3, 2.35),     # E: irreducible loss
        (0.7, 16.0),     # A: parameter coefficient
        (0.22, 0.68),    # α: parameter exponent
        (0.7, 16.0),     # B: data coefficient
        (0.22, 0.68),    # β: data exponent
        (0.4, 12.0),     # C: unique token coefficient
        (0.16, 0.58)     # γ: unique token exponent
    ]
    
    # Strategic initializations from top performers
    inits = [
        np.array([1.69, 2.9, 0.39, 2.4, 0.34, 1.9, 0.29]),
        np.array([1.65, 2.6, 0.37, 2.2, 0.32, 1.75, 0.27]),
        np.array([1.73, 3.2, 0.41, 2.6, 0.36, 2.1, 0.31])
    ]
    
    best_params = inits[0]
    best_score = float(&#x27;inf&#x27;)
    
    # Multi-start L-BFGS-B
    for init in inits:
        res = minimize(
            objective, init, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
            options={&#x27;maxiter&#x27;: 600, &#x27;ftol&#x27;: 1e-10}
        )
        if res.success and res.fun &lt; best_score:
            best_score = res.fun
            best_params = res.x
    
    # Global refinement with DE
    res_de = differential_evolution(
        objective, bounds, seed=42, maxiter=240, popsize=12,
        atol=1e-8, tol=1e-8, workers=1, polish=False,
        strategy=&#x27;best1bin&#x27;, mutation=(0.6, 1.4), recombination=0.75
    )
    
    if res_de.success and res_de.fun &lt; best_score:
        best_params = res_de.x
        best_score = res_de.fun
    
    # Final polish
    res_final = minimize(
        objective, best_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
        options={&#x27;maxiter&#x27;: 900, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
    )
    
    return res_final.x if (res_final.success and res_final.fun &lt; best_score) else best_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #32cd32; color: black"> R² = 0.876982 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Simplified robust scaling law with token diversity interaction
L = A/P^α + B/D^β * (1 + C*(U/D)^γ) + E
Focus on numerical stability and efficient optimization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law with token diversity: L = A/P^α + B/D^β * (1 + C*(U/D)^γ) + E
    7 parameters: [A, α, B, β, C, γ, E]
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64).ravel()
    
    # Extract features
    U = data_points[:, 0]  # unique_tokens
    P = data_points[:, 1]  # model_params
    D = data_points[:, 2]  # tokens
    
    # Unpack parameters
    A, alpha, B, beta, C, gamma, E = params[:7]
    
    # Numerical stability
    eps = 1e-12
    
    # Clamp exponents for stability
    alpha_safe = np.clip(alpha, 0.01, 2.0)
    beta_safe = np.clip(beta, 0.01, 2.0)
    gamma_safe = np.clip(gamma, 0.01, 3.0)
    
    # Parameter term: A/P^α
    param_term = A / (np.power(np.maximum(P, eps), alpha_safe) + eps)
    
    # Diversity ratio: U/D (bounded)
    diversity_ratio = np.clip(U / (D + eps), eps, 0.999)
    
    # Data term with diversity: B/D^β * (1 + C*(U/D)^γ)
    base_term = B / (np.power(np.maximum(D, eps), beta_safe) + eps)
    diversity_factor = 1.0 + C * np.power(diversity_ratio, gamma_safe)
    diversity_factor = np.clip(diversity_factor, 0.1, 10.0)
    data_term = base_term * diversity_factor
    
    # Total loss
    loss = param_term + data_term + E
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Streamlined optimization with proven hyperparameters
    &quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    loss_values = np.asarray(loss_values, dtype=np.float64).ravel()
    
    # Statistics
    min_loss = np.min(loss_values)
    max_loss = np.max(loss_values)
    loss_range = max_loss - min_loss
    
    # Objective with balanced regularization
    def objective(params):
        try:
            pred = scaling_law_func(data_points, params)
            mse = np.mean((pred - loss_values) ** 2)
            
            # Extract params for regularization
            A, alpha, B, beta, C, gamma, E = params
            
            # Light L1 on coefficients
            coeff_reg = 0.0001 * (np.abs(A) + np.abs(B) + np.abs(C))
            
            # Prefer typical scaling law exponents
            exp_reg = 0.001 * ((alpha - 0.34)**2 + (beta - 0.28)**2 + np.abs(gamma - 0.5))
            
            return mse + coeff_reg + exp_reg
        except:
            return 1e10
    
    # Tight bounds based on best performers
    bounds = [
        (0.001, 300.0),              # A
        (0.05, 0.8),                 # α
        (0.001, 300.0),              # B
        (0.05, 0.8),                 # β
        (-10.0, 10.0),               # C (allow negative)
        (0.05, 2.5),                 # γ
        (min_loss * 0.3, max_loss * 1.2)  # E
    ]
    
    # Chinchilla-inspired initialization
    init = np.array([
        loss_range * 0.35,
        0.34,
        loss_range * 0.35,
        0.28,
        0.5,
        0.5,
        min_loss * 0.95
    ])
    
    # Primary: Differential Evolution with proven settings
    result_de = differential_evolution(
        objective,
        bounds,
        maxiter=500,
        seed=42,
        workers=1,
        polish=True,
        atol=1e-8,
        tol=1e-8,
        popsize=25,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.5, 1.8),
        recombination=0.75
    )
    
    best_params = result_de.x
    best_score = result_de.fun
    
    # Refinement: L-BFGS-B from DE result
    result_local = minimize(
        objective,
        best_params,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 3000, &#x27;ftol&#x27;: 1e-10, &#x27;gtol&#x27;: 1e-8}
    )
    
    if result_local.success and result_local.fun &lt; best_score:
        best_params = result_local.x
        best_score = result_local.fun
    
    # Try from smart initialization
    result_init = minimize(
        objective,
        init,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-10}
    )
    
    if result_init.success and result_init.fun &lt; best_score:
        best_params = result_init.x
        best_score = result_init.fun
    
    # Multi-start refinement (2 iterations)
    for i in range(2):
        try:
            perturb = best_params * (1 + np.random.RandomState(42 + i).randn(7) * 0.03)
            perturb = np.clip(perturb, [b[0] for b in bounds], [b[1] for b in bounds])
            
            result = minimize(
                objective,
                perturb,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 2000, &#x27;ftol&#x27;: 1e-10}
            )
            
            if result.success and result.fun &lt; best_score:
                best_params = result.x
                best_score = result.fun
        except:
            pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #32cd32; color: black"> R² = 0.852179 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Optimized scaling law with proven data efficiency formulation
Form: L = A/U^α + B/P^β + C/(D^γ * U^0.15) + E
Focuses on numerical stability and robust parameter fitting
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = A/U^α + B/P^β + C/(D^γ * U^0.15) + E
    
    Components:
    - A/U^α: Unique token diversity effect
    - B/P^β: Model capacity (Chinchilla-style)
    - C/(D^γ * U^0.15): Data efficiency moderated by unique content
    - E: Irreducible loss floor
    
    Parameters: [A, α, B, β, C, γ, E] (7 params)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if params.size != 7:
        params = np.array([8.5, 0.29, 92.0, 0.21, 46.0, 0.17, 2.05])
    
    U = np.maximum(X[:, 0], 1e6)  # unique_tokens
    P = np.maximum(X[:, 1], 1e7)  # params
    D = np.maximum(X[:, 2], 1e8)  # tokens
    
    A, alpha, B, beta, C, gamma, E = params
    
    # Constrain exponents for stability
    alpha = np.clip(alpha, 0.05, 0.8)
    beta = np.clip(beta, 0.05, 0.7)
    gamma = np.clip(gamma, 0.05, 0.6)
    
    # Core terms
    term1 = A / np.power(U, alpha)
    term2 = B / np.power(P, beta)
    
    # Data efficiency with fixed U exponent for stability
    denom = np.power(D, gamma) * np.power(U, 0.15)
    term3 = C / np.maximum(denom, 1.0)
    
    return term1 + term2 + term3 + E


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Robust hybrid optimization with enhanced convergence
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    # Data statistics
    U_med = np.median(X[:, 0])
    P_med = np.median(X[:, 1])
    y_min, y_max = np.min(y), np.max(y)
    y_range = y_max - y_min
    y_mean = np.mean(y)
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            
            if not np.all(np.isfinite(pred)):
                return 1e10
            
            # Primary MSE loss
            mse = np.mean((pred - y) ** 2)
            
            # Minimal regularization
            reg = 1e-9 * (params[0]**2/8000 + params[2]**2/8000 + params[4]**2/8000)
            
            # Soft penalties
            penalty = 0.0
            if np.any(pred &lt; 0):
                penalty += 80.0 * np.sum(pred[pred &lt; 0]**2)
            
            over_mask = pred &gt; y_max + 2.2 * y_range
            if np.any(over_mask):
                penalty += 8.0 * np.sum((pred[over_mask] - y_max)**2)
            
            return mse + reg + penalty
        except:
            return 1e10
    
    # Adaptive bounds
    bounds = [
        (0.01, y_range * np.power(U_med, 0.42)),
        (0.05, 0.8),
        (0.01, y_range * np.power(P_med, 0.32)),
        (0.05, 0.7),
        (0.01, y_range * 140),
        (0.05, 0.6),
        (y_min * 0.45, y_max * 1.25)
    ]
    
    best_result = None
    best_score = float(&#x27;inf&#x27;)
    
    # Global search with differential evolution
    for seed_val in [42, 99, 333, 777]:
        try:
            result = differential_evolution(
                objective,
                bounds,
                maxiter=480,
                popsize=23,
                seed=seed_val,
                atol=1e-9,
                tol=1e-9,
                workers=1,
                polish=True,
                strategy=&#x27;best1bin&#x27;,
                updating=&#x27;deferred&#x27;
            )
            
            if result.fun &lt; best_score:
                best_score = result.fun
                best_result = result.x
        except:
            continue
    
    # Multi-start local optimization
    init_points = [
        [8.5, 0.29, 92.0, 0.21, 46.0, 0.17, y_mean],
        [9.5, 0.31, 98.0, 0.23, 49.0, 0.19, y_mean * 0.97],
        [7.0, 0.26, 88.0, 0.19, 43.0, 0.15, y_mean * 1.03],
        [11.0, 0.33, 105.0, 0.25, 52.0, 0.21, y_mean * 0.94],
        [6.5, 0.24, 95.0, 0.20, 48.0, 0.18, y_mean * 1.06],
    ]
    
    if best_result is not None:
        init_points.insert(0, best_result)
    
    for init in init_points:
        try:
            result = minimize(
                objective,
                init,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 3500, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10}
            )
            
            if result.fun &lt; best_score:
                best_score = result.fun
                best_result = result.x
        except:
            continue
    
    # Final ultra-fine refinement
    if best_result is not None and best_score &lt; 0.15:
        try:
            result = minimize(
                objective,
                best_result,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 2500, &#x27;ftol&#x27;: 1e-14, &#x27;gtol&#x27;: 1e-12}
            )
            
            if result.fun &lt; best_score:
                best_result = result.x
        except:
            pass
    
    # Fallback
    if best_result is None:
        best_result = np.array([8.5, 0.29, 92.0, 0.21, 46.0, 0.17, 2.05])
    
    return best_result
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
