<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - Data-Constrained Scaling Law - SLDAgent + Claude Sonnet 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>Data-Constrained Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Sonnet 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.995271 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.983285</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.946354</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.995271 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined Chinchilla-style scaling law with stable interaction modulation
L = A/U^alpha + B/P^beta + C/D^gamma * (D/U)^delta + E
Uses ratio exponent delta to capture data efficiency without numerical instability
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = A/U^alpha + B/P^beta + C/D^gamma * (D/U)^delta + E
    where U=unique_tokens, P=params, D=tokens
    
    7 parameters: [A, alpha, B, beta, C, gamma, delta]
    E is derived from minimum loss to reduce parameters
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).flatten()
    
    if len(params) &lt; 7:
        params = np.pad(params, (0, 7 - len(params)), constant_values=0.1)
    
    # Extract and stabilize features
    U = np.maximum(X[:, 0], 1e6)  # unique_tokens
    P = np.maximum(X[:, 1], 1e7)  # params
    D = np.maximum(X[:, 2], 1e8)  # tokens
    
    A, alpha, B, beta, C, gamma, delta = params[:7]
    
    # Stabilize exponents with tighter bounds
    alpha_safe = np.clip(alpha, 0.01, 0.75)
    beta_safe = np.clip(beta, 0.01, 0.75)
    gamma_safe = np.clip(gamma, 0.01, 0.75)
    delta_safe = np.clip(delta, -0.15, 0.15)  # Small interaction effect
    
    # Compute ratio safely
    ratio_DU = np.clip(D / U, 1.0, 1e4)
    
    # Main scaling terms
    term_U = np.abs(A) / np.power(U, alpha_safe)
    term_P = np.abs(B) / np.power(P, beta_safe)
    
    # Token term with multiplicative interaction
    # When delta &gt; 0: penalty for high repetition (D/U large)
    # When delta &lt; 0: bonus for data efficiency (D/U small)
    term_D = np.abs(C) / np.power(D, gamma_safe) * np.power(ratio_DU, delta_safe)
    
    # Irreducible loss estimated as 1.5 (typical minimum for transformers)
    E = 1.5
    
    loss = np.clip(term_U + term_P + term_D + E, 0.5, 12.0)
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Enhanced three-stage optimization with robust initialization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).flatten()
    
    min_loss = np.min(y)
    max_loss = np.max(y)
    loss_range = max_loss - min_loss
    
    def objective(params):
        try:
            pred = scaling_law_func(X, params)
            if not np.all(np.isfinite(pred)):
                return 1e10
            
            # MSE loss
            mse = np.mean((pred - y) ** 2)
            
            # Regularization: favor Chinchilla-optimal exponents (0.28-0.35)
            reg_exp = 4e-5 * (
                (params[1] - 0.29) ** 2 +  # alpha
                (params[3] - 0.34) ** 2 +  # beta
                (params[5] - 0.26) ** 2    # gamma
            )
            
            # Penalize extreme interaction strength
            reg_delta = 1e-4 * params[6] ** 2
            
            # Light L2 on coefficients
            reg_coef = 5e-9 * (params[0] ** 2 + params[2] ** 2 + params[4] ** 2)
            
            return mse + reg_exp + reg_delta + reg_coef
        except:
            return 1e10
    
    # Bounds: [A, alpha, B, beta, C, gamma, delta]
    bounds = [
        (0.001, loss_range * 180),  # A: unique tokens coefficient
        (0.01, 0.7),                # alpha
        (0.001, loss_range * 180),  # B: model params coefficient
        (0.01, 0.7),                # beta
        (0.001, loss_range * 180),  # C: tokens coefficient
        (0.01, 0.7),                # gamma
        (-0.12, 0.12)               # delta: interaction exponent
    ]
    
    # Smart initialization based on top performers
    init_params = np.array([
        loss_range * 9.5,   # A: unique tokens (primary constraint)
        0.28,               # alpha: slightly lower
        loss_range * 14.0,  # B: model params (secondary)
        0.35,               # beta: Chinchilla optimal
        loss_range * 5.0,   # C: total tokens (tertiary)
        0.25,               # gamma: lower
        -0.04               # delta: slight penalty for repetition
    ])
    
    # Stage 1: Global search with differential evolution
    result_global = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=420,
        popsize=21,
        atol=1e-11,
        tol=1e-11,
        workers=1,
        updating=&#x27;deferred&#x27;,
        init=&#x27;latinhypercube&#x27;,
        strategy=&#x27;best1bin&#x27;,
        recombination=0.7,
        mutation=(0.5, 1.2)
    )
    
    # Stage 2: Multi-start local refinement
    best_score = result_global.fun
    best_params = result_global.x
    
    start_points = [
        result_global.x,
        init_params,
        result_global.x * 0.96,
        result_global.x * 1.04,
        (result_global.x + init_params) / 2
    ]
    
    for start in start_points:
        # Ensure start point is within bounds
        start = np.clip(start, [b[0] for b in bounds], [b[1] for b in bounds])
        
        try:
            result = minimize(
                objective,
                start,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 3500, &#x27;ftol&#x27;: 1e-13, &#x27;gtol&#x27;: 1e-11}
            )
            if result.success and result.fun &lt; best_score:
                best_score = result.fun
                best_params = result.x
        except:
            continue
    
    # Stage 3: Final refinement with TNC
    try:
        result_final = minimize(
            objective,
            best_params,
            method=&#x27;TNC&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1500, &#x27;ftol&#x27;: 1e-13, &#x27;accuracy&#x27;: 1e-11}
        )
        if result_final.success and result_final.fun &lt; best_score:
            best_params = result_final.x
    except:
        pass
    
    return best_params

# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.994459 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Streamlined Chinchilla scaling law: L = E + A/N^α + B/D^β + C/U^γ
Simplified optimization with enhanced convergence through balanced 3-stage fitting.
Uses exactly 7 parameters with optimized regularization.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Pure power law: L = E + A/N^α + B/D^β + C/U^γ
    params: [E, A, alpha, B, beta, C, gamma] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    params = np.asarray(params, dtype=np.float64)
    
    if params.ndim == 1:
        params = params[None, :]
    
    U = np.maximum(X[:, 0], 1e-14)
    N = np.maximum(X[:, 1], 1e-14)
    D = np.maximum(X[:, 2], 1e-14)
    
    pred_list = []
    for p in params:
        E, A, alpha, B, beta, C, gamma = p
        
        # Enforce validity
        A, B, C = np.abs(A), np.abs(B), np.abs(C)
        alpha = np.clip(alpha, 0.01, 0.99)
        beta = np.clip(beta, 0.01, 0.99)
        gamma = np.clip(gamma, 0.01, 0.99)
        
        # Three power terms with stability
        t1 = np.minimum(A / np.power(N, alpha), 1e4)
        t2 = np.minimum(B / np.power(D, beta), 1e4)
        t3 = np.minimum(C / np.power(U, gamma), 1e4)
        
        loss = E + t1 + t2 + t3
        pred_list.append(loss)
    
    pred = np.array(pred_list).T
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Optimized 3-stage fitting: DE → L-BFGS-B → Nelder-Mead
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    y2d = y[:, None] if y.ndim == 1 else y
    T = y2d.shape[1]
    params_all = []
    
    for t in range(T):
        y_target = y2d[:, t]
        y_min, y_max = np.min(y_target), np.max(y_target)
        y_range = y_max - y_min
        
        # Adaptive bounds
        coef_max = max(1200, y_range * 120)
        
        bounds = [
            (y_min * 0.25, y_min * 1.5),
            (1e-3, coef_max),
            (0.01, 0.99),
            (1e-3, coef_max),
            (0.01, 0.99),
            (1e-3, coef_max),
            (0.01, 0.99)
        ]
        
        def objective(p):
            try:
                pred = scaling_law_func(X, p)
                residuals = pred - y_target
                
                # Balanced MSE-MAE
                mse = np.mean(residuals ** 2)
                mae = np.mean(np.abs(residuals))
                primary = 0.75 * mse + 0.25 * mae
                
                # Light regularization toward typical exponents
                reg = 2e-7 * np.sum((p[[2,4,6]] - 0.37) ** 2) + 2e-9 * np.sum(p[[1,3,5]] ** 2)
                
                return primary + reg
            except:
                return 1e12
        
        # Stage 1: Global search
        result_de = differential_evolution(
            objective, bounds, seed=42, maxiter=750, popsize=35,
            atol=1e-10, tol=1e-10, polish=True, strategy=&#x27;best1bin&#x27;,
            mutation=(0.6, 1.95), recombination=0.92, updating=&#x27;deferred&#x27;
        )
        
        # Stage 2: Gradient refinement
        result_lbfgs = minimize(
            objective, result_de.x, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
            options={&#x27;maxiter&#x27;: 1500, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11}
        )
        
        # Stage 3: Final polish
        result_nm = minimize(
            objective, result_lbfgs.x, method=&#x27;Nelder-Mead&#x27;,
            options={&#x27;maxiter&#x27;: 700, &#x27;xatol&#x27;: 1e-11, &#x27;fatol&#x27;: 1e-12}
        )
        
        # Select best
        candidates = [
            (result_de.fun, result_de.x),
            (result_lbfgs.fun, result_lbfgs.x),
            (result_nm.fun, result_nm.x)
        ]
        best_params = min(candidates, key=lambda x: x[0])[1]
        params_all.append(best_params)
    
    params_opt = np.array(params_all)
    return params_opt[0] if T == 1 else params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.990344 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined Chinchilla scaling law: L = A/P^α + B/D^β + C/U^γ + E
Optimized for accuracy with streamlined implementation
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = A/P^α + B/D^β + C/U^γ + E
    params = [A, α, B, β, C, γ, E] (7 parameters)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    p = np.asarray(params, dtype=np.float64).ravel()[:7]
    
    U, P, D = X[:, 0], X[:, 1], X[:, 2]
    
    # Numerical stability with minimal overhead
    eps = 1e-10
    U, P, D = np.maximum(U, eps), np.maximum(P, eps), np.maximum(D, eps)
    
    # Extract parameters - enforce positivity for coefficients
    A, alpha = np.abs(p[0]) + eps, np.abs(p[1])
    B, beta = np.abs(p[2]) + eps, np.abs(p[3])
    C, gamma = np.abs(p[4]) + eps, np.abs(p[5])
    E = p[6]
    
    return A / np.power(P, alpha) + B / np.power(D, beta) + C / np.power(U, gamma) + E


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Two-stage optimization: global search + local refinement
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points, dtype=np.float64))
    y = np.asarray(loss_values, dtype=np.float64)
    
    # Filter invalid data
    valid = np.isfinite(y) &amp; (y &gt; 0) &amp; np.all(np.isfinite(X), axis=1)
    X, y = X[valid], y[valid]
    
    if len(y) &lt; 3:
        return np.array([100.0, 0.15, 100.0, 0.15, 100.0, 0.15, 2.0])
    
    # Data statistics for adaptive bounds
    min_loss, mean_loss, std_loss = np.min(y), np.mean(y), np.std(y)
    
    def objective(p):
        try:
            pred = scaling_law_func(X, p)
            mse = np.mean((pred - y) ** 2)
            
            # Regularization: guide exponents toward [0.05, 0.5] (Chinchilla-inspired)
            # Using optimized strength from top performers
            reg_strength = 0.0005
            exp_reg = reg_strength * sum(
                max(0, p[i] - 0.5)**2 + max(0, 0.05 - p[i])**2 
                for i in [1, 3, 5]
            )
            
            # Minimal coefficient regularization to prevent extreme values
            coef_reg = 3e-9 * (p[0]**2 + p[2]**2 + p[4]**2)
            
            return mse + exp_reg + coef_reg
        except:
            return 1e10
    
    # Adaptive bounds based on data characteristics
    bounds = [
        (0.01, 1000.0), (0.01, 1.0),           # A, α
        (0.01, 1000.0), (0.01, 1.0),           # B, β
        (0.01, 1000.0), (0.01, 1.0),           # C, γ
        (min_loss * 0.35, mean_loss * 1.25)    # E
    ]
    
    # Global optimization with tuned hyperparameters
    result = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=400,
        popsize=20,
        atol=1e-8,
        tol=1e-8,
        polish=True,
        workers=1,
        strategy=&#x27;best1bin&#x27;,
        mutation=(0.5, 1.2),
        recombination=0.7
    )
    
    # Smart initialization for fallback
    scale = np.sqrt(mean_loss * std_loss) if std_loss &gt; 0 else mean_loss
    params_opt = result.x if result.success else np.array([
        scale * 4.5, 0.15, scale * 4.5, 0.15, 
        scale * 4.5, 0.15, min_loss * 0.7
    ])
    
    # Local refinement with aggressive convergence
    try:
        local = minimize(
            objective,
            params_opt,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 900, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-10}
        )
        if local.success and local.fun &lt; objective(params_opt):
            params_opt = local.x
    except:
        pass
    
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.989999 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Streamlined scaling law for data-constrained LLM training.
Chinchilla-inspired form: L = E + A/N^α + B/D^β + C/U^γ
Optimized for numerical stability and convergence speed.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = E + A/N^α + B/D^β + C/U^γ
    N=model_params, D=tokens, U=unique_tokens
    Parameters: [E, A, α, B, β, C, γ] (7 total)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    params = np.asarray(params).flatten()
    
    # Extract and stabilize features
    U = np.maximum(X[:, 0], 1e-10)  # unique_tokens
    N = np.maximum(X[:, 1], 1e-10)  # model_params
    D = np.maximum(X[:, 2], 1e-10)  # tokens
    
    # Unpack parameters
    E, A, alpha, B, beta, C, gamma = params
    
    # Compute loss with power law terms
    loss = E + A * np.power(N, -np.abs(alpha)) + \
               B * np.power(D, -np.abs(beta)) + \
               C * np.power(U, -np.abs(gamma))
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Efficient multi-start optimization with targeted exploration.
    &quot;&quot;&quot;
    np.random.seed(42)  # For reproducibility
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).flatten()
    
    y_min = np.min(y)
    y_std = np.std(y)
    y_range = np.max(y) - y_min
    
    def objective(params):
        pred = scaling_law_func(X, params)
        mse = np.mean((pred - y) ** 2)
        
        # Adaptive regularization targeting empirical exponents
        reg_strength = 0.00025 * (y_std / y_range)
        exponent_reg = reg_strength * (
            (params[2] - 0.41) ** 2 +
            (params[4] - 0.37) ** 2 +
            (params[6] - 0.44) ** 2
        )
        
        # Light L2 on log-coefficients to prevent extremes
        coeff_reg = 1e-8 * np.sum(np.log1p(np.abs(params[[1,3,5]])) ** 2)
        
        return mse + exponent_reg + coeff_reg
    
    bounds = [
        (0.78, 2.55),   # E: Irreducible loss
        (1e-5, 9e2),    # A: Model coefficient
        (0.06, 1.45),   # α: Model exponent
        (1e-5, 9e2),    # B: Token coefficient
        (0.06, 1.45),   # β: Token exponent
        (1e-5, 9e2),    # C: Unique token coefficient
        (0.06, 1.45),   # γ: Unique token exponent
    ]
    
    # Strategic initialization covering diverse scenarios
    init_sets = [
        [y_min * 0.89, 11.0, 0.41, 11.0, 0.37, 14.0, 0.44],  # Chinchilla-based
        [y_min * 0.94, 42.0, 0.29, 38.0, 0.31, 48.0, 0.51],  # Data-rich
        [y_min * 0.85, 6.5, 0.57, 7.5, 0.54, 6.8, 0.33],     # Parameter-limited
        [1.55, 17.0, 0.47, 21.0, 0.43, 19.5, 0.39],          # Mid-range balanced
        [y_min * 0.91, 13.5, 0.37, 15.5, 0.35, 27.0, 0.57],  # Unique-token focus
        [y_min * 0.87, 9.5, 0.45, 12.5, 0.41, 17.5, 0.37],   # Conservative
        [y_min * 0.96, 33.0, 0.31, 29.0, 0.34, 38.0, 0.47],  # High baseline
        [y_min * 0.92, 15.0, 0.39, 18.0, 0.39, 22.0, 0.42],  # Symmetric
    ]
    
    best_result = None
    best_loss = np.inf
    
    # Multi-start optimization
    for init in init_sets:
        result = minimize(
            objective, init, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
            options={&#x27;maxiter&#x27;: 2600, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9}
        )
        if result.fun &lt; best_loss:
            best_loss = result.fun
            best_result = result
    
    # Targeted random exploration
    n_random = 6 if best_loss &gt; 0.0108 else 4
    
    for i in range(n_random):
        random_init = []
        for j, b in enumerate(bounds):
            if j == 0:  # E: cluster near y_min
                random_init.append(np.random.uniform(y_min * 0.83, y_min * 0.98))
            elif j % 2 == 1:  # Coefficients: log-uniform
                random_init.append(10 ** np.random.uniform(np.log10(b[0] + 1e-6), np.log10(b[1])))
            else:  # Exponents: Gaussian around targets
                targets = [0.41, 0.37, 0.44]
                target = targets[(j-2)//2]
                val = np.random.normal(target, 0.16)
                random_init.append(np.clip(val, b[0], b[1]))
        
        result = minimize(
            objective, random_init, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
            options={&#x27;maxiter&#x27;: 2200, &#x27;ftol&#x27;: 1e-10}
        )
        if result.fun &lt; best_loss:
            best_loss = result.fun
            best_result = result
    
    # Final ultra-refinement
    if best_result is not None and best_loss &lt; 0.014:
        final_result = minimize(
            objective, best_result.x, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
            options={&#x27;maxiter&#x27;: 1600, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-11}
        )
        if final_result.success and final_result.fun &lt; best_loss:
            return final_result.x
    
    return best_result.x if best_result is not None else init_sets[0]
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #228b22; color: white"> R² = 0.946354 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Enhanced scaling law with interaction terms for data-constrained LLM training
Uses power law base with multiplicative correction for unique token effects
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = E + (A/N^alpha + B/D^beta) * (1 + C/U^gamma)
    where N=params, D=tokens, U=unique_tokens
    This captures interaction between data availability and unique token constraints
    Total: 7 parameters (E, A, B, C, alpha, beta, gamma)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    unique_tokens = X[:, 0]
    model_params = X[:, 1]
    tokens = X[:, 2]
    
    params = np.asarray(params).ravel()
    
    E = params[0]      # Irreducible loss
    A = params[1]      # Model size coefficient
    B = params[2]      # Data size coefficient
    C = params[3]      # Unique token interaction coefficient
    alpha = params[4]  # Model size exponent
    beta = params[5]   # Data size exponent
    gamma = params[6]  # Unique token exponent
    
    eps = 1e-12
    
    # Base scaling from model size and data
    base_term = A / (model_params ** alpha + eps) + B / (tokens ** beta + eps)
    
    # Multiplicative correction for unique token constraints
    unique_correction = 1.0 + C / (unique_tokens ** gamma + eps)
    
    loss = E + base_term * unique_correction
    
    return loss


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit scaling law parameters using multi-start optimization
    with improved initialization and adaptive bounds
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values).ravel()
    
    # Robust normalization using log-space
    unique_tokens = X[:, 0]
    model_params = X[:, 1]
    tokens = X[:, 2]
    
    # Use geometric mean for better scaling
    unique_scale = np.exp(np.mean(np.log(unique_tokens + 1)))
    params_scale = np.exp(np.mean(np.log(model_params + 1)))
    tokens_scale = np.exp(np.mean(np.log(tokens + 1)))
    
    X_normalized = X.copy()
    X_normalized[:, 0] /= unique_scale
    X_normalized[:, 1] /= params_scale
    X_normalized[:, 2] /= tokens_scale
    
    def objective(params):
        try:
            pred = scaling_law_func(X_normalized, params)
            residuals = pred - y
            
            # Huber loss for robustness to outliers
            delta = 0.5
            abs_res = np.abs(residuals)
            huber = np.where(abs_res &lt;= delta,
                           0.5 * residuals ** 2,
                           delta * (abs_res - 0.5 * delta))
            loss = np.mean(huber)
            
            # Adaptive regularization
            reg = 1e-7 * (np.sum(params[:4] ** 2) + 
                         np.sum((params[4:7] - 0.35) ** 2))
            
            return loss + reg
        except:
            return 1e10
    
    # Adaptive bounds based on expected parameter ranges
    bounds = [
        (1.6, 2.8),      # E: tighter around expected irreducible loss
        (0.01, 8.0),     # A (scaled)
        (0.01, 8.0),     # B (scaled)
        (0.001, 5.0),    # C (interaction term, typically smaller)
        (0.1, 0.7),      # alpha
        (0.1, 0.7),      # beta
        (0.1, 0.7),      # gamma
    ]
    
    # Multi-start differential evolution with different strategies
    best_result = None
    best_score = np.inf
    
    for strategy in [&#x27;best1bin&#x27;, &#x27;rand1bin&#x27;]:
        result_de = differential_evolution(
            objective, 
            bounds, 
            seed=42,
            strategy=strategy,
            maxiter=400,
            popsize=20,
            atol=1e-9,
            tol=1e-9,
            workers=1
        )
        
        if result_de.fun &lt; best_score:
            best_score = result_de.fun
            best_result = result_de
    
    # Refine with L-BFGS-B
    result = minimize(
        objective,
        best_result.x,
        method=&#x27;L-BFGS-B&#x27;,
        bounds=bounds,
        options={&#x27;maxiter&#x27;: 1000, &#x27;ftol&#x27;: 1e-10}
    )
    
    params_opt = result.x if result.success else best_result.x
    
    # Rescale coefficients back to original scale
    params_opt[1] *= params_scale ** params_opt[4]
    params_opt[2] *= tokens_scale ** params_opt[5]
    params_opt[3] *= unique_scale ** params_opt[6]
    
    return params_opt
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
