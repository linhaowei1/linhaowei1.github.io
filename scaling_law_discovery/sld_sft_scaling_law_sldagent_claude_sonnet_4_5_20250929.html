<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SLD - SFT Scaling Law - SLDAgent + Claude Sonnet 4.5</title>
    <link rel="icon" type="image/png" href="assets/sld_logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&family=Sora:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <style>
      :root {
        --bg-primary: #ffffff;
        --bg-secondary: #f8f9fa;
        --accent-primary: #2563eb;
        --accent-secondary: #3b82f6;
        --accent-gradient: linear-gradient(135deg, #2563eb 0%, #3b82f6 50%, #60a5fa 100%);
        --text-primary: #1f2937;
        --text-secondary: #4b5563;
        --border-subtle: rgba(0, 0, 0, 0.1);
        --glass-bg: rgba(0, 0, 0, 0.02);
        --success: #10b981;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family:
          "Sora",
          -apple-system,
          BlinkMacSystemFont,
          sans-serif;
        background: var(--bg-primary);
        min-height: 100vh;
        color: var(--text-primary);
      }

      .bg-pattern {
        display: none;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 2rem;
        position: relative;
        z-index: 1;
      }

      .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        color: var(--accent-primary);
        text-decoration: none;
        font-size: 0.9rem;
        margin-bottom: 1.5rem;
        transition: color 0.2s;
      }

      .back-link:hover {
        color: var(--accent-secondary);
      }

      .header {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 16px;
        padding: 2rem;
        margin-bottom: 2rem;
        backdrop-filter: blur(10px);
      }

      .header h1 {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        background: var(--accent-gradient);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .meta-row {
        display: flex;
        flex-wrap: wrap;
        gap: 1.5rem;
        margin-top: 1rem;
      }

      .meta-item {
        display: flex;
        align-items: center;
        gap: 0.5rem;
      }

      .meta-label {
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .meta-value {
        font-weight: 600;
        color: var(--text-primary);
      }

      .r2-badge {
        display: inline-block;
        padding: 0.3rem 0.6rem;
        border-radius: 6px;
        font-weight: 600;
        font-size: 0.85rem;
        font-family: "JetBrains Mono", monospace;
      }

      .section-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--text-primary);
      }

      .runs-container {
        display: flex;
        flex-direction: column;
        gap: 1rem;
      }

      .run-card {
        background: var(--glass-bg);
        border: 1px solid var(--border-subtle);
        border-radius: 12px;
        overflow: hidden;
        transition: border-color 0.2s;
      }

      .run-card:hover {
        border-color: rgba(99, 102, 241, 0.3);
      }

      .run-card.best-run {
        border-color: var(--success);
        box-shadow: 0 0 20px rgba(16, 185, 129, 0.1);
      }

      .run-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem 1.25rem;
        background: rgba(255, 255, 255, 0.02);
        cursor: pointer;
        transition: background 0.2s;
      }

      .run-header:hover {
        background: rgba(255, 255, 255, 0.04);
      }

      .run-info {
        display: flex;
        align-items: center;
        gap: 1rem;
      }

      .run-badge {
        padding: 0.25rem 0.6rem;
        border-radius: 6px;
        font-size: 0.75rem;
        font-weight: 600;
        background: rgba(255, 255, 255, 0.1);
        color: var(--text-secondary);
      }

      .run-badge.best-badge {
        background: var(--success);
        color: white;
      }

      .run-label {
        font-weight: 500;
        color: var(--text-primary);
      }

      .expand-icon {
        color: var(--text-muted);
        font-size: 0.8rem;
        transition: transform 0.2s;
      }

      .run-header.expanded .expand-icon {
        transform: rotate(180deg);
      }

      .run-content {
        border-top: 1px solid var(--border-subtle);
      }

      .code-container {
        overflow: hidden;
      }

      .code-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 0.75rem 1.25rem;
        background: rgba(0, 0, 0, 0.2);
        border-bottom: 1px solid var(--border-subtle);
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .copy-btn {
        padding: 0.35rem 0.75rem;
        background: rgba(99, 102, 241, 0.2);
        border: 1px solid rgba(99, 102, 241, 0.3);
        border-radius: 6px;
        color: var(--accent-primary);
        font-size: 0.75rem;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s;
        font-family: inherit;
      }

      .copy-btn:hover {
        background: rgba(99, 102, 241, 0.3);
      }

      .code-container pre {
        margin: 0;
        padding: 1.25rem;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.8rem;
        line-height: 1.6;
        overflow-x: auto;
        background: transparent !important;
      }

      .footer {
        text-align: center;
        margin-top: 3rem;
        padding-top: 2rem;
        border-top: 1px solid var(--border-subtle);
        color: var(--text-secondary);
        font-size: 0.85rem;
      }

      .footer a {
        color: var(--accent-primary);
        text-decoration: none;
      }

      @media (max-width: 768px) {
        .container {
          padding: 1rem;
        }

        .header h1 {
          font-size: 1.25rem;
        }

        .meta-row {
          flex-direction: column;
          gap: 0.75rem;
        }

        .run-info {
          flex-wrap: wrap;
          gap: 0.5rem;
        }
      }
    </style>
  </head>
  <body>
    <div class="bg-pattern"></div>

    <div class="container">
      <a href="sld_index.html" class="back-link"> ← Back to Leaderboard </a>

      <div class="header">
        <h1>SFT Scaling Law</h1>
        <div class="meta-row">
          <div class="meta-item">
            <span class="meta-label">Agent:</span>
            <span class="meta-value">SLDAgent</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Model:</span>
            <span class="meta-value">Claude Sonnet 4.5</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Best R²:</span>
            <span class="r2-badge" style="background-color: #006400; color: white"> 0.994992 </span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Mean R²:</span>
            <span class="meta-value">0.983403</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Min R²:</span>
            <span class="meta-value">0.959688</span>
          </div>
          <div class="meta-item">
            <span class="meta-label">Runs:</span>
            <span class="meta-value">5</span>
          </div>
        </div>
      </div>

      <h2 class="section-title">All Runs (sorted by R²)</h2>

      <div class="runs-container">
        <div class="run-card best-run">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge best-badge">Best</span>
              <span class="run-label">Run 2</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.994992 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: block">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-0')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-0"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined 4-parameter scaling law: L(D) = L_inf + A / (D^alpha + B)

Parameters:
- L_inf: irreducible loss (asymptotic minimum)
- A: scale coefficient
- alpha: power law exponent (0.15-0.65 range)
- B: stability term

Optimizations:
1. Conservative bounds calibrated to prevent overfitting
2. Two-stage optimization: global exploration + local refinement
3. Minimal but effective regularization
4. Robust error handling
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;Scaling law: L(D) = L_inf + A / (D^alpha + B)&quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    data_size = data_points[:, 0]
    
    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :]
    
    L_inf, A, alpha, B = params[:, 0], params[:, 1], params[:, 2], params[:, 3]
    
    # Numerically stable computation
    D_alpha = np.power(data_size[:, None], alpha[None, :])
    denominator = np.maximum(D_alpha + B[None, :], 1e-12)
    pred = L_inf[None, :] + A[None, :] / denominator
    
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;Fit scaling law using two-stage optimization&quot;&quot;&quot;
    data_points = np.atleast_2d(np.asarray(data_points))
    data_size = data_points[:, 0]
    loss_values = np.asarray(loss_values)
    
    if loss_values.ndim == 1:
        loss_values = loss_values[:, None]
    
    T = loss_values.shape[1]
    min_loss = np.min(loss_values, axis=0)
    max_loss = np.max(loss_values, axis=0)
    loss_range = max_loss - min_loss
    min_data, max_data = np.min(data_size), np.max(data_size)
    
    all_params = []
    
    for t in range(T):
        y_t = loss_values[:, t]
        
        # Smart initialization based on empirical LLM behavior
        L_inf_init = min_loss[t] * 0.895
        alpha_init = 0.355
        A_init = loss_range[t] * np.power(min_data, alpha_init) * 1.25
        B_init = np.power(min_data, alpha_init) * 0.45
        
        init = np.array([L_inf_init, A_init, alpha_init, B_init])
        
        # Conservative bounds to ensure good generalization
        bounds = [
            (0.0, min_loss[t] + 0.085 * loss_range[t]),
            (1e-8, loss_range[t] * np.power(max_data, 0.66)),
            (0.15, 0.65),
            (1e-8, np.power(max_data, 0.56))
        ]
        
        def objective(params):
            pred = scaling_law_func(data_points, params)
            mse = np.mean((pred - y_t) ** 2)
            
            # Minimal regularization for stability
            alpha_reg = 4e-7 * (params[2] - 0.355) ** 2
            linf_reg = 1.2e-7 * max(0, params[0] - min_loss[t]) ** 2
            B_reg = 4e-10 * params[3] ** 2
            
            return mse + alpha_reg + linf_reg + B_reg
        
        # Two-stage optimization
        best_params = init
        try:
            # Stage 1: Global search
            result_de = differential_evolution(
                objective, bounds, seed=42, maxiter=310,
                atol=1e-9, tol=1e-9, workers=1, polish=True,
                strategy=&#x27;best1bin&#x27;, popsize=15
            )
            best_params = result_de.x
        except:
            pass
        
        try:
            # Stage 2: Local refinement
            result_local = minimize(
                objective, best_params, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                options={&#x27;maxiter&#x27;: 1600, &#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10}
            )
            params_opt = result_local.x if result_local.success else best_params
        except:
            params_opt = best_params
        
        all_params.append(params_opt)
    
    params_array = np.array(all_params)
    return params_array[0] if T == 1 else params_array
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#2</span>
              <span class="run-label">Run 5</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.994986 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-1')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-1"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Refined scaling law with adaptive power-sigmoid blend
Form: L = a + b / (1 + (N/s)^c)^(0.75*c)
Simplified optimization with focused hyperparameters
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Adaptive scaling law: L = a + b / (1 + (N/s)^c)^d where d = 0.75*c
    - a: asymptotic minimum loss
    - b: scale of loss decrease  
    - c: transition sharpness
    - s: characteristic data size scale
    &quot;&quot;&quot;
    N = np.asarray(data_points).flatten()
    a, b, c, s = params[:4]
    
    # Stability enforcement
    b = abs(b)
    c = np.clip(c, 0.1, 1.2)
    s = max(10.0, abs(s))
    d = 0.75 * c  # Slightly lower ratio for better generalization
    
    # Core scaling function
    ratio = np.power(N / s, c)
    pred = a + b / np.power(1.0 + ratio, d)
    
    return pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Streamlined two-stage optimization
    &quot;&quot;&quot;
    N = np.asarray(data_points).flatten()
    L = np.asarray(loss_values).flatten()
    
    L_min, L_max = np.min(L), np.max(L)
    L_range = L_max - L_min
    N_median = np.median(N)
    N_max = np.max(N)
    
    # Tighter, data-driven bounds
    bounds = [
        (L_min - 0.35 * L_range, L_max + 0.05 * L_range),
        (0.03 * L_range, 2.8 * L_range),
        (0.1, 1.15),
        (N_median * 0.08, N_max * 2.0)
    ]
    
    def obj(p):
        try:
            return np.mean((scaling_law_func(N, p) - L) ** 2)
        except:
            return 1e10
    
    # Stage 1: Global search with tuned DE
    res = differential_evolution(
        obj, bounds, maxiter=230, seed=42,
        atol=1e-11, tol=1e-11, popsize=17,
        strategy=&#x27;best1bin&#x27;, mutation=(0.6, 1.65), recombination=0.87
    )
    
    best_p, best_s = res.x, res.fun
    
    # Stage 2: Focused multi-start local refinement
    for i in range(8):
        if i == 0:
            start = best_p
        else:
            rng = np.random.RandomState(42 + i)
            perturb = rng.randn(4) * 0.14
            start = best_p * (1 + perturb)
            start = np.clip(start, [b[0] for b in bounds], [b[1] for b in bounds])
        
        r = minimize(obj, start, method=&#x27;L-BFGS-B&#x27;, bounds=bounds,
                    options={&#x27;maxiter&#x27;: 2800, &#x27;ftol&#x27;: 1e-13, &#x27;gtol&#x27;: 1e-11})
        
        if r.fun &lt; best_s:
            best_p, best_s = r.x, r.fun
    
    # Final refinement
    r = minimize(obj, best_p, method=&#x27;Nelder-Mead&#x27;,
                options={&#x27;maxiter&#x27;: 1500, &#x27;xatol&#x27;: 1e-12, &#x27;fatol&#x27;: 1e-12})
    
    if r.fun &lt; best_s:
        best_p = r.x
    
    return best_p
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#3</span>
              <span class="run-label">Run 1</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.987692 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-2')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-2"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Chinchilla-inspired scaling law optimized for SFT scenarios
L = L_inf + A / (N^alpha + B)
Uses 4 parameters with robust multi-stage optimization
Simplified from best performers to reduce complexity
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import differential_evolution, minimize

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = L_inf + A / (N^alpha + B)
    params: [L_inf, A, alpha, B]
    - L_inf: irreducible loss (asymptotic minimum)
    - A: scale of improvement with data
    - alpha: power law exponent (decay rate)
    - B: saturation parameter (flexibility at small N)
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    N = X[:, 0]
    
    params = np.asarray(params)
    if params.ndim == 1:
        params = params[None, :]
    
    L_inf = params[:, 0]
    A = params[:, 1]
    alpha = params[:, 2]
    B = params[:, 3]
    
    # Ensure numerical stability
    alpha_safe = np.clip(alpha[None, :], 0.16, 0.85)
    B_safe = np.maximum(np.abs(B[None, :]), 1.0)
    A_safe = np.maximum(A[None, :], 1e-10)
    
    denominator = np.power(N[:, None], alpha_safe) + B_safe
    pred = L_inf[None, :] + A_safe / denominator
    
    return pred[:, 0] if pred.shape[1] == 1 else pred


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Robust two-stage optimization with smart initialization
    &quot;&quot;&quot;
    X = np.atleast_2d(np.asarray(data_points))
    y = np.asarray(loss_values)
    N = X[:, 0]
    
    if y.ndim == 1:
        y = y[:, None]
    
    y_min, y_max = np.min(y, axis=0), np.max(y, axis=0)
    y_range = y_max - y_min
    N_min, N_max = np.min(N), np.max(N)
    N_geomean = np.sqrt(N_min * N_max)
    
    all_params = []
    
    for t in range(y.shape[1]):
        y_t = y[:, t]
        
        # Smart initialization from data
        L_inf_init = y_min[t] - 0.13 * y_range[t]
        A_init = y_range[t] * np.power(N_min, 0.39)
        alpha_init = 0.39
        B_init = np.power(N_geomean, 0.37)
        
        bounds = [
            (y_min[t] - 0.5 * y_range[t], y_min[t] + 0.13 * y_range[t]),
            (y_range[t] * 0.005, y_range[t] * N_max * 1.4),
            (0.16, 0.74),
            (1.0, np.power(N_max, 0.52))
        ]
        
        def objective(p):
            pred = scaling_law_func(X, p)
            mse = np.mean((pred - y_t) ** 2)
            reg = 1e-10 * np.sum(p ** 2)
            return mse + reg
        
        # Stage 1: Global optimization
        result_global = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=440,
            atol=1e-9,
            tol=1e-8,
            popsize=14,
            workers=1,
            strategy=&#x27;best1bin&#x27;,
            mutation=(0.5, 1.2),
            recombination=0.76,
            polish=False
        )
        
        best_params = result_global.x
        best_loss = result_global.fun
        
        # Stage 2: Local refinement
        if result_global.success:
            result_local = minimize(
                objective,
                result_global.x,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;ftol&#x27;: 1e-12, &#x27;gtol&#x27;: 1e-10, &#x27;maxiter&#x27;: 480}
            )
            if result_local.success and result_local.fun &lt; best_loss:
                best_params = result_local.x
                best_loss = result_local.fun
        
        # Stage 3: Fallback with smart init if needed
        if not result_global.success or best_loss &gt; 0.013:
            init_guess = np.array([L_inf_init, A_init, alpha_init, B_init])
            for i in range(4):
                init_guess[i] = np.clip(init_guess[i], bounds[i][0], bounds[i][1])
            
            result_init = minimize(
                objective,
                init_guess,
                method=&#x27;L-BFGS-B&#x27;,
                bounds=bounds,
                options={&#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-9, &#x27;maxiter&#x27;: 450}
            )
            if result_init.success and result_init.fun &lt; best_loss:
                best_params = result_init.x
        
        all_params.append(best_params)
    
    params_array = np.array(all_params)
    return params_array[0] if params_array.shape[0] == 1 else params_array
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#4</span>
              <span class="run-label">Run 4</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.979657 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-3')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-3"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Enhanced scaling law with refined initialization: L(N) = a / (N^b + c) + d
Improved parameter estimation and multi-strategy optimization for better convergence.
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L(N) = a / (N^b + c) + d
    4 parameters: [a, b, c, d]
    - a: scale coefficient
    - b: power law exponent  
    - c: shift parameter (stabilizes at small N)
    - d: irreducible loss
    &quot;&quot;&quot;
    data_points = np.atleast_1d(np.asarray(data_points).flatten())
    params = np.asarray(params).flatten()
    
    a, b, c, d = params
    N = np.maximum(data_points, 1.0)
    
    return a / np.maximum(N ** b + c, 1e-10) + d


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Multi-strategy optimization with robust initialization
    &quot;&quot;&quot;
    data_points = np.atleast_1d(np.asarray(data_points).flatten())
    loss_values = np.atleast_1d(np.asarray(loss_values).flatten())
    
    N_min, N_max = np.min(data_points), np.max(data_points)
    L_min, L_max = np.min(loss_values), np.max(loss_values)
    L_range = L_max - L_min
    
    def objective(params):
        a, b, c, d = params
        N = np.maximum(data_points, 1.0)
        pred = a / np.maximum(N ** b + c, 1e-10) + d
        mse = np.mean((pred - loss_values) ** 2)
        # Very light regularization for stability
        reg = 1e-8 * (a**2 + (b - 0.3)**2 + c**2)
        return mse + reg
    
    # Enhanced asymptote estimation from multiple strategies
    low_loss_mask = loss_values &lt;= np.percentile(loss_values, 20)
    if np.any(low_loss_mask):
        init_d = np.mean(loss_values[low_loss_mask]) * 0.91
    else:
        init_d = L_min * 0.88
    
    # Robust power law estimation with multiple fallbacks
    high_idx = data_points &gt; np.percentile(data_points, 55)
    if np.sum(high_idx) &gt;= 3:
        N_high = data_points[high_idx]
        L_high = loss_values[high_idx]
        delta_L = np.maximum(L_high - init_d, 1e-6)
        
        log_N = np.log(N_high)
        log_delta = np.log(delta_L)
        
        # Try weighted regression first
        try:
            # Use inverse variance weighting if possible
            weights = 1.0 / (delta_L + 1e-6)
            weights = weights / np.sum(weights)
            
            mean_log_N = np.sum(weights * log_N)
            mean_log_delta = np.sum(weights * log_delta)
            
            cov = np.sum(weights * (log_N - mean_log_N) * (log_delta - mean_log_delta))
            var = np.sum(weights * (log_N - mean_log_N) ** 2)
            
            if var &gt; 1e-10:
                init_b = np.clip(-cov / var, 0.08, 0.85)
                init_a = np.exp(mean_log_delta) * (np.exp(mean_log_N) ** init_b)
            else:
                # Fallback to polyfit
                coeffs = np.polyfit(log_N, log_delta, 1)
                init_b = np.clip(-coeffs[0], 0.08, 0.85)
                init_a = np.exp(coeffs[1]) * (N_high[0] ** init_b)
        except:
            init_b = 0.32
            init_a = L_range * (N_max ** 0.32)
    else:
        init_b = 0.32
        init_a = L_range * (N_max ** 0.32)
    
    # Adaptive shift parameter
    init_c = (N_min ** init_b) * 0.008
    init_params = np.array([init_a, init_b, init_c, init_d])
    
    # Refined bounds based on top performers
    bounds = [
        (1e-7, L_range * 1e8),
        (0.05, 1.5),
        (0.0, N_max ** 0.6),
        (L_min - L_range * 0.5, L_max * 1.1)
    ]
    
    best_params = init_params
    best_loss = objective(init_params)
    
    # Stage 1: Enhanced differential evolution
    try:
        result_de = differential_evolution(
            objective, 
            bounds, 
            seed=42,
            maxiter=420,
            popsize=13,
            atol=1e-8,
            tol=1e-8,
            workers=1,
            updating=&#x27;deferred&#x27;,
            strategy=&#x27;best1bin&#x27;,
            mutation=(0.55, 1.3),
            recombination=0.72
        )
        if result_de.fun &lt; best_loss:
            best_params = result_de.x
            best_loss = result_de.fun
    except:
        pass
    
    # Stage 2: L-BFGS-B with tighter tolerances
    try:
        result_local = minimize(
            objective,
            best_params,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 2800, &#x27;ftol&#x27;: 1e-11, &#x27;gtol&#x27;: 1e-10}
        )
        if result_local.success and result_local.fun &lt; best_loss:
            best_params = result_local.x
            best_loss = result_local.fun
    except:
        pass
    
    # Stage 3: Powell for final polish
    try:
        result_powell = minimize(
            objective,
            best_params,
            method=&#x27;Powell&#x27;,
            options={&#x27;maxiter&#x27;: 1400, &#x27;ftol&#x27;: 1e-11}
        )
        if result_powell.fun &lt; best_loss:
            # Clip to bounds
            clipped = np.clip(
                result_powell.x, 
                [b[0] for b in bounds], 
                [b[1] for b in bounds]
            )
            if objective(clipped) &lt; best_loss:
                best_params = clipped
    except:
        pass
    
    # Stage 4: Additional TNC refinement if not converged well
    if best_loss &gt; 1e-4:
        try:
            result_tnc = minimize(
                objective,
                best_params,
                method=&#x27;TNC&#x27;,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 1200, &#x27;ftol&#x27;: 1e-11, &#x27;xtol&#x27;: 1e-11}
            )
            if result_tnc.success and result_tnc.fun &lt; best_loss:
                best_params = result_tnc.x
        except:
            pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>

        <div class="run-card">
          <div class="run-header" onclick="toggleRun(this)">
            <div class="run-info">
              <span class="run-badge">#5</span>
              <span class="run-label">Run 3</span>
              <span class="r2-badge" style="background-color: #006400; color: white"> R² = 0.959688 </span>
            </div>
            <span class="expand-icon">▼</span>
          </div>
          <div class="run-content" style="display: none">
            <div class="code-container">
              <div class="code-header">
                <span>Python</span>
                <button class="copy-btn" onclick="copyCode(event, 'code-4')">Copy</button>
              </div>
              <pre><code class="language-python" id="code-4"># EVOLVE-BLOCK-START
&quot;&quot;&quot;
Scaling law: L = a/(N+b)^c + d
Streamlined with robust multi-strategy initialization
&quot;&quot;&quot;
import numpy as np
from scipy.optimize import minimize, differential_evolution

def scaling_law_func(data_points, params):
    &quot;&quot;&quot;
    Scaling law: L = a/(N+b)^c + d
    4 parameters: [a, b, c, d]
    &quot;&quot;&quot;
    N = np.asarray(data_points).flatten()
    a, b, c, d = np.asarray(params).flatten()
    
    denominator = np.maximum(N + b, 1e-10)
    c_safe = np.clip(c, 0.01, 2.0)
    
    return a * np.power(denominator, -c_safe) + d


def fit_scaling_law(data_points, loss_values):
    &quot;&quot;&quot;
    Fit using enhanced initialization and efficient optimization
    &quot;&quot;&quot;
    N = np.asarray(data_points).flatten()
    L = np.asarray(loss_values).flatten()
    
    L_min, L_max = np.min(L), np.max(L)
    N_min, N_max = np.min(N), np.max(N)
    L_range = L_max - L_min
    
    # Robust floor estimation from multiple percentiles
    d_candidates = [np.percentile(L, p) for p in [8, 12, 16]]
    d_init = np.median(d_candidates)
    d_init = np.clip(d_init, L_min * 0.68, L_min * 1.1)
    
    # Multi-offset initialization for robustness
    valid_mask = (L &gt; d_init * 1.02) &amp; (N &gt; 0)
    c_init, a_init, b_init = 0.5, L_range * (N_min ** 0.5), N_min * 0.15
    
    if np.sum(valid_mask) &gt;= 4:
        N_valid, L_valid = N[valid_mask], L[valid_mask]
        best_error = np.inf
        
        for b_test in [0, N_min * 0.08, N_min * 0.18, N_min * 0.32]:
            try:
                log_N = np.log(N_valid + b_test)
                log_L_diff = np.log(np.maximum(L_valid - d_init, 1e-10))
                
                coeffs = np.polyfit(log_N, log_L_diff, 1)
                c_est = np.clip(-coeffs[0], 0.22, 0.88)
                a_est = np.exp(coeffs[1])
                
                pred_test = a_est * np.power(N_valid + b_test, -c_est) + d_init
                error = np.mean((pred_test - L_valid) ** 2)
                
                if error &lt; best_error:
                    best_error, c_init, a_init, b_init = error, c_est, a_est, b_test
            except:
                continue
    
    init = [a_init, b_init, c_init, d_init]
    
    bounds = [
        (L_range * 0.0008, L_range * N_max * 2.8),
        (0.0, N_max * 0.48),
        (0.14, 1.05),
        (L_min * 0.58, L_min * 1.25)
    ]
    
    def objective(p):
        try:
            pred = scaling_law_func(N, p)
            mse = np.mean((pred - L) ** 2)
            reg = 1e-9 * ((p[2] - 0.48) ** 2 + (p[3] - d_init) ** 2)
            return mse + reg
        except:
            return 1e12
    
    # Efficient global search
    result_de = differential_evolution(
        objective,
        bounds,
        seed=42,
        maxiter=340,
        popsize=14,
        atol=1e-10,
        tol=1e-10,
        workers=1,
        mutation=(0.52, 1.05),
        recombination=0.72,
        polish=False,
        strategy=&#x27;best1bin&#x27;
    )
    
    best_params = result_de.x
    best_score = result_de.fun
    
    # Dual-method local refinement
    for method in [&#x27;L-BFGS-B&#x27;, &#x27;Powell&#x27;]:
        try:
            result = minimize(
                objective,
                best_params,
                method=method,
                bounds=bounds,
                options={&#x27;maxiter&#x27;: 1300, &#x27;ftol&#x27;: 1e-11}
            )
            if result.success and result.fun &lt; best_score:
                best_params = result.x
                best_score = result.fun
        except:
            pass
    
    # Backup from smart initialization
    try:
        result_init = minimize(
            objective,
            init,
            method=&#x27;L-BFGS-B&#x27;,
            bounds=bounds,
            options={&#x27;maxiter&#x27;: 1100, &#x27;ftol&#x27;: 1e-11}
        )
        if result_init.success and result_init.fun &lt; best_score:
            best_params = result_init.x
    except:
        pass
    
    return best_params
# EVOLVE-BLOCK-END</code></pre>
            </div>
          </div>
        </div>
      </div>

      <footer class="footer">
        <p>
          SLDBench | <a href="https://arxiv.org/abs/2507.21184" target="_blank">arXiv:2507.21184</a> |
          <a href="https://github.com/linhaowei1/SLD" target="_blank">GitHub</a>
        </p>
      </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
      function toggleRun(header) {
        const content = header.nextElementSibling;
        const isExpanded = content.style.display === "block";
        content.style.display = isExpanded ? "none" : "block";
        header.classList.toggle("expanded", !isExpanded);
      }

      function copyCode(event, codeId) {
        event.stopPropagation();
        const code = document.getElementById(codeId).textContent;
        navigator.clipboard.writeText(code).then(() => {
          const btn = event.target;
          btn.textContent = "Copied!";
          setTimeout(() => (btn.textContent = "Copy"), 2000);
        });
      }

      Prism.highlightAll();
    </script>
  </body>
</html>
